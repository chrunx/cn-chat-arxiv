<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.CV.xml</link><description>This is arxiv RSS feed for cs.CV</description><item><title>该文章提出了一种将激光雷达事件立体匹配与事件幻觉结合的方法，旨在解决由于缺乏亮度变化导致的对应关系难题，通过故意将立体事件相机与固定频率的激光雷达结合，能够在缺少亮度变化的情况下生成深度数据，显著提高了立体匹配的性能，并超越了现有的立体匹配融合方法。</title><link>https://arxiv.org/abs/2408.04633</link><description>&lt;p&gt;
LiDAR-Event Stereo Fusion with Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04633
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种将激光雷达事件立体匹配与事件幻觉结合的方法，旨在解决由于缺乏亮度变化导致的对应关系难题，通过故意将立体事件相机与固定频率的激光雷达结合，能够在缺少亮度变化的情况下生成深度数据，显著提高了立体匹配的性能，并超越了现有的立体匹配融合方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04633v1 Announce Type: new  Abstract: Event stereo matching is an emerging technique to estimate depth from neuromorphic cameras; however, events are unlikely to trigger in the absence of motion or the presence of large, untextured regions, making the correspondence problem extremely challenging. Purposely, we propose integrating a stereo event camera with a fixed-frequency active sensor -- e.g., a LiDAR -- collecting sparse depth measurements, overcoming the aforementioned limitations. Such depth hints are used by hallucinating -- i.e., inserting fictitious events -- the stacks or raw input streams, compensating for the lack of information in the absence of brightness changes. Our techniques are general, can be adapted to any structured representation to stack events and outperform state-of-the-art fusion methods applied to event-based stereo.
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为Arctic-TILT的模型，它在处理基于PDF或扫描内容的问题时，其准确度与比自身大1000倍大小的模型相当。在单个24GB GPU上可以进行微调并部署，大大降低了运营成本，同时能够处理含有高达400k个词汇的视觉丰富文档。Arctic-TILT在七个不同的文档理解基准测试中取得了领先的结果，并且提供了可靠的置信分数和快速的推理速度，这些都是处理大规模或紧急企业环境中的文件所必需的。</title><link>https://arxiv.org/abs/2408.04632</link><description>&lt;p&gt;
Arctic-TILT. Business Document Understanding at Sub-Billion Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04632
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为Arctic-TILT的模型，它在处理基于PDF或扫描内容的问题时，其准确度与比自身大1000倍大小的模型相当。在单个24GB GPU上可以进行微调并部署，大大降低了运营成本，同时能够处理含有高达400k个词汇的视觉丰富文档。Arctic-TILT在七个不同的文档理解基准测试中取得了领先的结果，并且提供了可靠的置信分数和快速的推理速度，这些都是处理大规模或紧急企业环境中的文件所必需的。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04632v1 Announce Type: cross  Abstract: The vast portion of workloads employing LLMs involves answering questions grounded on PDF or scan content. We introduce the Arctic-TILT achieving accuracy on par with models 1000$\times$ its size on these use cases. It can be fine-tuned and deployed on a single 24GB GPU, lowering operational costs while processing Visually Rich Documents with up to 400k tokens. The model establishes state-of-the-art results on seven diverse Document Understanding benchmarks, as well as provides reliable confidence scores and quick inference, which are essential for processing files in large-scale or time-sensitive enterprise environments.
&lt;/p&gt;</description></item><item><title>该文章开发了一种名为Puppet-Master的交互式视频生成模型，该模型能够作为部分动态的先验知识。在测试阶段，仅凭一张图像和部分运动轨迹（即拖动操作），该模型能够生成一个视频，其中描绘的现实主义部分动态忠实地反映了给定的拖动操作。这种能力是通过对大型预训练视频扩散模型进行微调实现的，并且提出了一个新式的条件化架构，以有效注入拖动控制。此外，作者还引入了一种全部到第一次的注意力机制，这是一种空间注意力模块的替代方案，它通过解决现有模型中出现的视觉外观和背景问题，显著提高了生成质量。与训练于自然场景视频且主要移动整个物体的其他运动条件视频生成器不同，Puppet-Master是在新构建的Objaverse-Animation-HQ数据集上进行训练的，该数据集专门用于动作和动画的高质量视频数据。</title><link>https://arxiv.org/abs/2408.04631</link><description>&lt;p&gt;
Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04631
&lt;/p&gt;
&lt;p&gt;
该文章开发了一种名为Puppet-Master的交互式视频生成模型，该模型能够作为部分动态的先验知识。在测试阶段，仅凭一张图像和部分运动轨迹（即拖动操作），该模型能够生成一个视频，其中描绘的现实主义部分动态忠实地反映了给定的拖动操作。这种能力是通过对大型预训练视频扩散模型进行微调实现的，并且提出了一个新式的条件化架构，以有效注入拖动控制。此外，作者还引入了一种全部到第一次的注意力机制，这是一种空间注意力模块的替代方案，它通过解决现有模型中出现的视觉外观和背景问题，显著提高了生成质量。与训练于自然场景视频且主要移动整个物体的其他运动条件视频生成器不同，Puppet-Master是在新构建的Objaverse-Animation-HQ数据集上进行训练的，该数据集专门用于动作和动画的高质量视频数据。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04631v1 Announce Type: new  Abstract: We present Puppet-Master, an interactive video generative model that can serve as a motion prior for part-level dynamics. At test time, given a single image and a sparse set of motion trajectories (i.e., drags), Puppet-Master can synthesize a video depicting realistic part-level motion faithful to the given drag interactions. This is achieved by fine-tuning a large-scale pre-trained video diffusion model, for which we propose a new conditioning architecture to inject the dragging control effectively. More importantly, we introduce the all-to-first attention mechanism, a drop-in replacement for the widely adopted spatial attention modules, which significantly improves generation quality by addressing the appearance and background issues in existing models. Unlike other motion-conditioned video generators that are trained on in-the-wild videos and mostly move an entire object, Puppet-Master is learned from Objaverse-Animation-HQ, a new dat
&lt;/p&gt;</description></item><item><title>该文章介绍了LogogramNLP，一个首个基准研究，它使得自然语言处理分析古代象形文字成为可能，并且包含了四种书写系统的转录和视觉数据集，以及对诸如分类等各种任务的注释标注。</title><link>https://arxiv.org/abs/2408.04628</link><description>&lt;p&gt;
LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04628
&lt;/p&gt;
&lt;p&gt;
该文章介绍了LogogramNLP，一个首个基准研究，它使得自然语言处理分析古代象形文字成为可能，并且包含了四种书写系统的转录和视觉数据集，以及对诸如分类等各种任务的注释标注。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04628v1 Announce Type: cross  Abstract: Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription -- this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing.   This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, 
&lt;/p&gt;</description></item><item><title>该文章研究了年龄和性别等人口变化如何影响腹部CT图像的分割，提出了一个评估这一影响的框架，并利用大型公开数据集进行了首次分析。</title><link>https://arxiv.org/abs/2408.04610</link><description>&lt;p&gt;
Quantifying the Impact of Population Shift Across Age and Sex for Abdominal Organ Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04610
&lt;/p&gt;
&lt;p&gt;
该文章研究了年龄和性别等人口变化如何影响腹部CT图像的分割，提出了一个评估这一影响的框架，并利用大型公开数据集进行了首次分析。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04610v1 Announce Type: cross  Abstract: Deep learning-based medical image segmentation has seen tremendous progress over the last decade, but there is still relatively little transfer into clinical practice. One of the main barriers is the challenge of domain generalisation, which requires segmentation models to maintain high performance across a wide distribution of image data. This challenge is amplified by the many factors that contribute to the diverse appearance of medical images, such as acquisition conditions and patient characteristics. The impact of shifting patient characteristics such as age and sex on segmentation performance remains relatively under-studied, especially for abdominal organs, despite that this is crucial for ensuring the fairness of the segmentation model. We perform the first study to determine the impact of population shift with respect to age and sex on abdominal CT image segmentation, by leveraging two large public datasets, and introduce a no
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EPPNet的增强型原型部分网络架构，用于通过发现可解释的图像分类原型增强机器学习系统的透明度和信任度。该架构通过引入一种新的原型损失函数来提高原型的相关性，从而在保持高水平准确性同时实现了良好的人性化解释性。此外，文章还提出了一个公正性评分，用于评价基于发现的原型结果的解释能力。</title><link>https://arxiv.org/abs/2408.04606</link><description>&lt;p&gt;
Enhanced Prototypical Part Network (EPPNet) For Explainable Image Classification Via Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04606
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EPPNet的增强型原型部分网络架构，用于通过发现可解释的图像分类原型增强机器学习系统的透明度和信任度。该架构通过引入一种新的原型损失函数来提高原型的相关性，从而在保持高水平准确性同时实现了良好的人性化解释性。此外，文章还提出了一个公正性评分，用于评价基于发现的原型结果的解释能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04606v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (xAI) has the potential to enhance the transparency and trust of AI-based systems. Although accurate predictions can be made using Deep Neural Networks (DNNs), the process used to arrive at such predictions is usually hard to explain. In terms of perceptibly human-friendly representations, such as word phrases in text or super-pixels in images, prototype-based explanations can justify a model's decision. In this work, we introduce a DNN architecture for image classification, the Enhanced Prototypical Part Network (EPPNet), which achieves strong performance while discovering relevant prototypes that can be used to explain the classification results. This is achieved by introducing a novel cluster loss that helps to discover more relevant human-understandable prototypes. We also introduce a faithfulness score to evaluate the explainability of the results based on the discovered prototypes. Our score not 
&lt;/p&gt;</description></item><item><title>该文章提出了一种采用YOLOv8变体模型的工业设施工伤检测系统，通过数据集扩增来优化模型性能，最终选定具有2590万参数和79.1 GFLOPs的YOLOv8m模型实现了0.971的均一平均精度（mAP），并在提示词“Fall Detected”和“Human in Motion”下达到50%的交并比（IoU）。</title><link>https://arxiv.org/abs/2408.04605</link><description>&lt;p&gt;
Fall Detection for Industrial Setups Using YOLOv8 Variants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04605
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种采用YOLOv8变体模型的工业设施工伤检测系统，通过数据集扩增来优化模型性能，最终选定具有2590万参数和79.1 GFLOPs的YOLOv8m模型实现了0.971的均一平均精度（mAP），并在提示词“Fall Detected”和“Human in Motion”下达到50%的交并比（IoU）。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04605v1 Announce Type: new  Abstract: This paper presents the development of an industrial fall detection system utilizing YOLOv8 variants, enhanced by our proposed augmentation pipeline to increase dataset variance and improve detection accuracy. Among the models evaluated, the YOLOv8m model, consisting of 25.9 million parameters and 79.1 GFLOPs, demonstrated a respectable balance between computational efficiency and detection performance, achieving a mean Average Precision (mAP) of 0.971 at 50% Intersection over Union (IoU) across both "Fall Detected" and "Human in Motion" categories. Although the YOLOv8l and YOLOv8x models presented higher precision and recall, particularly in fall detection, their higher computational demands and model size make them less suitable for resource-constrained environments.
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的基于群特征的网络架构Group3AD，通过设计群内同质性网络（IUN）有效解决高级Transformer方法在捕获HRPCD信息时的特征退化问题，以及异常区域占比不高导致的特征难以刻画的问题。</title><link>https://arxiv.org/abs/2408.04604</link><description>&lt;p&gt;
Towards High-resolution 3D Anomaly Detection via Group-Level Feature Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04604
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的基于群特征的网络架构Group3AD，通过设计群内同质性网络（IUN）有效解决高级Transformer方法在捕获HRPCD信息时的特征退化问题，以及异常区域占比不高导致的特征难以刻画的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04604v1 Announce Type: new  Abstract: High-resolution point clouds~(HRPCD) anomaly detection~(AD) plays a critical role in precision machining and high-end equipment manufacturing. Despite considerable 3D-AD methods that have been proposed recently, they still cannot meet the requirements of the HRPCD-AD task. There are several challenges: i) It is difficult to directly capture HRPCD information due to large amounts of points at the sample level; ii) The advanced transformer-based methods usually obtain anisotropic features, leading to degradation of the representation; iii) The proportion of abnormal areas is very small, which makes it difficult to characterize. To address these challenges, we propose a novel group-level feature-based network, called Group3AD, which has a significantly efficient representation ability. First, we design an Intercluster Uniformity Network~(IUN) to present the mapping of different groups in the feature space as several clusters, and obtain a m
&lt;/p&gt;</description></item><item><title>该文章提出了一种简单的框架，通过引入一种称为"解释一致性"的新度量，能够在不需额外监督的情况下，获得更具解释性的激活热图并同时提高模型性能。通过重新加权训练样本，该框架确保了模型对原始样本的解释与对抗样本的解释相似，从而提高了模型的透明度和表现。</title><link>https://arxiv.org/abs/2408.04600</link><description>&lt;p&gt;
Improving Network Interpretability via Explanation Consistency Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04600
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种简单的框架，通过引入一种称为"解释一致性"的新度量，能够在不需额外监督的情况下，获得更具解释性的激活热图并同时提高模型性能。通过重新加权训练样本，该框架确保了模型对原始样本的解释与对抗样本的解释相似，从而提高了模型的透明度和表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04600v1 Announce Type: new  Abstract: While deep neural networks have achieved remarkable performance, they tend to lack transparency in prediction. The pursuit of greater interpretability in neural networks often results in a degradation of their original performance. Some works strive to improve both interpretability and performance, but they primarily depend on meticulously imposed conditions. In this paper, we propose a simple yet effective framework that acquires more explainable activation heatmaps and simultaneously increase the model performance, without the need for any extra supervision. Specifically, our concise framework introduces a new metric, i.e., explanation consistency, to reweight the training samples adaptively in model learning. The explanation consistency metric is utilized to measure the similarity between the model's visual explanations of the original samples and those of semantic-preserved adversarial samples, whose background regions are perturbed 
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一种名为Img-Diff的高质量跨模态数据集，该数据集通过对比学习方法和图像差异描述，增强了大型语言模型在图像识别方面的精细度。文章采用稳定的扩散模型和技术来创建相似图像对，并利用自动生成的差异描述来训练状态最先进的MLLMs，实现了显著的性能提升。</title><link>https://arxiv.org/abs/2408.04594</link><description>&lt;p&gt;
Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04594
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一种名为Img-Diff的高质量跨模态数据集，该数据集通过对比学习方法和图像差异描述，增强了大型语言模型在图像识别方面的精细度。文章采用稳定的扩散模型和技术来创建相似图像对，并利用自动生成的差异描述来训练状态最先进的MLLMs，实现了显著的性能提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04594v1 Announce Type: new  Abstract: High-performance Multimodal Large Language Models (MLLMs) rely heavily on data quality. This study introduces a novel dataset named Img-Diff, designed to enhance fine-grained image recognition in MLLMs by leveraging insights from contrastive learning and image difference captioning. By analyzing object differences between similar images, we challenge models to identify both matching and distinct components. We utilize the Stable-Diffusion-XL model and advanced image editing techniques to create pairs of similar images that highlight object replacements. Our methodology includes a Difference Area Generator for object differences identifying, followed by a Difference Captions Generator for detailed difference descriptions. The result is a relatively small but high-quality dataset of "object replacement" samples. We use the the proposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B, yielding comprehensive improvements of
&lt;/p&gt;</description></item><item><title>该文章探讨了Segment Anything Model 2 (SAM 2)在机器人手术中的应用，进行了一次实证评价，特别是评估了其在手术视频分割中的鲁棒性和泛化能力。研究显示，SAM 2在处理视频跟踪和对象遮挡问题时表现出色，并且在互动分割（包括图像和视频）方面取得了显著成果。此外，文章还评估了SAM 2在不同类型提示下的零 shot分割性能，并测试了其在真实世界干扰情况下的表现。通过对MICCAI EndoVis 2017和EndoVis 2018基准测试的数据集进行广泛实验，发现使用边界框提示的SAM 2在对比评估中优于当前的先进技术。使用点提示的结果也显示了潜力。总的来说，SAM 2在手术视频分割中的应用展现了强大的能力，为机器人手术领域提供了一个有前景的解决方案。</title><link>https://arxiv.org/abs/2408.04593</link><description>&lt;p&gt;
SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and Generalization in Surgical Video Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04593
&lt;/p&gt;
&lt;p&gt;
该文章探讨了Segment Anything Model 2 (SAM 2)在机器人手术中的应用，进行了一次实证评价，特别是评估了其在手术视频分割中的鲁棒性和泛化能力。研究显示，SAM 2在处理视频跟踪和对象遮挡问题时表现出色，并且在互动分割（包括图像和视频）方面取得了显著成果。此外，文章还评估了SAM 2在不同类型提示下的零 shot分割性能，并测试了其在真实世界干扰情况下的表现。通过对MICCAI EndoVis 2017和EndoVis 2018基准测试的数据集进行广泛实验，发现使用边界框提示的SAM 2在对比评估中优于当前的先进技术。使用点提示的结果也显示了潜力。总的来说，SAM 2在手术视频分割中的应用展现了强大的能力，为机器人手术领域提供了一个有前景的解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04593v1 Announce Type: cross  Abstract: The recent Segment Anything Model (SAM) 2 has demonstrated remarkable foundational competence in semantic segmentation, with its memory mechanism and mask decoder further addressing challenges in video tracking and object occlusion, thereby achieving superior results in interactive segmentation for both images and videos. Building upon our previous empirical studies, we further explore the zero-shot segmentation performance of SAM 2 in robot-assisted surgery based on prompts, alongside its robustness against real-world corruption. For static images, we employ two forms of prompts: 1-point and bounding box, while for video sequences, the 1-point prompt is applied to the initial frame. Through extensive experimentation on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding box prompts, outperforms state-of-the-art (SOTA) methods in comparative evaluations. The results with point prompts also exhibit a subs
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为HiLo的框架，用于在跨域变化下进行的一般化类别发现任务。该框架提取高阶语义和低阶域特征，并通过最小化两种特征表示之间的互信息来确保类别信息与域信息不相关。通过特殊设计的数据增强和基于课程学习的策略，该框架在处理来自不同域的未标注数据方面取得了显著成果。</title><link>https://arxiv.org/abs/2408.04591</link><description>&lt;p&gt;
HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04591
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为HiLo的框架，用于在跨域变化下进行的一般化类别发现任务。该框架提取高阶语义和低阶域特征，并通过最小化两种特征表示之间的互信息来确保类别信息与域信息不相关。通过特殊设计的数据增强和基于课程学习的策略，该框架在处理来自不同域的未标注数据方面取得了显著成果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04591v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) is a challenging task in which, given a partially labelled dataset, models must categorize all unlabelled instances, regardless of whether they come from labelled categories or from new ones. In this paper, we challenge a remaining assumption in this task: that all images share the same domain. Specifically, we introduce a new task and method to handle GCD when the unlabelled data also contains images from different domains to the labelled set. Our proposed `HiLo' networks extract High-level semantic and Low-level domain features, before minimizing the mutual information between the representations. Our intuition is that the clusterings based on domain information and semantic information should be independent. We further extend our method with a specialized domain augmentation tailored for the GCD task, as well as a curriculum learning approach. Finally, we construct a benchmark from corrupted fine-g
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的算法，用于从场景的不规则采样网格中合成新的视角，这种方法首先将每个采样的视角扩展到一个局部光场，然后通过混合相邻的局部光场来渲染新的视角，为用户提供了如何可靠地合成高质量新视角的具体指导。</title><link>https://arxiv.org/abs/2408.04586</link><description>&lt;p&gt;
Sampling for View Synthesis: From Local Light Field Fusion to Neural Radiance Fields and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04586
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的算法，用于从场景的不规则采样网格中合成新的视角，这种方法首先将每个采样的视角扩展到一个局部光场，然后通过混合相邻的局部光场来渲染新的视角，为用户提供了如何可靠地合成高质量新视角的具体指导。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04586v1 Announce Type: cross  Abstract: Capturing and rendering novel views of complex real-world scenes is a long-standing problem in computer graphics and vision, with applications in augmented and virtual reality, immersive experiences and 3D photography. The advent of deep learning has enabled revolutionary advances in this area, classically known as image-based rendering. However, previous approaches require intractably dense view sampling or provide little or no guidance for how users should sample views of a scene to reliably render high-quality novel views. Local light field fusion proposes an algorithm for practical view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image scene representation, then renders novel views by blending adjacent local light fields. Crucially, we extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should s
&lt;/p&gt;</description></item><item><title>该文章介绍了SAM2-Adapter，一个专门设计用于克服SAM2在特定下游任务中持续存在的问题的适配器，如在医学图像分割、隐藏物体检测和阴影检测方面实现了新的最先进结果。</title><link>https://arxiv.org/abs/2408.04579</link><description>&lt;p&gt;
SAM2-Adapter: Evaluating &amp;amp; Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04579
&lt;/p&gt;
&lt;p&gt;
该文章介绍了SAM2-Adapter，一个专门设计用于克服SAM2在特定下游任务中持续存在的问题的适配器，如在医学图像分割、隐藏物体检测和阴影检测方面实现了新的最先进结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04579v1 Announce Type: new  Abstract: The advent of large models, also known as foundation models, has significantly transformed the AI research landscape, with models like Segment Anything (SAM) achieving notable success in diverse image segmentation scenarios. Despite its advancements, SAM encountered limitations in handling some complex low-level segmentation tasks like camouflaged object and medical imaging. In response, in 2023, we introduced SAM-Adapter, which demonstrated improved performance on these challenging tasks. Now, with the release of Segment Anything 2 (SAM2), a successor with enhanced architecture and a larger training corpus, we reassess these challenges. This paper introduces SAM2-Adapter, the first adapter designed to overcome the persistent limitations observed in SAM2 and achieve new state-of-the-art (SOTA) results in specific downstream tasks including medical image segmentation, camouflaged (concealed) object detection, and shadow detection. SAM2-Ad
&lt;/p&gt;</description></item><item><title>该文章提出了一种自动生成交互式3D游戏场景的方法，使用户可以通过简单的手绘草图来创建场景。方法首先使用预训练的2D去噪扩散模型生成场景的2D概念图像，并通过等距投影模式来忽略未知相机位置，从而获取场景布局。随后，通过预训练的图像理解技术对图像进行分割，并生成具有特定功能的设计草图。最终，这种方法将设计草图转换为完整的3D游戏场景，实现了用户与虚拟环境的高效互动。</title><link>https://arxiv.org/abs/2408.04567</link><description>&lt;p&gt;
Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04567
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种自动生成交互式3D游戏场景的方法，使用户可以通过简单的手绘草图来创建场景。方法首先使用预训练的2D去噪扩散模型生成场景的2D概念图像，并通过等距投影模式来忽略未知相机位置，从而获取场景布局。随后，通过预训练的图像理解技术对图像进行分割，并生成具有特定功能的设计草图。最终，这种方法将设计草图转换为完整的3D游戏场景，实现了用户与虚拟环境的高效互动。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04567v1 Announce Type: new  Abstract: 3D Content Generation is at the heart of many computer graphics applications, including video gaming, film-making, virtual and augmented reality, etc. This paper proposes a novel deep-learning based approach for automatically generating interactive and playable 3D game scenes, all from the user's casual prompts such as a hand-drawn sketch. Sketch-based input offers a natural, and convenient way to convey the user's design intention in the content creation process. To circumvent the data-deficient challenge in learning (i.e. the lack of large training data of 3D scenes), our method leverages a pre-trained 2D denoising diffusion model to generate a 2D image of the scene as the conceptual guidance. In this process, we adopt the isometric projection mode to factor out unknown camera poses while obtaining the scene layout. From the generated isometric image, we use a pre-trained image understanding method to segment the image into meaningful 
&lt;/p&gt;</description></item><item><title>该文章提出了一个新方法“Depth Any Canopy”，它通过在远程 sensing数据上对机器学习模型进行微调来估计全球树冠高，这种方法利用了深度告示模型，克服了之前需要在多种情况下提供大量训练数据的限制。</title><link>https://arxiv.org/abs/2408.04523</link><description>&lt;p&gt;
Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04523
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个新方法“Depth Any Canopy”，它通过在远程 sensing数据上对机器学习模型进行微调来估计全球树冠高，这种方法利用了深度告示模型，克服了之前需要在多种情况下提供大量训练数据的限制。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04523v1 Announce Type: new  Abstract: Estimating global tree canopy height is crucial for forest conservation and climate change applications. However, capturing high-resolution ground truth canopy height using LiDAR is expensive and not available globally. An efficient alternative is to train a canopy height estimator to operate on single-view remotely sensed imagery. The primary obstacle to this approach is that these methods require significant training data to generalize well globally and across uncommon edge cases. Recent monocular depth estimation foundation models have show strong zero-shot performance even for complex scenes. In this paper we leverage the representations learned by these models to transfer to the remote sensing domain for measuring canopy height. Our findings suggest that our proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2 model for canopy height estimation, provides a performant and efficient solution, surpassing the curre
&lt;/p&gt;</description></item><item><title>该文章创新在于分析了当前模型在教育视频中的性能，识别了现有技术的局限性，并指出了未来研究的方向。</title><link>https://arxiv.org/abs/2408.04515</link><description>&lt;p&gt;
Saliency Detection in Educational Videos: Analyzing the Performance of Current Models, Identifying Limitations and Advancement Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04515
&lt;/p&gt;
&lt;p&gt;
该文章创新在于分析了当前模型在教育视频中的性能，识别了现有技术的局限性，并指出了未来研究的方向。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04515v1 Announce Type: new  Abstract: Identifying the regions of a learning resource that a learner pays attention to is crucial for assessing the material's impact and improving its design and related support systems. Saliency detection in videos addresses the automatic recognition of attention-drawing regions in single frames. In educational settings, the recognition of pertinent regions in a video's visual stream can enhance content accessibility and information retrieval tasks such as video segmentation, navigation, and summarization. Such advancements can pave the way for the development of advanced AI-assisted technologies that support learning with greater efficacy. However, this task becomes particularly challenging for educational videos due to the combination of unique characteristics such as text, voice, illustrations, animations, and more. To the best of our knowledge, there is currently no study that evaluates saliency detection approaches in educational videos.
&lt;/p&gt;</description></item><item><title>该文章提出了一种新型的深度学习模型，用于在MRI图像中实现对肝脏硬化症的准确分割。该模型整合了连续和离散的 latent space，以捕捉图像的复杂特征交互，并在一个包含628个样本的私有数据集上取得了相较于 baseline 模型的显著提升。</title><link>https://arxiv.org/abs/2408.04491</link><description>&lt;p&gt;
Towards Synergistic Deep Learning Models for Volumetric Cirrhotic Liver Segmentation in MRIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04491
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新型的深度学习模型，用于在MRI图像中实现对肝脏硬化症的准确分割。该模型整合了连续和离散的 latent space，以捕捉图像的复杂特征交互，并在一个包含628个样本的私有数据集上取得了相较于 baseline 模型的显著提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04491v1 Announce Type: new  Abstract: Liver cirrhosis, a leading cause of global mortality, requires precise segmentation of ROIs for effective disease monitoring and treatment planning. Existing segmentation models often fail to capture complex feature interactions and generalize across diverse datasets. To address these limitations, we propose a novel synergistic theory that leverages complementary latent spaces for enhanced feature interaction modeling. Our proposed architecture, nnSynergyNet3D integrates continuous and discrete latent spaces for 3D volumes and features auto-configured training. This approach captures both fine-grained and coarse features, enabling effective modeling of intricate feature interactions. We empirically validated nnSynergyNet3D on a private dataset of 628 high-resolution T1 abdominal MRI scans from 339 patients. Our model outperformed the baseline nnUNet3D by approximately 2%. Additionally, zero-shot testing on healthy liver CT scans from the
&lt;/p&gt;</description></item><item><title>该文章提出了一个基于可解释主动学习的 semantic segmentation 模型 "SegXAL"，该模型能够有效利用未标记数据，促进人类参与的 "Human-in-the-loop" 模式，并使模型决策变得可解释，尤其适用于驾驶场景中的图像识别任务。</title><link>https://arxiv.org/abs/2408.04482</link><description>&lt;p&gt;
SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04482
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个基于可解释主动学习的 semantic segmentation 模型 "SegXAL"，该模型能够有效利用未标记数据，促进人类参与的 "Human-in-the-loop" 模式，并使模型决策变得可解释，尤其适用于驾驶场景中的图像识别任务。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04482v1 Announce Type: cross  Abstract: Most of the sophisticated AI models utilize huge amounts of annotated data and heavy training to achieve high-end performance. However, there are certain challenges that hinder the deployment of AI models "in-the-wild" scenarios, i.e., inefficient use of unlabeled data, lack of incorporation of human expertise, and lack of interpretation of the results. To mitigate these challenges, we propose a novel Explainable Active Learning (XAL) model, XAL-based semantic segmentation model "SegXAL", that can (i) effectively utilize the unlabeled data, (ii) facilitate the "Human-in-the-loop" paradigm, and (iii) augment the model decisions in an interpretable way. In particular, we investigate the application of the SegXAL model for semantic segmentation in driving scene scenarios. The SegXAL model proposes the image regions that require labeling assistance from Oracle by dint of explainable AI (XAI) and uncertainty measures in a weakly-supervised 
&lt;/p&gt;</description></item><item><title>该文章提出的LumiGauss技术通过2D高斯点法线计算和增强的阴影质量，实现了户外场景的高保真光照重映射，无需约束场景，可直接从集合中重构场景和环境光照，大幅加快了游戏引擎中大规模复杂场景的3D assets创建过程，减少了依赖手动劳动的时间，通过有效的光照合成和高质量的阴影处理，提高了输出的实用性和视觉效果。</title><link>https://arxiv.org/abs/2408.04474</link><description>&lt;p&gt;
LumiGauss: High-Fidelity Outdoor Relighting with 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04474
&lt;/p&gt;
&lt;p&gt;
该文章提出的LumiGauss技术通过2D高斯点法线计算和增强的阴影质量，实现了户外场景的高保真光照重映射，无需约束场景，可直接从集合中重构场景和环境光照，大幅加快了游戏引擎中大规模复杂场景的3D assets创建过程，减少了依赖手动劳动的时间，通过有效的光照合成和高质量的阴影处理，提高了输出的实用性和视觉效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04474v1 Announce Type: new  Abstract: Decoupling lighting from geometry using unconstrained photo collections is notoriously challenging. Solving it would benefit many users, as creating complex 3D assets takes days of manual labor. Many previous works have attempted to address this issue, often at the expense of output fidelity, which questions the practicality of such methods.   We introduce LumiGauss, a technique that tackles 3D reconstruction of scenes and environmental lighting through 2D Gaussian Splatting. Our approach yields high-quality scene reconstructions and enables realistic lighting synthesis under novel environment maps. We also propose a method for enhancing the quality of shadows, common in outdoor scenes, by exploiting spherical harmonics properties. Our approach facilitates seamless integration with game engines and enables the use of fast precomputed radiance transfer.   We validate our method on the NeRF-OSR dataset, demonstrating superior performance o
&lt;/p&gt;</description></item><item><title>该文章提出了一种简单有效的方案，能够预测并使用自然语言描述计算机视觉模型遇到潜在失败模式时的性能。这有助于开发者理解和测试新环境的适用性，同时也为未来在这一领域的研究奠定了基础。</title><link>https://arxiv.org/abs/2408.04471</link><description>&lt;p&gt;
What could go wrong? Discovering and describing failure modes in computer vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04471
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种简单有效的方案，能够预测并使用自然语言描述计算机视觉模型遇到潜在失败模式时的性能。这有助于开发者理解和测试新环境的适用性，同时也为未来在这一领域的研究奠定了基础。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04471v1 Announce Type: new  Abstract: Deep learning models are effective, yet brittle. Even carefully trained, their behavior tends to be hard to predict when confronted with out-of-distribution samples. In this work, our goal is to propose a simple yet effective solution to predict and describe via natural language potential failure modes of computer vision models. Given a pretrained model and a set of samples, our aim is to find sentences that accurately describe the visual conditions in which the model underperforms. In order to study this important topic and foster future research on it, we formalize the problem of Language-Based Error Explainability (LBEE) and propose a set of metrics to evaluate and compare different methods for this task. We propose solutions that operate in a joint vision-and-language embedding space, and can characterize through language descriptions model failures caused, e.g., by objects unseen during training or adverse visual conditions. We expe
&lt;/p&gt;</description></item><item><title>该文章通过深度学习方法，对心音图信号中的收缩期波形进行了跨数据集分析，证明了在真实世界条件下，通过对模型进行个性化调整，能够有效识别出收缩期波形，提高了心音图信号分析的准确性。</title><link>https://arxiv.org/abs/2408.04439</link><description>&lt;p&gt;
Deep Learning for identifying systolic complexes in SCG traces: a cross-dataset analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04439
&lt;/p&gt;
&lt;p&gt;
该文章通过深度学习方法，对心音图信号中的收缩期波形进行了跨数据集分析，证明了在真实世界条件下，通过对模型进行个性化调整，能够有效识别出收缩期波形，提高了心音图信号分析的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04439v1 Announce Type: new  Abstract: The seismocardiographic signal is a promising alternative to the traditional ECG in the analysis of the cardiac activity. In particular, the systolic complex is known to be the most informative part of the seismocardiogram, thus requiring further analysis. State-of-art solutions to detect the systolic complex are based on Deep Learning models, which have been proven effective in pioneering studies. However, these solutions have only been tested in a controlled scenario considering only clean signals acquired from users maintained still in supine position. On top of that, all these studies consider data coming from a single dataset, ignoring the benefits and challenges related to a cross-dataset scenario. In this work, a cross-dataset experimental analysis was performed considering also data from a real-world scenario. Our findings prove the effectiveness of a deep learning solution, while showing the importance of a personalization step 
&lt;/p&gt;</description></item><item><title>该文章详细介绍了基于NeRF（Neural Radiance Fields）和3D-GS（Gaussian splatting）的3D重建技术对于可变形组织在机器人手术中的应用，并对其创新性和贡献进行了总结。文章强调这些技术在提高手术场景重建质量方面的潜力，尤其在处理动态场景和手术工具遮挡方面。通过对SOTA方法的探讨，文章提供了对这些方法的深入理解，并通过测试和评估进一步验证了这些技术的有效性。文章的贡献在于它为机器人手术中3D重建技术的实施和优化提供了详细的讨论和测试结果，为临床应用提供了参考。</title><link>https://arxiv.org/abs/2408.04426</link><description>&lt;p&gt;
A Review of 3D Reconstruction Techniques for Deformable Tissues in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04426
&lt;/p&gt;
&lt;p&gt;
该文章详细介绍了基于NeRF（Neural Radiance Fields）和3D-GS（Gaussian splatting）的3D重建技术对于可变形组织在机器人手术中的应用，并对其创新性和贡献进行了总结。文章强调这些技术在提高手术场景重建质量方面的潜力，尤其在处理动态场景和手术工具遮挡方面。通过对SOTA方法的探讨，文章提供了对这些方法的深入理解，并通过测试和评估进一步验证了这些技术的有效性。文章的贡献在于它为机器人手术中3D重建技术的实施和优化提供了详细的讨论和测试结果，为临床应用提供了参考。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04426v1 Announce Type: cross  Abstract: As a crucial and intricate task in robotic minimally invasive surgery, reconstructing surgical scenes using stereo or monocular endoscopic video holds immense potential for clinical applications. NeRF-based techniques have recently garnered attention for the ability to reconstruct scenes implicitly. On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly using 3D Gaussians and projects them onto a 2D plane as a replacement for the complex volume rendering in NeRF. However, these methods face challenges regarding surgical scene reconstruction, such as slow inference, dynamic scenes, and surgical tool occlusion. This work explores and reviews state-of-the-art (SOTA) approaches, discussing their innovations and implementation principles. Furthermore, we replicate the models and conduct testing and evaluation on two datasets. The test results demonstrate that with advancements in these techniques, achieving real-time
&lt;/p&gt;</description></item><item><title>该文章提出了一种深度学习方法，用于自动识别卫星图像中的环境遮蔽类型，这一创新有助于提高无线通信传播模型中遮蔽信息的准确性。</title><link>https://arxiv.org/abs/2408.04407</link><description>&lt;p&gt;
Clutter Classification Using Deep Learning in Multiple Stages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04407
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种深度学习方法，用于自动识别卫星图像中的环境遮蔽类型，这一创新有助于提高无线通信传播模型中遮蔽信息的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04407v1 Announce Type: cross  Abstract: Path loss prediction for wireless communications is highly dependent on the local environment. Propagation models including clutter information have been shown to significantly increase model accuracy. This paper explores the application of deep learning to satellite imagery to identify environmental clutter types automatically. Recognizing these clutter types has numerous uses, but our main application is to use clutter information to enhance propagation prediction models. Knowing the type of obstruction (tree, building, and further classifications) can improve the prediction accuracy of key propagation metrics such as path loss.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为MultiViPerFrOG的全球优化多视角感知框架，该框架能够从移动深度相机捕获的信息中有效地重建3D场景的形状，特别是在处理组织变形时。该框架能够同时估计多个摄像机的运动和场景的绝对流，并通过结合低级感知模块的输出和先验知识，成功解决单视角下同时估计相机运动和组织变形的问题。</title><link>https://arxiv.org/abs/2408.04367</link><description>&lt;p&gt;
MultiViPerFrOG: A Globally Optimized Multi-Viewpoint Perception Framework for Camera Motion and Tissue Deformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04367
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为MultiViPerFrOG的全球优化多视角感知框架，该框架能够从移动深度相机捕获的信息中有效地重建3D场景的形状，特别是在处理组织变形时。该框架能够同时估计多个摄像机的运动和场景的绝对流，并通过结合低级感知模块的输出和先验知识，成功解决单视角下同时估计相机运动和组织变形的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04367v1 Announce Type: new  Abstract: Reconstructing the 3D shape of a deformable environment from the information captured by a moving depth camera is highly relevant to surgery. The underlying challenge is the fact that simultaneously estimating camera motion and tissue deformation in a fully deformable scene is an ill-posed problem, especially from a single arbitrarily moving viewpoint. Current solutions are often organ-specific and lack the robustness required to handle large deformations. Here we propose a multi-viewpoint global optimization framework that can flexibly integrate the output of low-level perception modules (data association, depth, and relative scene flow) with kinematic and scene-modeling priors to jointly estimate multiple camera motions and absolute scene flow. We use simulated noisy data to show three practical examples that successfully constrain the convergence to a unique solution. Overall, our method shows robustness to combined noisy input measur
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用对象检测和深度估计的深度学习框架，可以利用手持设备如手机快速准确地检测汽车速度，提供了一种新型的无需使用传统技术即可监控车辆速度的方法。</title><link>https://arxiv.org/abs/2408.04360</link><description>&lt;p&gt;
Detecting Car Speed using Object Detection and Depth Estimation: A Deep Learning Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04360
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用对象检测和深度估计的深度学习框架，可以利用手持设备如手机快速准确地检测汽车速度，提供了一种新型的无需使用传统技术即可监控车辆速度的方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04360v1 Announce Type: cross  Abstract: Road accidents are quite common in almost every part of the world, and, in majority, fatal accidents are attributed to over speeding of vehicles. The tendency to over speeding is usually tried to be controlled using check points at various parts of the road but not all traffic police have the device to check speed with existing speed estimating devices such as LIDAR based, or Radar based guns. The current project tries to address the issue of vehicle speed estimation with handheld devices such as mobile phones or wearable cameras with network connection to estimate the speed using deep learning frameworks.
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于图像旋转的自监督学习方法，通过将每个图像定义的旋转视为新类进行训练，并在推断时聚合所有图像旋转预测结果，以此提高类增量学习中的性能。研究发现该方法能够促进神经网络对物体内部特征的关注，从而显著提高类增量学习中的特征学习效果。该研究进一步证明，这种方法可以集成到各种类增量学习框架中，作为一种可直接使用的模块，并且能够显著提升这些学习框架在不同类增量学习任务和数据集上的表现。</title><link>https://arxiv.org/abs/2408.04347</link><description>&lt;p&gt;
AggSS: An Aggregated Self-Supervised Approach for Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04347
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于图像旋转的自监督学习方法，通过将每个图像定义的旋转视为新类进行训练，并在推断时聚合所有图像旋转预测结果，以此提高类增量学习中的性能。研究发现该方法能够促进神经网络对物体内部特征的关注，从而显著提高类增量学习中的特征学习效果。该研究进一步证明，这种方法可以集成到各种类增量学习框架中，作为一种可直接使用的模块，并且能够显著提升这些学习框架在不同类增量学习任务和数据集上的表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04347v1 Announce Type: new  Abstract: This paper investigates the impact of self-supervised learning, specifically image rotations, on various class-incremental learning paradigms. Here, each image with a predefined rotation is considered as a new class for training. At inference, all image rotation predictions are aggregated for the final prediction, a strategy we term Aggregated Self-Supervision (AggSS). We observe a shift in the deep neural network's attention towards intrinsic object features as it learns through AggSS strategy. This learning approach significantly enhances class-incremental learning by promoting robust feature learning. AggSS serves as a plug-and-play module that can be seamlessly incorporated into any class-incremental learning framework, leveraging its powerful feature learning capabilities to enhance performance across various class-incremental learning approaches. Extensive experiments conducted on standard incremental learning datasets CIFAR-100 an
&lt;/p&gt;</description></item><item><title>该文章探讨了如何使用大型语言模型（LLMs）和大型多模态模型（LMMs）来增强新闻文章的图像描述质量，从而提升新闻报道的效率和质量。通过实验对比了多种模型的性能，包括BLIP-2、GPT-4v、LLaVA、OFA、ViT-GPT2以及GPT-4和LLaMA，并分析了不同类型上下文（整个新闻文章或抽取的命名实体）对模型效果的影响。文章发现，尽管不同模型的选择对描述质量有显著影响，但使用LLMs进行后续的上下文整合是一个有效的改进策略。</title><link>https://arxiv.org/abs/2408.04331</link><description>&lt;p&gt;
Enhancing Journalism with AI: A Study of Contextualized Image Captioning for News Articles using LLMs and LMMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04331
&lt;/p&gt;
&lt;p&gt;
该文章探讨了如何使用大型语言模型（LLMs）和大型多模态模型（LMMs）来增强新闻文章的图像描述质量，从而提升新闻报道的效率和质量。通过实验对比了多种模型的性能，包括BLIP-2、GPT-4v、LLaVA、OFA、ViT-GPT2以及GPT-4和LLaMA，并分析了不同类型上下文（整个新闻文章或抽取的命名实体）对模型效果的影响。文章发现，尽管不同模型的选择对描述质量有显著影响，但使用LLMs进行后续的上下文整合是一个有效的改进策略。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04331v1 Announce Type: cross  Abstract: Large language models (LLMs) and large multimodal models (LMMs) have significantly impacted the AI community, industry, and various economic sectors. In journalism, integrating AI poses unique challenges and opportunities, particularly in enhancing the quality and efficiency of news reporting. This study explores how LLMs and LMMs can assist journalistic practice by generating contextualised captions for images accompanying news articles. We conducted experiments using the GoodNews dataset to evaluate the ability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of context: entire news articles, or extracted named entities. In addition, we compared their performance to a two-stage pipeline composed of a captioning model (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs (GPT-4 or LLaMA). We assess a diversity of models, and we find that while the choice of contextualisation model is a significant fact
&lt;/p&gt;</description></item><item><title>该文章提出了一种增强多尺度信息和细节的Segment Anything Model（MDSAM），用于解决Salient Object Detection（SOD）中的问题。MDSAM通过引入轻量级多尺度适配器（LMSA），有效增强了模型的多尺度学习能力，同时保证了模型的高性能和良好的泛化能力，尤其是在复杂场景中。此外，MDSAM还能够充分利用细节信息，提高了SOD的准确性。</title><link>https://arxiv.org/abs/2408.04326</link><description>&lt;p&gt;
Multi-Scale and Detail-Enhanced Segment Anything Model for Salient Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04326
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种增强多尺度信息和细节的Segment Anything Model（MDSAM），用于解决Salient Object Detection（SOD）中的问题。MDSAM通过引入轻量级多尺度适配器（LMSA），有效增强了模型的多尺度学习能力，同时保证了模型的高性能和良好的泛化能力，尤其是在复杂场景中。此外，MDSAM还能够充分利用细节信息，提高了SOD的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04326v1 Announce Type: new  Abstract: Salient Object Detection (SOD) aims to identify and segment the most prominent objects in images. Advanced SOD methods often utilize various Convolutional Neural Networks (CNN) or Transformers for deep feature extraction. However, these methods still deliver low performance and poor generalization in complex cases. Recently, Segment Anything Model (SAM) has been proposed as a visual fundamental model, which gives strong segmentation and generalization capabilities. Nonetheless, SAM requires accurate prompts of target objects, which are unavailable in SOD. Additionally, SAM lacks the utilization of multi-scale and multi-level information, as well as the incorporation of fine-grained details. To address these shortcomings, we propose a Multi-scale and Detail-enhanced SAM (MDSAM) for SOD. Specifically, we first introduce a Lightweight Multi-Scale Adapter (LMSA), which allows SAM to learn multi-scale information with very few trainable param
&lt;/p&gt;</description></item><item><title>该文章利用深度迁移学习方法在缺乏大量标注数据的情况下实现了对肾脏癌的自动诊断，开辟了在小数据条件下提高疾病诊断准确性的新型路径。</title><link>https://arxiv.org/abs/2408.04318</link><description>&lt;p&gt;
Deep Transfer Learning for Kidney Cancer Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04318
&lt;/p&gt;
&lt;p&gt;
该文章利用深度迁移学习方法在缺乏大量标注数据的情况下实现了对肾脏癌的自动诊断，开辟了在小数据条件下提高疾病诊断准确性的新型路径。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04318v1 Announce Type: cross  Abstract: Many incurable diseases prevalent across global societies stem from various influences, including lifestyle choices, economic conditions, social factors, and genetics. Research predominantly focuses on these diseases due to their widespread nature, aiming to decrease mortality, enhance treatment options, and improve healthcare standards. Among these, kidney disease stands out as a particularly severe condition affecting men and women worldwide. Nonetheless, there is a pressing need for continued research into innovative, early diagnostic methods to develop more effective treatments for such diseases. Recently, automatic diagnosis of Kidney Cancer has become an important challenge especially when using deep learning (DL) due to the importance of training medical datasets, which in most cases are difficult and expensive to obtain. Furthermore, in most cases, algorithms require data from the same domain and a powerful computer with effici
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为NL-RAN的3D注意力非局部网络，用于快速且可解释地诊断COVID-19。通过对CT图像进行直接分析并输出分类结果，该网络能够捕获全局信息和关注病变细节，增加模型的解释性。该网络在包含4079个来自不同分类的3D CT扫描的训练集中进行验证，展示了其在新型冠状病毒感染、普通肺炎和正常肺部CT图像分类方面的有效性。</title><link>https://arxiv.org/abs/2408.04300</link><description>&lt;p&gt;
An Explainable Non-local Network for COVID-19 Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04300
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为NL-RAN的3D注意力非局部网络，用于快速且可解释地诊断COVID-19。通过对CT图像进行直接分析并输出分类结果，该网络能够捕获全局信息和关注病变细节，增加模型的解释性。该网络在包含4079个来自不同分类的3D CT扫描的训练集中进行验证，展示了其在新型冠状病毒感染、普通肺炎和正常肺部CT图像分类方面的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04300v1 Announce Type: cross  Abstract: The CNN has achieved excellent results in the automatic classification of medical images. In this study, we propose a novel deep residual 3D attention non-local network (NL-RAN) to classify CT images included COVID-19, common pneumonia, and normal to perform rapid and explainable COVID-19 diagnosis. We built a deep residual 3D attention non-local network that could achieve end-to-end training. The network is embedded with a nonlocal module to capture global information, while a 3D attention module is embedded to focus on the details of the lesion so that it can directly analyze the 3D lung CT and output the classification results. The output of the attention module can be used as a heat map to increase the interpretability of the model. 4079 3D CT scans were included in this study. Each scan had a unique label (novel coronavirus pneumonia, common pneumonia, and normal). The CT scans cohort was randomly split into a training set of 3263
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为呼吸运动减去的图像处理方法，用于评估肺肿瘤微波消融治疗的疗效。该方法通过精炼的刚性配准和进一步的非刚性配准技术，结合术前和术后的影像资料，生成减影图像，以增强对微波消融治疗效果的临床评估。</title><link>https://arxiv.org/abs/2408.04299</link><description>&lt;p&gt;
Respiratory Subtraction for Pulmonary Microwave Ablation Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04299
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为呼吸运动减去的图像处理方法，用于评估肺肿瘤微波消融治疗的疗效。该方法通过精炼的刚性配准和进一步的非刚性配准技术，结合术前和术后的影像资料，生成减影图像，以增强对微波消融治疗效果的临床评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04299v1 Announce Type: new  Abstract: Currently, lung cancer is a leading cause of global cancer mortality, often necessitating minimally invasive interventions. Microwave ablation (MWA) is extensively utilized for both primary and secondary lung tumors. Although numerous clinical guidelines and standards for MWA have been established, the clinical evaluation of ablation surgery remains challenging and requires long-term patient follow-up for confirmation. In this paper, we propose a method termed respiratory subtraction to evaluate lung tumor ablation therapy performance based on pre- and post-operative image guidance. Initially, preoperative images undergo coarse rigid registration to their corresponding postoperative positions, followed by further non-rigid registration. Subsequently, subtraction images are generated by subtracting the registered preoperative images from the postoperative ones. Furthermore, to enhance the clinical assessment of MWA treatment performance, 
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于生成自监督学习的双分支 PolSAR 图像分类方法，该方法通过生成掩码自编码器学习超级像素级别的极化特征，并通过融合超级像素和像素级别的特征实现了精细的分类效果。</title><link>https://arxiv.org/abs/2408.04294</link><description>&lt;p&gt;
Dual-branch PolSAR Image Classification Based on GraphMAE and Local Feature Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04294
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于生成自监督学习的双分支 PolSAR 图像分类方法，该方法通过生成掩码自编码器学习超级像素级别的极化特征，并通过融合超级像素和像素级别的特征实现了精细的分类效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04294v1 Announce Type: new  Abstract: The annotation of polarimetric synthetic aperture radar (PolSAR) images is a labor-intensive and time-consuming process. Therefore, classifying PolSAR images with limited labels is a challenging task in remote sensing domain. In recent years, self-supervised learning approaches have proven effective in PolSAR image classification with sparse labels. However, we observe a lack of research on generative selfsupervised learning in the studied task. Motivated by this, we propose a dual-branch classification model based on generative self-supervised learning in this paper. The first branch is a superpixel-branch, which learns superpixel-level polarimetric representations using a generative self-supervised graph masked autoencoder. To acquire finer classification results, a convolutional neural networks-based pixel-branch is further incorporated to learn pixel-level features. Classification with fused dual-branch features is finally performed 
&lt;/p&gt;</description></item><item><title>该文章提出了一种结合深度学习和transformer注意力机制的全新方法，用于从X光胸片高效准确地检测肺炎，该方法通过使用具有特殊自研Transformer模块的TransUNet模型进行肺部分割，减少了模型参数数量，同时保持了性能，并通过使用预训练的ResNet模型进行多尺度特征提取，提高分类任务性能，有效解决了传统诊断方法在区域变量和解读不同医生之间的挑战。</title><link>https://arxiv.org/abs/2408.04290</link><description>&lt;p&gt;
Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale Transformer Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04290
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种结合深度学习和transformer注意力机制的全新方法，用于从X光胸片高效准确地检测肺炎，该方法通过使用具有特殊自研Transformer模块的TransUNet模型进行肺部分割，减少了模型参数数量，同时保持了性能，并通过使用预训练的ResNet模型进行多尺度特征提取，提高分类任务性能，有效解决了传统诊断方法在区域变量和解读不同医生之间的挑战。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04290v1 Announce Type: cross  Abstract: Pneumonia, a severe respiratory disease, poses significant diagnostic challenges, especially in underdeveloped regions. Traditional diagnostic methods, such as chest X-rays, suffer from variability in interpretation among radiologists, necessitating reliable automated tools. In this study, we propose a novel approach combining deep learning and transformer-based attention mechanisms to enhance pneumonia detection from chest X-rays. Our method begins with lung segmentation using a TransUNet model that integrates our specialized transformer module, which has fewer parameters compared to common transformers while maintaining performance. This model is trained on the "Chest Xray Masks and Labels" dataset and then applied to the Kermany and Cohen datasets to isolate lung regions, enhancing subsequent classification tasks. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, 
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于语义引导的JND预测器（SG-JND），能够在考虑到图像内容特征的基础上，对图像压缩过程中的人眼可觉察误差（JND）进行更加精确的预测。</title><link>https://arxiv.org/abs/2408.04273</link><description>&lt;p&gt;
SG-JND: Semantic-Guided Just Noticeable Distortion Predictor For Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04273
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于语义引导的JND预测器（SG-JND），能够在考虑到图像内容特征的基础上，对图像压缩过程中的人眼可觉察误差（JND）进行更加精确的预测。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04273v1 Announce Type: cross  Abstract: Just noticeable distortion (JND), representing the threshold of distortion in an image that is minimally perceptible to the human visual system (HVS), is crucial for image compression algorithms to achieve a trade-off between transmission bit rate and image quality. However, traditional JND prediction methods only rely on pixel-level or sub-band level features, lacking the ability to capture the impact of image content on JND. To bridge this gap, we propose a Semantic-Guided JND (SG-JND) network to leverage semantic information for JND prediction. In particular, SG-JND consists of three essential modules: the image preprocessing module extracts semantic-level patches from images, the feature extraction module extracts multi-layer features by utilizing the cross-scale attention layers, and the JND prediction module regresses the extracted features into the final JND value. Experimental results show that SG-JND achieves the state-of-the-
&lt;/p&gt;</description></item><item><title>该文章创新性地对比了现代3D场景重建方法中的NeRF与基于高斯的重建方法，并探索了这些方法与传统SLAM系统之间的差异，提出了在Replica和ScanNet等数据集上的性能评估标准。研究发现，NeRF虽然在生成新视角方面表现出色，但在处理速度上却较慢，而基于高斯的算法虽然处理速度快，但在场景完整性和实时性上有所欠缺。文章还介绍了NICE-SLAM和SplaTAM等新方法，这些方法不仅在鲁棒性上超过了ORB-SLAM2等旧方法，而且在动态复杂环境下也表现出了更好的性能。</title><link>https://arxiv.org/abs/2408.04268</link><description>&lt;p&gt;
Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs Gaussian-Based Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04268
&lt;/p&gt;
&lt;p&gt;
该文章创新性地对比了现代3D场景重建方法中的NeRF与基于高斯的重建方法，并探索了这些方法与传统SLAM系统之间的差异，提出了在Replica和ScanNet等数据集上的性能评估标准。研究发现，NeRF虽然在生成新视角方面表现出色，但在处理速度上却较慢，而基于高斯的算法虽然处理速度快，但在场景完整性和实时性上有所欠缺。文章还介绍了NICE-SLAM和SplaTAM等新方法，这些方法不仅在鲁棒性上超过了ORB-SLAM2等旧方法，而且在动态复杂环境下也表现出了更好的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04268v1 Announce Type: new  Abstract: Exploring the capabilities of Neural Radiance Fields (NeRF) and Gaussian-based methods in the context of 3D scene reconstruction, this study contrasts these modern approaches with traditional Simultaneous Localization and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we assess performance based on tracking accuracy, mapping fidelity, and view synthesis. Findings reveal that NeRF excels in view synthesis, offering unique capabilities in generating new perspectives from existing data, albeit at slower processing speeds. Conversely, Gaussian-based methods provide rapid processing and significant expressiveness but lack comprehensive scene completion. Enhanced by global optimization and loop closure techniques, newer methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as ORB-SLAM2 in terms of robustness but also demonstrate superior performance in dynamic and complex environments. This comparat
&lt;/p&gt;</description></item><item><title>该文章提出了CoBooM框架，这是针对医疗图像分析的自我监督学习方法的一个创新。CoBooM框架通过整合代码本与自我监督学习，有效利用了医疗图像中的内在解剖学相似性，从而在捕捉细致解剖学特征的同时，构建了强大而通用的特征表示。</title><link>https://arxiv.org/abs/2408.04262</link><description>&lt;p&gt;
CoBooM: Codebook Guided Bootstrapping for Medical Image Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04262
&lt;/p&gt;
&lt;p&gt;
该文章提出了CoBooM框架，这是针对医疗图像分析的自我监督学习方法的一个创新。CoBooM框架通过整合代码本与自我监督学习，有效利用了医疗图像中的内在解剖学相似性，从而在捕捉细致解剖学特征的同时，构建了强大而通用的特征表示。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04262v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as a promising paradigm for medical image analysis by harnessing unannotated data. Despite their potential, the existing SSL approaches overlook the high anatomical similarity inherent in medical images. This makes it challenging for SSL methods to capture diverse semantic content in medical images consistently. This work introduces a novel and generalized solution that implicitly exploits anatomical similarities by integrating codebooks in SSL. The codebook serves as a concise and informative dictionary of visual patterns, which not only aids in capturing nuanced anatomical details but also facilitates the creation of robust and generalized feature representations. In this context, we propose CoBooM, a novel framework for self-supervised medical image learning by integrating continuous and discrete representations. The continuous component ensures the preservation of fine-grained details, while
&lt;/p&gt;</description></item><item><title>该文章提出了一种针对对抗式视觉信息隐藏(AVIH)的攻击，揭示了隐藏在加密图像中的隐藏信息，并讨论了AVIH方法中使用的独特密钥模型在实际应用中的安全和效率问题。</title><link>https://arxiv.org/abs/2408.04261</link><description>&lt;p&gt;
Unveiling Hidden Visual Information: A Reconstruction Attack Against Adversarial Visual Information Hiding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04261
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种针对对抗式视觉信息隐藏(AVIH)的攻击，揭示了隐藏在加密图像中的隐藏信息，并讨论了AVIH方法中使用的独特密钥模型在实际应用中的安全和效率问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04261v1 Announce Type: new  Abstract: This paper investigates the security vulnerabilities of adversarial-example-based image encryption by executing data reconstruction (DR) attacks on encrypted images. A representative image encryption method is the adversarial visual information hiding (AVIH), which uses type-I adversarial example training to protect gallery datasets used in image recognition tasks. In the AVIH method, the type-I adversarial example approach creates images that appear completely different but are still recognized by machines as the original ones. Additionally, the AVIH method can restore encrypted images to their original forms using a predefined private key generative model. For the best security, assigning a unique key to each image is recommended; however, storage limitations may necessitate some images sharing the same key model. This raises a crucial security question for AVIH: How many images can safely share the same key model without being comprom
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为UHNet的超轻量级和高速度的边缘检测网络。它具有极低的参数数量、快速的计算速度和极低的预训练成本，同时还能保持出色的性能表现。通过创新的特征提取模块和优化残差连接方法，UHNet显著降低了模型复杂性和计算需求，为在资源受限设备上应用的医学图像处理提供了有效解决方案。</title><link>https://arxiv.org/abs/2408.04258</link><description>&lt;p&gt;
UHNet: An Ultra-Lightweight and High-Speed Edge Detection Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04258
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为UHNet的超轻量级和高速度的边缘检测网络。它具有极低的参数数量、快速的计算速度和极低的预训练成本，同时还能保持出色的性能表现。通过创新的特征提取模块和优化残差连接方法，UHNet显著降低了模型复杂性和计算需求，为在资源受限设备上应用的医学图像处理提供了有效解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04258v1 Announce Type: new  Abstract: Edge detection is crucial in medical image processing, enabling precise extraction of structural information to support lesion identification and image analysis. Traditional edge detection models typically rely on complex Convolutional Neural Networks and Vision Transformer architectures. Due to their numerous parameters and high computational demands, these models are limited in their application on resource-constrained devices. This paper presents an ultra-lightweight edge detection model (UHNet), characterized by its minimal parameter count, rapid computation speed, negligible of pre-training costs, and commendable performance. UHNet boasts impressive performance metrics with 42.3k parameters, 166 FPS, and 0.79G FLOPs. By employing an innovative feature extraction module and optimized residual connection method, UHNet significantly reduces model complexity and computational requirements. Additionally, a lightweight feature fusion stra
&lt;/p&gt;</description></item><item><title>该文章提出“InstantStyleGaussian”，一种基于3D Gaussian Splatting（3DGS）场景表示的快速艺术风格转移方法。通过输入目标风格图像，它能够迅速生成新的3DGS场景。该方法在预重建的3DGS场景之上运行，结合扩散模型和改进的迭代数据集更新策略。它使用扩散模型生成目标风格图像，将这些新图像添加到训练数据集中，并使用该数据集进行迭代更新和优化3DGS场景。大量实验结果表明，这种方法确保了高质量的风格化场景，同时在风格转移速度和一致性方面提供了显著优势。</title><link>https://arxiv.org/abs/2408.04249</link><description>&lt;p&gt;
InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04249
&lt;/p&gt;
&lt;p&gt;
该文章提出“InstantStyleGaussian”，一种基于3D Gaussian Splatting（3DGS）场景表示的快速艺术风格转移方法。通过输入目标风格图像，它能够迅速生成新的3DGS场景。该方法在预重建的3DGS场景之上运行，结合扩散模型和改进的迭代数据集更新策略。它使用扩散模型生成目标风格图像，将这些新图像添加到训练数据集中，并使用该数据集进行迭代更新和优化3DGS场景。大量实验结果表明，这种方法确保了高质量的风格化场景，同时在风格转移速度和一致性方面提供了显著优势。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04249v1 Announce Type: new  Abstract: We present InstantStyleGaussian, an innovative 3D style transfer method based on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target style image, it quickly generates new 3D GS scenes. Our approach operates on pre-reconstructed GS scenes, combining diffusion models with an improved iterative dataset update strategy. It utilizes diffusion models to generate target style images, adds these new images to the training dataset, and uses this dataset to iteratively update and optimize the GS scenes. Extensive experimental results demonstrate that our method ensures high-quality stylized scenes while offering significant advantages in style transfer speed and consistency.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为MU-MAE的算法，其结合了多模态 masked autoencoders 用于单例学习，通过同步的遮罩策略强化了传感器数据的特征捕捉，无需额外数据即可实现有效的自监督预训练，同时利用多模态遮罩 autoencoders 的表示作为提供的先验信息输入到跨注意力机制的多模态模型中，有效提升了人类活动识别的准确性。</title><link>https://arxiv.org/abs/2408.04243</link><description>&lt;p&gt;
MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04243
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为MU-MAE的算法，其结合了多模态 masked autoencoders 用于单例学习，通过同步的遮罩策略强化了传感器数据的特征捕捉，无需额外数据即可实现有效的自监督预训练，同时利用多模态遮罩 autoencoders 的表示作为提供的先验信息输入到跨注意力机制的多模态模型中，有效提升了人类活动识别的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04243v1 Announce Type: new  Abstract: With the exponential growth of multimedia data, leveraging multimodal sensors presents a promising approach for improving accuracy in human activity recognition. Nevertheless, accurately identifying these activities using both video data and wearable sensor data presents challenges due to the labor-intensive data annotation, and reliance on external pretrained models or additional data. To address these challenges, we introduce Multimodal Masked Autoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal masked autoencoder with a synchronized masking strategy tailored for wearable sensors. This masking strategy compels the networks to capture more meaningful spatiotemporal features, which enables effective self-supervised pretraining without the need for external data. Furthermore, Mu-MAE leverages the representation extracted from multimodal masked autoencoders as prior information input to a cross-attention multimodal
&lt;/p&gt;</description></item><item><title>该文章介绍了LLDif，一个专门为低光照面部表情识别问题设计的扩散模型框架。通过两种训练策略结合，高效地将低光照图像中的噪点转化为清晰的表情识别结果。</title><link>https://arxiv.org/abs/2408.04235</link><description>&lt;p&gt;
LLDif: Diffusion Models for Low-light Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04235
&lt;/p&gt;
&lt;p&gt;
该文章介绍了LLDif，一个专门为低光照面部表情识别问题设计的扩散模型框架。通过两种训练策略结合，高效地将低光照图像中的噪点转化为清晰的表情识别结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04235v1 Announce Type: new  Abstract: This paper introduces LLDif, a novel diffusion-based facial expression recognition (FER) framework tailored for extremely low-light (LL) environments. Images captured under such conditions often suffer from low brightness and significantly reduced contrast, presenting challenges to conventional methods. These challenges include poor image quality that can significantly reduce the accuracy of emotion recognition. LLDif addresses these issues with a novel two-stage training process that combines a Label-aware CLIP (LA-CLIP), an embedding prior network (PNET), and a transformer-based network adept at handling the noise of low-light images. The first stage involves LA-CLIP generating a joint embedding prior distribution (EPD) to guide the LLformer in label recovery. In the second stage, the diffusion model (DM) refines the EPD inference, ultilising the compactness of EPD for precise predictions. Experimental evaluations on various LL-FER dat
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于物理先验的协作学习框架，它能够联合估计大气湍流强度并恢复红外视频，通过模型的相互协作和物理先验知识，使得湍流强度的测量更加精确，同时提高了图像的恢复质量。</title><link>https://arxiv.org/abs/2408.04227</link><description>&lt;p&gt;
Physical prior guided cooperative learning framework for joint turbulence degradation estimation and infrared video restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04227
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于物理先验的协作学习框架，它能够联合估计大气湍流强度并恢复红外视频，通过模型的相互协作和物理先验知识，使得湍流强度的测量更加精确，同时提高了图像的恢复质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04227v1 Announce Type: cross  Abstract: Infrared imaging and turbulence strength measurements are in widespread demand in many fields. This paper introduces a Physical Prior Guided Cooperative Learning (P2GCL) framework to jointly enhance atmospheric turbulence strength estimation and infrared image restoration. P2GCL involves a cyclic collaboration between two models, i.e., a TMNet measures turbulence strength and outputs the refractive index structure constant (Cn2) as a physical prior, a TRNet conducts infrared image sequence restoration based on Cn2 and feeds the restored images back to the TMNet to boost the measurement accuracy. A novel Cn2-guided frequency loss function and a physical constraint loss are introduced to align the training process with physical theories. Experiments demonstrate P2GCL achieves the best performance for both turbulence strength estimation (improving Cn2 MAE by 0.0156, enhancing R2 by 0.1065) and image restoration (enhancing PSNR by 0.2775 d
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为GPG2A的模型，该模型通过预测地面图像的鸟瞰视图分割（BEV布局图）来生成真实的空中图像。GPG2A模型包括两个阶段，第一阶段从地面图像预测BEV分割，第二阶段使用文本描述和预测的BEV布局图合成空中图像。通过这种方式，GPG2A模型有效地解决了地面到空中图像合成中遇到的各种挑战，如视角变化、遮挡和视距范围等问题。</title><link>https://arxiv.org/abs/2408.04224</link><description>&lt;p&gt;
Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and Text Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04224
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为GPG2A的模型，该模型通过预测地面图像的鸟瞰视图分割（BEV布局图）来生成真实的空中图像。GPG2A模型包括两个阶段，第一阶段从地面图像预测BEV分割，第二阶段使用文本描述和预测的BEV布局图合成空中图像。通过这种方式，GPG2A模型有效地解决了地面到空中图像合成中遇到的各种挑战，如视角变化、遮挡和视距范围等问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04224v1 Announce Type: new  Abstract: Aerial imagery analysis is critical for many research fields. However, obtaining frequent high-quality aerial images is not always accessible due to its high effort and cost requirements. One solution is to use the Ground-to-Aerial (G2A) technique to synthesize aerial images from easily collectible ground images. However, G2A is rarely studied, because of its challenges, including but not limited to, the drastic view changes, occlusion, and range of visibility. In this paper, we present a novel Geometric Preserving Ground-to-Aerial (G2A) image synthesis (GPG2A) model that can generate realistic aerial images from ground images. GPG2A consists of two stages. The first stage predicts the Bird's Eye View (BEV) segmentation (referred to as the BEV layout map) from the ground image. The second stage synthesizes the aerial image from the predicted BEV layout map and text descriptions of the ground image. To train our model, we present a new mu
&lt;/p&gt;</description></item><item><title>该文章通过实证研究，探究了大型语言模型（LLMs）在视频处理中的表现，尤其是视频问答（VideoQA）任务。研究揭示了这些模型在处理视频内容和回答问题方面的优势，但也指出了其在处理视频中的时间性方面存在的不足。此外，文章还发现，即使在面对简单的干扰和问题的细微变化时，这些模型也未能表现出更为自然的行为。总的来说，这项研究为video-llms在视频理解和问答方面的表现提供了深入的见解，并为改进这些模型以更接近人类水平的能力提供了指导。</title><link>https://arxiv.org/abs/2408.04223</link><description>&lt;p&gt;
VideoQA in the Era of LLMs: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04223
&lt;/p&gt;
&lt;p&gt;
该文章通过实证研究，探究了大型语言模型（LLMs）在视频处理中的表现，尤其是视频问答（VideoQA）任务。研究揭示了这些模型在处理视频内容和回答问题方面的优势，但也指出了其在处理视频中的时间性方面存在的不足。此外，文章还发现，即使在面对简单的干扰和问题的细微变化时，这些模型也未能表现出更为自然的行为。总的来说，这项研究为video-llms在视频理解和问答方面的表现提供了深入的见解，并为改进这些模型以更接近人类水平的能力提供了指导。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04223v1 Announce Type: new  Abstract: Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks. As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing. This work conducts a timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering. Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments. Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also, they do not neces
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一个综合性的视角，分析了信号-噪声扩散模型（S2N），并通过信息论的理论连接了对噪声调度器的研究。作者开发了一个通用的逆向扩散方程，从而改进了当前扩散模型的扩散机制和推理过程。</title><link>https://arxiv.org/abs/2408.04221</link><description>&lt;p&gt;
Connective Viewpoints of Signal-to-Noise Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04221
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一个综合性的视角，分析了信号-噪声扩散模型（S2N），并通过信息论的理论连接了对噪声调度器的研究。作者开发了一个通用的逆向扩散方程，从而改进了当前扩散模型的扩散机制和推理过程。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04221v1 Announce Type: new  Abstract: Diffusion models (DM) have become fundamental components of generative models, excelling across various domains such as image creation, audio generation, and complex data interpolation. Signal-to-Noise diffusion models constitute a diverse family covering most state-of-the-art diffusion models. While there have been several attempts to study Signal-to-Noise (S2N) diffusion models from various perspectives, there remains a need for a comprehensive study connecting different viewpoints and exploring new perspectives. In this study, we offer a comprehensive perspective on noise schedulers, examining their role through the lens of the signal-to-noise ratio (SNR) and its connections to information theory. Building upon this framework, we have developed a generalized backward equation to enhance the performance of the inference process.
&lt;/p&gt;</description></item><item><title>该文章主要创新点在于评估了Segment Anything Model 2（SAM 2）在医学图像 segmentation 领域的性能，并与先前的 SAM 模型进行了比较。通过在不同影像模态（如MRI、CT、超声等）的多个人体组织结构的图像数据集中使用不同的提示策略，该研究展示了 SAM 2 在零-shot 条件下进行医学图像分割的能力相较于 SAM 更为优越。此外，研究还探索了 SAM 2 在视频分割方面的额外能力。此研究强调了两个模型的实际应用潜力，并为医学影像处理领域的开发者提供了重要参考。</title><link>https://arxiv.org/abs/2408.04212</link><description>&lt;p&gt;
Is SAM 2 Better than SAM in Medical Image Segmentation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04212
&lt;/p&gt;
&lt;p&gt;
该文章主要创新点在于评估了Segment Anything Model 2（SAM 2）在医学图像 segmentation 领域的性能，并与先前的 SAM 模型进行了比较。通过在不同影像模态（如MRI、CT、超声等）的多个人体组织结构的图像数据集中使用不同的提示策略，该研究展示了 SAM 2 在零-shot 条件下进行医学图像分割的能力相较于 SAM 更为优越。此外，研究还探索了 SAM 2 在视频分割方面的额外能力。此研究强调了两个模型的实际应用潜力，并为医学影像处理领域的开发者提供了重要参考。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04212v1 Announce Type: cross  Abstract: Segment Anything Model (SAM) demonstrated impressive performance in zero-shot promptable segmentation on natural images. The recently released Segment Anything Model 2 (SAM 2) model claims to have better performance than SAM on images while extending the model's capabilities to video segmentation. It is important to evaluate the recent model's ability in medical image segmentation in a zero-shot promptable manner. In this work, we performed extensive studies with multiple datasets from different imaging modalities to compare the performance between SAM and SAM 2. We used two point prompt strategies: (i) single positive prompt near the centroid of the target structure and (ii) additional positive prompts placed randomly within the target structure. The evaluation included 21 unique organ-modality combinations including abdominal structures, cardiac structures, and fetal head images acquired from publicly available MRI, CT, and Ultrasoun
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为MedGraphRAG的医学领域特定的大型语言模型生成框架，通过图检索增强生成方法，旨在提高处理敏感医疗信息的安全性和可靠性。该框架采用了一种结合静态和语义方法的分块技术，提高了上下文信息的捕获能力。提取的实体被用于创建一个三级层次的图结构，将实体与来自医学论文和词典的基础医学知识相连接。这些实体被相互连接形成元图，这些元图根据语义相似性合并为一个全球性的图结构，支持精确的信息检索和响应生成。检索过程使用了一种名为U-retrieve的方法，以实现更高的检索效率和准确性，并通过这种方法增强生成模型的知识表示能力，确保生成结果的可靠性和相关性。</title><link>https://arxiv.org/abs/2408.04187</link><description>&lt;p&gt;
Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04187
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为MedGraphRAG的医学领域特定的大型语言模型生成框架，通过图检索增强生成方法，旨在提高处理敏感医疗信息的安全性和可靠性。该框架采用了一种结合静态和语义方法的分块技术，提高了上下文信息的捕获能力。提取的实体被用于创建一个三级层次的图结构，将实体与来自医学论文和词典的基础医学知识相连接。这些实体被相互连接形成元图，这些元图根据语义相似性合并为一个全球性的图结构，支持精确的信息检索和响应生成。检索过程使用了一种名为U-retrieve的方法，以实现更高的检索效率和准确性，并通过这种方法增强生成模型的知识表示能力，确保生成结果的可靠性和相关性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04187v1 Announce Type: new  Abstract: We introduce a novel graph-based Retrieval-Augmented Generation (RAG) framework specifically designed for the medical domain, called \textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM) capabilities and generating evidence-based results, thereby improving safety and reliability when handling private medical data. Our comprehensive pipeline begins with a hybrid static-semantic approach to document chunking, significantly improving context capture over traditional methods. Extracted entities are used to create a three-tier hierarchical graph structure, linking entities to foundational medical knowledge sourced from medical papers and dictionaries. These entities are then interconnected to form meta-graphs, which are merged based on semantic similarities to develop a comprehensive global graph. This structure supports precise information retrieval and response generation. The retrieval process employs a U-retrieve method to b
&lt;/p&gt;</description></item><item><title>该文章介绍了pyBregMan——一个Python库，用于处理Bregman流形，旨在提供对信息科学中常见Bregman流形的通用操作实现。</title><link>https://arxiv.org/abs/2408.04175</link><description>&lt;p&gt;
pyBregMan: A Python library for Bregman Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04175
&lt;/p&gt;
&lt;p&gt;
该文章介绍了pyBregMan——一个Python库，用于处理Bregman流形，旨在提供对信息科学中常见Bregman流形的通用操作实现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04175v1 Announce Type: cross  Abstract: A Bregman manifold is a synonym for a dually flat space in information geometry which admits as a canonical divergence a Bregman divergence. Bregman manifolds are induced by smooth strictly convex functions like the cumulant or partition functions of regular exponential families, the negative entropy of mixture families, or the characteristic functions of regular cones just to list a few such convex Bregman generators. We describe the design of pyBregMan, a library which implements generic operations on Bregman manifolds and instantiate several common Bregman manifolds used in information sciences. At the core of the library is the notion of Legendre-Fenchel duality inducing a canonical pair of dual potential functions and dual Bregman divergences. The library also implements the Fisher-Rao manifolds of categorical/multinomial distributions and multivariate normal distributions. To demonstrate the use of the pyBregMan kernel manipulati
&lt;/p&gt;</description></item><item><title>该文章提出了MultiColor方法，这是一种新的学习型方法，旨在使用单个颜色模型对单色图像进行自动着色。通过结合多维颜色空间的信息，它利用了不同颜色空间互补的特点，为图像着色任务带来了益处。该方法包括多个专用的颜色空间着色组件，每个组件都由一个采用变换器解码器的模块组成，用于改进颜色查询嵌入并产生颜色图。</title><link>https://arxiv.org/abs/2408.04172</link><description>&lt;p&gt;
MultiColor: Image Colorization by Learning from Multiple Color Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04172
&lt;/p&gt;
&lt;p&gt;
该文章提出了MultiColor方法，这是一种新的学习型方法，旨在使用单个颜色模型对单色图像进行自动着色。通过结合多维颜色空间的信息，它利用了不同颜色空间互补的特点，为图像着色任务带来了益处。该方法包括多个专用的颜色空间着色组件，每个组件都由一个采用变换器解码器的模块组成，用于改进颜色查询嵌入并产生颜色图。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04172v1 Announce Type: new  Abstract: Deep networks have shown impressive performance in the image restoration tasks, such as image colorization. However, we find that previous approaches rely on the digital representation from single color model with a specific mapping function, a.k.a., color space, during the colorization pipeline. In this paper, we first investigate the modeling of different color spaces, and find each of them exhibiting distinctive characteristics with unique distribution of colors. The complementarity among multiple color spaces leads to benefits for the image colorization task.   We present MultiColor, a new learning-based approach to automatically colorize grayscale images that combines clues from multiple color spaces. Specifically, we employ a set of dedicated colorization modules for individual color space. Within each module, a transformer decoder is first employed to refine color query embeddings and then a color mapper produces color channel pre
&lt;/p&gt;</description></item><item><title>该文章提出一种基于几何关系的旋转中心识别方法，为非盲式旋转运动去模糊提供了精准的旋转中心，改进了现有方法的性能，并通过实验验证了其在实际旋转运动模糊图像中的有效性。</title><link>https://arxiv.org/abs/2408.04171</link><description>&lt;p&gt;
Rotation center identification based on geometric relationships for rotary motion deblurring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04171
&lt;/p&gt;
&lt;p&gt;
该文章提出一种基于几何关系的旋转中心识别方法，为非盲式旋转运动去模糊提供了精准的旋转中心，改进了现有方法的性能，并通过实验验证了其在实际旋转运动模糊图像中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04171v1 Announce Type: new  Abstract: Non-blind rotary motion deblurring (RMD) aims to recover the latent clear image from a rotary motion blurred (RMB) image. The rotation center is a crucial input parameter in non-blind RMD methods. Existing methods directly estimate the rotation center from the RMB image. However they always suffer significant errors, and the performance of RMD is limited. For the assembled imaging systems, the position of the rotation center remains fixed. Leveraging this prior knowledge, we propose a geometric-based method for rotation center identification and analyze its error range. Furthermore, we construct a RMB imaging system. The experiment demonstrates that our method achieves less than 1-pixel error along a single axis (x-axis or y-axis). We utilize the constructed imaging system to capture real RMB images, and experimental results show that our method can help existing RMD approaches yield better RMD images.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为M2EF-NNs的神经网络模型，用于多模态多实例证据融合的癌症生存预测，通过利用预训练的Vision Transformer模型捕获图像全局信息，并引入了一种多模态注意力模块来学习基因组信息与图像信息之间的交互作用，从而提高了预测的准确性。</title><link>https://arxiv.org/abs/2408.04170</link><description>&lt;p&gt;
M2EF-NNs: Multimodal Multi-instance Evidence Fusion Neural Networks for Cancer Survival Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04170
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为M2EF-NNs的神经网络模型，用于多模态多实例证据融合的癌症生存预测，通过利用预训练的Vision Transformer模型捕获图像全局信息，并引入了一种多模态注意力模块来学习基因组信息与图像信息之间的交互作用，从而提高了预测的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04170v1 Announce Type: new  Abstract: Accurate cancer survival prediction is crucial for assisting clinical doctors in formulating treatment plans. Multimodal data, including histopathological images and genomic data, offer complementary and comprehensive information that can greatly enhance the accuracy of this task. However, the current methods, despite yielding promising results, suffer from two notable limitations: they do not effectively utilize global context and disregard modal uncertainty. In this study, we put forward a neural network model called M2EF-NNs, which leverages multimodal and multi-instance evidence fusion techniques for accurate cancer survival prediction. Specifically, to capture global information in the images, we use a pre-trained Vision Transformer (ViT) model to obtain patch feature embeddings of histopathological images. Then, we introduce a multimodal attention module that uses genomic embeddings as queries and learns the co-attention mapping be
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Entropy Attention and Receptive Field Augmentation (EARFA)的模型，旨在解决传统的Transformer-based SISR模型在计算效率和SR性能之间的矛盾。EARFA模型中采用了新颖的Entropy Attention（EA）和Shifted Large Kernel Attention（SLKA）技术，通过增加中间特征的熵来实现更高效的计算，并扩展了模型的感受野。这种方法在保持模型效率的同时，提高了超分辨率任务的性能。</title><link>https://arxiv.org/abs/2408.04158</link><description>&lt;p&gt;
Efficient Single Image Super-Resolution with Entropy Attention and Receptive Field Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04158
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Entropy Attention and Receptive Field Augmentation (EARFA)的模型，旨在解决传统的Transformer-based SISR模型在计算效率和SR性能之间的矛盾。EARFA模型中采用了新颖的Entropy Attention（EA）和Shifted Large Kernel Attention（SLKA）技术，通过增加中间特征的熵来实现更高效的计算，并扩展了模型的感受野。这种方法在保持模型效率的同时，提高了超分辨率任务的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04158v1 Announce Type: cross  Abstract: Transformer-based deep models for single image super-resolution (SISR) have greatly improved the performance of lightweight SISR tasks in recent years. However, they often suffer from heavy computational burden and slow inference due to the complex calculation of multi-head self-attention (MSA), seriously hindering their practical application and deployment. In this work, we present an efficient SR model to mitigate the dilemma between model efficiency and SR performance, which is dubbed Entropy Attention and Receptive Field Augmentation network (EARFA), and composed of a novel entropy attention (EA) and a shifting large kernel attention (SLKA). From the perspective of information theory, EA increases the entropy of intermediate features conditioned on a Gaussian distribution, providing more informative input for subsequent reasoning. On the other hand, SLKA extends the receptive field of SR models with the assistance of channel shifti
&lt;/p&gt;</description></item><item><title>该文章提出了一种轻量级、无需额外损失函数且对架构不敏感的集成学习方法，通过使用能够正则化模型结构的结构相关的适配器在半监督学习中实现有效的集成学习。</title><link>https://arxiv.org/abs/2408.04150</link><description>&lt;p&gt;
Decorrelating Structure via Adapters Makes Ensemble Learning Practical for Semi-supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04150
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种轻量级、无需额外损失函数且对架构不敏感的集成学习方法，通过使用能够正则化模型结构的结构相关的适配器在半监督学习中实现有效的集成学习。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04150v1 Announce Type: new  Abstract: In computer vision, traditional ensemble learning methods exhibit either a low training efficiency or the limited performance to enhance the reliability of deep neural networks. In this paper, we propose a lightweight, loss-function-free, and architecture-agnostic ensemble learning by the Decorrelating Structure via Adapters (DSA) for various visual tasks. Concretely, the proposed DSA leverages the structure-diverse adapters to decorrelate multiple prediction heads without any tailed regularization or loss. This allows DSA to be easily extensible to architecture-agnostic networks for a range of computer vision tasks. Importantly, the theoretically analysis shows that the proposed DSA has a lower bias and variance than that of the single head based method (which is adopted by most of the state of art approaches). Consequently, the DSA makes deep networks reliable and robust for the various real-world challenges, \textit{e.g.}, data corrup
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为ComKD-CLIP的综合知识蒸馏方法，用于从较大的教师CLIP模型迁移知识到较小的学生模型，通过图像特征对齐（IFAlign）和教育注意力（EduAttention）机制实现这一目标，有效地减小了模型大小并保持了较高的性能。</title><link>https://arxiv.org/abs/2408.04145</link><description>&lt;p&gt;
ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive Language-Image Pre-traning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04145
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为ComKD-CLIP的综合知识蒸馏方法，用于从较大的教师CLIP模型迁移知识到较小的学生模型，通过图像特征对齐（IFAlign）和教育注意力（EduAttention）机制实现这一目标，有效地减小了模型大小并保持了较高的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04145v1 Announce Type: new  Abstract: Contrastive Language-Image Pre-training (CLIP) excels in integrating semantic information between images and text through contrastive learning techniques. It has achieved remarkable performance in various multimodal tasks. However, the deployment of large CLIP models is hindered in resource-limited environments, while smaller models frequently fall short of meeting performance benchmarks necessary for practical applications. In this paper, we propose a novel approach, coined as ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive Language-Image Pre-traning Model, which aims to comprehensively distill the knowledge from a large teacher CLIP model into a smaller student model, ensuring comparable performance with significantly reduced parameters. ComKD-CLIP is composed of two key mechanisms: Image Feature Alignment (IFAlign) and Educational Attention (EduAttention). IFAlign makes the image features extracted by the student mode
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为InPhea的模型，该模型将生物节律特征整合到遥感图像变化检测的框架中，通过一个包含差异性注意力模块的检测器来提高变化信息的特征表示，并采用高分辨率特征提取和空间金字塔块来增强性能。此外，模型还包括一个由四个约束模块和一个多阶段对比学习方法组成的约束器，以帮助模型理解生物节律特性。实验验证了该模型在HRSCD、SECD和PSCD-Wuhan数据集上的有效性。</title><link>https://arxiv.org/abs/2408.04144</link><description>&lt;p&gt;
Integrated Dynamic Phenological Feature for Remote Sensing Image Land Cover Change Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04144
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为InPhea的模型，该模型将生物节律特征整合到遥感图像变化检测的框架中，通过一个包含差异性注意力模块的检测器来提高变化信息的特征表示，并采用高分辨率特征提取和空间金字塔块来增强性能。此外，模型还包括一个由四个约束模块和一个多阶段对比学习方法组成的约束器，以帮助模型理解生物节律特性。实验验证了该模型在HRSCD、SECD和PSCD-Wuhan数据集上的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04144v1 Announce Type: new  Abstract: Remote sensing image change detection (CD) is essential for analyzing land surface changes over time, with a significant challenge being the differentiation of actual changes from complex scenes while filtering out pseudo-changes. A primary contributor to this challenge is the intra-class dynamic changes due to phenological characteristics in natural areas. To overcome this, we introduce the InPhea model, which integrates phenological features into a remote sensing image CD framework. The model features a detector with a differential attention module for improved feature representation of change information, coupled with high-resolution feature extraction and spatial pyramid blocks to enhance performance. Additionally, a constrainer with four constraint modules and a multi-stage contrastive learning approach is employed to aid in the model's understanding of phenological characteristics. Experiments on the HRSCD, SECD, and PSCD-Wuhan dat
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RadPrompt的策略，结合了规则系统和多轮提示策略，使用一个基于不确定性的信息模式优化了规则，显著提高了大型语言模型在放射学报告分类中的零样本预测性能。</title><link>https://arxiv.org/abs/2408.04121</link><description>&lt;p&gt;
Can Rule-Based Insights Enhance LLMs for Radiology Report Classification? Introducing the RadPrompt Methodology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04121
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RadPrompt的策略，结合了规则系统和多轮提示策略，使用一个基于不确定性的信息模式优化了规则，显著提高了大型语言模型在放射学报告分类中的零样本预测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04121v1 Announce Type: cross  Abstract: Developing imaging models capable of detecting pathologies from chest X-rays can be cost and time-prohibitive for large datasets as it requires supervision to attain state-of-the-art performance. Instead, labels extracted from radiology reports may serve as distant supervision since these are routinely generated as part of clinical practice. Despite their widespread use, current rule-based methods for label extraction rely on extensive rule sets that are limited in their robustness to syntactic variability. To alleviate these limitations, we introduce RadPert, a rule-based system that integrates an uncertainty-aware information schema with a streamlined set of rules, enhancing performance. Additionally, we have developed RadPrompt, a multi-turn prompting strategy that leverages RadPert to bolster the zero-shot predictive capabilities of large language models, achieving a statistically significant improvement in weighted average F1 scor
&lt;/p&gt;</description></item><item><title>该文章提出了首个针对路面状况评估的多模态框架——PaveCap，使用密集描述和PCI估计技术，通过YOLOv8、SAM模型和卷积神经网络进行量化PCI预测，同时采用YOLOv8、Transformer和卷积模块进行定性描述。通过在带有边界框和文本注释的路面数据集上训练和评估这些网络，研究成果显示了PaveCap在路面状况自动评估中的有效性。</title><link>https://arxiv.org/abs/2408.04110</link><description>&lt;p&gt;
PaveCap: The First Multimodal Framework for Comprehensive Pavement Condition Assessment with Dense Captioning and PCI Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04110
&lt;/p&gt;
&lt;p&gt;
该文章提出了首个针对路面状况评估的多模态框架——PaveCap，使用密集描述和PCI估计技术，通过YOLOv8、SAM模型和卷积神经网络进行量化PCI预测，同时采用YOLOv8、Transformer和卷积模块进行定性描述。通过在带有边界框和文本注释的路面数据集上训练和评估这些网络，研究成果显示了PaveCap在路面状况自动评估中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04110v1 Announce Type: new  Abstract: This research introduces the first multimodal approach for pavement condition assessment, providing both quantitative Pavement Condition Index (PCI) predictions and qualitative descriptions. We introduce PaveCap, a novel framework for automated pavement condition assessment. The framework consists of two main parts: a Single-Shot PCI Estimation Network and a Dense Captioning Network. The PCI Estimation Network uses YOLOv8 for object detection, the Segment Anything Model (SAM) for zero-shot segmentation, and a four-layer convolutional neural network to predict PCI. The Dense Captioning Network uses a YOLOv8 backbone, a Transformer encoder-decoder architecture, and a convolutional feed-forward module to generate detailed descriptions of pavement conditions. To train and evaluate these networks, we developed a pavement dataset with bounding box annotations, textual annotations, and PCI values. The results of our PCI Estimation Network showe
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了将意识形态分歧纳入视觉情感分析框架的方法，通过整合不同政党的视觉情感差异，训练了一个多任务多类深度学习模型，以预测从多个政治观点出发的图像情感。这种方法有效地捕捉了移民相关图像在不同政党观点下的情感反应，并通过将多样性观点纳入标注和模型训练过程中，解决了标签模糊的问题，并提高了视觉情感预测的准确性。</title><link>https://arxiv.org/abs/2408.04103</link><description>&lt;p&gt;
Decoding Visual Sentiment of Political Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04103
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了将意识形态分歧纳入视觉情感分析框架的方法，通过整合不同政党的视觉情感差异，训练了一个多任务多类深度学习模型，以预测从多个政治观点出发的图像情感。这种方法有效地捕捉了移民相关图像在不同政党观点下的情感反应，并通过将多样性观点纳入标注和模型训练过程中，解决了标签模糊的问题，并提高了视觉情感预测的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04103v1 Announce Type: new  Abstract: How can we define visual sentiment when viewers systematically disagree on their perspectives? This study introduces a novel approach to visual sentiment analysis by integrating attitudinal differences into visual sentiment classification. Recognizing that societal divides, such as partisan differences, heavily influence sentiment labeling, we developed a dataset that reflects these divides. We then trained a deep learning multi-task multi-class model to predict visual sentiment from different ideological viewpoints. Applied to immigration-related images, our approach captures perspectives from both Democrats and Republicans. By incorporating diverse perspectives into the labeling and model training process, our strategy addresses the limitation of label ambiguity and demonstrates improved accuracy in visual sentiment predictions. Overall, our study advocates for a paradigm shift in decoding visual sentiment toward creating classifiers t
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为ArtVLM的方法，通过采用大型预训练的视觉语言模型（VLM），实现了在零样本条件下对视觉属性的识别。这种方法通过改造一种条件概率图模型，将识别任务转化为依赖性敏感的语言建模问题，有效解决了之前在视觉语言模型中未能捕捉到的对象属性关系问题。</title><link>https://arxiv.org/abs/2408.04102</link><description>&lt;p&gt;
ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04102
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为ArtVLM的方法，通过采用大型预训练的视觉语言模型（VLM），实现了在零样本条件下对视觉属性的识别。这种方法通过改造一种条件概率图模型，将识别任务转化为依赖性敏感的语言建模问题，有效解决了之前在视觉语言模型中未能捕捉到的对象属性关系问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04102v1 Announce Type: new  Abstract: Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications. While large vision language representations like CLIP had largely resolved the task of zero-shot object recognition, zero-shot visual attribute recognition remains a challenge because CLIP's contrastively-learned vision-language representation cannot effectively capture object-attribute dependencies. In this paper, we target this weakness and propose a sentence generation-based retrieval formulation for attribute recognition that is novel in 1) explicitly modeling a to-be-measured and retrieved object-attribute relation as a conditional probability graph, which converts the recognition problem into a dependency-sensitive language-modeling problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this reformulation and naturally distilling its knowledge of image-object-attribute relations to use towards attri
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于3D CycleGAN的全新框架，能够将不适合人体但能够提供高分辨率细胞细节的活体OCT图像转换成适合人体但成像效果受限的死体confocal microscopy图像。这一创新解决了以往活体OCT图像清晰度和移动伪影问题，以及在研治疗方法中关于图像分辨率的需求。通过创建包含小鼠OCT和confocal retinal图像的独特数据集OCT2Confocal，该工作为跨模态图像转换领域的研究提供了新基准，标志着在早期发现和诊断视网膜疾病领域的一个重大进步。</title><link>https://arxiv.org/abs/2408.04091</link><description>&lt;p&gt;
The Quest for Early Detection of Retinal Disease: 3D CycleGAN-based Translation of Optical Coherence Tomography into Confocal Microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04091
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于3D CycleGAN的全新框架，能够将不适合人体但能够提供高分辨率细胞细节的活体OCT图像转换成适合人体但成像效果受限的死体confocal microscopy图像。这一创新解决了以往活体OCT图像清晰度和移动伪影问题，以及在研治疗方法中关于图像分辨率的需求。通过创建包含小鼠OCT和confocal retinal图像的独特数据集OCT2Confocal，该工作为跨模态图像转换领域的研究提供了新基准，标志着在早期发现和诊断视网膜疾病领域的一个重大进步。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04091v1 Announce Type: cross  Abstract: Optical coherence tomography (OCT) and confocal microscopy are pivotal in retinal imaging, offering distinct advantages and limitations. In vivo OCT offers rapid, non-invasive imaging but can suffer from clarity issues and motion artifacts, while ex vivo confocal microscopy, providing high-resolution, cellular-detailed color images, is invasive and raises ethical concerns. To bridge the benefits of both modalities, we propose a novel framework based on unsupervised 3D CycleGAN for translating unpaired in vivo OCT to ex vivo confocal microscopy images. This marks the first attempt to exploit the inherent 3D information of OCT and translate it into the rich, detailed color domain of confocal microscopy. We also introduce a unique dataset, OCT2Confocal, comprising mouse OCT and confocal retinal images, facilitating the development of and establishing a benchmark for cross-modal image translation research. Our model has been evaluated both
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为PushPull-Conv的新型计算单元，用于改进ResNet架构的第一层。这种单元通过在激发和抑制方面学习，提高了网络对图像 corruption的鲁棒性。</title><link>https://arxiv.org/abs/2408.04077</link><description>&lt;p&gt;
PushPull-Net: Inhibition-driven ResNet robust to image corruptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04077
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为PushPull-Conv的新型计算单元，用于改进ResNet架构的第一层。这种单元通过在激发和抑制方面学习，提高了网络对图像 corruption的鲁棒性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04077v1 Announce Type: new  Abstract: We introduce a novel computational unit, termed PushPull-Conv, in the first layer of a ResNet architecture, inspired by the anti-phase inhibition phenomenon observed in the primary visual cortex. This unit redefines the traditional convolutional layer by implementing a pair of complementary filters: a trainable push kernel and its counterpart, the pull kernel. The push kernel (analogous to traditional convolution) learns to respond to specific stimuli, while the pull kernel reacts to the same stimuli but of opposite contrast. This configuration enhances stimulus selectivity and effectively inhibits response in regions lacking preferred stimuli. This effect is attributed to the push and pull kernels, which produce responses of comparable magnitude in such regions, thereby neutralizing each other. The incorporation of the PushPull-Conv into ResNets significantly increases their robustness to image corruption. Our experiments with benchmark
&lt;/p&gt;</description></item><item><title>该文章提出了一种多尺度结构复杂性度量方法，该方法通过对图像在不同层次上的差异性进行量化来评估视觉复杂性。应用此方法到一组具有人类评定复杂性的开放图像数据集上，结果显示该方法在预测主观复杂性方面与现有方法相当，且因其定义直观、跨类别一致并易于计算而具有优势。研究还探讨了客观视觉复杂性与主观认知之间的差异，并指出多尺度特性使该方法在进一步分析视觉复杂性的各个方面方面具有潜力。</title><link>https://arxiv.org/abs/2408.04076</link><description>&lt;p&gt;
Multi-scale structural complexity as a quantitative measure of visual complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04076
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种多尺度结构复杂性度量方法，该方法通过对图像在不同层次上的差异性进行量化来评估视觉复杂性。应用此方法到一组具有人类评定复杂性的开放图像数据集上，结果显示该方法在预测主观复杂性方面与现有方法相当，且因其定义直观、跨类别一致并易于计算而具有优势。研究还探讨了客观视觉复杂性与主观认知之间的差异，并指出多尺度特性使该方法在进一步分析视觉复杂性的各个方面方面具有潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04076v1 Announce Type: cross  Abstract: While intuitive for humans, the concept of visual complexity is hard to define and quantify formally. We suggest adopting the multi-scale structural complexity (MSSC) measure, an approach that defines structural complexity of an object as the amount of dissimilarities between distinct scales in its hierarchical organization. In this work, we apply MSSC to the case of visual stimuli, using an open dataset of images with subjective complexity scores obtained from human participants (SAVOIAS). We demonstrate that MSSC correlates with subjective complexity on par with other computational complexity measures, while being more intuitive by definition, consistent across categories of images, and easier to compute. We discuss objective and subjective elements inherently present in human perception of complexity and the domains where the two are more likely to diverge. We show how the multi-scale nature of MSSC allows further investigation of c
&lt;/p&gt;</description></item><item><title>该文章提出AEye工具，它是一个用于图像数据集的可视化工具，能够通过对比学习模型生成图像的高维语义表示，并投射到二维平面进行交互式浏览，用户可通过多种方式进行搜索和探索，具有重要的创新和贡献。</title><link>https://arxiv.org/abs/2408.04072</link><description>&lt;p&gt;
AEye: A Visualization Tool for Image Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04072
&lt;/p&gt;
&lt;p&gt;
该文章提出AEye工具，它是一个用于图像数据集的可视化工具，能够通过对比学习模型生成图像的高维语义表示，并投射到二维平面进行交互式浏览，用户可通过多种方式进行搜索和探索，具有重要的创新和贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04072v1 Announce Type: new  Abstract: Image datasets serve as the foundation for machine learning models in computer vision, significantly influencing model capabilities, performance, and biases alongside architectural considerations. Therefore, understanding the composition and distribution of these datasets has become increasingly crucial. To address the need for intuitive exploration of these datasets, we propose AEye, an extensible and scalable visualization tool tailored to image datasets. AEye utilizes a contrastively trained model to embed images into semantically meaningful high-dimensional representations, facilitating data clustering and organization. To visualize the high-dimensional representations, we project them onto a two-dimensional plane and arrange images in layers so users can seamlessly navigate and explore them interactively. AEye facilitates semantic search functionalities for both text and image queries, enabling users to search for content. We open-s
&lt;/p&gt;</description></item><item><title>该文章探索了通过优化损失函数景观的尖锐度来提升医疗图像分析中深度学习模型的泛化性能的可能性。通过比较不同类型的尖锐性校正优化器对在健康医疗应用中部署的深度学习模型泛化能力的潜在影响，研究揭示了尖锐性校正优化器在提高模型在图像数据集中的性能方面可能取得的改进。该研究成果对于优化医疗图像分析中深度学习模型的泛化能力具有重要意义。</title><link>https://arxiv.org/abs/2408.04065</link><description>&lt;p&gt;
Do Sharpness-based Optimizers Improve Generalization in Medical Image Analysis?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04065
&lt;/p&gt;
&lt;p&gt;
该文章探索了通过优化损失函数景观的尖锐度来提升医疗图像分析中深度学习模型的泛化性能的可能性。通过比较不同类型的尖锐性校正优化器对在健康医疗应用中部署的深度学习模型泛化能力的潜在影响，研究揭示了尖锐性校正优化器在提高模型在图像数据集中的性能方面可能取得的改进。该研究成果对于优化医疗图像分析中深度学习模型的泛化能力具有重要意义。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04065v1 Announce Type: cross  Abstract: Effective clinical deployment of deep learning models in healthcare demands high generalization performance to ensure accurate diagnosis and treatment planning. In recent years, significant research has focused on improving the generalization of deep learning models by regularizing the sharpness of the loss landscape. Among the optimization approaches that explicitly minimize sharpness, Sharpness-Aware Minimization (SAM) has shown potential in enhancing generalization performance on general domain image datasets. This success has led to the development of several advanced sharpness-based algorithms aimed at addressing the limitations of SAM, such as Adaptive SAM, surrogate-Gap SAM, Weighted SAM, and Curvature Regularized SAM. These sharpness-based optimizers have shown improvements in model generalization compared to conventional stochastic gradient descent optimizers and their variants on general domain image datasets, but they have n
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的任务——在3D场景中进行任务导向的顺序定位，其中代理必须遵循详细的逐步指示，在室内场景中找到一系列目标对象以完成日常活动。文章还介绍了SG3D，一个包含22346个任务、112236个步骤的大型数据集，这些任务分布在4895个真实世界3D场景中。该数据集通过结合来自各种3D场景的数据集的RGB-D扫描和自动化任务生成管道以及人工验证，为3D视觉定位的真实和动态需求提供了进一步研究的基础。</title><link>https://arxiv.org/abs/2408.04034</link><description>&lt;p&gt;
Task-oriented Sequential Grounding in 3D Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04034
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的任务——在3D场景中进行任务导向的顺序定位，其中代理必须遵循详细的逐步指示，在室内场景中找到一系列目标对象以完成日常活动。文章还介绍了SG3D，一个包含22346个任务、112236个步骤的大型数据集，这些任务分布在4895个真实世界3D场景中。该数据集通过结合来自各种3D场景的数据集的RGB-D扫描和自动化任务生成管道以及人工验证，为3D视觉定位的真实和动态需求提供了进一步研究的基础。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04034v1 Announce Type: new  Abstract: Grounding natural language in physical 3D environments is essential for the advancement of embodied artificial intelligence. Current datasets and models for 3D visual grounding predominantly focus on identifying and localizing objects from static, object-centric descriptions. These approaches do not adequately address the dynamic and sequential nature of task-oriented grounding necessary for practical applications. In this work, we propose a new task: Task-oriented Sequential Grounding in 3D scenes, wherein an agent must follow detailed step-by-step instructions to complete daily activities by locating a sequence of target objects in indoor scenes. To facilitate this task, we introduce SG3D, a large-scale dataset containing 22,346 tasks with 112,236 steps across 4,895 real-world 3D scenes. The dataset is constructed using a combination of RGB-D scans from various 3D scene datasets and an automated task generation pipeline, followed by hu
&lt;/p&gt;</description></item><item><title>该文章开发了一种基于Swin Transformer和Low-Rank Adaptation的图像到LaTeX转换器，能够将数学公式和文本的图像转换为LaTeX代码，并在现有模型基础上提高了准确性和实用性。</title><link>https://arxiv.org/abs/2408.04015</link><description>&lt;p&gt;
Image-to-LaTeX Converter for Mathematical Formulas and Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.04015
&lt;/p&gt;
&lt;p&gt;
该文章开发了一种基于Swin Transformer和Low-Rank Adaptation的图像到LaTeX转换器，能够将数学公式和文本的图像转换为LaTeX代码，并在现有模型基础上提高了准确性和实用性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.04015v1 Announce Type: cross  Abstract: In this project, we train a vision encoder-decoder model to generate LaTeX code from images of mathematical formulas and text. Utilizing a diverse collection of image-to-LaTeX data, we build two models: a base model with a Swin Transformer encoder and a GPT-2 decoder, trained on machine-generated images, and a fine-tuned version enhanced with Low-Rank Adaptation (LoRA) trained on handwritten formulas. We then compare the BLEU performance of our specialized model on a handwritten test set with other similar models, such as Pix2Text, TexTeller, and Sumen. Through this project, we contribute open-source models for converting images to LaTeX and provide from-scratch code for building these models with distributed training and GPU optimizations.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为HiRISE的高分辨率图像缩放系统，用于边缘机器学习，通过模拟传感器内的图像缩放以及选择性区域关注（ROI）功能，该系统能够显著降低内存需求，实现高达17.7倍的传输数据量和能量的减少。</title><link>https://arxiv.org/abs/2408.03956</link><description>&lt;p&gt;
HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03956
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为HiRISE的高分辨率图像缩放系统，用于边缘机器学习，通过模拟传感器内的图像缩放以及选择性区域关注（ROI）功能，该系统能够显著降低内存需求，实现高达17.7倍的传输数据量和能量的减少。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03956v1 Announce Type: new  Abstract: With the rise of tiny IoT devices powered by machine learning (ML), many researchers have directed their focus toward compressing models to fit on tiny edge devices. Recent works have achieved remarkable success in compressing ML models for object detection and image classification on microcontrollers with small memory, e.g., 512kB SRAM. However, there remain many challenges prohibiting the deployment of ML systems that require high-resolution images. Due to fundamental limits in memory capacity for tiny IoT devices, it may be physically impossible to store large images without external hardware. To this end, we propose a high-resolution image scaling system for edge ML, called HiRISE, which is equipped with selective region-of-interest (ROI) capability leveraging analog in-sensor image scaling. Our methodology not only significantly reduces the peak memory requirements, but also achieves up to 17.7x reduction in data transfer and energy
&lt;/p&gt;</description></item><item><title>该文章提出了一种利用基于大规模未标注病理图像预训练的基准模型特征聚合的方法，用于预测患者对癌症治疗的响应，该方法通过自监督学习在大型数据集上训练，旨在提高癌症诊断相关任务的效果。</title><link>https://arxiv.org/abs/2408.03954</link><description>&lt;p&gt;
Histopathology image embedding based on foundation models features aggregation for patient treatment response prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03954
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种利用基于大规模未标注病理图像预训练的基准模型特征聚合的方法，用于预测患者对癌症治疗的响应，该方法通过自监督学习在大型数据集上训练，旨在提高癌症诊断相关任务的效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03954v1 Announce Type: new  Abstract: Predicting the response of a patient to a cancer treatment is of high interest. Nonetheless, this task is still challenging from a medical point of view due to the complexity of the interaction between the patient organism and the considered treatment. Recent works on foundation models pre-trained with self-supervised learning on large-scale unlabeled histopathology datasets have opened a new direction towards the development of new methods for cancer diagnosis related tasks. In this article, we propose a novel methodology for predicting Diffuse Large B-Cell Lymphoma patients treatment response from Whole Slide Images. Our method exploits several foundation models as feature extractors to obtain a local representation of the image corresponding to a small region of the tissue, then, a global representation of the image is obtained by aggregating these local representations using attention-based Multiple Instance Learning. Our experimenta
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为“税范畴驱动的快速对抗训练”（TDAT）的方法，该方法在优化学习目标、损失函数和初始化方法的同时，能够提升神经网络对抗训练的鲁棒性。</title><link>https://arxiv.org/abs/2408.03944</link><description>&lt;p&gt;
Taxonomy Driven Fast Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03944
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为“税范畴驱动的快速对抗训练”（TDAT）的方法，该方法在优化学习目标、损失函数和初始化方法的同时，能够提升神经网络对抗训练的鲁棒性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03944v1 Announce Type: new  Abstract: Adversarial training (AT) is an effective defense method against gradient-based attacks to enhance the robustness of neural networks. Among them, single-step AT has emerged as a hotspot topic due to its simplicity and efficiency, requiring only one gradient propagation in generating adversarial examples. Nonetheless, the problem of catastrophic overfitting (CO) that causes training collapse remains poorly understood, and there exists a gap between the robust accuracy achieved through single- and multi-step AT. In this paper, we present a surprising finding that the taxonomy of adversarial examples reveals the truth of CO. Based on this conclusion, we propose taxonomy driven fast adversarial training (TDAT) which jointly optimizes learning objective, loss function, and initialization method, thereby can be regarded as a new paradigm of single-step AT. Compared with other fast AT methods, TDAT can boost the robustness of neural networks, a
&lt;/p&gt;</description></item><item><title>该文章介绍了GMAI-MMBench：一个旨在对一般医疗人工智能进行综合的多模态评估基准。这是一个全面的多模态评估基准，旨在评估能够处理包括医学图像、文本和生理信号等多类型数据的通用医疗人工智能模型。</title><link>https://arxiv.org/abs/2408.03361</link><description>&lt;p&gt;
GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03361
&lt;/p&gt;
&lt;p&gt;
该文章介绍了GMAI-MMBench：一个旨在对一般医疗人工智能进行综合的多模态评估基准。这是一个全面的多模态评估基准，旨在评估能够处理包括医学图像、文本和生理信号等多类型数据的通用医疗人工智能模型。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03361v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 285 datasets across 39 medical image mod
&lt;/p&gt;</description></item><item><title>该文章通过结合背景信息并用对比学习的方法重新审视通道注意机制，解决了夜间行人检测中的低光性能问题，提出了FBCA（Fore-Background Contrast Attention）模型，该模型通过利用区域背景和行人对象的区别来提高检测性能。</title><link>https://arxiv.org/abs/2408.03030</link><description>&lt;p&gt;
Nighttime Pedestrian Detection Based on Fore-Background Contrast Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03030
&lt;/p&gt;
&lt;p&gt;
该文章通过结合背景信息并用对比学习的方法重新审视通道注意机制，解决了夜间行人检测中的低光性能问题，提出了FBCA（Fore-Background Contrast Attention）模型，该模型通过利用区域背景和行人对象的区别来提高检测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03030v1 Announce Type: new  Abstract: The significance of background information is frequently overlooked in contemporary research concerning channel attention mechanisms. This study addresses the issue of suboptimal single-spectral nighttime pedestrian detection performance under low-light conditions by incorporating background information into the channel attention mechanism. Despite numerous studies focusing on the development of efficient channel attention mechanisms, the relevance of background information has been largely disregarded. By adopting a contrast learning approach, we reexamine channel attention with regard to pedestrian objects and background information for nighttime pedestrian detection, resulting in the proposed Fore-Background Contrast Attention (FBCA). FBCA possesses two primary attributes: (1) channel descriptors form remote dependencies with global spatial feature information; (2) the integration of background information enhances the distinction bet
&lt;/p&gt;</description></item><item><title>该文章详细介绍了以NPU-ASLP命名的用于CNVSRC 2024视觉语音识别挑战的所有四个轨道的系统设计，包括单一说话者VSR任务和多说话者VSR任务的固定和开放轨道。数据处理方面，该系统采用基础线上的唇动提取器产生了多尺度视频数据。在训练期间，各种增强技术被应用，包括速度偏移、随机旋转、水平翻转和颜色变换。VSR模型采用端到端架构，带有联合CTC/注意力损失，引入了增强的ResNet3D视觉前端、E-Branchformer编码器和双方向Transformer解码器。该系统在单一说话者任务的开放轨道上取得了30.47%的错误率（CER），在多说话者任务的开放轨道上取得了34.30%的CER，在单一说话者任务的固定轨道上取得了32.51%的CER，在多说话者任务的固定轨道上取得了35.28%的CER，从而在该挑战中赢得了第二名，证明了其创新的语音识别系统的有效性和实用性。</title><link>https://arxiv.org/abs/2408.02369</link><description>&lt;p&gt;
The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02369
&lt;/p&gt;
&lt;p&gt;
该文章详细介绍了以NPU-ASLP命名的用于CNVSRC 2024视觉语音识别挑战的所有四个轨道的系统设计，包括单一说话者VSR任务和多说话者VSR任务的固定和开放轨道。数据处理方面，该系统采用基础线上的唇动提取器产生了多尺度视频数据。在训练期间，各种增强技术被应用，包括速度偏移、随机旋转、水平翻转和颜色变换。VSR模型采用端到端架构，带有联合CTC/注意力损失，引入了增强的ResNet3D视觉前端、E-Branchformer编码器和双方向Transformer解码器。该系统在单一说话者任务的开放轨道上取得了30.47%的错误率（CER），在多说话者任务的开放轨道上取得了34.30%的CER，在单一说话者任务的固定轨道上取得了32.51%的CER，在多说话者任务的固定轨道上取得了35.28%的CER，从而在该挑战中赢得了第二名，证明了其创新的语音识别系统的有效性和实用性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta
&lt;/p&gt;</description></item><item><title>该文章提出一种名为SSHD-Net的单一点监督高分辨率动态网络，通过使用单一点监督达到了现有方法难以达到的红外小目标检测性能。SSHD-Net通过设计一种高分辨率交叉特征提取模块（HCEM）和动态坐标融合模块（DCFM），实现了生物向特征交互和高分辨率深层红外小目标信息的维护，进而提高了红外小目标检测的准确性和抗干扰能力。</title><link>https://arxiv.org/abs/2408.01976</link><description>&lt;p&gt;
Single-Point Supervised High-Resolution Dynamic Network for Infrared Small Target Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01976
&lt;/p&gt;
&lt;p&gt;
该文章提出一种名为SSHD-Net的单一点监督高分辨率动态网络，通过使用单一点监督达到了现有方法难以达到的红外小目标检测性能。SSHD-Net通过设计一种高分辨率交叉特征提取模块（HCEM）和动态坐标融合模块（DCFM），实现了生物向特征交互和高分辨率深层红外小目标信息的维护，进而提高了红外小目标检测的准确性和抗干扰能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01976v1 Announce Type: new  Abstract: Infrared small target detection (IRSTD) tasks are extremely challenging for two main reasons: 1) it is difficult to obtain accurate labelling information that is critical to existing methods, and 2) infrared (IR) small target information is easily lost in deep networks. To address these issues, we propose a single-point supervised high-resolution dynamic network (SSHD-Net). In contrast to existing methods, we achieve state-of-the-art (SOTA) detection performance using only single-point supervision. Specifically, we first design a high-resolution cross-feature extraction module (HCEM), that achieves bi-directional feature interaction through stepped feature cascade channels (SFCC). It balances network depth and feature resolution to maintain deep IR small-target information. Secondly, the effective integration of global and local features is achieved through the dynamic coordinate fusion module (DCFM), which enhances the anti-interference
&lt;/p&gt;</description></item><item><title>该文章提出了一个大型视频地面真实数据集，名为“SynopGround”，该数据集结合了流行的电视剧视频和详细的剧情概要，以促进对视频内容的深层次多模态理解。</title><link>https://arxiv.org/abs/2408.01669</link><description>&lt;p&gt;
SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01669
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个大型视频地面真实数据集，名为“SynopGround”，该数据集结合了流行的电视剧视频和详细的剧情概要，以促进对视频内容的深层次多模态理解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01669v1 Announce Type: new  Abstract: Video grounding is a fundamental problem in multimodal content understanding, aiming to localize specific natural language queries in an untrimmed video. However, current video grounding datasets merely focus on simple events and are either limited to shorter videos or brief sentences, which hinders the model from evolving toward stronger multimodal understanding capabilities. To address these limitations, we present a large-scale video grounding dataset named SynopGround, in which more than 2800 hours of videos are sourced from popular TV dramas and are paired with accurately localized human-written synopses. Each paragraph in the synopsis serves as a language query and is manually annotated with precise temporal boundaries in the long video. These paragraph queries are tightly correlated to each other and contain a wealth of abstract expressions summarizing video storylines and specific descriptions portraying event details, which enab
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EnPrompt的框架，通过在VLMs（预训练视觉-语言模型）中集成文本外部层（EnLa），以增加模型的泛化能力。EnLa包含了预训练CLIP的有效文本嵌入和可学习的视觉嵌入，旨在为下游任务提供更好的适应性。通过这种方式，文章旨在平衡视觉和文本分支的学习能力，并通过一种独特的协同学习机制来增强它们之间的交互作用。这种设计允许模型在保持灵活性的同时，能够学习到更加有效的任务特异性文本嵌入，从而提升在各种视觉相关任务上的性能。</title><link>https://arxiv.org/abs/2407.19674</link><description>&lt;p&gt;
Advancing Prompt Learning through an External Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19674
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EnPrompt的框架，通过在VLMs（预训练视觉-语言模型）中集成文本外部层（EnLa），以增加模型的泛化能力。EnLa包含了预训练CLIP的有效文本嵌入和可学习的视觉嵌入，旨在为下游任务提供更好的适应性。通过这种方式，文章旨在平衡视觉和文本分支的学习能力，并通过一种独特的协同学习机制来增强它们之间的交互作用。这种设计允许模型在保持灵活性的同时，能够学习到更加有效的任务特异性文本嵌入，从而提升在各种视觉相关任务上的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19674v3 Announce Type: replace  Abstract: Prompt learning represents a promising method for adapting pre-trained vision-language models (VLMs) to various downstream tasks by learning a set of text embeddings. One challenge inherent to these methods is the poor generalization performance due to the invalidity of the learned text embeddings for unseen tasks. A straightforward approach to bridge this gap is to freeze the text embeddings in prompts, which results in a lack of capacity to adapt VLMs for downstream tasks. To address this dilemma, we propose a paradigm called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a textual external layer and learnable visual embeddings for adapting VLMs to downstream tasks. The learnable external layer is built upon valid embeddings of pre-trained CLIP. This design considers the balance of learning capabilities between the two branches. To align the textual and visual features, we propose a novel two-pronged approach
&lt;/p&gt;</description></item><item><title>该文章提出了一种全新的量化框架，旨在高效地对扩散模型进行后训练量化。该框架通过维护基于时间信息的基本（Temporal Information Basis）特征，解决了传统量化方法对时间步长敏感的特征优化不足问题。框架中的三个策略有效降低了量化过程中对时间特征的干扰，提高了压缩效率。</title><link>https://arxiv.org/abs/2407.19547</link><description>&lt;p&gt;
Temporal Feature Matters: A Framework for Diffusion Model Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19547
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种全新的量化框架，旨在高效地对扩散模型进行后训练量化。该框架通过维护基于时间信息的基本（Temporal Information Basis）特征，解决了传统量化方法对时间步长敏感的特征优化不足问题。框架中的三个策略有效降低了量化过程中对时间特征的干扰，提高了压缩效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19547v2 Announce Type: replace  Abstract: The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information B
&lt;/p&gt;</description></item><item><title>该文章介绍了Perm，一个用于3D头发建模的参数化表示，它能够支持各种头发相关应用。文章提出了一种新的方法来分离头发的大致形状和局部细丝细节，这种方法使用频率域中的PCA（主分量分析）基细丝表示，从而允许更精确的编辑和输出控制。通过使用一个专门的细丝表示来适合和分解头发的几何纹理，以及将其分割成低频到高频的头发结构，并使用不同的生成模型对这些分解后的纹理进行参数化，从而模拟头发建模过程中常见的步骤。通过广泛的实验验证了Perm架构的设计，并最终将其作为通用先验部署到诸如3D头发参数化、发型插值和单视角渲染等任务中，进一步展示了它在众多任务中的灵活性和优越性。</title><link>https://arxiv.org/abs/2407.19451</link><description>&lt;p&gt;
Perm: A Parametric Representation for Multi-Style 3D Hair Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19451
&lt;/p&gt;
&lt;p&gt;
该文章介绍了Perm，一个用于3D头发建模的参数化表示，它能够支持各种头发相关应用。文章提出了一种新的方法来分离头发的大致形状和局部细丝细节，这种方法使用频率域中的PCA（主分量分析）基细丝表示，从而允许更精确的编辑和输出控制。通过使用一个专门的细丝表示来适合和分解头发的几何纹理，以及将其分割成低频到高频的头发结构，并使用不同的生成模型对这些分解后的纹理进行参数化，从而模拟头发建模过程中常见的步骤。通过广泛的实验验证了Perm架构的设计，并最终将其作为通用先验部署到诸如3D头发参数化、发型插值和单视角渲染等任务中，进一步展示了它在众多任务中的灵活性和优越性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19451v3 Announce Type: replace  Abstract: We present Perm, a learned parametric model of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair shape and local strand details, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair modeling process. We conduct extensive experiments to validate the architecture design of \textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as 3D hair parameterization, hairstyle interpolation, single-view
&lt;/p&gt;</description></item><item><title>该文章提出了一种无需手动清理或标注数据的 unsupervised 学习框架，用于合成术前 CT 扫描中的 mastoidectomy 体积。这种方法使用术后 CT 扫描进行模型训练，即便区域在术中移除后存在金属干扰和低信噪比问题。</title><link>https://arxiv.org/abs/2407.15787</link><description>&lt;p&gt;
Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using Highly Noisy Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15787
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种无需手动清理或标注数据的 unsupervised 学习框架，用于合成术前 CT 扫描中的 mastoidectomy 体积。这种方法使用术后 CT 扫描进行模型训练，即便区域在术中移除后存在金属干扰和低信噪比问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15787v2 Announce Type: replace  Abstract: Cochlear Implant (CI) procedures involve inserting an array of electrodes into the cochlea located inside the inner ear. Mastoidectomy is a surgical procedure that uses a high-speed drill to remove part of the mastoid region of the temporal bone, providing safe access to the cochlea through the middle and inner ear. We aim to develop an intraoperative navigation system that registers plans created using 3D preoperative Computerized Tomography (CT) volumes with the 2D surgical microscope view. Herein, we propose a method to synthesize the mastoidectomy volume using only the preoperative CT scan, where the mastoid is intact. We introduce an unsupervised learning framework designed to synthesize mastoidectomy. For model training purposes, this method uses postoperative CT scans to avoid manual data cleaning or labeling, even when the region removed during mastoidectomy is visible but affected by metal artifacts, low signal-to-noise rati
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为DBF（Dynamic Backbone Freezing）的动态特性骨干冻结方法，用于解决在遥感物体检测中特征骨干的精细调节问题。这种方法旨在解决在遥感领域中，常见的将骨干网络在ImageNet自然场景预训练后再用于遥感图像的特征提取可能会导致基础视觉特征提取能力下降的问题，限制了性能的进一步提升。通过动态调节骨干网络的冻结策略，DBF方法能够在保持骨干网络低级通用的特征提取能力的同时，提高其领域特定的知识提取效率，从而提高了遥感物体检测的性能。</title><link>https://arxiv.org/abs/2407.15143</link><description>&lt;p&gt;
Rethinking Feature Backbone Fine-tuning for Remote Sensing Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15143
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为DBF（Dynamic Backbone Freezing）的动态特性骨干冻结方法，用于解决在遥感物体检测中特征骨干的精细调节问题。这种方法旨在解决在遥感领域中，常见的将骨干网络在ImageNet自然场景预训练后再用于遥感图像的特征提取可能会导致基础视觉特征提取能力下降的问题，限制了性能的进一步提升。通过动态调节骨干网络的冻结策略，DBF方法能够在保持骨干网络低级通用的特征提取能力的同时，提高其领域特定的知识提取效率，从而提高了遥感物体检测的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15143v2 Announce Type: replace  Abstract: Recently, numerous methods have achieved impressive performance in remote sensing object detection, relying on convolution or transformer architectures. Such detectors typically have a feature backbone to extract useful features from raw input images. For the remote sensing domain, a common practice among current detectors is to initialize the backbone with pre-training on ImageNet consisting of natural scenes. Fine-tuning the backbone is then typically required to generate features suitable for remote-sensing images. However, this could hinder the extraction of basic visual features in long-term training, thus restricting performance improvement. To mitigate this issue, we propose a novel method named DBF (Dynamic Backbone Freezing) for feature backbone fine-tuning on remote sensing object detection. Our method aims to handle the dilemma of whether the backbone should extract low-level generic features or possess specific knowledge 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为ESP-MedSAM的 efficient self-prompting SAM（Segment Anything Model），针对医疗图像 segmentation 的三个主要挑战：显著降低计算成本、简化数据标注过程并提升对不同医疗模态的适应性。通过 Multi-Modal Decoupled Knowledge Distillation（MMDKD）策略，文章创新性地将通用的图像知识和特定于医学领域的知识从基础模型中提炼出来，用于训练一个轻量级的图像编码器和一种称为 modality controller 的模块，以此减轻了专家标注的负担并提高了模型在不同医学模态上的性能。 Additionally, a self-patch prompting generator was introduced to further enhance the model's ability to adapt to diverse medical imaging tasks, making ESP-MedSAM a significant improvement over previous SAM models in terms of efficiency and applicability for medical image segmentation.</title><link>https://arxiv.org/abs/2407.14153</link><description>&lt;p&gt;
ESP-MedSAM: Efficient Self-Prompting SAM for Universal Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.14153
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为ESP-MedSAM的 efficient self-prompting SAM（Segment Anything Model），针对医疗图像 segmentation 的三个主要挑战：显著降低计算成本、简化数据标注过程并提升对不同医疗模态的适应性。通过 Multi-Modal Decoupled Knowledge Distillation（MMDKD）策略，文章创新性地将通用的图像知识和特定于医学领域的知识从基础模型中提炼出来，用于训练一个轻量级的图像编码器和一种称为 modality controller 的模块，以此减轻了专家标注的负担并提高了模型在不同医学模态上的性能。 Additionally, a self-patch prompting generator was introduced to further enhance the model's ability to adapt to diverse medical imaging tasks, making ESP-MedSAM a significant improvement over previous SAM models in terms of efficiency and applicability for medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.14153v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) has demonstrated outstanding adaptation to medical image segmentation but still faces three major challenges. Firstly, the huge computational costs of SAM limit its real-world applicability. Secondly, SAM depends on manual annotations (e.g., points, boxes) as prompts, which are laborious and impractical in clinical scenarios. Thirdly, SAM handles all segmentation targets equally, which is suboptimal for diverse medical modalities with inherent heterogeneity. To address these issues, we propose an Efficient Self-Prompting SAM for universal medical image segmentation, named ESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD) strategy to distil common image knowledge and domain-specific medical knowledge from the foundation model to train a lightweight image encoder and a modality controller. Further, they combine with the additionally introduced Self-Patch Prompt Generator
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的自我监督学习方法，通过模拟人类观察物体时所进行的动作，即改变观察角度的动作，来增强视觉表示学习的鲁棒性。这种方法通过提取视频数据集中的动作信息，并开发了一个新的损失函数来学习视觉和动作嵌入，使得动作与两个图像的表示进行对齐，从而将动作有意安排进隐含的视觉表示中。实验结果表明，该方法在多个视频数据集上均显著优于现有方法，表明了动作信息在自我监督学习中的积极作用。</title><link>https://arxiv.org/abs/2407.06704</link><description>&lt;p&gt;
Self-supervised visual learning from interactions with objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.06704
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的自我监督学习方法，通过模拟人类观察物体时所进行的动作，即改变观察角度的动作，来增强视觉表示学习的鲁棒性。这种方法通过提取视频数据集中的动作信息，并开发了一个新的损失函数来学习视觉和动作嵌入，使得动作与两个图像的表示进行对齐，从而将动作有意安排进隐含的视觉表示中。实验结果表明，该方法在多个视频数据集上均显著优于现有方法，表明了动作信息在自我监督学习中的积极作用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.06704v2 Announce Type: replace  Abstract: Self-supervised learning (SSL) has revolutionized visual representation learning, but has not achieved the robustness of human vision. A reason for this could be that SSL does not leverage all the data available to humans during learning. When learning about an object, humans often purposefully turn or move around objects and research suggests that these interactions can substantially enhance their learning. Here we explore whether such object-related actions can boost SSL. For this, we extract the actions performed to change from one ego-centric view of an object to another in four video datasets. We then introduce a new loss function to learn visual and action embeddings by aligning the performed action with the representations of two images extracted from the same clip. This permits the performed actions to structure the latent visual representation. Our experiments show that our method consistently outperforms previous methods on
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RealTalk的基于音频驱动的通用面部生成框架，通过考虑身份和个人内部变化特征，可以使用增强的跨模态注意力和面部先验信息，实现更高的唇形同步精度，同时确保在实时性能中生成高质量的面部渲染。</title><link>https://arxiv.org/abs/2406.18284</link><description>&lt;p&gt;
RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D Facial Prior-guided Identity Alignment Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.18284
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RealTalk的基于音频驱动的通用面部生成框架，通过考虑身份和个人内部变化特征，可以使用增强的跨模态注意力和面部先验信息，实现更高的唇形同步精度，同时确保在实时性能中生成高质量的面部渲染。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.18284v2 Announce Type: replace  Abstract: Person-generic audio-driven face generation is a challenging task in computer vision. Previous methods have achieved remarkable progress in audio-visual synchronization, but there is still a significant gap between current results and practical applications. The challenges are two-fold: 1) Preserving unique individual traits for achieving high-precision lip synchronization. 2) Generating high-quality facial renderings in real-time performance. In this paper, we propose a novel generalized audio-driven framework RealTalk, which consists of an audio-to-expression transformer and a high-fidelity expression-to-face renderer. In the first component, we consider both identity and intra-personal variation features related to speaking lip movements. By incorporating cross-modal attention on the enriched facial priors, we can effectively align lip movements with audio, thus attaining greater precision in expression prediction. In the second c
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SMG-Learning的学习范式，旨在同时提高模型对不同数据源的记忆能力和对新数据源的泛化能力。文章通过创建定向梯度对齐策略，确保模型能够记忆过去数据源的信息，并通过任意梯度对齐策略提高对新数据源的泛化能力。通过这种平行梯度对齐技术，模型能够在不遗忘先前学习内容的同时，在新数据源上表现出良好的泛化性能。</title><link>https://arxiv.org/abs/2406.18037</link><description>&lt;p&gt;
Towards Synchronous Memorizability and Generalizability with Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.18037
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SMG-Learning的学习范式，旨在同时提高模型对不同数据源的记忆能力和对新数据源的泛化能力。文章通过创建定向梯度对齐策略，确保模型能够记忆过去数据源的信息，并通过任意梯度对齐策略提高对新数据源的泛化能力。通过这种平行梯度对齐技术，模型能够在不遗忘先前学习内容的同时，在新数据源上表现出良好的泛化性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.18037v2 Announce Type: replace  Abstract: The ability to learn sequentially from different data sites is crucial for a deep network in solving practical medical image diagnosis problems due to privacy restrictions and storage limitations. However, adapting on incoming site leads to catastrophic forgetting on past sites and decreases generalizablity on unseen sites. Existing Continual Learning (CL) and Domain Generalization (DG) methods have been proposed to solve these two challenges respectively, but none of them can address both simultaneously. Recognizing this limitation, this paper proposes a novel training paradigm, learning towards Synchronous Memorizability and Generalizability (SMG-Learning). To achieve this, we create the orientational gradient alignment to ensure memorizability on previous sites, and arbitrary gradient alignment to enhance generalizability on unseen sites. This approach is named as Parallel Gradient Alignment (PGA). Furthermore, we approximate the 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Toffee的效率百万级别的数据集构建方法，用于针对特定主题的文本到图像生成。通过该算法，可以高效地训练文本到图像生成模型，使其能够在无需对测试图像进行进一步微调的情况下，直接根据任意文本生成主题相关的图像，从而降低创造大型规模数据集所需的大量GPU计算资源。</title><link>https://arxiv.org/abs/2406.09305</link><description>&lt;p&gt;
Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.09305
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Toffee的效率百万级别的数据集构建方法，用于针对特定主题的文本到图像生成。通过该算法，可以高效地训练文本到图像生成模型，使其能够在无需对测试图像进行进一步微调的情况下，直接根据任意文本生成主题相关的图像，从而降低创造大型规模数据集所需的大量GPU计算资源。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.09305v2 Announce Type: replace  Abstract: In subject-driven text-to-image generation, recent works have achieved superior performance by training the model on synthetic datasets containing numerous image pairs. Trained on these datasets, generative models can produce text-aligned images for specific subject from arbitrary testing image in a zero-shot manner. They even outperform methods which require additional fine-tuning on testing images. However, the cost of creating such datasets is prohibitive for most researchers. To generate a single training pair, current methods fine-tune a pre-trained text-to-image model on the subject image to capture fine-grained details, then use the fine-tuned model to create images for the same subject based on creative text prompts. Consequently, constructing a large-scale dataset with millions of subjects can require hundreds of thousands of GPU hours. To tackle this problem, we propose Toffee, an efficient method to construct datasets for 
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为"GaussianForest"的3D场景建模框架，该框架通过混合的3D高斯分割来优化存储，并在复杂区域保持高细节的同时降低了所需的参数数量。</title><link>https://arxiv.org/abs/2406.08759</link><description>&lt;p&gt;
GaussianForest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.08759
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为"GaussianForest"的3D场景建模框架，该框架通过混合的3D高斯分割来优化存储，并在复杂区域保持高细节的同时降低了所需的参数数量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.08759v2 Announce Type: replace  Abstract: The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为“文本前瞻”的预训练目标，用于学习用户界面的表示，通过生成未来用户界面状态的全球文本描述来改善界面元素和整个屏幕的联合推理。这种方法在生成任务上取得了显著的改进，在较少的图像数量下，与现有技术相比，生成了更精确的用户界面描述。此外，该研究还构建了名为“OpenApp”的移动应用程序数据集，为该领域提供了首个公开的移动应用程序数据集。</title><link>https://arxiv.org/abs/2406.07822</link><description>&lt;p&gt;
Tell Me What's Next: Textual Foresight for Generic UI Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.07822
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为“文本前瞻”的预训练目标，用于学习用户界面的表示，通过生成未来用户界面状态的全球文本描述来改善界面元素和整个屏幕的联合推理。这种方法在生成任务上取得了显著的改进，在较少的图像数量下，与现有技术相比，生成了更精确的用户界面描述。此外，该研究还构建了名为“OpenApp”的移动应用程序数据集，为该领域提供了首个公开的移动应用程序数据集。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.07822v2 Announce Type: replace  Abstract: Mobile app user interfaces (UIs) are rich with action, text, structure, and image content that can be utilized to learn generic UI representations for tasks like automating user commands, summarizing content, and evaluating the accessibility of user interfaces. Prior work has learned strong visual representations with local or global captioning losses, but fails to retain both granularities. To combat this, we propose Textual Foresight, a novel pretraining objective for learning UI screen representations. Textual Foresight generates global text descriptions of future UI states given a current UI and local action taken. Our approach requires joint reasoning over elements and entire screens, resulting in improved UI features: on generation tasks, UI agents trained with Textual Foresight outperform state-of-the-art by 2% with 28x fewer images. We train with our newly constructed mobile app dataset, OpenApp, which results in the first pu
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为VISTA3D的通用成像分割和注释模型，它在3D计算机断层扫描图像上实现了高准确度的自动分割，覆盖了多种人体解剖结构。VISTA3D通过在大量图像数据上进行训练，能够在无需额外调整的情况下直接应用于临床，同时具有适应新结构和类别进行分割的能力，极大地促进了以CT图像为基础的医疗研究和诊断。</title><link>https://arxiv.org/abs/2406.05285</link><description>&lt;p&gt;
VISTA3D: Versatile Imaging SegmenTation and Annotation model for 3D Computed Tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.05285
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为VISTA3D的通用成像分割和注释模型，它在3D计算机断层扫描图像上实现了高准确度的自动分割，覆盖了多种人体解剖结构。VISTA3D通过在大量图像数据上进行训练，能够在无需额外调整的情况下直接应用于临床，同时具有适应新结构和类别进行分割的能力，极大地促进了以CT图像为基础的医疗研究和诊断。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.05285v2 Announce Type: replace  Abstract: Medical image segmentation is a core component of precision medicine, and 3D computed tomography (CT) is one of the most important imaging techniques. A highly accurate and clinically applicable segmentation foundation model will greatly facilitate clinicians and researchers using CT images. Although existing foundation models have attracted great interest, none are adequate for 3D CT, either because they lack accurate automatic segmentation for large cohort analysis or the ability to segment novel classes. An ideal segmentation solution should possess two features: accurate out-of-the-box performance covering major organ classes, and effective adaptation or zero-shot ability to novel structures. To achieve this goal, we introduce Versatile Imaging SegmenTation and Annotation model (VISTA3D). VISTA3D is trained systematically on 11454 volumes and provides accurate out-of-the-box segmentation for 127 common types of human anatomical s
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为CMR-Net的跨模态重建网络，它通过结合可微渲染技术和光学图像的跨模态监督，能够将多基线稀疏SAR三维图像中的车辆目标重建为结构清晰的高分辨率图像。通过精心设计网络结构和训练策略，该网络显著提高了稀疏多基线SAR三维成像的质量。</title><link>https://arxiv.org/abs/2406.04158</link><description>&lt;p&gt;
Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04158
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为CMR-Net的跨模态重建网络，它通过结合可微渲染技术和光学图像的跨模态监督，能够将多基线稀疏SAR三维图像中的车辆目标重建为结构清晰的高分辨率图像。通过精心设计网络结构和训练策略，该网络显著提高了稀疏多基线SAR三维成像的质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04158v2 Announce Type: replace  Abstract: Multi-baseline SAR 3D imaging faces significant challenges due to data sparsity. In recent years, deep learning techniques have achieved notable success in enhancing the quality of sparse SAR 3D imaging. However, previous work typically rely on full-aperture high-resolution radar images to supervise the training of deep neural networks (DNNs), utilizing only single-modal information from radar data. Consequently, imaging performance is limited, and acquiring full-aperture data for multi-baseline SAR is costly and sometimes impractical in real-world applications. In this paper, we propose a Cross-Modal Reconstruction Network (CMR-Net), which integrates differentiable render and cross-modal supervision with optical images to reconstruct highly sparse multi-baseline SAR 3D images of vehicle targets into visually structured and high-resolution images. We meticulously designed the network architecture and training strategies to enhance ne
&lt;/p&gt;</description></item><item><title>该文章提出FastLGS方法，通过使用特征网格映射技术，显著加快了语言嵌入高斯分布的计算速度，实现了在超高分辨率下实时支持开放词汇查询的3D场景理解应用。</title><link>https://arxiv.org/abs/2406.01916</link><description>&lt;p&gt;
FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.01916
&lt;/p&gt;
&lt;p&gt;
该文章提出FastLGS方法，通过使用特征网格映射技术，显著加快了语言嵌入高斯分布的计算速度，实现了在超高分辨率下实时支持开放词汇查询的3D场景理解应用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.01916v2 Announce Type: replace  Abstract: The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications. However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields. In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution. We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries. Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance con
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为长期和短期无监督分类器指导的长和短指导策略，用于基于分对的文本到图像生成模型，显著提高了训练效率并提升了生成质量和速度。</title><link>https://arxiv.org/abs/2406.01561</link><description>&lt;p&gt;
Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.01561
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为长期和短期无监督分类器指导的长和短指导策略，用于基于分对的文本到图像生成模型，显著提高了训练效率并提升了生成质量和速度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.01561v3 Announce Type: replace  Abstract: Diffusion-based text-to-image generation models trained on extensive text-image pairs have shown the capacity to generate photorealistic images consistent with textual descriptions. However, a significant limitation of these models is their slow sample generation, which requires iterative refinement through the same network. In this paper, we enhance Score identity Distillation (SiD) by developing long and short classifier-free guidance (LSG) to efficiently distill pretrained Stable Diffusion models without using real training data. SiD aims to optimize a model-based explicit score matching loss, utilizing a score-identity-based approximation alongside the proposed LSG for practical computation. By training exclusively with fake images synthesized with its one-step generator, SiD equipped with LSG rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Specifically,
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为3DSS-Mamba的框架，通过结合状态空间模型和3维（3D）光谱-空间信息，解决了传统CNN和Transformer在处理高光谱图像分类时的局限性，提高了全局光谱-空间关系建模的效率并减少了计算开销。</title><link>https://arxiv.org/abs/2405.12487</link><description>&lt;p&gt;
3DSS-Mamba: 3D-Spectral-Spatial Mamba for Hyperspectral Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.12487
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为3DSS-Mamba的框架，通过结合状态空间模型和3维（3D）光谱-空间信息，解决了传统CNN和Transformer在处理高光谱图像分类时的局限性，提高了全局光谱-空间关系建模的效率并减少了计算开销。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.12487v2 Announce Type: replace  Abstract: Hyperspectral image (HSI) classification constitutes the fundamental research in remote sensing fields. Convolutional Neural Networks (CNNs) and Transformers have demonstrated impressive capability in capturing spectral-spatial contextual dependencies. However, these architectures suffer from limited receptive fields and quadratic computational complexity, respectively. Fortunately, recent Mamba architectures built upon the State Space Model integrate the advantages of long-range sequence modeling and linear computational efficiency, exhibiting substantial potential in low-dimensional scenarios. Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba (3DSS-Mamba) framework for HSI classification, allowing for global spectral-spatial relationship modeling with greater computational efficiency. Technically, a spectral-spatial token generation (SSTG) module is designed to convert the HSI cube into a set of 3D spectral-spatial to
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SAM3D的新方法，它结合了现有的Segment Anything Model，用于在3D医学图像中实现半自动化的零样本分割。该方法通过用户在图像上绘制3D多边形来提示，并根据划分的体积进行切片，然后使用预训练模型对切片进行全量推理，最后在三维空间中进行重组和细化工作。该方法在多个成像模态和不同解剖结构上进行了评估。SAM3D在腹部盆腔CT和大脑MRI图像上的特定结构分割方面取得了显著的性能。最重要的是，该方法无需进行任何模型训练或微调，非常适用于数据标注不足的细分任务。通过允许用户快速生成对新数据的3D分割，SAM3D有望在手术规划和教育、诊断以及医学研究中发挥重要作用。</title><link>https://arxiv.org/abs/2405.06786</link><description>&lt;p&gt;
SAM3D: Zero-Shot Semi-Automatic Segmentation in 3D Medical Images with the Segment Anything Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.06786
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SAM3D的新方法，它结合了现有的Segment Anything Model，用于在3D医学图像中实现半自动化的零样本分割。该方法通过用户在图像上绘制3D多边形来提示，并根据划分的体积进行切片，然后使用预训练模型对切片进行全量推理，最后在三维空间中进行重组和细化工作。该方法在多个成像模态和不同解剖结构上进行了评估。SAM3D在腹部盆腔CT和大脑MRI图像上的特定结构分割方面取得了显著的性能。最重要的是，该方法无需进行任何模型训练或微调，非常适用于数据标注不足的细分任务。通过允许用户快速生成对新数据的3D分割，SAM3D有望在手术规划和教育、诊断以及医学研究中发挥重要作用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.06786v2 Announce Type: replace-cross  Abstract: We introduce SAM3D, a new approach to semi-automatic zero-shot segmentation of 3D images building on the existing Segment Anything Model. We achieve fast and accurate segmentations in 3D images with a four-step strategy involving: user prompting with 3D polylines, volume slicing along multiple axes, slice-wide inference with a pretrained model, and recomposition and refinement in 3D. We evaluated SAM3D performance qualitatively on an array of imaging modalities and anatomical structures and quantify performance for specific structures in abdominal pelvic CT and brain MRI. Notably, our method achieves good performance with zero model training or finetuning, making it particularly useful for tasks with a scarcity of preexisting labeled data. By enabling users to create 3D segmentations of unseen data quickly and with dramatically reduced manual input, these methods have the potential to aid surgical planning and education, diagno
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于压缩实现的深度结构网络（CRDS），通过引入与经典压缩编码器架构中三个主要过程相匹配的三个经验性偏差，有效增强了压缩视频的质量。这种设计同时融合了经典编码器和深度神经网络的优点，并实现了对视频质量提升任务的有效解析和优化。</title><link>https://arxiv.org/abs/2405.06342</link><description>&lt;p&gt;
Compression-Realized Deep Structural Network for Video Quality Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.06342
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于压缩实现的深度结构网络（CRDS），通过引入与经典压缩编码器架构中三个主要过程相匹配的三个经验性偏差，有效增强了压缩视频的质量。这种设计同时融合了经典编码器和深度神经网络的优点，并实现了对视频质量提升任务的有效解析和优化。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.06342v2 Announce Type: replace  Abstract: This paper focuses on the task of quality enhancement for compressed videos. Although deep network-based video restorers achieve impressive progress, most of the existing methods lack a structured design to optimally leverage the priors within compression codecs. Since the quality degradation of the video is primarily induced by the compression algorithm, a new paradigm is urgently needed for a more ``conscious'' process of quality enhancement. As a result, we propose the Compression-Realized Deep Structural Network (CRDS), introducing three inductive biases aligned with the three primary processes in the classic compression codec, merging the strengths of classical encoder architecture with deep network capabilities. Inspired by the residual extraction and domain transformation process in the codec, a pre-trained Latent Degradation Residual Auto-Encoder is proposed to transform video frames into a latent feature space, and the mutua
&lt;/p&gt;</description></item><item><title>该文章提出了一种平滑深层注意力图的方法，减少从卷积下采样产生的噪声，提高了基于梯度注意力图的解译性，通过在隐藏层计算的注意力图与输入层和GradCAM的对比，发现隐藏层计算的注意力图在三种类型的模型上表现更好，并在ImageNet1K图像分类和Camelyon16以及真实世界数字病理学扫描的肿瘤检测中验证了该方法。</title><link>https://arxiv.org/abs/2404.02282</link><description>&lt;p&gt;
Smooth Deep Saliency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02282
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种平滑深层注意力图的方法，减少从卷积下采样产生的噪声，提高了基于梯度注意力图的解译性，通过在隐藏层计算的注意力图与输入层和GradCAM的对比，发现隐藏层计算的注意力图在三种类型的模型上表现更好，并在ImageNet1K图像分类和Camelyon16以及真实世界数字病理学扫描的肿瘤检测中验证了该方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02282v3 Announce Type: replace  Abstract: In this work, we investigate methods to reduce the noise in deep saliency maps coming from convolutional downsampling. Those methods make the investigated models more interpretable for gradient-based saliency maps, computed in hidden layers. We evaluate the faithfulness of those methods using insertion and deletion metrics, finding that saliency maps computed in hidden layers perform better compared to both the input layer and GradCAM. We test our approach on different models trained for image classification on ImageNet1K, and models trained for tumor detection on Camelyon16 and in-house real-world digital pathology scans of stained tissue samples. Our results show that the checkerboard noise in the gradient gets reduced, resulting in smoother and therefore easier to interpret saliency maps.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为HARMamba的创新高效穿戴式传感器人类活动识别方法，该方法是基于双向Mamba模型的，它可以为资源受限的移动健康应用提供轻量级和灵活的解决方案。</title><link>https://arxiv.org/abs/2403.20183</link><description>&lt;p&gt;
HARMamba: Efficient and Lightweight Wearable Sensor Human Activity Recognition Based on Bidirectional Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20183
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为HARMamba的创新高效穿戴式传感器人类活动识别方法，该方法是基于双向Mamba模型的，它可以为资源受限的移动健康应用提供轻量级和灵活的解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20183v3 Announce Type: replace  Abstract: Wearable sensor-based human activity recognition (HAR) is a critical research domain in activity perception. However, achieving high efficiency and long sequence recognition remains a challenge. Despite the extensive investigation of temporal deep learning models, such as CNNs, RNNs, and transformers, their extensive parameters often pose significant computational and memory constraints, rendering them less suitable for resource-constrained mobile health applications. This study introduces HARMamba, an innovative light-weight and versatile HAR architecture that combines selective bidirectional State Spaces Model and hardware-aware design. To optimize real-time resource consumption in practical scenarios, HARMamba employs linear recursive mechanisms and parameter discretization, allowing it to selectively focus on relevant input sequences while efficiently fusing scan and recompute operations. The model employs independent channels to
&lt;/p&gt;</description></item><item><title>该文章提出FOOL技术，通过在卫星边缘计算场景中进行特征压缩，有效缓解了因大量传感器数据导致的下行链路瓶颈问题。FOOL通过最大化传输效率和利用上下文信息，降低了数据传输成本，同时保持了预测性能。</title><link>https://arxiv.org/abs/2403.16677</link><description>&lt;p&gt;
FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16677
&lt;/p&gt;
&lt;p&gt;
该文章提出FOOL技术，通过在卫星边缘计算场景中进行特征压缩，有效缓解了因大量传感器数据导致的下行链路瓶颈问题。FOOL通过最大化传输效率和利用上下文信息，降低了数据传输成本，同时保持了预测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16677v2 Announce Type: replace-cross  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on quality measures at lowe
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为GenAD的基于扩散模型的预测模型，它能够在无需额外训练的情况下对多种未见过的驾驶场景进行零样本预测，并展现出优于其他模型的性能，为自动驾驶技术的应用提供了新的可能。</title><link>https://arxiv.org/abs/2403.09630</link><description>&lt;p&gt;
GenAD: Generalized Predictive Model for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09630
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为GenAD的基于扩散模型的预测模型，它能够在无需额外训练的情况下对多种未见过的驾驶场景进行零样本预测，并展现出优于其他模型的性能，为自动驾驶技术的应用提供了新的可能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09630v2 Announce Type: replace  Abstract: In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline. To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions. The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios. Inheriting the merits from recent latent diffusion models, our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal reasoning blocks. We showcase that it can generalize to various unseen driving datasets in a zero-shot manner, surpassing general or driving-specific video prediction counterparts. Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-w
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为P2LHAP的全新框架，通过单一模型实现了对传感器数据进行活动识别、分割和预测。框架通过将数据流划分为系列“片断”并预测未来的活动来实现上述功能，同时通过周围片断标签的平滑技术精确地识别活动边界。该模型利用通道独立的Transformer编码器和解码器学习片断级别的表示，且所有的通道共享嵌入和Transformer权重，为健康管理及辅助生活等领域提供了实时理解活动信息的可能性。</title><link>https://arxiv.org/abs/2403.08214</link><description>&lt;p&gt;
P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08214
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为P2LHAP的全新框架，通过单一模型实现了对传感器数据进行活动识别、分割和预测。框架通过将数据流划分为系列“片断”并预测未来的活动来实现上述功能，同时通过周围片断标签的平滑技术精确地识别活动边界。该模型利用通道独立的Transformer编码器和解码器学习片断级别的表示，且所有的通道共享嵌入和Transformer权重，为健康管理及辅助生活等领域提供了实时理解活动信息的可能性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08214v2 Announce Type: replace  Abstract: Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of "patches", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on th
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于点云的机器人抓取轨迹优化方法，该方法通过将机器人表示为链接表面的3D点，以及使用深度传感器获得的点云来表示机器人任务空间。通过这种点云表示，抓取目标定位可以简化为点匹配问题，而碰撞回避则可通过查询机器人点在场景点 signed distance 场中的 signed distance 值来实现。这种方法通过对关节运动和抓取规划问题进行约束的非线性优化来解决。其主要创新之处在于该方法对任何类型的机器人和环境都具有普适性。文章通过在桌面和架子上进行的实验展示了该方法的有效性。</title><link>https://arxiv.org/abs/2403.05466</link><description>&lt;p&gt;
Grasping Trajectory Optimization with Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05466
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于点云的机器人抓取轨迹优化方法，该方法通过将机器人表示为链接表面的3D点，以及使用深度传感器获得的点云来表示机器人任务空间。通过这种点云表示，抓取目标定位可以简化为点匹配问题，而碰撞回避则可通过查询机器人点在场景点 signed distance 场中的 signed distance 值来实现。这种方法通过对关节运动和抓取规划问题进行约束的非线性优化来解决。其主要创新之处在于该方法对任何类型的机器人和环境都具有普适性。文章通过在桌面和架子上进行的实验展示了该方法的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05466v2 Announce Type: replace  Abstract: We introduce a new trajectory optimization method for robotic grasping based on a point-cloud representation of robots and task spaces. In our method, robots are represented by 3D points on their link surfaces. The task space of a robot is represented by a point cloud that can be obtained from depth sensors. Using the point-cloud representation, goal reaching in grasping can be formulated as point matching, while collision avoidance can be efficiently achieved by querying the signed distance values of the robot points in the signed distance field of the scene points. Consequently, a constrained nonlinear optimization problem is formulated to solve the joint motion and grasp planning problem. The advantage of our method is that the point-cloud representation is general to be used with any robot in any environment. We demonstrate the effectiveness of our method by performing experiments on a tabletop scene and a shelf scene for graspin
&lt;/p&gt;</description></item><item><title>该文章提出了一种高效的多模态语言模型，通过数据增强和扩展，使其具有了逐步理解和推理的能力。该模型能够从文档图像中逐步产生问题解答对，并通过 high-performance LLM 进行错误检测以过滤噪音数据。最终，该模型训练出了一个人类化的文档理解与推理系统，解决了现有模型直接生成答案而忽略证据及缺乏解释性的问题。</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
Read and Think: An Efficient Step-wise Multimodal Language Model for Document Understanding and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种高效的多模态语言模型，通过数据增强和扩展，使其具有了逐步理解和推理的能力。该模型能够从文档图像中逐步产生问题解答对，并通过 high-performance LLM 进行错误检测以过滤噪音数据。最终，该模型训练出了一个人类化的文档理解与推理系统，解决了现有模型直接生成答案而忽略证据及缺乏解释性的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v2 Announce Type: replace-cross  Abstract: Understanding the contents of multimodal documents is essential to accurately extract relevant evidence and use it for reasoning. Existing document understanding models tend to generate answers with a single word or phrase directly, ignoring the source document's evidence and lacking interpretability. In this work, we address the lack of step-wise capabilities through data augmentation and extension. Specifically, We use Multi-modal Large Language Models (MLLMs), which have strong visual understanding and reasoning abilities, as data generators to generate step-wise question-and-answer pairs for document images and use a high-performance LLM as the error detector to filter out noisy data. This step-wise data generation pipeline is implemented using both template-based and few-shot methods. We then use the generated high-quality data to train a humanized document understanding and reasoning model, specifically designed to solve 
&lt;/p&gt;</description></item><item><title>该文章的核心创新在于提出了一种名为EMO的框架，它使用了一种直接的音频-视频合成方法，无需中间的3D模型或面部特征点，能够增强说话和唱歌视频生成中的现实感和表达力。该方法确保了视频中帧的连续性以及身份的一致性，生成了高度逼真和自然的动画。实验结果表明，EMO在表达性和领先技术上均得到了显著提升。</title><link>https://arxiv.org/abs/2402.17485</link><description>&lt;p&gt;
EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17485
&lt;/p&gt;
&lt;p&gt;
该文章的核心创新在于提出了一种名为EMO的框架，它使用了一种直接的音频-视频合成方法，无需中间的3D模型或面部特征点，能够增强说话和唱歌视频生成中的现实感和表达力。该方法确保了视频中帧的连续性以及身份的一致性，生成了高度逼真和自然的动画。实验结果表明，EMO在表达性和领先技术上均得到了显著提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17485v2 Announce Type: replace  Abstract: In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of express
&lt;/p&gt;</description></item><item><title>该文章提出了一种双阶段框架，用于在大型互联网数据集上实现既能兼顾隐私又能取得高性能的签名语言翻译系统。</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种双阶段框架，用于在大型互联网数据集上实现既能兼顾隐私又能取得高性能的签名语言翻译系统。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v2 Announce Type: replace-cross  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experime
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RRWNet的端到端深度学习框架，用于改进对眼底彩色影像中动脉和静脉的自动分割和分类，尤其是通过递归修正策略解决了现有的自动化方法在分类和拓扑一致性方面的问题。</title><link>https://arxiv.org/abs/2402.03166</link><description>&lt;p&gt;
RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03166
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RRWNet的端到端深度学习框架，用于改进对眼底彩色影像中动脉和静脉的自动分割和分类，尤其是通过递归修正策略解决了现有的自动化方法在分类和拓扑一致性方面的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.03166v4 Announce Type: replace-cross  Abstract: The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions. A thorough analysis of the retinal vasculature requires the segmentation of the blood vessels and their classification into arteries and veins, typically performed on color fundus images obtained by retinography. However, manually performing these tasks is labor-intensive and prone to human error. While several automated methods have been proposed to address this task, the current state of art faces challenges due to manifest classification errors affecting the topological consistency of segmentation maps. In this work, we introduce RRWNet, a novel end-to-end deep learning framework that addresses this limitation. The framework consists of a fully convolutional neural network that recursively refines semantic segmentation maps, correcting manifest classification errors and thus improving topological 
&lt;/p&gt;</description></item><item><title>该文章提出了一种结合空间和频谱表示的深度学习方法，以增强医学图像分割的领域泛化能力。该方法通过引入新颖的谱相关系数目标函数，极大地提升了模型捕捉中尺度特征和上下文长程依赖的能力，有效解决了现有模型在处理同类图像内部差异性和不同类图像之间关联性不足的问题，从而提高了区分不同对象的能力，减少了误检情况的出现。通过优化该目标函数与现有架构（如）的结合，实验结果表明该方法在提升医学图像分割准确性和泛化能力方面取得了显著成效。</title><link>https://arxiv.org/abs/2401.10373</link><description>&lt;p&gt;
Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.10373
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种结合空间和频谱表示的深度学习方法，以增强医学图像分割的领域泛化能力。该方法通过引入新颖的谱相关系数目标函数，极大地提升了模型捕捉中尺度特征和上下文长程依赖的能力，有效解决了现有模型在处理同类图像内部差异性和不同类图像之间关联性不足的问题，从而提高了区分不同对象的能力，减少了误检情况的出现。通过优化该目标函数与现有架构（如）的结合，实验结果表明该方法在提升医学图像分割准确性和泛化能力方面取得了显著成效。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.10373v2 Announce Type: replace-cross  Abstract: Deep learning has demonstrated remarkable achievements in medical image segmentation. However, prevailing deep learning models struggle with poor generalization due to (i) intra-class variations, where the same class appears differently in different samples, and (ii) inter-class independence, resulting in difficulties capturing intricate relationships between distinct objects, leading to higher false negative cases. This paper presents a novel approach that synergies spatial and spectral representations to enhance domain-generalized medical image segmentation. We introduce the innovative Spectral Correlation Coefficient objective to improve the model's capacity to capture middle-order features and contextual long-range dependencies. This objective complements traditional spatial objectives by incorporating valuable spectral information. Extensive experiments reveal that optimizing this objective with existing architectures like
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SVIPTR的视觉可变提取器，用于快速且高效的场景文本识别。SVIPTR通过一种视觉-语义提取器，结合了自注意力的局部和全局层级，以及输入文本长度的动态调整，实现了在提高性能的同时保持了快速的推理速度。</title><link>https://arxiv.org/abs/2401.10110</link><description>&lt;p&gt;
SVIPTR: Fast and Efficient Scene Text Recognition with Vision Permutable Extractor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.10110
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SVIPTR的视觉可变提取器，用于快速且高效的场景文本识别。SVIPTR通过一种视觉-语义提取器，结合了自注意力的局部和全局层级，以及输入文本长度的动态调整，实现了在提高性能的同时保持了快速的推理速度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.10110v4 Announce Type: replace  Abstract: Scene Text Recognition (STR) is an important and challenging upstream task for building structured information databases, that involves recognizing text within images of natural scenes. Although current state-of-the-art (SOTA) models for STR exhibit high performance, they typically suffer from low inference efficiency due to their reliance on hybrid architectures comprised of visual encoders and sequence decoders. In this work, we propose a VIsion Permutable extractor for fast and efficient Scene Text Recognition (SVIPTR), which achieves an impressive balance between high performance and rapid inference speeds in the domain of STR. Specifically, SVIPTR leverages a visual-semantic extractor with a pyramid structure, characterized by the Permutation and combination of local and global self-attention layers. This design results in a lightweight and efficient model and its inference is insensitive to input length. Extensive experimental 
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为“双视图语义关系指导下的少样本图像识别数据幻化”的框架，它通过利用文本模态中的语义信息，对少样本图像识别中的数据幻化问题进行了有效解决，使得生成的新数据样本更加合理且多样化。</title><link>https://arxiv.org/abs/2401.07061</link><description>&lt;p&gt;
Dual-View Data Hallucination with Semantic Relation Guidance for Few-Shot Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07061
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为“双视图语义关系指导下的少样本图像识别数据幻化”的框架，它通过利用文本模态中的语义信息，对少样本图像识别中的数据幻化问题进行了有效解决，使得生成的新数据样本更加合理且多样化。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07061v2 Announce Type: replace  Abstract: Learning to recognize novel concepts from just a few image samples is very challenging as the learned model is easily overfitted on the few data and results in poor generalizability. One promising but underexplored solution is to compensate the novel classes by generating plausible samples. However, most existing works of this line exploit visual information only, rendering the generated data easy to be distracted by some challenging factors contained in the few available samples. Being aware of the semantic information in the textual modality that reflects human concepts, this work proposes a novel framework that exploits semantic relations to guide dual-view data hallucination for few-shot image recognition. The proposed framework enables generating more diverse and reasonable data samples for novel classes through effective information transfer from base classes. Specifically, an instance-view data hallucination module hallucinate
&lt;/p&gt;</description></item><item><title>该文章提出了DreamTalk框架，利用强大的扩散模型在生成情绪化的说话头像时实现了突破。它通过精心设计的组件，如 denoising 网络、风格指导的唇部专家和一个风格预测器，能够一致地生成高质量的情感驱动的面部动作，并且能够通过个性化的情绪指定来增强 lip-sync 过程的精确度和情感表达的丰富性。</title><link>https://arxiv.org/abs/2312.09767</link><description>&lt;p&gt;
DreamTalk: When Emotional Talking Head Generation Meets Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09767
&lt;/p&gt;
&lt;p&gt;
该文章提出了DreamTalk框架，利用强大的扩散模型在生成情绪化的说话头像时实现了突破。它通过精心设计的组件，如 denoising 网络、风格指导的唇部专家和一个风格预测器，能够一致地生成高质量的情感驱动的面部动作，并且能够通过个性化的情绪指定来增强 lip-sync 过程的精确度和情感表达的丰富性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09767v2 Announce Type: replace  Abstract: Emotional talking head generation has attracted growing attention. Previous methods, which are mainly GAN-based, still struggle to consistently produce satisfactory results across diverse emotions and cannot conveniently specify personalized emotions. In this work, we leverage powerful diffusion models to address the issue and propose DreamTalk, a framework that employs meticulous design to unlock the potential of diffusion models in generating emotional talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network can consistently synthesize high-quality audio-driven face motions across diverse emotions. To enhance lip-motion accuracy and emotional fullness, we introduce a style-aware lip expert that can guide lip-sync while preserving emotion intensity. To more conveniently specify personalized emotions, a diff
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的多标签胸病影像分类方案，名为MS-Twins，这是一种基于多尺度深度自注意力网络的方案。文章通过迭代融合多尺度信息，增强了对高概率病灶区域的关注，从而有效提取数据中的有价值信息。并且在只有图像级别标注的情况下，仅通过分类性能的提升。同时，文章还设计了一种新的损失函数，以进一步提升分类结果。</title><link>https://arxiv.org/abs/2312.07128</link><description>&lt;p&gt;
MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07128
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的多标签胸病影像分类方案，名为MS-Twins，这是一种基于多尺度深度自注意力网络的方案。文章通过迭代融合多尺度信息，增强了对高概率病灶区域的关注，从而有效提取数据中的有价值信息。并且在只有图像级别标注的情况下，仅通过分类性能的提升。同时，文章还设计了一种新的损失函数，以进一步提升分类结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07128v2 Announce Type: replace-cross  Abstract: Chest X-ray is one of the most common radiological examination types for the diagnosis of chest diseases. Nowadays, the automatic classification technology of radiological images has been widely used in clinical diagnosis and treatment plans. However, each disease has its own different response characteristic receptive field region, which is the main challenge for chest disease classification tasks. Besides, the imbalance of sample data categories further increases the difficulty of tasks. To solve these problems, we propose a new multi-label chest disease image classification scheme based on a multi-scale attention network. In this scheme, multi-scale information is iteratively fused to focus on regions with a high probability of disease, to effectively mine more meaningful information from data, and the classification performance can be improved only by image level annotation. We also designed a new loss function to improve t
&lt;/p&gt;</description></item><item><title>该文章提出一种名为Cascade-Zero123的框架，能够从单一图像中生成高度一致的多视角3D模型。通过两阶段的Zero-1-to-3模型，该框架首先生成一系列邻近视图，然后利用这些视图和源图像作为条件信息，对图像进行更深入的3D信息提取。这种自促进的方法显著增强了模型的视图一致性，尤其是在处理复杂对象时。</title><link>https://arxiv.org/abs/2312.04424</link><description>&lt;p&gt;
Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04424
&lt;/p&gt;
&lt;p&gt;
该文章提出一种名为Cascade-Zero123的框架，能够从单一图像中生成高度一致的多视角3D模型。通过两阶段的Zero-1-to-3模型，该框架首先生成一系列邻近视图，然后利用这些视图和源图像作为条件信息，对图像进行更深入的3D信息提取。这种自促进的方法显著增强了模型的视图一致性，尤其是在处理复杂对象时。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04424v2 Announce Type: replace  Abstract: Synthesizing multi-view 3D from one single image is a significant but challenging task. Zero-1-to-3 methods have achieved great success by lifting a 2D latent diffusion model to the 3D scope. The target view image is generated with a single-view source image and the camera pose as condition information. However, due to the high sparsity of the single input image, Zero-1-to-3 tends to produce geometry and appearance inconsistency across views, especially for complex objects. To tackle this issue, we propose to supply more condition information for the generation model but in a self-prompt way. A cascade framework is constructed with two Zero-1-to-3 models, named Cascade-Zero123, which progressively extract 3D information from the source image. Specifically, several nearby views are first generated by the first model and then fed into the second-stage model along with the source image as generation conditions. With amplified self-promp
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SPOC的架构，通过在模拟环境中模仿最短路径规划器来训练机器人代理，使其能够仅通过RGB传感器数据在现实世界中进行导航、探索和物体操作，并且不需要依赖深度地图或GPS坐标。这种训练方法使得机器人能够在实际环境中有效执行任务，这主要得益于大量的模拟数据、强有力的视觉编码器以及广泛的数据增强技术。</title><link>https://arxiv.org/abs/2312.02976</link><description>&lt;p&gt;
SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02976
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SPOC的架构，通过在模拟环境中模仿最短路径规划器来训练机器人代理，使其能够仅通过RGB传感器数据在现实世界中进行导航、探索和物体操作，并且不需要依赖深度地图或GPS坐标。这种训练方法使得机器人能够在实际环境中有效执行任务，这主要得益于大量的模拟数据、强有力的视觉编码器以及广泛的数据增强技术。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02976v2 Announce Type: replace  Abstract: Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents. RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks. While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive. In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates). This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path
&lt;/p&gt;</description></item><item><title>该文章提出的WoVoGen系统使用World Volume作为基础，结合4D世界体积信息，提出了一种控制多摄像头驾驶场景生成的方法，以确保生成的传感器数据既具有世界内部的一致性也有跨传感器的连贯性。</title><link>https://arxiv.org/abs/2312.02934</link><description>&lt;p&gt;
WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02934
&lt;/p&gt;
&lt;p&gt;
该文章提出的WoVoGen系统使用World Volume作为基础，结合4D世界体积信息，提出了一种控制多摄像头驾驶场景生成的方法，以确保生成的传感器数据既具有世界内部的一致性也有跨传感器的连贯性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02934v4 Announce Type: replace  Abstract: Generating multi-camera street-view videos is critical for augmenting autonomous driving datasets, addressing the urgent demand for extensive and varied data. Due to the limitations in diversity and challenges in handling lighting conditions, traditional rendering-based methods are increasingly being supplanted by diffusion-based methods. However, a significant challenge in diffusion-based methods is ensuring that the generated sensor data preserve both intra-world consistency and inter-sensor coherence. To address these challenges, we combine an additional explicit world volume and propose the World Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system is specifically designed to leverage 4D world volume as a foundational element for video generation. Our model operates in two distinct phases: (i) envisioning the future 4D temporal world volume based on vehicle control sequences, and (ii) generating multi-camera v
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为TPA3D的三角形注意力机制，用于快速文本到3D模型的转换，这通过GAN模型的改进提高了3D生成的速度和准确性，尽管缺少大规模的文本与3D数据对应的数据集。通过这种改进的模型，可以在不牺牲质量的条件下，大大加快文本到3D模型的转换过程。</title><link>https://arxiv.org/abs/2312.02647</link><description>&lt;p&gt;
TPA3D: Triplane Attention for Fast Text-to-3D Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02647
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为TPA3D的三角形注意力机制，用于快速文本到3D模型的转换，这通过GAN模型的改进提高了3D生成的速度和准确性，尽管缺少大规模的文本与3D数据对应的数据集。通过这种改进的模型，可以在不牺牲质量的条件下，大大加快文本到3D模型的转换过程。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02647v2 Announce Type: replace  Abstract: Due to the lack of large-scale text-3D correspondence data, recent text-to-3D generation works mainly rely on utilizing 2D diffusion models for synthesizing 3D data. Since diffusion-based methods typically require significant optimization time for both training and inference, the use of GAN-based models would still be desirable for fast 3D generation. In this work, we propose Triplane Attention for text-guided 3D generation (TPA3D), an end-to-end trainable GAN-based deep learning model for fast text-to-3D generation. With only 3D shape data and their rendered 2D images observed during training, our TPA3D is designed to retrieve detailed visual descriptions for synthesizing the corresponding 3D mesh data. This is achieved by the proposed attention mechanisms on the extracted sentence and word-level text features. In our experiments, we show that TPA3D generates high-quality 3D textured shapes aligned with fine-grained descriptions, wh
&lt;/p&gt;</description></item><item><title>该文章提出了一个新型的基于点云的渲染框架，能够实现真实的场景光照效果。通过将法线、BRDF参数以及多方向光照与3D高斯分布关联，优化后的点云能够根据不同方向的光线实时调整颜色和纹理。此外，该框架还采用了物理为基础的渲染和高效的点级光线追踪技术，可以在不增加成本的情况下为模型提供正确的阴影效果。通过与当前最佳方法进行对比，文章展示了其在BRDF估算、视图合成以及光照效果方面的显著改进。</title><link>https://arxiv.org/abs/2311.16043</link><description>&lt;p&gt;
Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16043
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个新型的基于点云的渲染框架，能够实现真实的场景光照效果。通过将法线、BRDF参数以及多方向光照与3D高斯分布关联，优化后的点云能够根据不同方向的光线实时调整颜色和纹理。此外，该框架还采用了物理为基础的渲染和高效的点级光线追踪技术，可以在不增加成本的情况下为模型提供正确的阴影效果。通过与当前最佳方法进行对比，文章展示了其在BRDF估算、视图合成以及光照效果方面的显著改进。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16043v2 Announce Type: replace  Abstract: In this paper, we present a novel differentiable point-based rendering framework to achieve photo-realistic relighting. To make the reconstructed scene relightable, we enhance vanilla 3D Gaussians by associating extra properties, including normal vectors, BRDF parameters, and incident lighting from various directions. From a collection of multi-view images, the 3D scene is optimized through 3D Gaussian Splatting while BRDF and lighting are decomposed by physically based differentiable rendering. To produce plausible shadow effects in photo-realistic relighting, we introduce an innovative point-based ray tracing with the bounding volume hierarchies for efficient visibility pre-computation. Extensive experiments demonstrate our improved BRDF estimation, novel view synthesis and relighting results compared to state-of-the-art approaches. The proposed framework showcases the potential to revolutionize the mesh-based graphics pipeline wit
&lt;/p&gt;</description></item><item><title>该文章指出，尽管医学图像生成模型的新趋势是采用专门针对医疗图像的feature extractors，但研究结果表明，基于ImageNet的feature extractors在评估合成图像质量时更可靠且与人类判断一致，尤其是在使用SwAV提取器时，其计算的Fr\'echet距离与专家评估结果有着显著的相关性。</title><link>https://arxiv.org/abs/2311.13717</link><description>&lt;p&gt;
Feature Extraction for Generative Medical Imaging Evaluation: New Evidence Against an Evolving Trend
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13717
&lt;/p&gt;
&lt;p&gt;
该文章指出，尽管医学图像生成模型的新趋势是采用专门针对医疗图像的feature extractors，但研究结果表明，基于ImageNet的feature extractors在评估合成图像质量时更可靠且与人类判断一致，尤其是在使用SwAV提取器时，其计算的Fr\'echet距离与专家评估结果有着显著的相关性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13717v4 Announce Type: replace  Abstract: Fr\'echet Inception Distance (FID) is a widely used metric for assessing synthetic image quality. It relies on an ImageNet-based feature extractor, making its applicability to medical imaging unclear. A recent trend is to adapt FID to medical imaging through feature extractors trained on medical images. Our study challenges this practice by demonstrating that ImageNet-based extractors are more consistent and aligned with human judgment than their RadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across four medical imaging modalities and four data augmentation techniques with Fr\'echet distances (FDs) computed using eleven ImageNet or RadImageNet-trained feature extractors. Comparison with human judgment via visual Turing tests revealed that ImageNet-based extractors produced rankings consistent with human judgment, with the FD derived from the ImageNet-trained SwAV extractor significantly correlating with expert eval
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为GMISeg的通用医学图像分割方法，无需重新训练即可实现医学图像的准确分割。该方法在保持性能的同时，有效减少了重新训练的时间和成本。</title><link>https://arxiv.org/abs/2311.12539</link><description>&lt;p&gt;
GMISeg: General Medical Image Segmentation without Re-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12539
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为GMISeg的通用医学图像分割方法，无需重新训练即可实现医学图像的准确分割。该方法在保持性能的同时，有效减少了重新训练的时间和成本。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12539v3 Announce Type: replace-cross  Abstract: The online shopping behavior has the characteristics of rich granularity dimension and data sparsity and previous researches on user behavior prediction did not seriously discuss feature selection and ensemble design. In this paper, we proposed a SE-Stacking model based on information fusion and ensemble learning for user purchase behavior prediction. After successfully utilizing the ensemble feature selection method to screen purchase-related factors, we used the Stacking algorithm for user purchase behavior prediction. In our efforts to avoid the deviation of prediction results, we optimized the model by selecting ten different kinds of models as base learners and modifying relevant parameters specifically for them. The experiments conducted on a publicly-available dataset shows that the SE-Stacking model can achieve a 98.40% F1-score, about 0.09% higher than the optimal base models. The SE-Stacking model not only has a good 
&lt;/p&gt;</description></item><item><title>该文章以自我监督的ViT模型为背景，提出了一个自我监督预训练特征的利用方法，能够在不事先知道图像中存在哪些物体的情况下，进行无监督的物体定位任务。</title><link>https://arxiv.org/abs/2310.12904</link><description>&lt;p&gt;
Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12904
&lt;/p&gt;
&lt;p&gt;
该文章以自我监督的ViT模型为背景，提出了一个自我监督预训练特征的利用方法，能够在不事先知道图像中存在哪些物体的情况下，进行无监督的物体定位任务。
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12904v3 Announce Type: replace  Abstract: The recent enthusiasm for open-world vision systems show the high interest of the community to perform perception tasks outside of the closed-vocabulary benchmark setups which have been so popular until now. Being able to discover objects in images/videos without knowing in advance what objects populate the dataset is an exciting prospect. But how to find objects without knowing anything about them? Recent works show that it is possible to perform class-agnostic unsupervised object localization by exploiting self-supervised pre-trained features. We propose here a survey of unsupervised object localization methods that discover objects in images without requiring any manual annotation in the era of self-supervised ViTs. We gather links of discussed methods in the repository https://github.com/valeoai/Awesome-Unsupervised-Object-Localization.
&lt;/p&gt;</description></item><item><title>该文章提出了一种在2D照片中基于3D结构指导的牙齿对齐网络，该网络能够将智能手机拍摄的照片中的牙齿自动对齐，以创造出一张具有改善后美观效果的牙齿排列的对比照片，从而帮助医生和患者进行更有效的沟通，并提高患者接受矫正治疗的可能性。</title><link>https://arxiv.org/abs/2310.11106</link><description>&lt;p&gt;
3D Structure-guided Network for Tooth Alignment in 2D Photograph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11106
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种在2D照片中基于3D结构指导的牙齿对齐网络，该网络能够将智能手机拍摄的照片中的牙齿自动对齐，以创造出一张具有改善后美观效果的牙齿排列的对比照片，从而帮助医生和患者进行更有效的沟通，并提高患者接受矫正治疗的可能性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.11106v2 Announce Type: replace  Abstract: Orthodontics focuses on rectifying misaligned teeth (i.e., malocclusions), affecting both masticatory function and aesthetics. However, orthodontic treatment often involves complex, lengthy procedures. As such, generating a 2D photograph depicting aligned teeth prior to orthodontic treatment is crucial for effective dentist-patient communication and, more importantly, for encouraging patients to accept orthodontic intervention. In this paper, we propose a 3D structure-guided tooth alignment network that takes 2D photographs as input (e.g., photos captured by smartphones) and aligns the teeth within the 2D image space to generate an orthodontic comparison photograph featuring aesthetically pleasing, aligned teeth. Notably, while the process operates within a 2D image space, our method employs 3D intra-oral scanning models collected in clinics to learn about orthodontic treatment, i.e., projecting the pre- and post-orthodontic 3D tooth
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SafeDreamer的算法，它利用Lagrangian方法结合世界模型规划，在Dreamer框架中实现了在复杂任务中的近零成本性能，尤其是对于低维度和纯视觉输入的任务，其在Safety-Gymnasium基准测试中展现了其在进行强化学习时既能保证性能又能确保安全的能力。</title><link>https://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
SafeDreamer: Safe Reinforcement Learning with World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SafeDreamer的算法，它利用Lagrangian方法结合世界模型规划，在Dreamer框架中实现了在复杂任务中的近零成本性能，尤其是对于低维度和纯视觉输入的任务，其在Safety-Gymnasium基准测试中展现了其在进行强化学习时既能保证性能又能确保安全的能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.07176v3 Announce Type: replace-cross  Abstract: The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details can be found i
&lt;/p&gt;</description></item><item><title>该文章全面介绍了深度学习中常用的损失函数和性能度量，适用于各种任务，包括回归、分类、计算机视觉及自然语言处理，旨在为研究者和实践者提供参考，帮助他们在深度学习项目选择合适的损失函数和度量方法。</title><link>https://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
Loss Functions and Metrics in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
该文章全面介绍了深度学习中常用的损失函数和性能度量，适用于各种任务，包括回归、分类、计算机视觉及自然语言处理，旨在为研究者和实践者提供参考，帮助他们在深度学习项目选择合适的损失函数和度量方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02694v3 Announce Type: replace-cross  Abstract: When training or evaluating deep learning models, two essential parts are picking the proper loss function and deciding on performance metrics. In this paper, we provide a comprehensive overview of the most common loss functions and metrics used across many different types of deep learning tasks, from general tasks such as regression and classification to more specific tasks in Computer Vision and Natural Language Processing. We introduce the formula for each loss and metric, discuss their strengths and limitations, and describe how these methods can be applied to various problems within deep learning. We hope this work serves as a reference for researchers and practitioners in the field, helping them make informed decisions when selecting the most appropriate loss function and performance metrics for their deep learning projects.
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为VATEX-EDIT的开放领域基准数据集，用于自动修订现有视频描述，以适应用户的多层次请求。用户请求被设计为由“操作、位置和属性”组成的精炼三元组，实现了从粗粒度到细粒度的用户需求覆盖。通过这种方式，该任务允许用户对视频描述进行动态编辑和修订，从而满足不同的用户意图。</title><link>https://arxiv.org/abs/2305.08389</link><description>&lt;p&gt;
Edit As You Wish: Video Caption Editing with Multi-grained User Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08389
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为VATEX-EDIT的开放领域基准数据集，用于自动修订现有视频描述，以适应用户的多层次请求。用户请求被设计为由“操作、位置和属性”组成的精炼三元组，实现了从粗粒度到细粒度的用户需求覆盖。通过这种方式，该任务允许用户对视频描述进行动态编辑和修订，从而满足不同的用户意图。
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08389v3 Announce Type: replace  Abstract: Automatically narrating videos in natural language complying with user requests, i.e. Controllable Video Captioning task, can help people manage massive videos with desired intentions. However, existing works suffer from two shortcomings: 1) the control signal is single-grained which can not satisfy diverse user intentions; 2) the video description is generated in a single round which can not be further edited to meet dynamic needs. In this paper, we propose a novel \textbf{V}ideo \textbf{C}aption \textbf{E}diting \textbf{(VCE)} task to automatically revise an existing video description guided by multi-grained user requests. Inspired by human writing-revision habits, we design the user command as a pivotal triplet \{\textit{operation, position, attribute}\} to cover diverse user needs from coarse-grained to fine-grained. To facilitate the VCE task, we \textit{automatically} construct an open-domain benchmark dataset named VATEX-EDIT 
&lt;/p&gt;</description></item><item><title>该文章介绍了利用深度卷积神经网络进行稀疏视角计算机断层扫描（CT）图像中的artifact（artifact，噪声）消除方法，并评估了这种方法对自动检测颅内出血的影响。通过训练U-Net网络对模拟的稀疏视角CT扫描进行artifact去除，并以此提高了自动化出血检测的准确性。</title><link>https://arxiv.org/abs/2303.09340</link><description>&lt;p&gt;
Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.09340
&lt;/p&gt;
&lt;p&gt;
该文章介绍了利用深度卷积神经网络进行稀疏视角计算机断层扫描（CT）图像中的artifact（artifact，噪声）消除方法，并评估了这种方法对自动检测颅内出血的影响。通过训练U-Net网络对模拟的稀疏视角CT扫描进行artifact去除，并以此提高了自动化出血检测的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.09340v4 Announce Type: replace-cross  Abstract: This is a preprint. The latest version has been published here: https://pubs.rsna.org/doi/10.1148/ryai.230275   Purpose: Sparse-view computed tomography (CT) is an effective way to reduce dose by lowering the total number of views acquired, albeit at the expense of image quality, which, in turn, can impact the ability to detect diseases. We explore deep learning-based artifact reduction in sparse-view cranial CT scans and its impact on automated hemorrhage detection. Methods: We trained a U-Net for artefact reduction on simulated sparse-view cranial CT scans from 3000 patients obtained from a public dataset and reconstructed with varying levels of sub-sampling. Additionally, we trained a convolutional neural network on fully sampled CT data from 17,545 patients for automated hemorrhage detection. We evaluated the classification performance using the area under the receiver operator characteristic curves (AUC-ROCs) with correspo
&lt;/p&gt;</description></item><item><title>该文章分析了基于深度学习的对象检测技术，并探讨了如何通过采用不对称感受野来提高对象检测的速度和准确性，还考虑了不同应用场景下的具体要求，提供了对现有技术的优化方案。</title><link>https://arxiv.org/abs/2303.08995</link><description>&lt;p&gt;
Fast and Accurate Object Detection on Asymmetrical Receptive Field
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.08995
&lt;/p&gt;
&lt;p&gt;
该文章分析了基于深度学习的对象检测技术，并探讨了如何通过采用不对称感受野来提高对象检测的速度和准确性，还考虑了不同应用场景下的具体要求，提供了对现有技术的优化方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.08995v2 Announce Type: replace  Abstract: Object detection has been used in a wide range of industries. For example, in autonomous driving, the task of object detection is to accurately and efficiently identify and locate a large number of predefined classes of object instances (vehicles, pedestrians, traffic signs, etc.) from videos of roads. In robotics, the industry robot needs to recognize specific machine elements. In the security field, the camera should accurately recognize each face of people. With the wide application of deep learning, the accuracy and efficiency of object detection have been greatly improved, but object detection based on deep learning still faces challenges. Different applications of object detection have different requirements, including highly accurate detection, multi-category object detection, real-time detection, robustness to occlusions, etc. To address the above challenges, based on extensive literature research, this paper analyzes methods
&lt;/p&gt;</description></item><item><title>该文章构建了一个含有真实色彩失真数据的立体视频数据库，并提出了一种基于深度学习的算法，能够高质量地纠正立体视视频中的色彩失真问题，提升了用户观看体验。</title><link>https://arxiv.org/abs/2303.06657</link><description>&lt;p&gt;
Color Mismatches in Stereoscopic Video: Real-World Dataset and Deep Correction Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.06657
&lt;/p&gt;
&lt;p&gt;
该文章构建了一个含有真实色彩失真数据的立体视频数据库，并提出了一种基于深度学习的算法，能够高质量地纠正立体视视频中的色彩失真问题，提升了用户观看体验。
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.06657v3 Announce Type: replace  Abstract: Stereoscopic videos can contain color mismatches between the left and right views due to minor variations in camera settings, lenses, and even object reflections captured from different positions. The presence of color mismatches can lead to viewer discomfort and headaches. This problem can be solved by transferring color between stereoscopic views, but traditional methods often lack quality, while neural-network-based methods can easily overfit on artificial data. The scarcity of stereoscopic videos with real-world color mismatches hinders the evaluation of different methods' performance. Therefore, we filmed a video dataset, which includes both distorted frames with color mismatches and ground-truth data, using a beam-splitter. Our second contribution is a deep multiscale neural network that solves the color-mismatch-correction task by leveraging stereo correspondences. The experimental results demonstrate the effectiveness of the 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Reference Twice (RefT)的统一框架，该框架在少样本实例分割（FSIS）任务中通过简单的跨注意力机制，利用支持示例与查询特征之间的关系，避免传统RPN方法的双重问题：过度拟合和复杂的空间相关策略。通过使用新的Transformer-based基础模型，RefT框架避免了过度拟合的问题，并展示了对查询特征的双重增强，即特征级别和查询级别的增强，从而不需要复杂的空间相关策略。此外，RefT框架通过在基础训练后发现的支持对象查询中编码关键因素，实现了对查询特征的增强，从而在FSIS和相关任务中提供了一种新的简单而统一的基准方法。</title><link>https://arxiv.org/abs/2301.01156</link><description>&lt;p&gt;
Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.01156
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Reference Twice (RefT)的统一框架，该框架在少样本实例分割（FSIS）任务中通过简单的跨注意力机制，利用支持示例与查询特征之间的关系，避免传统RPN方法的双重问题：过度拟合和复杂的空间相关策略。通过使用新的Transformer-based基础模型，RefT框架避免了过度拟合的问题，并展示了对查询特征的双重增强，即特征级别和查询级别的增强，从而不需要复杂的空间相关策略。此外，RefT框架通过在基础训练后发现的支持对象查询中编码关键因素，实现了对查询特征的增强，从而在FSIS和相关任务中提供了一种新的简单而统一的基准方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.01156v3 Announce Type: replace  Abstract: Few-Shot Instance Segmentation (FSIS) requires detecting and segmenting novel classes with limited support examples. Existing methods based on Region Proposal Networks (RPNs) face two issues: 1) Overfitting suppresses novel class objects; 2) Dual-branch models require complex spatial correlation strategies to prevent spatial information loss when generating class prototypes. We introduce a unified framework, Reference Twice (RefT), to exploit the relationship between support and query features for FSIS and related tasks. Our three main contributions are: 1) A novel transformer-based baseline that avoids overfitting, offering a new direction for FSIS; 2) Demonstrating that support object queries encode key factors after base training, allowing query features to be enhanced twice at both feature and query levels using simple cross-attention, thus avoiding complex spatial correlation interaction; 3) Introducing a class-enhanced base kno
&lt;/p&gt;</description></item><item><title>该文章研究了在深度伪造视频中检测行为标志的创新方法，能够通过控制视觉外观并转移行为信号来自其他源的方式，来区分一个人与其他说话者的行为标志。</title><link>https://arxiv.org/abs/2208.03561</link><description>&lt;p&gt;
Study of detecting behavioral signatures within DeepFake videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.03561
&lt;/p&gt;
&lt;p&gt;
该文章研究了在深度伪造视频中检测行为标志的创新方法，能够通过控制视觉外观并转移行为信号来自其他源的方式，来区分一个人与其他说话者的行为标志。
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.03561v2 Announce Type: replace  Abstract: There is strong interest in the generation of synthetic video imagery of people talking for various purposes, including entertainment, communication, training, and advertisement. With the development of deep fake generation models, synthetic video imagery will soon be visually indistinguishable to the naked eye from a naturally capture video. In addition, many methods are continuing to improve to avoid more careful, forensic visual analysis. Some deep fake videos are produced through the use of facial puppetry, which directly controls the head and face of the synthetic image through the movements of the actor, allow the actor to 'puppet' the image of another. In this paper, we address the question of whether one person's movements can be distinguished from the original speaker by controlling the visual appearance of the speaker but transferring the behavior signals from another source. We conduct a study by comparing synthetic imager
&lt;/p&gt;</description></item><item><title>该文章提出了一种新型的U-Attention Vision Transformer，用于实现对多种类型纹理的无监督合成。该模型通过使用自注意力机制来捕捉纹理中的长期依赖关系，能在一次推理过程中合成出具有特定结构的不同纹理。论文中的模型架构包括一个层次化的“hourglass”网络，它能够在一系列分辨率的架构中分别关注纹理的宏观结构和微观细节，并且在细化合成结果的过程中实现从粗到精再到粗的多次操作。通过在不同的尺度上使用信息传播和融合的设计，该模型有效地结合了对纹理特征的注意力处理，并在结构和细节上实现了更好的合成效果。实验结果表明，该模型在纹理合成任务上取得了显著的性能提升，并且没有进行任何形式的微调。</title><link>https://arxiv.org/abs/2202.11703</link><description>&lt;p&gt;
Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal Texture Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.11703
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新型的U-Attention Vision Transformer，用于实现对多种类型纹理的无监督合成。该模型通过使用自注意力机制来捕捉纹理中的长期依赖关系，能在一次推理过程中合成出具有特定结构的不同纹理。论文中的模型架构包括一个层次化的“hourglass”网络，它能够在一系列分辨率的架构中分别关注纹理的宏观结构和微观细节，并且在细化合成结果的过程中实现从粗到精再到粗的多次操作。通过在不同的尺度上使用信息传播和融合的设计，该模型有效地结合了对纹理特征的注意力处理，并在结构和细节上实现了更好的合成效果。实验结果表明，该模型在纹理合成任务上取得了显著的性能提升，并且没有进行任何形式的微调。
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.11703v3 Announce Type: replace  Abstract: We present a novel U-Attention vision Transformer for universal texture synthesis. We exploit the natural long-range dependencies enabled by the attention mechanism to allow our approach to synthesize diverse textures while preserving their structures in a single inference. We propose a hierarchical hourglass backbone that attends to the global structure and performs patch mapping at varying scales in a coarse-to-fine-to-coarse stream. Completed by skip connection and convolution designs that propagate and fuse information at different scales, our hierarchical U-Attention architecture unifies attention to features from macro structures to micro details, and progressively refines synthesis results at successive stages. Our method achieves stronger 2$\times$ synthesis than previous work on both stochastic and structured textures while generalizing to unseen textures without fine-tuning. Ablation studies demonstrate the effectiveness of
&lt;/p&gt;</description></item><item><title>该文章总结了近年来在半监督学习领域中提出的深入半监督学习方法及相关研究。它解释了半监督学习中常用的关键假设，包括流形假设、簇假设和连续性假设，并对使用深度神经网络进行半监督学习的最新方法进行了重点讨论。文章通过综合分类和解释现有研究，提供了一种全面的视角来了解半监督学习的发展趋势和应用潜力。</title><link>https://arxiv.org/abs/2106.11528</link><description>&lt;p&gt;
Recent Deep Semi-supervised Learning Approaches and Related Works
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.11528
&lt;/p&gt;
&lt;p&gt;
该文章总结了近年来在半监督学习领域中提出的深入半监督学习方法及相关研究。它解释了半监督学习中常用的关键假设，包括流形假设、簇假设和连续性假设，并对使用深度神经网络进行半监督学习的最新方法进行了重点讨论。文章通过综合分类和解释现有研究，提供了一种全面的视角来了解半监督学习的发展趋势和应用潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.11528v3 Announce Type: replace-cross  Abstract: This work proposes an overview of the recent semi-supervised learning approaches and related works. Despite the remarkable success of neural networks in various applications, there exist a few formidable constraints, including the need for a large amount of labeled data. Therefore, semi-supervised learning, which is a learning scheme in which scarce labels and a larger amount of unlabeled data are utilized to train models (e.g., deep neural networks), is getting more important. Based on the key assumptions of semi-supervised learning, which are the manifold assumption, cluster assumption, and continuity assumption, the work reviews the recent semi-supervised learning approaches. In particular, the methods in regard to using deep neural networks in a semi-supervised learning setting are primarily discussed. In addition, the existing works are first classified based on the underlying idea and explained, then the holistic approach
&lt;/p&gt;</description></item></channel></rss>