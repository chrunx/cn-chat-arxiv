<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.CV.xml</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;LLaVA-OneVision&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21333;&#22270;&#20687;&#12289;&#22810;&#22270;&#20687;&#21644;&#35270;&#39057;&#22330;&#26223;&#20013;&#21516;&#26102;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23637;&#29616;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23558;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#25104;&#21151;&#36801;&#31227;&#21040;&#35270;&#39057;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.03326</link><description>&lt;p&gt;
LLaVA-OneVision: Easy Visual Task Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03326
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;LLaVA-OneVision&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21333;&#22270;&#20687;&#12289;&#22810;&#22270;&#20687;&#21644;&#35270;&#39057;&#22330;&#26223;&#20013;&#21516;&#26102;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23637;&#29616;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23558;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#25104;&#21151;&#36801;&#31227;&#21040;&#35270;&#39057;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03326v1 Announce Type: new  Abstract: We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Segment Anything Model 2&#65288;SAM2&#65289;&#30340;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#36890;&#36807;&#24555;&#36895;&#24494;&#35843;&#24555;&#36895;&#36866;&#24212;&#21307;&#23398;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#21307;&#23398;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;3D&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03322</link><description>&lt;p&gt;
Segment Anything in Medical Images and Videos: Benchmark and Deployment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Segment Anything Model 2&#65288;SAM2&#65289;&#30340;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#36890;&#36807;&#24555;&#36895;&#24494;&#35843;&#24555;&#36895;&#36866;&#24212;&#21307;&#23398;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#21307;&#23398;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;3D&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03322v1 Announce Type: cross  Abstract: Recent advances in segmentation foundation models have enabled accurate and efficient segmentation across a wide range of natural images and videos, but their utility to medical data remains unclear. In this work, we first present a comprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11 medical image modalities and videos and point out its strengths and weaknesses by comparing it to SAM1 and MedSAM. Then, we develop a transfer learning pipeline and demonstrate SAM2 can be quickly adapted to medical domain by fine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio API for efficient 3D image and video segmentation. The code has been made publicly available at \url{https://github.com/bowang-lab/MedSAM}.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDT-A2G&#30340;&#26032;&#22411;Masked Diffusion Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#27493;&#25163;&#21183;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Masked Diffusion Transformer&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;CNNs&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#27169;&#24335;&#25513;&#30721;&#26469;&#21152;&#24378;&#21160;&#20316;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20851;&#32852;&#23398;&#20064;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#21516;&#27493;&#25163;&#21183;&#29983;&#25104;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#23454;&#26045;&#20110;&#25163;&#21183;&#24207;&#21015;&#30340;&#36864;&#28779;&#36807;&#31243;&#26469;&#25552;&#39640;&#25972;&#20010;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.03312</link><description>&lt;p&gt;
MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDT-A2G&#30340;&#26032;&#22411;Masked Diffusion Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#27493;&#25163;&#21183;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Masked Diffusion Transformer&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;CNNs&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#27169;&#24335;&#25513;&#30721;&#26469;&#21152;&#24378;&#21160;&#20316;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20851;&#32852;&#23398;&#20064;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#21516;&#27493;&#25163;&#21183;&#29983;&#25104;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#23454;&#26045;&#20110;&#25163;&#21183;&#24207;&#21015;&#30340;&#36864;&#28779;&#36807;&#31243;&#26469;&#25552;&#39640;&#25972;&#20010;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03312v1 Announce Type: new  Abstract: Recent advancements in the field of Diffusion Transformers have substantially improved the generation of high-quality 2D images, 3D videos, and 3D shapes. However, the effectiveness of the Transformer architecture in the domain of co-speech gesture generation remains relatively unexplored, as prior methodologies have predominantly employed the Convolutional Neural Network (CNNs) or simple a few transformer layers. In an attempt to bridge this research gap, we introduce a novel Masked Diffusion Transformer for co-speech gesture generation, referred to as MDT-A2G, which directly implements the denoising process on gesture sequences. To enhance the contextual reasoning capability of temporally aligned speech-driven gestures, we incorporate a novel Masked Diffusion Transformer. This model employs a mask modeling scheme specifically designed to strengthen temporal relation learning among sequence gestures, thereby expediting the learning proc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20154;&#31867;&#25351;&#23548;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#36755;&#20837;&#30340;&#38656;&#27714;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#27880;&#30340;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.03304</link><description>&lt;p&gt;
Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20154;&#31867;&#25351;&#23548;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#36755;&#20837;&#30340;&#38656;&#27714;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#27880;&#30340;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03304v1 Announce Type: new  Abstract: Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improveme
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TextIM&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#20132;&#20114;&#36523;&#20307;&#37096;&#20301;&#21644;&#20132;&#20114;&#21547;&#20041;&#23545;&#40784;&#65292;&#29983;&#25104;&#26356;&#21152;&#31934;&#30830;&#21644;&#33258;&#28982;&#30340;&#20132;&#20114;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2408.03302</link><description>&lt;p&gt;
TextIM: Part-aware Interactive Motion Synthesis from Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TextIM&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#20132;&#20114;&#36523;&#20307;&#37096;&#20301;&#21644;&#20132;&#20114;&#21547;&#20041;&#23545;&#40784;&#65292;&#29983;&#25104;&#26356;&#21152;&#31934;&#30830;&#21644;&#33258;&#28982;&#30340;&#20132;&#20114;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03302v1 Announce Type: new  Abstract: In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics. Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes. To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions. Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion. Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion. We design a spatial co
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DopQ-ViT&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;TanQ&#30340;&#20998;&#24067;&#21451;&#22909;&#30340;Tan&#37327;&#21270;&#22120;&#65292;&#25913;&#21892;&#20102;&#23545;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65292;&#20854;&#29305;&#21035;&#20851;&#27880;&#25509;&#36817;1&#30340;&#20540;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20445;&#30041;&#20102;&#21518;Softmax&#28608;&#27963;&#30340;&#24130;&#24459;&#20998;&#24067;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#65292;&#20419;&#36827;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#22823;&#33539;&#22260;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.03291</link><description>&lt;p&gt;
DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DopQ-ViT&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;TanQ&#30340;&#20998;&#24067;&#21451;&#22909;&#30340;Tan&#37327;&#21270;&#22120;&#65292;&#25913;&#21892;&#20102;&#23545;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65292;&#20854;&#29305;&#21035;&#20851;&#27880;&#25509;&#36817;1&#30340;&#20540;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20445;&#30041;&#20102;&#21518;Softmax&#28608;&#27963;&#30340;&#24130;&#24459;&#20998;&#24067;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#65292;&#20419;&#36827;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#22823;&#33539;&#22260;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03291v1 Announce Type: new  Abstract: Vision transformers (ViTs) have garnered significant attention for their performance in vision tasks; however, the high computational cost and significant latency issues have hinder widespread adoption. Post-training quantization (PTQ), a promising method for model compression, still faces accuracy degradation challenges with ViTs. There are two reasons for this: the existing quantization paradigm does not fit the power-law distribution of post-Softmax activations well, and accuracy inevitably decreases after reparameterizing post-LayerNorm activations. We propose a Distribution-Friendly and Outlier-Aware Post-training Quantization method for Vision Transformers, named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more on values near 1, more accurately preserving the power-law distribution of post-Softmax activations, and achieves favora
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BioSAM 2&#30340;&#22686;&#24378;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#21270;&#33258;SAM 2&#65292;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#35774;&#35745;&#65292;&#24182;&#22312;&#21333;&#24103;&#22270;&#20687;&#21644;&#22810;&#24103;&#35270;&#39057;&#20998;&#21106;&#20013;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#19982;&#19987;&#38376;&#27169;&#22411;&#30456;&#21305;&#25932;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#25928;&#33021;&#21644;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.03286</link><description>&lt;p&gt;
Biomedical SAM 2: Segment Anything in Biomedical Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BioSAM 2&#30340;&#22686;&#24378;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#21270;&#33258;SAM 2&#65292;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#35774;&#35745;&#65292;&#24182;&#22312;&#21333;&#24103;&#22270;&#20687;&#21644;&#22810;&#24103;&#35270;&#39057;&#20998;&#21106;&#20013;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#19982;&#19987;&#38376;&#27169;&#22411;&#30456;&#21305;&#25932;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#25928;&#33021;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03286v1 Announce Type: new  Abstract: Medical image segmentation and video object segmentation are essential for diagnosing and analyzing diseases by identifying and measuring biological structures. Recent advances in natural domain have been driven by foundation models like the Segment Anything Model 2 (SAM 2). To explore the performance of SAM 2 in biomedical applications, we designed two evaluation pipelines for single-frame image segmentation and multi-frame video segmentation with varied prompt designs, revealing SAM 2's limitations in medical contexts. Consequently, we developed BioSAM 2, an enhanced foundation model optimized for biomedical data based on SAM 2. Our experiments show that BioSAM 2 not only surpasses the performance of existing state-of-the-art foundation models but also matches or even exceeds specialist models, demonstrating its efficacy and potential in the medical domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ReSyncer&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#38899;&#39057;&#21644;&#35270;&#39057;&#20013;&#30340;&#38754;&#37096;&#20449;&#24687;&#21516;&#27493;&#12290;&#36890;&#36807;&#37325;&#26500;&#39118;&#26684;&#29983;&#25104;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#19982;3D&#38754;&#37096;&#21160;&#24577;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#25991;&#31456;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#30340;&#38899;&#39057;&#33267;&#35270;&#39057;&#38754;&#37096;&#21516;&#27493;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#25903;&#25345;&#22810;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03284</link><description>&lt;p&gt;
ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ReSyncer&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#38899;&#39057;&#21644;&#35270;&#39057;&#20013;&#30340;&#38754;&#37096;&#20449;&#24687;&#21516;&#27493;&#12290;&#36890;&#36807;&#37325;&#26500;&#39118;&#26684;&#29983;&#25104;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#19982;3D&#38754;&#37096;&#21160;&#24577;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#25991;&#31456;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#30340;&#38899;&#39057;&#33267;&#35270;&#39057;&#38754;&#37096;&#21516;&#27493;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#25903;&#25345;&#22810;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03284v1 Announce Type: new  Abstract: Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers. While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts. In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information. The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer. By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training. Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing prope
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AMES&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#27493;&#21644;&#35760;&#24518;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#20363;&#32423;&#21035;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#25345;&#36739;&#20302;&#20869;&#23384;&#28040;&#32791;&#65288;&#27599;&#20010;&#22270;&#20687;&#20165;&#20351;&#29992;1KB&#65289;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;AMES&#27169;&#22411;&#36824;&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#21363;&#36827;&#34892;&#24322;&#27493;&#30456;&#20284;&#24615;&#20272;&#35745;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#26412;&#22320;&#25551;&#36848;&#31526;&#25968;&#37327;&#26469;&#23454;&#29616;&#30340;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#19981;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#36866;&#24212;&#19981;&#21516;&#24212;&#29992;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#33021;&#22815;&#28789;&#27963;&#35843;&#25972;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#26412;&#22320;&#25551;&#36848;&#31526;&#25968;&#37327;&#12290;](/backtranslated)</title><link>https://arxiv.org/abs/2408.03282</link><description>&lt;p&gt;
AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AMES&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#27493;&#21644;&#35760;&#24518;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#20363;&#32423;&#21035;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#25345;&#36739;&#20302;&#20869;&#23384;&#28040;&#32791;&#65288;&#27599;&#20010;&#22270;&#20687;&#20165;&#20351;&#29992;1KB&#65289;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;AMES&#27169;&#22411;&#36824;&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#21363;&#36827;&#34892;&#24322;&#27493;&#30456;&#20284;&#24615;&#20272;&#35745;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#26412;&#22320;&#25551;&#36848;&#31526;&#25968;&#37327;&#26469;&#23454;&#29616;&#30340;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#19981;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#36866;&#24212;&#19981;&#21516;&#24212;&#29992;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#33021;&#22815;&#28789;&#27963;&#35843;&#25972;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#26412;&#22320;&#25551;&#36848;&#31526;&#25968;&#37327;&#12290;](/backtranslated)
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03282v1 Announce Type: new  Abstract: This work investigates the problem of instance-level image retrieval re-ranking with the constraint of memory efficiency, ultimately aiming to limit memory usage to 1KB per image. Departing from the prevalent focus on performance enhancements, this work prioritizes the crucial trade-off between performance and memory requirements. The proposed model uses a transformer-based architecture designed to estimate image-to-image similarity by capturing interactions within and across images based on their local descriptors. A distinctive property of the model is the capability for asymmetric similarity estimation. Database images are represented with a smaller number of descriptors compared to query images, enabling performance improvements without increasing memory consumption. To ensure adaptability across different applications, a universal model is introduced that adjusts to a varying number of local descriptors during the testing phase. Res
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAC-Net&#30340;&#32447;&#24615;&#34701;&#21512;&#27880;&#24847;&#21147;&#25351;&#23548;&#21367;&#31215;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;RGB&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#26469;&#33258;&#28145;&#24230;&#22270;&#20687;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23545;&#36974;&#25377;&#22330;&#26223;&#20013;&#30446;&#26631;&#23545;&#35937;&#30340;&#20934;&#30830;&#25235;&#21462;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.03238</link><description>&lt;p&gt;
LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03238
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAC-Net&#30340;&#32447;&#24615;&#34701;&#21512;&#27880;&#24847;&#21147;&#25351;&#23548;&#21367;&#31215;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;RGB&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#26469;&#33258;&#28145;&#24230;&#22270;&#20687;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23545;&#36974;&#25377;&#22330;&#26223;&#20013;&#30446;&#26631;&#23545;&#35937;&#30340;&#20934;&#30830;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03238v1 Announce Type: new  Abstract: This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-m
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MoCo v2&#26694;&#26550;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;CLIC&#65289;&#65292;&#29992;&#20110;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#21363;&#21487;&#23398;&#20064;&#22270;&#20687;&#22797;&#26434;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21457;&#29616;&#22270;&#20687;&#19981;&#21516;&#23616;&#37096;&#21306;&#22495;&#23384;&#22312;&#22797;&#26434;&#24615;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#38543;&#26426;&#35009;&#21098;&#21644;&#28151;&#21512;&#31574;&#30053;&#65288;RCM&#65289;&#26469;&#20135;&#29983;&#22810;&#23610;&#24230;&#23616;&#37096;crop&#30340;&#23545;&#27604;&#26679;&#26412;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#35757;&#32451;&#38598;&#24182;&#22686;&#24378;&#20102;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;CLIC&#30456;&#27604;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03230</link><description>&lt;p&gt;
Contrastive Learning for Image Complexity Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MoCo v2&#26694;&#26550;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;CLIC&#65289;&#65292;&#29992;&#20110;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#21363;&#21487;&#23398;&#20064;&#22270;&#20687;&#22797;&#26434;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21457;&#29616;&#22270;&#20687;&#19981;&#21516;&#23616;&#37096;&#21306;&#22495;&#23384;&#22312;&#22797;&#26434;&#24615;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#38543;&#26426;&#35009;&#21098;&#21644;&#28151;&#21512;&#31574;&#30053;&#65288;RCM&#65289;&#26469;&#20135;&#29983;&#22810;&#23610;&#24230;&#23616;&#37096;crop&#30340;&#23545;&#27604;&#26679;&#26412;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#35757;&#32451;&#38598;&#24182;&#22686;&#24378;&#20102;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;CLIC&#30456;&#27604;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03230v1 Announce Type: new  Abstract: Quantifying and evaluating image complexity can be instrumental in enhancing the performance of various computer vision tasks. Supervised learning can effectively learn image complexity features from well-annotated datasets. However, creating such datasets requires expensive manual annotation costs. The models may learn human subjective biases from it. In this work, we introduce the MoCo v2 framework. We utilize contrastive learning to represent image complexity, named CLIC (Contrastive Learning for Image Complexity). We find that there are complexity differences between different local regions of an image, and propose Random Crop and Mix (RCM), which can produce positive samples consisting of multi-scale local crops. RCM can also expand the train set and increase data diversity without introducing additional data. We conduct extensive experiments with CLIC, comparing it with both unsupervised and supervised methods. The results demonstr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30456;&#26426;&#30340;6&#33258;&#30001;&#24230;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#32447;&#20449;&#24687;&#26469;&#20272;&#35745;&#21644;&#36319;&#36394;&#24179;&#38754;&#25110;&#38750;&#24179;&#38754;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#21160;&#24577;&#33539;&#22260;&#22330;&#26223;&#21644;&#39640;&#36895;&#24230;&#36816;&#21160;&#65292;&#20026;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.03225</link><description>&lt;p&gt;
Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30456;&#26426;&#30340;6&#33258;&#30001;&#24230;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#32447;&#20449;&#24687;&#26469;&#20272;&#35745;&#21644;&#36319;&#36394;&#24179;&#38754;&#25110;&#38750;&#24179;&#38754;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#21160;&#24577;&#33539;&#22260;&#22330;&#26223;&#21644;&#39640;&#36895;&#24230;&#36816;&#21160;&#65292;&#20026;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03225v1 Announce Type: new  Abstract: Pose estimation and tracking of objects is a fundamental application in 3D vision. Event cameras possess remarkable attributes such as high dynamic range, low latency, and resilience against motion blur, which enables them to address challenging high dynamic range scenes or high-speed motion. These features make event cameras an ideal complement over standard cameras for object pose estimation. In this work, we propose a line-based robust pose estimation and tracking method for planar or non-planar objects using an event camera. Firstly, we extract object lines directly from events, then provide an initial pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line correspondences are not known in advance. Subsequently, we utilize event-line matching to establish correspondences between 2D events and 3D models. Furthermore, object poses are refined and continuously tracked by minimizing event-line distances. Events are assi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20351;&#20248;&#21270;&#22120;&#20803;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#23398;</title><link>https://arxiv.org/abs/2408.03219</link><description>&lt;p&gt;
Learning to Learn without Forgetting using Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20351;&#20248;&#21270;&#22120;&#20803;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#23398;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03219v1 Announce Type: cross  Abstract: Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchma
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;IPAdapter-Instruct&#65292;&#19968;&#31181;&#32467;&#21512;&#33258;&#28982;&#22270;&#20687;&#26465;&#20214;&#21644;&#8220;Instruct&#8221;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21516;&#19968;&#27969;&#31243;&#20013;&#28789;&#27963;&#20999;&#25442;&#30456;&#21516;&#26465;&#20214;&#22270;&#20687;&#30340;&#19981;&#21516;&#35299;&#37322;&#65292;&#21253;&#25324;&#39118;&#26684;&#36716;&#25442;&#12289;&#23545;&#35937;&#25552;&#21462;&#31561;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.03209</link><description>&lt;p&gt;
IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;IPAdapter-Instruct&#65292;&#19968;&#31181;&#32467;&#21512;&#33258;&#28982;&#22270;&#20687;&#26465;&#20214;&#21644;&#8220;Instruct&#8221;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21516;&#19968;&#27969;&#31243;&#20013;&#28789;&#27963;&#20999;&#25442;&#30456;&#21516;&#26465;&#20214;&#22270;&#20687;&#30340;&#19981;&#21516;&#35299;&#37322;&#65292;&#21253;&#25324;&#39118;&#26684;&#36716;&#25442;&#12289;&#23545;&#35937;&#25552;&#21462;&#31561;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03209v1 Announce Type: new  Abstract: Diffusion models continuously push the boundary of state-of-the-art image generation, but the process is hard to control with any nuance: practice proves that textual prompts are inadequate for accurately describing image style or fine structural details (such as faces). ControlNet and IPAdapter address this shortcoming by conditioning the generative process on imagery instead, but each individual instance is limited to modeling a single conditional posterior: for practical use-cases, where multiple different posteriors are desired within the same workflow, training and using multiple adapters is cumbersome. We propose IPAdapter-Instruct, which combines natural-image conditioning with ``Instruct'' prompts to swap between interpretations for the same conditioning image: style transfer, object extraction, both, or something else still? IPAdapterInstruct efficiently learns multiple tasks with minimal loss in quality compared to dedicated pe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFedSIS&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20808;&#39564;&#65292;&#32467;&#21512;&#20840;&#23616;&#20010;&#24615;&#21270; disentanglement&#12289;&#22806;&#35266;&#35843;&#33410;&#20010;&#24615;&#21270;&#22686;&#24378;&#21644;&#24418;&#29366;&#30456;&#20284;&#24615;&#20840;&#23616;&#22686;&#24378;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#22806;&#35266;&#22810;&#26679;&#24615;&#19982;&#22120;&#26800;&#24418;&#29366;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22836;&#26435;&#37325;&#20010;&#24615;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#35757;&#32451;&#31449;&#28857;&#29305;&#24449;&#30340;&#31934;&#30830;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#29420;&#31435;&#31449;&#28857;&#19978;&#30340;&#22120;&#26800;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03208</link><description>&lt;p&gt;
Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFedSIS&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20808;&#39564;&#65292;&#32467;&#21512;&#20840;&#23616;&#20010;&#24615;&#21270; disentanglement&#12289;&#22806;&#35266;&#35843;&#33410;&#20010;&#24615;&#21270;&#22686;&#24378;&#21644;&#24418;&#29366;&#30456;&#20284;&#24615;&#20840;&#23616;&#22686;&#24378;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#22806;&#35266;&#22810;&#26679;&#24615;&#19982;&#22120;&#26800;&#24418;&#29366;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22836;&#26435;&#37325;&#20010;&#24615;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#35757;&#32451;&#31449;&#28857;&#29305;&#24449;&#30340;&#31934;&#30830;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#29420;&#31435;&#31449;&#28857;&#19978;&#30340;&#22120;&#26800;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03208v1 Announce Type: cross  Abstract: Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#25552;&#20986;&#30340;&#31354;&#38388;&#39057;&#29575;&#32852;&#21512;&#26597;&#35810;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;SGSR&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23545;&#27604;&#24230;MRI&#36229;&#32423;&#20998;&#36776;&#29575;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#30340;&#32467;&#26500;&#26597;&#35810;&#25552;&#21462;&#12289;&#34701;&#21512;&#21644;&#31934;&#28860;&#22810;&#23545;&#27604;&#24230;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#20102;&#22810;&#23545;&#27604;&#24230;MRI&#20013;&#30340;&#32467;&#26500;&#21644;&#23545;&#27604;&#24230;&#19981;&#21464;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25991;&#31456;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#22914;&#20309;&#26356;&#22909;&#22320;&#32467;&#21512;&#32467;&#26500;&#20449;&#24687;&#21644;&#23545;&#27604;&#24230;&#20449;&#24687;&#65292;&#20197;&#25552;&#21319;MRI&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#30001;&#20110;&#36816;&#21160;&#24341;&#20837;&#30340; artifacts&#12290;</title><link>https://arxiv.org/abs/2408.03194</link><description>&lt;p&gt;
SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via Spatio-Frequency Co-Query Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#25552;&#20986;&#30340;&#31354;&#38388;&#39057;&#29575;&#32852;&#21512;&#26597;&#35810;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;SGSR&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23545;&#27604;&#24230;MRI&#36229;&#32423;&#20998;&#36776;&#29575;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#30340;&#32467;&#26500;&#26597;&#35810;&#25552;&#21462;&#12289;&#34701;&#21512;&#21644;&#31934;&#28860;&#22810;&#23545;&#27604;&#24230;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#20102;&#22810;&#23545;&#27604;&#24230;MRI&#20013;&#30340;&#32467;&#26500;&#21644;&#23545;&#27604;&#24230;&#19981;&#21464;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25991;&#31456;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#22914;&#20309;&#26356;&#22909;&#22320;&#32467;&#21512;&#32467;&#26500;&#20449;&#24687;&#21644;&#23545;&#27604;&#24230;&#20449;&#24687;&#65292;&#20197;&#25552;&#21319;MRI&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#30001;&#20110;&#36816;&#21160;&#24341;&#20837;&#30340; artifacts&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03194v1 Announce Type: cross  Abstract: Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide range of exams, where multiple contrast images are often acquired for characterizing different tissues. However, acquiring high-resolution MRI typically extends scan time, which can introduce motion artifacts. Super-resolution of MRI therefore emerges as a promising approach to mitigate these challenges. Earlier studies have investigated the use of multiple contrasts for MRI super-resolution (MCSR), whereas majority of them did not fully exploit the rich contrast-invariant structural information. To fully utilize such crucial prior knowledge of multi-contrast MRI, in this work, we propose a novel structure-guided MCSR (SGSR) framework based on a new spatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs attention on features of multiple contrasts with a shared structural query, which is particularly designed to extract, fuse, and refin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#32447;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#34987;&#29992;&#20110;&#22788;&#29702;&#24050;&#32463;&#23398;&#20250;&#30340;&#26679;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#36825;&#37096;&#20998;&#26679;&#26412;&#19981;&#20877;&#23545;&#27169;&#22411;&#26356;&#26032;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20316;&#32773;&#35782;&#21035;&#20986;&#38543;&#26426;&#26679;&#26412;&#30340;&#36870;&#20256;&#25773;&#26159;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#25991;&#31456;&#25552;&#20986;&#39318;&#20808;&#20351;&#29992;&#31532;&#19968;&#36941;&#21069;&#21521;&#25512;&#29702;&#26469;&#31579;&#36873;&#20986;"&#30828;&#26679;&#26412;"&#65292;&#21482;&#23545;&#36825;&#20123;&#26679;&#26412;&#26500;&#24314;&#35745;&#31639;&#22270;&#24182;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#31574;&#30053;&#33021;&#22815;&#26377;&#25928;&#33410;&#30465;&#35757;&#32451;NeRF&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2408.03193</link><description>&lt;p&gt;
Efficient NeRF Optimization -- Not All Samples Remain Equally Hard
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#32447;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#34987;&#29992;&#20110;&#22788;&#29702;&#24050;&#32463;&#23398;&#20250;&#30340;&#26679;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#36825;&#37096;&#20998;&#26679;&#26412;&#19981;&#20877;&#23545;&#27169;&#22411;&#26356;&#26032;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20316;&#32773;&#35782;&#21035;&#20986;&#38543;&#26426;&#26679;&#26412;&#30340;&#36870;&#20256;&#25773;&#26159;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#25991;&#31456;&#25552;&#20986;&#39318;&#20808;&#20351;&#29992;&#31532;&#19968;&#36941;&#21069;&#21521;&#25512;&#29702;&#26469;&#31579;&#36873;&#20986;"&#30828;&#26679;&#26412;"&#65292;&#21482;&#23545;&#36825;&#20123;&#26679;&#26412;&#26500;&#24314;&#35745;&#31639;&#22270;&#24182;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#31574;&#30053;&#33021;&#22815;&#26377;&#25928;&#33410;&#30465;&#35757;&#32451;NeRF&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03193v1 Announce Type: new  Abstract: We propose an application of online hard sample mining for efficient training of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality for many 3D reconstruction and rendering tasks but require substantial computational resources. The encoding of the scene information within the NeRF network parameters necessitates stochastic sampling. We observe that during the training, a major part of the compute time and memory usage is spent on processing already learnt samples, which no longer affect the model update significantly. We identify the backward pass on the stochastic samples as the computational bottleneck during the optimization. We thus perform the first forward pass in inference mode as a relatively low-cost search for hard samples. This is followed by building the computational graph and updating the NeRF network parameters using only the hard samples. To demonstrate the effectiveness of the proposed approach, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;3D&#27169;&#22411;&#31616;&#21270;&#20026;64x64&#20687;&#32032;&#22270;&#20687;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#24102;&#26377;UV&#26144;&#23556;&#30340;&#36924;&#30495;3D&#24418;&#29366;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;&#24418;&#29366;&#34987;&#36716;&#25442;&#25104;&#26356;&#26131;&#22788;&#29702;&#30340;&#20108;&#32500;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32593;&#26684;&#24418;&#29366;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#38750;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;3D&#24418;&#29366;&#65292;&#24182;&#22312;ABO&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#26368;&#36817;3D&#29983;&#25104;&#27169;&#22411;&#30456;&#20284;&#30340;&#28857;&#20113;FID&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03178</link><description>&lt;p&gt;
An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;3D&#27169;&#22411;&#31616;&#21270;&#20026;64x64&#20687;&#32032;&#22270;&#20687;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#24102;&#26377;UV&#26144;&#23556;&#30340;&#36924;&#30495;3D&#24418;&#29366;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;&#24418;&#29366;&#34987;&#36716;&#25442;&#25104;&#26356;&#26131;&#22788;&#29702;&#30340;&#20108;&#32500;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32593;&#26684;&#24418;&#29366;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#38750;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;3D&#24418;&#29366;&#65292;&#24182;&#22312;ABO&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#26368;&#36817;3D&#29983;&#25104;&#27169;&#22411;&#30456;&#20284;&#30340;&#28857;&#20113;FID&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03178v1 Announce Type: new  Abstract: We introduce a new approach for generating realistic 3D models with UV maps through a representation termed "Object Images." This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#38548;&#30340;&#31354;&#38388;&#33192;&#32960;&#21367;&#31215;&#65288;DCLS&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26631;&#20934;&#21644;&#33192;&#32960;&#21367;&#31215;&#65292;&#36824;&#22312;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#21487;&#35299;&#37322;&#24615;&#36890;&#36807;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#23545;&#24212;&#20851;&#31995;&#26469;&#34913;&#37327;&#65292;&#21363;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27169;&#22411;&#30340;GradCAM&#28909;&#22270;&#19982;&#21453;&#26144;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#24230;&#30340;ClickMe&#25968;&#25454;&#38598;&#28909;&#22270;&#20043;&#38388;&#30340;Spearman&#30456;&#20851;&#31995;&#25968;&#26469;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20998;&#65292;&#34920;&#26126;DCLS&#22686;&#21152;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03164</link><description>&lt;p&gt;
Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#38548;&#30340;&#31354;&#38388;&#33192;&#32960;&#21367;&#31215;&#65288;DCLS&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26631;&#20934;&#21644;&#33192;&#32960;&#21367;&#31215;&#65292;&#36824;&#22312;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#21487;&#35299;&#37322;&#24615;&#36890;&#36807;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#23545;&#24212;&#20851;&#31995;&#26469;&#34913;&#37327;&#65292;&#21363;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27169;&#22411;&#30340;GradCAM&#28909;&#22270;&#19982;&#21453;&#26144;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#24230;&#30340;ClickMe&#25968;&#25454;&#38598;&#28909;&#22270;&#20043;&#38388;&#30340;Spearman&#30456;&#20851;&#31995;&#25968;&#26469;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20998;&#65292;&#34920;&#26126;DCLS&#22686;&#21152;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03164v1 Announce Type: new  Abstract: Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random he
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#29616;&#20195;&#22810;&#27169;&#24577;&#25512;&#29702;&#27169;&#22411;&#22312;&#36741;&#21161;&#35270;&#35273;&#36741;&#21161;&#35774;&#22791;&#23436;&#25104;&#22810;&#27493;&#39588;&#26085;&#24120;&#27963;&#21160;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;LLM&#26041;&#27861;&#8212;&#8212;Socratic&#27169;&#22411;&#21644;&#35270;&#35273;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#65288;VCLM&#65289;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#23558;&#35270;&#35273;&#21382;&#21490;&#32534;&#30721;&#21270;&#21644;&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20165;&#20165;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#36741;&#21161;&#35774;&#22791;&#30340;&#21069;&#20004;&#20010;&#33021;&#21147;&#65292;&#24182;&#19981;&#33021;&#35780;&#20272;&#22312;&#29992;&#25143;&#21442;&#19982;&#19979;&#37325;&#26032;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#25143;&#21442;&#19982;&#24335;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;VCLM&#25552;&#20379;&#20102;&#23454;&#26102;&#29992;&#25143;&#20132;&#20114;&#30340;&#34917;&#20805;&#35780;&#20272;&#26041;&#24335;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;VCLM&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21644;&#21160;&#24577;&#34892;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.03160</link><description>&lt;p&gt;
User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#29616;&#20195;&#22810;&#27169;&#24577;&#25512;&#29702;&#27169;&#22411;&#22312;&#36741;&#21161;&#35270;&#35273;&#36741;&#21161;&#35774;&#22791;&#23436;&#25104;&#22810;&#27493;&#39588;&#26085;&#24120;&#27963;&#21160;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;LLM&#26041;&#27861;&#8212;&#8212;Socratic&#27169;&#22411;&#21644;&#35270;&#35273;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#65288;VCLM&#65289;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#23558;&#35270;&#35273;&#21382;&#21490;&#32534;&#30721;&#21270;&#21644;&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20165;&#20165;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#36741;&#21161;&#35774;&#22791;&#30340;&#21069;&#20004;&#20010;&#33021;&#21147;&#65292;&#24182;&#19981;&#33021;&#35780;&#20272;&#22312;&#29992;&#25143;&#21442;&#19982;&#19979;&#37325;&#26032;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#25143;&#21442;&#19982;&#24335;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;VCLM&#25552;&#20379;&#20102;&#23454;&#26102;&#29992;&#25143;&#20132;&#20114;&#30340;&#34917;&#20805;&#35780;&#20272;&#26041;&#24335;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;VCLM&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21644;&#21160;&#24577;&#34892;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03160v1 Announce Type: new  Abstract: Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches -- Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CT&#37325;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36864;&#21270;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#19982;&#36845;&#20195;CT&#37325;&#24314;&#30340;&#20445;&#30495;&#24230;&#25439;&#22833;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.03156</link><description>&lt;p&gt;
Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CT&#37325;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36864;&#21270;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#19982;&#36845;&#20195;CT&#37325;&#24314;&#30340;&#20445;&#30495;&#24230;&#25439;&#22833;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03156v1 Announce Type: new  Abstract: Image generative AI has garnered significant attention in recent years. In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity. In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference. We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data. Despite the simplicity of the implementation, the proposed method 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EGMS&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#32454;&#31890;&#24230;&#23454;&#20307;&#30693;&#35782;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#31934;&#32454;&#22320;&#25972;&#21512;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#25991;&#26412;&#25688;&#35201;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#36873;&#21462;&#22270;&#20687;&#26041;&#38754;&#36827;&#34892;&#31934;&#28860;&#22788;&#29702;&#12290;&#22312;BART&#30340;&#22522;&#30784;&#19978;&#65292;EGMS&#27169;&#22411;&#37319;&#29992;&#20102;&#21452;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#26435;&#37325;&#26469;&#22788;&#29702;&#25991;&#26412;-&#22270;&#20687;&#21644;&#23454;&#20307;-&#22270;&#20687;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#25968;&#25454;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;</title><link>https://arxiv.org/abs/2408.03149</link><description>&lt;p&gt;
Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03149
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EGMS&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#32454;&#31890;&#24230;&#23454;&#20307;&#30693;&#35782;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#31934;&#32454;&#22320;&#25972;&#21512;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#25991;&#26412;&#25688;&#35201;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#36873;&#21462;&#22270;&#20687;&#26041;&#38754;&#36827;&#34892;&#31934;&#28860;&#22788;&#29702;&#12290;&#22312;BART&#30340;&#22522;&#30784;&#19978;&#65292;EGMS&#27169;&#22411;&#37319;&#29992;&#20102;&#21452;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#26435;&#37325;&#26469;&#22788;&#29702;&#25991;&#26412;-&#22270;&#20687;&#21644;&#23454;&#20307;-&#22270;&#20687;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#25968;&#25454;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03149v1 Announce Type: new  Abstract: The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperSimpleNet&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26681;&#25454;SimpleNet&#27169;&#22411;&#28436;&#21464;&#32780;&#26469;&#65292;&#20027;&#35201;&#29992;&#20110;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#35782;&#21035;&#29289;&#20307;&#34920;&#38754;&#30340;&#32570;&#38519;&#12290;SuperSimpleNet&#33021;&#22815;&#22312;&#20165;&#20351;&#29992;&#27491;&#24120;&#35757;&#32451;&#22270;&#20687;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#22312;&#26377;&#24322;&#24120;&#26631;&#31614;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20004;&#31181;&#29615;&#22659;&#19979;&#22343;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#22791;&#30340;&#39640;&#24615;&#33021;&#12289;&#24555;&#36895;&#25805;&#20316;&#20197;&#21450;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.03143</link><description>&lt;p&gt;
SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast and Reliable Surface Defect Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperSimpleNet&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26681;&#25454;SimpleNet&#27169;&#22411;&#28436;&#21464;&#32780;&#26469;&#65292;&#20027;&#35201;&#29992;&#20110;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#35782;&#21035;&#29289;&#20307;&#34920;&#38754;&#30340;&#32570;&#38519;&#12290;SuperSimpleNet&#33021;&#22815;&#22312;&#20165;&#20351;&#29992;&#27491;&#24120;&#35757;&#32451;&#22270;&#20687;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#22312;&#26377;&#24322;&#24120;&#26631;&#31614;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20004;&#31181;&#29615;&#22659;&#19979;&#22343;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#22791;&#30340;&#39640;&#24615;&#33021;&#12289;&#24555;&#36895;&#25805;&#20316;&#20197;&#21450;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03143v1 Announce Type: new  Abstract: The aim of surface defect detection is to identify and localise abnormal regions on the surfaces of captured objects, a task that's increasingly demanded across various industries. Current approaches frequently fail to fulfil the extensive demands of these industries, which encompass high performance, consistency, and fast operation, along with the capacity to leverage the entirety of the available training data. Addressing these gaps, we introduce SuperSimpleNet, an innovative discriminative model that evolved from SimpleNet. This advanced model significantly enhances its predecessor's training consistency, inference time, as well as detection performance. SuperSimpleNet operates in an unsupervised manner using only normal training images but also benefits from labelled abnormal training images when they are available. SuperSimpleNet achieves state-of-the-art results in both the supervised and the unsupervised settings, as demonstrated 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#32508;&#36848;&#23545;&#29616;&#26377;&#26893;&#29289;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#39564;&#23460;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37326;&#22806;&#22270;&#20687;&#35782;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#28145;&#20837;&#21078;&#26512;&#65292;&#24182;&#38024;&#23545;&#26893;&#29289;&#30142;&#30149;&#30340;&#30456;&#20284;&#22806;&#35266;&#21644;&#19981;&#21516;&#30142;&#30149;&#22806;&#35266;&#30340;&#30683;&#30462;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#26368;&#22823;&#30340;&#26893;&#29289;&#30142;&#30149;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#35299;&#20915;&#37326;&#22806;&#35782;&#21035;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#30142;&#30149;&#30456;&#20284;&#21644;&#21516;&#19968;&#31181;&#30142;&#30149;&#22806;&#35266;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#20123;&#21162;&#21147;&#65292;&#25991;&#31456;&#26088;&#22312;&#25552;&#39640;&#26893;&#29289;&#30142;&#30149;&#37326;&#22806;&#22270;&#35782;&#21035;&#20013;&#27169;&#22411;&#22312;&#22788;&#29702;&#23567;&#31867;&#38388;&#24046;&#24322;&#21644;&#22823;&#31867;&#20869;&#24046;&#24322;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03120</link><description>&lt;p&gt;
Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#32508;&#36848;&#23545;&#29616;&#26377;&#26893;&#29289;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#39564;&#23460;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37326;&#22806;&#22270;&#20687;&#35782;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#28145;&#20837;&#21078;&#26512;&#65292;&#24182;&#38024;&#23545;&#26893;&#29289;&#30142;&#30149;&#30340;&#30456;&#20284;&#22806;&#35266;&#21644;&#19981;&#21516;&#30142;&#30149;&#22806;&#35266;&#30340;&#30683;&#30462;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#26368;&#22823;&#30340;&#26893;&#29289;&#30142;&#30149;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#35299;&#20915;&#37326;&#22806;&#35782;&#21035;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#30142;&#30149;&#30456;&#20284;&#21644;&#21516;&#19968;&#31181;&#30142;&#30149;&#22806;&#35266;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#20123;&#21162;&#21147;&#65292;&#25991;&#31456;&#26088;&#22312;&#25552;&#39640;&#26893;&#29289;&#30142;&#30149;&#37326;&#22806;&#22270;&#35782;&#21035;&#20013;&#27169;&#22411;&#22312;&#22788;&#29702;&#23567;&#31867;&#38388;&#24046;&#24322;&#21644;&#22823;&#31867;&#20869;&#24046;&#24322;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03120v1 Announce Type: new  Abstract: Existing plant disease classification models have achieved remarkable performance in recognizing in-laboratory diseased images. However, their performance often significantly degrades in classifying in-the-wild images. Furthermore, we observed that in-the-wild plant images may exhibit similar appearances across various diseases (i.e., small inter-class discrepancy) while the same diseases may look quite different (i.e., large intra-class variance). Motivated by this observation, we propose an in-the-wild multimodal plant disease recognition dataset that contains the largest number of disease classes but also text-based descriptions for each disease. Particularly, the newly provided text descriptions are introduced to provide rich information in textual modality and facilitate in-the-wild disease classification with small inter-class discrepancy and large intra-class variance issues. Therefore, our proposed dataset can be regarded as an i
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24494;&#25163;&#21183;&#20998;&#31867;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#20132;&#21449;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#21407;&#22411;&#31934;&#28860;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24494;&#25163;&#21183;&#29305;&#24449;&#30340;&#21306;&#20998;&#24230;&#65292;&#20174;&#32780;&#22312;IJCAI 2024&#30340;MiGA&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#30456;&#36739;&#20110;&#21435;&#24180;&#26368;&#20339;&#25104;&#32489;&#22823;&#24133;&#25552;&#21319;&#20102;6.13%&#30340;Top-1&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.03097</link><description>&lt;p&gt;
Prototype Learning for Micro-gesture Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24494;&#25163;&#21183;&#20998;&#31867;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#20132;&#21449;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#21407;&#22411;&#31934;&#28860;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24494;&#25163;&#21183;&#29305;&#24449;&#30340;&#21306;&#20998;&#24230;&#65292;&#20174;&#32780;&#22312;IJCAI 2024&#30340;MiGA&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#30456;&#36739;&#20110;&#21435;&#24180;&#26368;&#20339;&#25104;&#32489;&#22823;&#24133;&#25552;&#21319;&#20102;6.13%&#30340;Top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03097v1 Announce Type: new  Abstract: In this paper, we briefly introduce the solution developed by our team, HFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge at IJCAI 2024. The task of micro-gesture classification task involves recognizing the category of a given video clip, which focuses on more fine-grained and subtle body movements compared to typical action recognition tasks. Given the inherent complexity of micro-gesture recognition, which includes large intra-class variability and minimal inter-class differences, we utilize two innovative modules, i.e., the cross-modal fusion module and prototypical refinement module, to improve the discriminative ability of MG features, thereby improving the classification accuracy. Our solution achieved significant success, ranking 1st in the track of Micro-gesture Classification. We surpassed the performance of last year's leading team by a substantial margin, improving Top-1 accuracy by 6.13%.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25163;&#26415;&#24212;&#29992;&#30340;&#24320;&#25918;&#22495;&#21333;&#30446;&#35270;&#35273;SLAM&#26694;&#26550;BodySLAM&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21333;&#30446;&#25668;&#20687;&#22836;&#30340;&#36755;&#20837;&#65292;&#26080;&#38656;&#20219;&#20309;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25163;&#26415;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25805;&#32437;&#31934;&#20934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.03078</link><description>&lt;p&gt;
BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25163;&#26415;&#24212;&#29992;&#30340;&#24320;&#25918;&#22495;&#21333;&#30446;&#35270;&#35273;SLAM&#26694;&#26550;BodySLAM&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21333;&#30446;&#25668;&#20687;&#22836;&#30340;&#36755;&#20837;&#65292;&#26080;&#38656;&#20219;&#20309;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25163;&#26415;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25805;&#32437;&#31934;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03078v1 Announce Type: cross  Abstract: Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM) has emerged as a promising solution to address these limitations, its implementation in endoscopic procedures presents significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents a robust deep learning-based SLAM approach that combines state-of-the-art and newly developed models. It consists of three main parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method based on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the novel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous models to create a coherent surgical map. The performance of the procedure was rigorously eva
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOPE&#30340;&#21512;&#25104;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38598;&#20307;&#24863;&#30693;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#20256;&#24863;&#22120;&#27169;&#25311;&#65292;&#29305;&#21035;&#26159;&#29289;&#29702;&#20934;&#30830;&#30340;&#39118;&#38632;&#27169;&#25311;&#65292;&#20026;&#24320;&#21457;&#21644;&#27979;&#35797;&#26032;&#22411;&#38598;&#20307;&#24863;&#30693;&#25216;&#26415;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;SCOPE&#25968;&#25454;&#38598;&#36890;&#36807;&#27169;&#25311;&#29616;&#23454;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#22797;&#26434;&#30340;&#20132;&#36890;&#22330;&#26223;&#65292;&#20419;&#36827;&#20102;&#33258;&#21160;&#39550;&#39542;&#21644;&#36710;&#36742;&#24863;&#30693;&#39046;&#22495;&#30340;&#25216;&#26415;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2408.03065</link><description>&lt;p&gt;
SCOPE: A Synthetic Multi-Modal Dataset for Collective Perception Including Physical-Correct Weather Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOPE&#30340;&#21512;&#25104;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38598;&#20307;&#24863;&#30693;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#20256;&#24863;&#22120;&#27169;&#25311;&#65292;&#29305;&#21035;&#26159;&#29289;&#29702;&#20934;&#30830;&#30340;&#39118;&#38632;&#27169;&#25311;&#65292;&#20026;&#24320;&#21457;&#21644;&#27979;&#35797;&#26032;&#22411;&#38598;&#20307;&#24863;&#30693;&#25216;&#26415;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;SCOPE&#25968;&#25454;&#38598;&#36890;&#36807;&#27169;&#25311;&#29616;&#23454;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#22797;&#26434;&#30340;&#20132;&#36890;&#22330;&#26223;&#65292;&#20419;&#36827;&#20102;&#33258;&#21160;&#39550;&#39542;&#21644;&#36710;&#36742;&#24863;&#30693;&#39046;&#22495;&#30340;&#25216;&#26415;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03065v1 Announce Type: new  Abstract: Collective perception has received considerable attention as a promising approach to overcome occlusions and limited sensing ranges of vehicle-local perception in autonomous driving. In order to develop and test novel collective perception technologies, appropriate datasets are required. These datasets must include not only different environmental conditions, as they strongly influence the perception capabilities, but also a wide range of scenarios with different road users as well as realistic sensor models. Therefore, we propose the Synthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic multi-modal dataset that incorporates realistic camera and LiDAR models as well as parameterized and physically accurate weather simulations for both sensor types. The dataset contains 17,600 frames from over 40 diverse scenarios with up to 24 collaborative agents, infrastructure sensors, and passive traffic, including cyclists an
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Masked Gaussian Fields (MGFs)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#22810;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#30340;&#24314;&#31569;&#29289;&#34920;&#38754;&#37325;&#24314;&#12290;&#36890;&#36807;&#26377;&#25928;&#30340;&#31354;&#38388;&#37319;&#26679;&#21644;COLMAP&#30340;&#22810;&#23618;&#27425;&#24314;&#31569;&#29289;&#25513;&#30721;&#29983;&#25104;&#65292;MGFs&#33021;&#22815;&#22312;&#20445;&#25345;&#19977;&#32500;&#32454;&#33410;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#37325;&#24314;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.03060</link><description>&lt;p&gt;
MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Masked Gaussian Fields (MGFs)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#22810;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#30340;&#24314;&#31569;&#29289;&#34920;&#38754;&#37325;&#24314;&#12290;&#36890;&#36807;&#26377;&#25928;&#30340;&#31354;&#38388;&#37319;&#26679;&#21644;COLMAP&#30340;&#22810;&#23618;&#27425;&#24314;&#31569;&#29289;&#25513;&#30721;&#29983;&#25104;&#65292;MGFs&#33021;&#22815;&#22312;&#20445;&#25345;&#19977;&#32500;&#32454;&#33410;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#37325;&#24314;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03060v1 Announce Type: new  Abstract: Over the last few decades, image-based building surface reconstruction has garnered substantial research interest and has been applied across various fields, such as heritage preservation, architectural planning, etc. Compared to the traditional photogrammetric and NeRF-based solutions, recently, Gaussian fields-based methods have exhibited significant potential in generating surface meshes due to their time-efficient training and detailed 3D information preservation. However, most gaussian fields-based methods are trained with all image pixels, encompassing building and nonbuilding areas, which results in a significant noise for building meshes and degeneration in time efficiency. This paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to generate accurate surface reconstruction for building in a time-efficient way. The framework first applies EfficientSAM and COLMAP to generate multi-level masks of building and t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;CPD&#26694;&#26550;&#36890;&#36807;&#38598;&#25104;&#23618;&#27425;&#20381;&#36182;&#38382;&#39064;&#35299;&#20915;&#30340;&#8220;comb&#8221;&#27493;&#39588;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#32467;&#26500;&#26080;&#20851;&#30340;&#23618;&#32423;&#20381;&#36182;&#31361;&#30772;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;&#24615;&#21435;&#38500;&#21442;&#25968;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;&#22312;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#26356;&#21152;&#39640;&#25928;&#21644;&#36890;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.03046</link><description>&lt;p&gt;
Comb, Prune, Distill: Towards Unified Pruning for Vision Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;CPD&#26694;&#26550;&#36890;&#36807;&#38598;&#25104;&#23618;&#27425;&#20381;&#36182;&#38382;&#39064;&#35299;&#20915;&#30340;&#8220;comb&#8221;&#27493;&#39588;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#32467;&#26500;&#26080;&#20851;&#30340;&#23618;&#32423;&#20381;&#36182;&#31361;&#30772;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;&#24615;&#21435;&#38500;&#21442;&#25968;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;&#22312;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#26356;&#21152;&#39640;&#25928;&#21644;&#36890;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03046v1 Announce Type: new  Abstract: Lightweight and effective models are essential for devices with limited resources, such as intelligent vehicles. Structured pruning offers a promising approach to model compression and efficiency enhancement. However, existing methods often tie pruning techniques to specific model architectures or vision tasks. To address this limitation, we propose a novel unified pruning framework Comb, Prune, Distill (CPD), which addresses both model-agnostic and task-agnostic concerns simultaneously. Our framework employs a combing step to resolve hierarchical layer-wise dependency issues, enabling architecture independence. Additionally, the pruning pipeline adaptively remove parameters based on the importance scoring metrics regardless of vision tasks. To support the model in retaining its learned information, we introduce knowledge distillation during the pruning step. Extensive experiments demonstrate the generalizability of our framework, encomp
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#23545;&#22270;&#20687;&#30340;&#23616;&#37096;&#29702;&#35299;&#21644;&#25972;&#20307;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#21306;&#22495;&#21270;&#38382;&#39064;&#30340;&#22238;&#31572;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.03043</link><description>&lt;p&gt;
Targeted Visual Prompting for Medical Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#23545;&#22270;&#20687;&#30340;&#23616;&#37096;&#29702;&#35299;&#21644;&#25972;&#20307;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#21306;&#22495;&#21270;&#38382;&#39064;&#30340;&#22238;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03043v1 Announce Type: new  Abstract: With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation. However, simple visual errors cast doubt on the actual visual understanding abilities of these models. To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to se
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Free-Echo&#30340;&#26080;&#35757;&#32451;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#24515;&#21160;&#21608;&#26399;&#20998;&#21106;&#22270;&#21512;&#25104;&#20855;&#26377;&#31354;&#38388;-&#35821;&#20041;&#29305;&#24615;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;3D-Unet&#27169;&#22411;&#65292;&#20351;&#29992;&#35757;&#32451;&#33258;&#30001;&#30340;&#26465;&#20214;&#26041;&#27861;SDEdit&#23545;&#20998;&#21106;&#22270;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#24182;&#22312;CAMUS&#21644;EchoNet-Dynamic&#20004;&#20010;&#20844;&#24320;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#35757;&#32451;&#20381;&#36182;&#30340;CDM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#21512;&#25104;&#21512;&#20046;&#23454;&#38469;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03035</link><description>&lt;p&gt;
Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Free-Echo&#30340;&#26080;&#35757;&#32451;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#24515;&#21160;&#21608;&#26399;&#20998;&#21106;&#22270;&#21512;&#25104;&#20855;&#26377;&#31354;&#38388;-&#35821;&#20041;&#29305;&#24615;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;3D-Unet&#27169;&#22411;&#65292;&#20351;&#29992;&#35757;&#32451;&#33258;&#30001;&#30340;&#26465;&#20214;&#26041;&#27861;SDEdit&#23545;&#20998;&#21106;&#22270;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#24182;&#22312;CAMUS&#21644;EchoNet-Dynamic&#20004;&#20010;&#20844;&#24320;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#35757;&#32451;&#20381;&#36182;&#30340;CDM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#21512;&#25104;&#21512;&#20046;&#23454;&#38469;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03035v1 Announce Type: cross  Abstract: Conditional video diffusion models (CDM) have shown promising results for video synthesis, potentially enabling the generation of realistic echocardiograms to address the problem of data scarcity. However, current CDMs require a paired segmentation map and echocardiogram dataset. We present a new method called Free-Echo for generating realistic echocardiograms from a single end-diastolic segmentation map without additional training data. Our method is based on the 3D-Unet with Temporal Attention Layers model and is conditioned on the segmentation map using a training-free conditioning method based on SDEdit. We evaluate our model on two public echocardiogram datasets, CAMUS and EchoNet-Dynamic. We show that our model can generate plausible echocardiograms that are spatially aligned with the input segmentation map, achieving performance comparable to training-based CDMs. Our work opens up new possibilities for generating echocardiograms
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#32467;&#21512;&#32972;&#26223;&#20449;&#24687;&#24182;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22812;&#38388;&#34892;&#20154;&#26816;&#27979;&#20013;&#30340;&#20302;&#20809;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FBCA&#65288;Fore-Background Contrast Attention&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#21306;&#22495;&#32972;&#26223;&#21644;&#34892;&#20154;&#23545;&#35937;&#30340;&#21306;&#21035;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03030</link><description>&lt;p&gt;
Nighttime Pedestrian Detection Based on Fore-Background Contrast Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#32467;&#21512;&#32972;&#26223;&#20449;&#24687;&#24182;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22812;&#38388;&#34892;&#20154;&#26816;&#27979;&#20013;&#30340;&#20302;&#20809;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FBCA&#65288;Fore-Background Contrast Attention&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#21306;&#22495;&#32972;&#26223;&#21644;&#34892;&#20154;&#23545;&#35937;&#30340;&#21306;&#21035;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03030v1 Announce Type: new  Abstract: The significance of background information is frequently overlooked in contemporary research concerning channel attention mechanisms. This study addresses the issue of suboptimal single-spectral nighttime pedestrian detection performance under low-light conditions by incorporating background information into the channel attention mechanism. Despite numerous studies focusing on the development of efficient channel attention mechanisms, the relevance of background information has been largely disregarded. By adopting a contrast learning approach, we reexamine channel attention with regard to pedestrian objects and background information for nighttime pedestrian detection, resulting in the proposed Fore-Background Contrast Attention (FBCA). FBCA possesses two primary attributes: (1) channel descriptors form remote dependencies with global spatial feature information; (2) the integration of background information enhances the distinction bet
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cleansed k-Nearest Neighbor (CKNN)&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#8220;Anomaly Cluster&#8221;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#28165;&#27927;&#25216;&#26415;&#26377;&#25928;&#25490;&#38500;&#19981;&#33391;&#35757;&#32451;&#26679;&#26412;&#65292;&#20026;Unsupervised Video Anomaly Detection&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.03014</link><description>&lt;p&gt;
CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cleansed k-Nearest Neighbor (CKNN)&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#8220;Anomaly Cluster&#8221;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#28165;&#27927;&#25216;&#26415;&#26377;&#25928;&#25490;&#38500;&#19981;&#33391;&#35757;&#32451;&#26679;&#26412;&#65292;&#20026;Unsupervised Video Anomaly Detection&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03014v1 Announce Type: new  Abstract: In this paper, we address the problem of unsupervised video anomaly detection (UVAD). The task aims to detect abnormal events in test video using unlabeled videos as training data. The presence of anomalies in the training data poses a significant challenge in this task, particularly because they form clusters in the feature space. We refer to this property as the "Anomaly Cluster" issue. The condensed nature of these anomalies makes it difficult to distinguish between normal and abnormal data in the training set. Consequently, training conventional anomaly detection techniques using an unlabeled dataset often leads to sub-optimal results. To tackle this difficulty, we propose a new method called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out the Anomaly Clusters by cleansing the training dataset. Following the k-nearest neighbor algorithm in the feature space provides powerful anomaly detection capability. Although the
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36335;&#24452;&#21327;&#20316;&#29983;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#24773;&#24863;&#35270;&#39057; captioning&#12290;</title><link>https://arxiv.org/abs/2408.03006</link><description>&lt;p&gt;
Dual-path Collaborative Generation Network for Emotional Video Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36335;&#24452;&#21327;&#20316;&#29983;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#24773;&#24863;&#35270;&#39057; captioning&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03006v1 Announce Type: new  Abstract: Emotional Video Captioning is an emerging task that aims to describe factual content with the intrinsic emotions expressed in videos. The essential of the EVC task is to effectively perceive subtle and ambiguous visual emotional cues during the caption generation, which is neglected by the traditional video captioning. Existing emotional video captioning methods perceive global visual emotional cues at first, and then combine them with the video features to guide the emotional caption generation, which neglects two characteristics of the EVC task. Firstly, their methods neglect the dynamic subtle changes in the intrinsic emotions of the video, which makes it difficult to meet the needs of common scenes with diverse and changeable emotions. Secondly, as their methods incorporate emotional cues into each step, the guidance role of emotion is overemphasized, which makes factual content more or less ignored during generation. To this end, we
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#35843;&#35856;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#27169;&#25311;&#20154;&#33041;&#20013;&#29305;&#23450;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#20351;&#24471;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#22810;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#27573;&#12289;&#25991;&#26412;&#22686;&#24378;&#12289;&#22270;&#20687;&#25551;&#36848;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MMUD&#30340;&#20840;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#32479;&#19968;&#35843;&#35856;&#21644;&#39640;&#25928;&#25191;&#34892;&#65292;&#20026;&#22810;&#27169;&#24577;&#20219;&#21153;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2408.03001</link><description>&lt;p&gt;
Multitask and Multimodal Neural Tuning for Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#35843;&#35856;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#27169;&#25311;&#20154;&#33041;&#20013;&#29305;&#23450;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#20351;&#24471;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#22810;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#27573;&#12289;&#25991;&#26412;&#22686;&#24378;&#12289;&#22270;&#20687;&#25551;&#36848;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MMUD&#30340;&#20840;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#32479;&#19968;&#35843;&#35856;&#21644;&#39640;&#25928;&#25191;&#34892;&#65292;&#20026;&#22810;&#27169;&#24577;&#20219;&#21153;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03001v1 Announce Type: new  Abstract: In recent years, large-scale multimodal models have demonstrated impressive capabilities across various domains. However, enabling these models to effectively perform multiple multimodal tasks simultaneously remains a significant challenge. To address this, we introduce a novel tuning method called neural tuning, designed to handle diverse multimodal tasks concurrently, including reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. Neural tuning emulates sparse distributed representation in human brain, where only specific subsets of neurons are activated for each task. Additionally, we present a new benchmark, MMUD, where each sample is annotated with multiple task labels. By applying neural tuning to pretrained large models on the MMUD benchmark, we achieve simultaneous task handling in a streamlined and efficient manner. All models, code, and datasets will be publicly available after publicat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;DreamLCM&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Latent Consistency Model&#65292;&#35299;&#20915;&#20102; text-to-3D &#20219;&#21153;&#20013;&#22240;&#37319;&#26679;&#31574;&#30053;&#21644;&#22122;&#22768;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#23545;&#35937;&#36136;&#37327;&#38382;&#39064;&#12290;DreamLCM&#33021;&#22815;&#29983;&#25104;&#19968;&#33268;&#19988;&#26377;&#39640;&#28165;&#24230;&#30340;&#24341;&#23548;&#20449;&#21495;&#65292;&#20026;&#20248;&#21270;&#30446;&#26631;3D&#27169;&#22411;&#25552;&#20379;&#20102;&#31934;&#30830;&#21644;&#35814;&#32454;&#30340;&#26799;&#24230;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.02993</link><description>&lt;p&gt;
DreamLCM: Towards High-Quality Text-to-3D Generation via Latent Consistency Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02993
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;DreamLCM&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Latent Consistency Model&#65292;&#35299;&#20915;&#20102; text-to-3D &#20219;&#21153;&#20013;&#22240;&#37319;&#26679;&#31574;&#30053;&#21644;&#22122;&#22768;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#23545;&#35937;&#36136;&#37327;&#38382;&#39064;&#12290;DreamLCM&#33021;&#22815;&#29983;&#25104;&#19968;&#33268;&#19988;&#26377;&#39640;&#28165;&#24230;&#30340;&#24341;&#23548;&#20449;&#21495;&#65292;&#20026;&#20248;&#21270;&#30446;&#26631;3D&#27169;&#22411;&#25552;&#20379;&#20102;&#31934;&#30830;&#21644;&#35814;&#32454;&#30340;&#26799;&#24230;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02993v1 Announce Type: new  Abstract: Recently, the text-to-3D task has developed rapidly due to the appearance of the SDS method. However, the SDS method always generates 3D objects with poor quality due to the over-smooth issue. This issue is attributed to two factors: 1) the DDPM single-step inference produces poor guidance gradients; 2) the randomness from the input noises and timesteps averages the details of the 3D contents.In this paper, to address the issue, we propose DreamLCM which incorporates the Latent Consistency Model (LCM). DreamLCM leverages the powerful image generation capabilities inherent in LCM, enabling generating consistent and high-quality guidance, i.e., predicted noises or images. Powered by the improved guidance, the proposed method can provide accurate and detailed gradients to optimize the target 3D models.In addition, we propose two strategies to enhance the generation quality further. Firstly, we propose a guidance calibration strategy, utiliz
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#24230;&#36924;&#36817;&#30495;&#23454;&#29305;&#24449;&#30340;&#31867;&#20195;&#34920;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#22312;&#26080;&#31034;&#33539;&#22686;&#37327;&#23398;&#20064;&#20013;&#30001;&#20110;&#29305;&#24449;&#29983;&#25104;&#35268;&#21017;&#23548;&#33268;&#30340;&#20998;&#24067;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02983</link><description>&lt;p&gt;
Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#24230;&#36924;&#36817;&#30495;&#23454;&#29305;&#24449;&#30340;&#31867;&#20195;&#34920;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#22312;&#26080;&#31034;&#33539;&#22686;&#37327;&#23398;&#20064;&#20013;&#30001;&#20110;&#29305;&#24449;&#29983;&#25104;&#35268;&#21017;&#23548;&#33268;&#30340;&#20998;&#24067;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02983v1 Announce Type: new  Abstract: Non-exemplar class-incremental learning (NECIL) is to resist catastrophic forgetting without saving old class samples. Prior methodologies generally employ simple rules to generate features for replaying, suffering from large distribution gap between replayed features and real ones. To address the aforementioned issue, we propose a simple, yet effective \textbf{Diff}usion-based \textbf{F}eature \textbf{R}eplay (\textbf{DiffFR}) method for NECIL. First, to alleviate the limited representational capacity caused by fixing the feature extractor, we employ Siamese-based self-supervised learning for initial generalizable features. Second, we devise diffusion models to generate class-representative features highly similar to real features, which provides an effective way for exemplar-free knowledge memorization. Third, we introduce prototype calibration to direct the diffusion model's focus towards learning the distribution shapes of features, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#26679;&#26412;&#26080;&#20851;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;VLP&#27169;&#22411;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#20316;&#32773;&#24320;&#21019;&#24615;&#22320;&#25506;&#32034;&#20102;&#36890;&#36807;&#36328;&#27169;&#24577;&#20915;&#31574;&#36793;&#30028;&#26469;&#21019;&#24314;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#22270;&#20687;&#65292;&#20197;&#27492;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25239;&#25915;&#20987;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#32771;&#34385;&#20102;Top-k&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#20063;&#33021;&#22815;&#25104;&#21151;&#25269;&#24481;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2408.02980</link><description>&lt;p&gt;
Sample-agnostic Adversarial Perturbation for Vision-Language Pre-training Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02980
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#26679;&#26412;&#26080;&#20851;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;VLP&#27169;&#22411;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#20316;&#32773;&#24320;&#21019;&#24615;&#22320;&#25506;&#32034;&#20102;&#36890;&#36807;&#36328;&#27169;&#24577;&#20915;&#31574;&#36793;&#30028;&#26469;&#21019;&#24314;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#22270;&#20687;&#65292;&#20197;&#27492;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25239;&#25915;&#20987;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#32771;&#34385;&#20102;Top-k&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#20063;&#33021;&#22815;&#25104;&#21151;&#25269;&#24481;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02980v1 Announce Type: new  Abstract: Recent studies on AI security have highlighted the vulnerability of Vision-Language Pre-training (VLP) models to subtle yet intentionally designed perturbations in images and texts. Investigating multimodal systems' robustness via adversarial attacks is crucial in this field. Most multimodal attacks are sample-specific, generating a unique perturbation for each sample to construct adversarial samples. To the best of our knowledge, it is the first work through multimodal decision boundaries to explore the creation of a universal, sample-agnostic perturbation that applies to any image. Initially, we explore strategies to move sample points beyond the decision boundaries of linear classifiers, refining the algorithm to ensure successful attacks under the top $k$ accuracy metric. Based on this foundation, in visual-language tasks, we treat visual and textual modalities as reciprocal sample points and decision hyperplanes, guiding image embed
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;ASR-enhanced Multimodal Product Representation Learning&#65288;AMPere&#65289;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;LLM&#30340;ASR&#25991;&#26412;&#25688;&#35201;&#22120;&#31616;&#21270;&#20174;&#22122;&#22768;ASR&#25991;&#26412;&#20013;&#25552;&#21462;&#20135;&#21697;&#30456;&#20851;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#25968;&#25454;&#32852;&#21512;&#36755;&#20837;&#21040;&#19968;&#20010;&#22810;&#20998;&#25903;&#32593;&#32476;&#20013;&#65292;&#20197;&#29983;&#25104;&#32039;&#20945;&#30340;&#36328;&#22495;&#20135;&#21697; multimodal &#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2408.02978</link><description>&lt;p&gt;
ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;ASR-enhanced Multimodal Product Representation Learning&#65288;AMPere&#65289;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;LLM&#30340;ASR&#25991;&#26412;&#25688;&#35201;&#22120;&#31616;&#21270;&#20174;&#22122;&#22768;ASR&#25991;&#26412;&#20013;&#25552;&#21462;&#20135;&#21697;&#30456;&#20851;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#25968;&#25454;&#32852;&#21512;&#36755;&#20837;&#21040;&#19968;&#20010;&#22810;&#20998;&#25903;&#32593;&#32476;&#20013;&#65292;&#20197;&#29983;&#25104;&#32039;&#20945;&#30340;&#36328;&#22495;&#20135;&#21697; multimodal &#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02978v1 Announce Type: cross  Abstract: E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive exp
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#27531;&#24046;&#32534;&#30721;&#21644;INR&#20248;&#21270;&#30340;&#19977;&#32500;&#28857;&#20113;&#24555;&#36895;&#20960;&#20309;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;KNN&#26041;&#27861;&#30830;&#23450;&#21407;&#22987;&#34920;&#38754;&#28857;&#30340;&#37051;&#22495;&#20851;&#31995;&#65292;&#26377;&#25928;&#21033;&#29992;&#31639;&#26415;&#32534;&#30721;&#23545;3D&#28857;&#29305;&#24449;&#36827;&#34892;&#21387;&#32553;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#21387;&#32553;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;&#31895;&#31961;&#32467;&#26500;&#21644;&#31934;&#32454;&#32454;&#33410;&#30340;&#23618;&#27425;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#21387;&#32553;&#29575;&#30340;&#24179;&#34913;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.02966</link><description>&lt;p&gt;
Fast Point Cloud Geometry Compression with Context-based Residual Coding and INR-based Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02966
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#27531;&#24046;&#32534;&#30721;&#21644;INR&#20248;&#21270;&#30340;&#19977;&#32500;&#28857;&#20113;&#24555;&#36895;&#20960;&#20309;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;KNN&#26041;&#27861;&#30830;&#23450;&#21407;&#22987;&#34920;&#38754;&#28857;&#30340;&#37051;&#22495;&#20851;&#31995;&#65292;&#26377;&#25928;&#21033;&#29992;&#31639;&#26415;&#32534;&#30721;&#23545;3D&#28857;&#29305;&#24449;&#36827;&#34892;&#21387;&#32553;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#21387;&#32553;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;&#31895;&#31961;&#32467;&#26500;&#21644;&#31934;&#32454;&#32454;&#33410;&#30340;&#23618;&#27425;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#21387;&#32553;&#29575;&#30340;&#24179;&#34913;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02966v1 Announce Type: new  Abstract: Compressing a set of unordered points is far more challenging than compressing images/videos of regular sample grids, because of the difficulties in characterizing neighboring relations in an irregular layout of points. Many researchers resort to voxelization to introduce regularity, but this approach suffers from quantization loss. In this research, we use the KNN method to determine the neighborhoods of raw surface points. This gives us a means to determine the spatial context in which the latent features of 3D points are compressed by arithmetic coding. As such, the conditional probability model is adaptive to local geometry, leading to significant rate reduction. Additionally, we propose a dual-layer architecture where a non-learning base layer reconstructs the main structures of the point cloud at low complexity, while a learned refinement layer focuses on preserving fine details. This design leads to reductions in model complexity 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#35760;&#24518;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;MATR&#65289;&#33021;&#22815;&#21033;&#29992;&#21463;&#38480;&#20110;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#35270;&#39057;&#29255;&#27573;&#26469;&#35782;&#21035;&#22810;&#20010;&#21160;&#20316;&#23454;&#20363;&#65292;&#36890;&#36807;&#35760;&#24518;&#38431;&#21015;&#20445;&#23384;&#36807;&#21435;&#29255;&#27573;&#30340;&#29305;&#24449;&#26469;&#26377;&#25928;&#21033;&#29992;&#38271;&#26399;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#23450;&#20301;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#20316;&#32467;&#26463;&#26102;&#38388;&#21644;&#20174;&#35760;&#24518;&#38431;&#21015;&#20013;&#20272;&#35745;&#21160;&#20316;&#24320;&#22987;&#26102;&#38388;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#24050;&#23384;&#22312;&#30340;&#26041;&#27861;&#65292;&#22312;THUMOS14&#21644;MUSES&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26576;&#20123;&#20256;&#32479;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02957</link><description>&lt;p&gt;
Online Temporal Action Localization with Memory-Augmented Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#35760;&#24518;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;MATR&#65289;&#33021;&#22815;&#21033;&#29992;&#21463;&#38480;&#20110;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#35270;&#39057;&#29255;&#27573;&#26469;&#35782;&#21035;&#22810;&#20010;&#21160;&#20316;&#23454;&#20363;&#65292;&#36890;&#36807;&#35760;&#24518;&#38431;&#21015;&#20445;&#23384;&#36807;&#21435;&#29255;&#27573;&#30340;&#29305;&#24449;&#26469;&#26377;&#25928;&#21033;&#29992;&#38271;&#26399;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#23450;&#20301;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#20316;&#32467;&#26463;&#26102;&#38388;&#21644;&#20174;&#35760;&#24518;&#38431;&#21015;&#20013;&#20272;&#35745;&#21160;&#20316;&#24320;&#22987;&#26102;&#38388;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#24050;&#23384;&#22312;&#30340;&#26041;&#27861;&#65292;&#22312;THUMOS14&#21644;MUSES&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26576;&#20123;&#20256;&#32479;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02957v1 Announce Type: new  Abstract: Online temporal action localization (On-TAL) is the task of identifying multiple action instances given a streaming video. Since existing methods take as input only a video segment of fixed size per iteration, they are limited in considering long-term context and require tuning the segment size carefully. To overcome these limitations, we propose memory-augmented transformer (MATR). MATR utilizes the memory queue that selectively preserves the past segment features, allowing to leverage long-term context for inference. We also propose a novel action localization method that observes the current input segment to predict the end time of the ongoing action and accesses the memory queue to estimate the start time of the action. Our method outperformed existing methods on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the online setting but also some offline TAL methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FakeMix&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#26816;&#27979;&#35270;&#39057;&#21644;&#38899;&#39057;&#20013;&#30340;&#29255;&#27573;&#32423;&#28145;&#20266;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;Temporal Accuracy&#65288;TA&#65289;&#21644;Frame-wise Discrimination Metric&#65288;FDM&#65289;&#31561;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#25913;&#21892;&#23545;&#28145;&#20266;&#26816;&#27979;&#27169;&#22411;&#30340;&#21160;&#24577;&#26816;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#22635;&#34917;&#29616;&#26377;&#35270;&#39057;&#32423;&#20998;&#31867;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#22312;&#27979;&#35797;&#19981;&#21516;&#28145;&#20266;&#25915;&#20987;&#26102;&#23637;&#29616;&#20986;&#26356;&#22909;&#22320;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02954</link><description>&lt;p&gt;
WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal Deepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FakeMix&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#26816;&#27979;&#35270;&#39057;&#21644;&#38899;&#39057;&#20013;&#30340;&#29255;&#27573;&#32423;&#28145;&#20266;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;Temporal Accuracy&#65288;TA&#65289;&#21644;Frame-wise Discrimination Metric&#65288;FDM&#65289;&#31561;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#25913;&#21892;&#23545;&#28145;&#20266;&#26816;&#27979;&#27169;&#22411;&#30340;&#21160;&#24577;&#26816;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#22635;&#34917;&#29616;&#26377;&#35270;&#39057;&#32423;&#20998;&#31867;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#22312;&#27979;&#35797;&#19981;&#21516;&#28145;&#20266;&#25915;&#20987;&#26102;&#23637;&#29616;&#20986;&#26356;&#22909;&#22320;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02954v1 Announce Type: new  Abstract: All current benchmarks for multimodal deepfake detection manipulate entire frames using various generation techniques, resulting in oversaturated detection accuracies exceeding 94% at the video-level classification. However, these benchmarks struggle to detect dynamic deepfake attacks with challenging frame-by-frame alterations presented in real-world scenarios. To address this limitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed at identifying manipulated segments within both video and audio, providing insight into the origins of deepfakes. Furthermore, we propose novel evaluation metrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to assess the robustness of deepfake detection models. Evaluating state-of-the-art models against diverse deepfake benchmarks, particularly FakeMix, demonstrates the effectiveness of our approach comprehensively. Specifically, while achieving an Average Preci
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#26631;&#27880;&#31574;&#30053;&#8212;&#8212;Multi-Size Labeling&#65288;MSL&#65289;&#21644;Distance-Based Labeling&#65288;DBL&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#23567;&#33041;&#26775;&#22622;&#21306;&#22495;&#36827;&#34892;&#31934;&#30830;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;ATLAS v2.0&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#38598;&#25104;&#20026;Ensemble&#21518;&#65292;&#23545;&#20110;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#20197;&#21450;Dice&#20998;&#25968;&#30340;&#25552;&#21319;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#23567;&#22411;&#30149;&#21464;&#26041;&#38754;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02929</link><description>&lt;p&gt;
Segmenting Small Stroke Lesions with Novel Labeling Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#26631;&#27880;&#31574;&#30053;&#8212;&#8212;Multi-Size Labeling&#65288;MSL&#65289;&#21644;Distance-Based Labeling&#65288;DBL&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#23567;&#33041;&#26775;&#22622;&#21306;&#22495;&#36827;&#34892;&#31934;&#30830;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;ATLAS v2.0&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#38598;&#25104;&#20026;Ensemble&#21518;&#65292;&#23545;&#20110;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#20197;&#21450;Dice&#20998;&#25968;&#30340;&#25552;&#21319;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#23567;&#22411;&#30149;&#21464;&#26041;&#38754;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02929v1 Announce Type: new  Abstract: Deep neural networks have demonstrated exceptional efficacy in stroke lesion segmentation. However, the delineation of small lesions, critical for stroke diagnosis, remains a challenge. In this study, we propose two straightforward yet powerful approaches that can be seamlessly integrated into a variety of networks: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL), with the aim of enhancing the segmentation accuracy of small lesions. MSL divides lesion masks into various categories based on lesion volume while DBL emphasizes the lesion boundaries. Experimental evaluations on the Anatomical Tracings of Lesions After Stroke (ATLAS) v2.0 dataset showcase that an ensemble of MSL and DBL achieves consistently better or equal performance on recall (3.6% and 3.7%), F1 (2.4% and 1.5%), and Dice scores (1.3% and 0.0%) compared to the top-1 winner of the 2022 MICCAI ATLAS Challenge on both the subset only containing small lesions and th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#38416;&#36848;&#20102;Meta&#24320;&#21457;&#30340;Segment Anything Model 2 (SAM2)&#22312;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#27700;&#38754;&#20197;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;&#38382;&#39064;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#36793;&#30028;&#26694;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;SAM2&#22312;&#27700;&#24213;&#23454;&#20363;&#20998;&#21106;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;SAM2&#22312;&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#33258;&#21160;&#20998;&#21106;&#26102;&#65292;&#20854;&#24863;&#30693;&#21644;&#20998;&#21106;&#27700;&#38754;&#20197;&#19979;&#23454;&#20363;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2408.02924</link><description>&lt;p&gt;
Evaluation of Segment Anything Model 2: The Role of SAM2 in the Underwater Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#38416;&#36848;&#20102;Meta&#24320;&#21457;&#30340;Segment Anything Model 2 (SAM2)&#22312;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#27700;&#38754;&#20197;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;&#38382;&#39064;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#36793;&#30028;&#26694;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;SAM2&#22312;&#27700;&#24213;&#23454;&#20363;&#20998;&#21106;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;SAM2&#22312;&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#33258;&#21160;&#20998;&#21106;&#26102;&#65292;&#20854;&#24863;&#30693;&#21644;&#20998;&#21106;&#27700;&#38754;&#20197;&#19979;&#23454;&#20363;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02924v1 Announce Type: new  Abstract: With breakthroughs in large-scale modeling, the Segment Anything Model (SAM) and its extensions have been attempted for applications in various underwater visualization tasks in marine sciences, and have had a significant impact on the academic community. Recently, Meta has further developed the Segment Anything Model 2 (SAM2), which significantly improves running speed and segmentation accuracy compared to its predecessor. This report aims to explore the potential of SAM2 in marine science by evaluating it on the underwater instance segmentation benchmark datasets UIIS and USIS10K. The experiments show that the performance of SAM2 is extremely dependent on the type of user-provided prompts. When using the ground truth bounding box as prompt, SAM2 performed excellently in the underwater instance segmentation domain. However, when running in automatic mode, SAM2's ability with point prompts to sense and segment underwater instances is sig
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pose Magic&#30340;&#28151;&#21512;Mamba-GCN&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#32467;&#21512;Mamba&#30340;&#39640;&#36136;&#37327;&#38271;&#33539;&#22260;&#24314;&#27169;&#33021;&#21147;&#21644;GCN&#30340;&#23616;&#37096;&#22686;&#24378;&#25928;&#26524;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02922</link><description>&lt;p&gt;
Pose Magic: Efficient and Temporally Consistent Human Pose Estimation with a Hybrid Mamba-GCN Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pose Magic&#30340;&#28151;&#21512;Mamba-GCN&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#32467;&#21512;Mamba&#30340;&#39640;&#36136;&#37327;&#38271;&#33539;&#22260;&#24314;&#27169;&#33021;&#21147;&#21644;GCN&#30340;&#23616;&#37096;&#22686;&#24378;&#25928;&#26524;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02922v1 Announce Type: new  Abstract: Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are primarily based on Transformers. However, existing Transformer-based 3D HPE backbones often encounter a trade-off between accuracy and computational efficiency. To resolve the above dilemma, in this work, leveraging recent advances in state space models, we utilize Mamba for high-quality and efficient long-range modeling. Nonetheless, Mamba still faces challenges in precisely exploiting the local dependencies between joints. To address these issues, we propose a new attention-free hybrid spatiotemporal architecture named Hybrid Mamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN by capturing relationships between neighboring joints, thus producing new representations to complement Mamba's outputs. By adaptively fusing representations from Mamba and GCN, Pose Magic demonstrates superior capability in learning the underlying 3D structu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21452;&#35270;&#22270;&#37329;&#23383;&#22612;&#27744;&#21270;&#8221;&#65288;Dual-View Pyramid Pooling&#65292;DVPP&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#29305;&#24449;&#36827;&#34892;&#20840;&#38754;&#30340;&#32858;&#21512;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#30340;&#20449;&#24515;&#12290;&#36890;&#36807;&#20998;&#26512;&#31354;&#38388;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#29305;&#24449;&#30340;&#21306;&#21035;&#65292;&#35813;&#30740;&#31350;&#26469;&#35299;&#20915;&#20256;&#32479;&#27744;&#21270;&#26041;&#27861;&#22312;&#29305;&#24449;&#34920;&#36798;&#21644;&#20998;&#31867;&#31934;&#24230;&#19978;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02906</link><description>&lt;p&gt;
Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical Image Classification and Confidence Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21452;&#35270;&#22270;&#37329;&#23383;&#22612;&#27744;&#21270;&#8221;&#65288;Dual-View Pyramid Pooling&#65292;DVPP&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#29305;&#24449;&#36827;&#34892;&#20840;&#38754;&#30340;&#32858;&#21512;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#30340;&#20449;&#24515;&#12290;&#36890;&#36807;&#20998;&#26512;&#31354;&#38388;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#29305;&#24449;&#30340;&#21306;&#21035;&#65292;&#35813;&#30740;&#31350;&#26469;&#35299;&#20915;&#20256;&#32479;&#27744;&#21270;&#26041;&#27861;&#22312;&#29305;&#24449;&#34920;&#36798;&#21644;&#20998;&#31867;&#31934;&#24230;&#19978;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02906v1 Announce Type: new  Abstract: Spatial pooling (SP) and cross-channel pooling (CCP) operators have been applied to aggregate spatial features and pixel-wise features from feature maps in deep neural networks (DNNs), respectively. Their main goal is to reduce computation and memory overhead without visibly weakening the performance of DNNs. However, SP often faces the problem of losing the subtle feature representations, while CCP has a high possibility of ignoring salient feature representations, which may lead to both miscalibration of confidence issues and suboptimal medical classification results. To address these problems, we propose a novel dual-view framework, the first to systematically investigate the relative roles of SP and CCP by analyzing the difference between spatial features and pixel-wise features. Based on this framework, we propose a new pooling method, termed dual-view pyramid pooling (DVPP), to aggregate multi-scale dual-view features. DVPP aims to
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#22467;&#21450;&#36710;&#36742;&#29260;&#29031;&#19978;&#30340;&#38463;&#25289;&#20271;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#21487;&#38752;&#22320;&#23450;&#20301;&#29260;&#29031;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#23383;&#31526;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;99.3%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#20854;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#31649;&#29702;&#65292;&#22914;&#20132;&#36890;&#36829;&#35268;&#26816;&#27979;&#21644;&#20572;&#36710;&#22330;&#20248;&#21270;&#12290;&#26410;&#26469;&#30740;&#31350;&#23558;&#36827;&#19968;&#27493;&#36890;&#36807;&#26550;&#26500;&#25913;&#36827;&#12289;&#25193;&#22823;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#31995;&#32479;&#20381;&#36182;&#38382;&#39064;&#26469;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02904</link><description>&lt;p&gt;
Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#22467;&#21450;&#36710;&#36742;&#29260;&#29031;&#19978;&#30340;&#38463;&#25289;&#20271;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#21487;&#38752;&#22320;&#23450;&#20301;&#29260;&#29031;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#23383;&#31526;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;99.3%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#20854;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#31649;&#29702;&#65292;&#22914;&#20132;&#36890;&#36829;&#35268;&#26816;&#27979;&#21644;&#20572;&#36710;&#22330;&#20248;&#21270;&#12290;&#26410;&#26469;&#30740;&#31350;&#23558;&#36827;&#19968;&#27493;&#36890;&#36807;&#26550;&#26500;&#25913;&#36827;&#12289;&#25193;&#22823;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#31995;&#32479;&#20381;&#36182;&#38382;&#39064;&#26469;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02904v1 Announce Type: new  Abstract: This paper introduces a novel two-stage framework for accurate Egyptian Vehicle License Plate Recognition (EVLPR). The first stage employs image processing techniques to reliably localize license plates, while the second stage utilizes a custom-designed deep learning model for robust Arabic character recognition. The proposed system achieves a remarkable 99.3% accuracy on a diverse dataset, surpassing existing approaches. Its potential applications extend to intelligent traffic management, including traffic violation detection and parking optimization. Future research will focus on enhancing the system's capabilities through architectural refinements, expanded datasets, and addressing system dependencies.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;Lighthouse&#65292;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#21644;&#31361;&#20986;&#26816;&#27979;&#65288;MR-HD&#65289;&#29992;&#25143;&#21451;&#22909;&#22411;&#24211;&#12290;Lighthouse&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#20047;&#36328;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35270;&#39057;&#25991;&#26412;&#29305;&#24449;&#30340;&#20840;&#38754;&#21644;&#21487;&#22797;&#29616;&#23454;&#39564;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#32479;&#19968;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#24211;&#20013;&#28085;&#30422;&#22810;&#31181;&#35774;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21487;&#22797;&#29992;&#30340;&#20195;&#30721;&#22522;&#12289;&#20845;&#20010;&#27169;&#22411;&#12289;&#19977;&#31181;&#29305;&#24449;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#22312;API&#21644;&#32593;&#39029;&#28436;&#31034;&#19978;&#30340;&#26131;&#35775;&#38382;&#24615;&#65292;&#20351;&#24471;&#30740;&#31350;&#32773;&#21644;&#24320;&#21457;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02901</link><description>&lt;p&gt;
Lighthouse: A User-Friendly Library for Reproducible Video Moment Retrieval and Highlight Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;Lighthouse&#65292;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#21644;&#31361;&#20986;&#26816;&#27979;&#65288;MR-HD&#65289;&#29992;&#25143;&#21451;&#22909;&#22411;&#24211;&#12290;Lighthouse&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#20047;&#36328;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35270;&#39057;&#25991;&#26412;&#29305;&#24449;&#30340;&#20840;&#38754;&#21644;&#21487;&#22797;&#29616;&#23454;&#39564;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#32479;&#19968;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#24211;&#20013;&#28085;&#30422;&#22810;&#31181;&#35774;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21487;&#22797;&#29992;&#30340;&#20195;&#30721;&#22522;&#12289;&#20845;&#20010;&#27169;&#22411;&#12289;&#19977;&#31181;&#29305;&#24449;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#22312;API&#21644;&#32593;&#39029;&#28436;&#31034;&#19978;&#30340;&#26131;&#35775;&#38382;&#24615;&#65292;&#20351;&#24471;&#30740;&#31350;&#32773;&#21644;&#24320;&#21457;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02901v1 Announce Type: new  Abstract: We propose Lighthouse, a user-friendly library for reproducible video moment retrieval and highlight detection (MR-HD). Although researchers proposed various MR-HD approaches, the research community holds two main issues. The first is a lack of comprehensive and reproducible experiments across various methods, datasets, and video-text features. This is because no unified training and evaluation codebase covers multiple settings. The second is user-unfriendly design. Because previous works use different libraries, researchers set up individual environments. In addition, most works release only the training codes, requiring users to implement the whole inference process of MR-HD. Lighthouse addresses these issues by implementing a unified reproducible codebase that includes six models, three features, and five datasets. In addition, it provides an inference API and web demo to make these methods easily accessible for researchers and develo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;MedTrinity-25M&#65292;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;2500&#19975;&#24352;&#22270;&#20687;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#21547;10&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#19988;&#34987;&#26631;&#27880;&#20102;&#36229;&#36807;65&#31181;&#30142;&#30149;&#30340;&#22810;&#23618;&#27425;&#20449;&#24687;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#27880;&#19981;&#20165;&#21253;&#25324;&#20840;&#29699;&#24615;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#22914;&#30142;&#30149;/&#30149;&#21464;&#31867;&#22411;&#12289;&#27169;&#24577;&#12289;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#65292;&#36824;&#21253;&#25324;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROIs&#65289;&#30340;&#35814;&#32454;&#26412;&#22320;&#26631;&#27880;&#65292;&#21253;&#25324;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#25513;&#30721;&#12290;&#19982;&#37027;&#20123;&#20165;&#38480;&#20110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#31649;&#32447;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20219;&#20309;&#25991;&#26412;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#22810;&#23618;&#27425;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#26631;&#27880;&#65288;&#20197;&#22270;&#20687;-ROI-&#25551;&#36848;&#19977;&#20803;&#32452;&#30340;&#24418;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#26469;&#33258;&#20110;90&#22810;&#20010;&#19981;&#21516;&#30340;&#28304;&#22836;&#65292;&#24050;&#32463;&#36807;&#39044;&#22788;&#29702;&#21644;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#27169;&#22411;&#36827;&#34892;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2408.02900</link><description>&lt;p&gt;
MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;MedTrinity-25M&#65292;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;2500&#19975;&#24352;&#22270;&#20687;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#21547;10&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#19988;&#34987;&#26631;&#27880;&#20102;&#36229;&#36807;65&#31181;&#30142;&#30149;&#30340;&#22810;&#23618;&#27425;&#20449;&#24687;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#27880;&#19981;&#20165;&#21253;&#25324;&#20840;&#29699;&#24615;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#22914;&#30142;&#30149;/&#30149;&#21464;&#31867;&#22411;&#12289;&#27169;&#24577;&#12289;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#65292;&#36824;&#21253;&#25324;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROIs&#65289;&#30340;&#35814;&#32454;&#26412;&#22320;&#26631;&#27880;&#65292;&#21253;&#25324;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#25513;&#30721;&#12290;&#19982;&#37027;&#20123;&#20165;&#38480;&#20110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#31649;&#32447;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20219;&#20309;&#25991;&#26412;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#22810;&#23618;&#27425;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#26631;&#27880;&#65288;&#20197;&#22270;&#20687;-ROI-&#25551;&#36848;&#19977;&#20803;&#32452;&#30340;&#24418;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#26469;&#33258;&#20110;90&#22810;&#20010;&#19981;&#21516;&#30340;&#28304;&#22836;&#65292;&#24050;&#32463;&#36807;&#39044;&#22788;&#29702;&#21644;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#27169;&#22411;&#36827;&#34892;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02900v1 Announce Type: new  Abstract: This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert model
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32500;&#25252;&#25968;&#25454;&#35821;&#20041;&#21327;&#35843;&#24615;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#21319;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02891</link><description>&lt;p&gt;
Diverse Generation while Maintaining Semantic Coordination: A Diffusion-Based Data Augmentation Method for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32500;&#25252;&#25968;&#25454;&#35821;&#20041;&#21327;&#35843;&#24615;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#21319;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02891v1 Announce Type: new  Abstract: Recent studies emphasize the crucial role of data augmentation in enhancing the performance of object detection models. However,existing methodologies often struggle to effectively harmonize dataset diversity with semantic coordination.To bridge this gap, we introduce an innovative augmentation technique leveraging pre-trained conditional diffusion models to mediate this balance. Our approach encompasses the development of a Category Affinity Matrix, meticulously designed to enhance dataset diversity, and a Surrounding Region Alignment strategy, which ensures the preservation of semantic coordination in the augmented images. Extensive experimental evaluations confirm the efficacy of our method in enriching dataset diversity while seamlessly maintaining semantic coordination. Our method yields substantial average improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives on three distinct object detection models, respectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VizECGNet&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;ECG&#22270;&#20687;&#32593;&#32476;&#26469;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#25171;&#21360;&#30340;ECG&#22270;&#24418;&#20415;&#33021;&#22815;&#20934;&#30830;&#22320;&#23545;&#22810;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2408.02888</link><description>&lt;p&gt;
VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VizECGNet&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;ECG&#22270;&#20687;&#32593;&#32476;&#26469;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#25171;&#21360;&#30340;ECG&#22270;&#24418;&#20415;&#33021;&#22815;&#20934;&#30830;&#22320;&#23545;&#22810;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02888v1 Announce Type: new  Abstract: An electrocardiogram (ECG) captures the heart's electrical signal to assess various heart conditions. In practice, ECG data is stored as either digitized signals or printed images. Despite the emergence of numerous deep learning models for digitized signals, many hospitals prefer image storage due to cost considerations. Recognizing the unavailability of raw ECG signals in many clinical settings, we propose VizECGNet, which uses only printed ECG graphics to determine the prognosis of multiple cardiovascular diseases. During training, cross-modal attention modules (CMAM) are used to integrate information from two modalities - image and signal, while self-modality attention modules (SMAM) capture inherent long-range dependencies in ECG data of each modality. Additionally, we utilize knowledge distillation to improve the similarity between two distinct predictions from each modality stream. This innovative multi-modal deep learning architec
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#27169;&#25311;&#29616;&#23454;&#20154;&#29289;&#34892;&#20026;&#30340;&#20840;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#21253;&#25324;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#30340;&#20132;&#27969;&#33021;&#21147;&#65292;&#33021;&#22815;&#36827;&#34892;&#23454;&#26102;&#21452;&#24037;&#20132;&#27969;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#31995;&#32479;&#23384;&#22312;&#30340;&#29616;&#23454;&#20154;&#31867;&#20195;&#29702;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2408.02879</link><description>&lt;p&gt;
Body of Her: A Preliminary Study on End-to-End Humanoid Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#27169;&#25311;&#29616;&#23454;&#20154;&#29289;&#34892;&#20026;&#30340;&#20840;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#21253;&#25324;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#30340;&#20132;&#27969;&#33021;&#21147;&#65292;&#33021;&#22815;&#36827;&#34892;&#23454;&#26102;&#21452;&#24037;&#20132;&#27969;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#31995;&#32479;&#23384;&#22312;&#30340;&#29616;&#23454;&#20154;&#31867;&#20195;&#29702;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02879v1 Announce Type: new  Abstract: Interactive virtual humanoid agent is a crucial interface with the physical world. A relatively complete humanoid agent first needs to have face and body, then possess both verbal and non-verbal (such as eye contact, facial expression, lip motion, gesture, and manipulation) abilities, and finally, it is capable of real-time duplex communication, e.g., the ability to actively interrupt conversations. Most prior systems typically only consider a subset of these elements, leaving a gap from realistic humanoid agent. In this work, we propose a real-time, duplex, interactive end-to-end network capable of modeling realistic agent behaviors, including speech, full-body movements for talking, responding, idling, and manipulation. This system is a multimodal model integrating audio and visual inputs, extended from a pre-trained large language model (LLM). We collect approximately 200,000 hours of audio, around 130,000 hours of video data, and abo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;VisionUnite&#30340;&#20840;&#26032;&#38754;&#21521;&#30524;&#31185;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22686;&#24378;&#20102;&#20020;&#24202;&#30693;&#35782;&#65292;&#24182;&#22312;1.24&#20159;&#22270;&#20687;&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;VisionUnite&#22312;&#35786;&#26029;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#29983;&#25104;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-4V&#21644;Gemini Pro&#65292;&#24182;&#19988;&#22312;&#22810;&#30149;&#31181;&#35786;&#26029;&#12289;&#20020;&#24202;&#35299;&#37322;&#21644;&#30149;&#20154;&#20114;&#21160;&#31561;&#20020;&#24202;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#36741;&#21161;&#19987;&#31185;&#21307;&#24072;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.02865</link><description>&lt;p&gt;
VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;VisionUnite&#30340;&#20840;&#26032;&#38754;&#21521;&#30524;&#31185;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22686;&#24378;&#20102;&#20020;&#24202;&#30693;&#35782;&#65292;&#24182;&#22312;1.24&#20159;&#22270;&#20687;&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;VisionUnite&#22312;&#35786;&#26029;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#29983;&#25104;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-4V&#21644;Gemini Pro&#65292;&#24182;&#19988;&#22312;&#22810;&#30149;&#31181;&#35786;&#26029;&#12289;&#20020;&#24202;&#35299;&#37322;&#21644;&#30149;&#20154;&#20114;&#21160;&#31561;&#20020;&#24202;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#36741;&#21161;&#19987;&#31185;&#21307;&#24072;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02865v1 Announce Type: cross  Abstract: The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly vers
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Madeleine&#65288;Madeleine&#65289;&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#21253;&#25324;&#20813;&#30123;&#32452;&#21270;&#22312;&#20869;&#30340;&#22810;&#26631;&#35760;&#26579;&#33394;&#20999;&#29255;&#20013;&#33719;&#21462;&#20016;&#23500;&#30340;&#20219;&#21153;&#26080;&#20851;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#28023;&#24179;&#38754;&#24040;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#23398;&#20064;&#20840;&#38754;&#30340;&#12289;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2408.02859</link><description>&lt;p&gt;
Multistain Pretraining for Slide Representation Learning in Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Madeleine&#65288;Madeleine&#65289;&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#21253;&#25324;&#20813;&#30123;&#32452;&#21270;&#22312;&#20869;&#30340;&#22810;&#26631;&#35760;&#26579;&#33394;&#20999;&#29255;&#20013;&#33719;&#21462;&#20016;&#23500;&#30340;&#20219;&#21153;&#26080;&#20851;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#28023;&#24179;&#38754;&#24040;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#23398;&#20064;&#20840;&#38754;&#30340;&#12289;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02859v1 Announce Type: cross  Abstract: Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&amp;E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224 x 224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce Madeleine, a multimodal pretraining strategy for slide representation
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#27604;&#36739;&#20102;&#21033;&#29992;RGB-D&#30456;&#26426;&#25429;&#25417;&#30340;3D&#36816;&#21160;&#25968;&#25454;&#21644;&#36890;&#36807;OpenPose&#12289;BlazePose&#31561;&#31639;&#27861;&#20174;2DRGB&#35270;&#39057;&#20013;&#20272;&#35745;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#23545;&#29289;&#29702;&#24247;&#22797;&#35757;&#32451;&#20013;&#24739;&#32773;&#21160;&#20316;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Gaussian Mixture Model&#65288;GMM&#65289;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#65292;&#35813;&#30740;&#31350;&#20026;&#25968;&#25454;&#20998;&#26512;&#25928;&#29575;&#21644;&#26426;&#22120;&#20154;&#25945;&#32451;&#31995;&#32479;&#20013;&#31639;&#27861;&#24615;&#33021;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.02855</link><description>&lt;p&gt;
Analyzing Data Efficiency and Performance of Machine Learning Algorithms for Assessing Low Back Pain Physical Rehabilitation Exercises
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02855
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#27604;&#36739;&#20102;&#21033;&#29992;RGB-D&#30456;&#26426;&#25429;&#25417;&#30340;3D&#36816;&#21160;&#25968;&#25454;&#21644;&#36890;&#36807;OpenPose&#12289;BlazePose&#31561;&#31639;&#27861;&#20174;2DRGB&#35270;&#39057;&#20013;&#20272;&#35745;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#23545;&#29289;&#29702;&#24247;&#22797;&#35757;&#32451;&#20013;&#24739;&#32773;&#21160;&#20316;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Gaussian Mixture Model&#65288;GMM&#65289;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#65292;&#35813;&#30740;&#31350;&#20026;&#25968;&#25454;&#20998;&#26512;&#25928;&#29575;&#21644;&#26426;&#22120;&#20154;&#25945;&#32451;&#31995;&#32479;&#20013;&#31639;&#27861;&#24615;&#33021;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02855v1 Announce Type: cross  Abstract: Analyzing human motion is an active research area, with various applications. In this work, we focus on human motion analysis in the context of physical rehabilitation using a robot coach system. Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose estimation from RGB images had made impressive improvements, we aim to compare the assessment of physical rehabilitation exercises using movement data obtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB videos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is employed from position (and orientation) features, with performance metrics defined based on the log-likelihood values from GMM. The evaluation is performed on a medical database of 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;GAReT&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22320;&#29702;&#36866;&#37197;&#22120;(GeoAdapter)&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;(Auto-Regressive Transformers)&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#30456;&#26426;&#21644;&#37324;&#31243;&#35745;&#25968;&#25454;&#30340;&#36328;&#35270;&#35282;&#35270;&#39057;&#22320;&#29702;&#23450;&#20301;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CVGL&#26041;&#27861;&#38754;&#20020;&#30340;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#12289;&#35745;&#31639;&#25928;&#29575;&#20302;&#20197;&#21450;&#39044;&#27979;&#32467;&#26524;&#26102;&#38388;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02840</link><description>&lt;p&gt;
GAReT: Cross-view Video Geolocalization with Adapters and Auto-Regressive Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;GAReT&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22320;&#29702;&#36866;&#37197;&#22120;(GeoAdapter)&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;(Auto-Regressive Transformers)&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#30456;&#26426;&#21644;&#37324;&#31243;&#35745;&#25968;&#25454;&#30340;&#36328;&#35270;&#35282;&#35270;&#39057;&#22320;&#29702;&#23450;&#20301;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CVGL&#26041;&#27861;&#38754;&#20020;&#30340;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#12289;&#35745;&#31639;&#25928;&#29575;&#20302;&#20197;&#21450;&#39044;&#27979;&#32467;&#26524;&#26102;&#38388;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02840v1 Announce Type: new  Abstract: Cross-view video geo-localization (CVGL) aims to derive GPS trajectories from street-view videos by aligning them with aerial-view images. Despite their promising performance, current CVGL methods face significant challenges. These methods use camera and odometry data, typically absent in real-world scenarios. They utilize multiple adjacent frames and various encoders for feature extraction, resulting in high computational costs. Moreover, these approaches independently predict each street-view frame's location, resulting in temporally inconsistent GPS trajectories. To address these challenges, in this work, we propose GAReT, a fully transformer-based method for CVGL that does not require camera and odometry data. We introduce GeoAdapter, a transformer-adapter module designed to efficiently aggregate image-level representations and adapt them for video inputs. Specifically, we train a transformer encoder on video frames and aerial images
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DaCapo&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21152;&#36895;&#38024;&#23545;&#22823;&#35268;&#27169;&#36817;&#31561;&#36724;&#23545;&#31216;&#22270;&#20687;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;DaCapo&#26694;&#26550;&#29305;&#21035;&#20248;&#21270;&#20102;&#23545;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#27169;&#22359;&#21270;&#32467;&#26500;&#12289;&#26377;&#25928;&#30340;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;DaCapo&#22312;&#25913;&#21892;&#23545;&#22823;&#35268;&#27169;&#31561;&#36724;&#23545;&#31216;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#30340;&#35775;&#38382;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#40723;&#21169;&#31038;&#21306;&#23545;&#35813;&#24320;&#28304;&#39033;&#30446;&#36827;&#34892;&#25506;&#32034;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.02834</link><description>&lt;p&gt;
DaCapo: a modular deep learning framework for scalable 3D image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DaCapo&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21152;&#36895;&#38024;&#23545;&#22823;&#35268;&#27169;&#36817;&#31561;&#36724;&#23545;&#31216;&#22270;&#20687;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;DaCapo&#26694;&#26550;&#29305;&#21035;&#20248;&#21270;&#20102;&#23545;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#27169;&#22359;&#21270;&#32467;&#26500;&#12289;&#26377;&#25928;&#30340;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;DaCapo&#22312;&#25913;&#21892;&#23545;&#22823;&#35268;&#27169;&#31561;&#36724;&#23545;&#31216;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#30340;&#35775;&#38382;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#40723;&#21169;&#31038;&#21306;&#23545;&#35813;&#24320;&#28304;&#39033;&#30446;&#36827;&#34892;&#25506;&#32034;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02834v1 Announce Type: new  Abstract: DaCapo is a specialized deep learning library tailored to expedite the training and application of existing machine learning approaches on large, near-isotropic image data. In this correspondence, we introduce DaCapo's unique features optimized for this specific domain, highlighting its modular structure, efficient experiment management tools, and scalable deployment capabilities. We discuss its potential to improve access to large-scale, isotropic image segmentation and invite the community to explore and contribute to this open-source initiative.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20449;&#24515;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#23458;&#25143;&#31471;&#27169;&#22411;&#26356;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#24182;&#38450;&#24481;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#27169;&#22411;&#20013;&#27602;&#21644;&#25968;&#25454;&#20013;&#27602;&#31561;&#22810;&#31181;&#24694;&#24847;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#28508;&#22312;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#26816;&#27979;&#21644;&#22788;&#29702;&#33021;&#21147;&#65292;&#22914;&#26631;&#31614;&#32763;&#36716;&#21644;&#26631;&#31614;&#28151;&#20081;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24403;&#22320;&#24179;&#34913;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#22312;&#23545;&#25239;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02813</link><description>&lt;p&gt;
Mitigating Malicious Attacks in Federated Learning via Confidence-aware Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20449;&#24515;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#23458;&#25143;&#31471;&#27169;&#22411;&#26356;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#24182;&#38450;&#24481;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#27169;&#22411;&#20013;&#27602;&#21644;&#25968;&#25454;&#20013;&#27602;&#31561;&#22810;&#31181;&#24694;&#24847;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#28508;&#22312;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#26816;&#27979;&#21644;&#22788;&#29702;&#33021;&#21147;&#65292;&#22914;&#26631;&#31614;&#32763;&#36716;&#21644;&#26631;&#31614;&#28151;&#20081;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24403;&#22320;&#24179;&#34913;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#22312;&#23545;&#25239;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02813v1 Announce Type: cross  Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm that allows multiple clients to collaboratively train a global model without sharing private local data. However, FL systems are vulnerable to attacks from malicious clients, who can degrade the global model performance through data poisoning and model poisoning. Existing defense methods typically focus on a single type of attack, such as Byzantine attacks or backdoor attacks, and are often ineffective against potential data poisoning attacks like label flipping and label shuffling. Additionally, these methods often lack accuracy and robustness in detecting and handling malicious updates. To address these issues, we propose a novel method based on model confidence scores, which evaluates the uncertainty of client model updates to detect and defend against malicious clients. Our approach is comprehensively effective for both model poisoning and data poisoning a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;SiCo&#30340;&#34394;&#25311;&#35797;&#34915;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#36523;&#20307;&#23610;&#23544;&#21644;&#34915;&#29289;&#23610;&#23544;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#34915;&#29289;&#23637;&#31034;&#25928;&#26524;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#26356;&#26377;&#20449;&#24687;&#25903;&#25345;&#30340;&#36141;&#20080;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2408.02803</link><description>&lt;p&gt;
SiCo: A Size-Controllable Virtual Try-On Approach for Informed Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;SiCo&#30340;&#34394;&#25311;&#35797;&#34915;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#36523;&#20307;&#23610;&#23544;&#21644;&#34915;&#29289;&#23610;&#23544;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#34915;&#29289;&#23637;&#31034;&#25928;&#26524;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#26356;&#26377;&#20449;&#24687;&#25903;&#25345;&#30340;&#36141;&#20080;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02803v1 Announce Type: cross  Abstract: Virtual try-on (VTO) applications aim to improve the online shopping experience by allowing users to preview garments, before making purchase decisions. However, many VTO tools fail to consider the crucial relationship between a garment's size and the user's body size, often employing a one-size-fits-all approach when visualizing a clothing item. This results in poor size recommendations and purchase decisions leading to increased return rates. To address this limitation, we introduce SiCo, an online VTO system, where users can upload images of themselves and visualize how different sizes of clothing would look on their body to help make better-informed purchase decisions. Our user study shows SiCo's superiority over baseline VTO. The results indicate that our approach significantly enhances user ability to gauge the appearance of outfits on their bodies and boosts their confidence in selecting clothing sizes that match desired goals. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340; evidential learning &#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#35748;&#20026;&#22270;&#20687;&#25968;&#25454;&#24212;&#30001;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#32452;&#25104;&#65292;&#24182;&#37319;&#29992;&#36870;&#20285;&#39532;&#20998;&#24067;&#20316;&#20026;&#20013;&#38388;&#20808;&#39564;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#30830;&#24230;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312; Scene Flow&#12289;KITTI 2015 &#21644; Middlebury 2014 &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.02796</link><description>&lt;p&gt;
Gaussian Mixture based Evidential Learning for Stereo Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02796
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340; evidential learning &#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#35748;&#20026;&#22270;&#20687;&#25968;&#25454;&#24212;&#30001;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#32452;&#25104;&#65292;&#24182;&#37319;&#29992;&#36870;&#20285;&#39532;&#20998;&#24067;&#20316;&#20026;&#20013;&#38388;&#20808;&#39564;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#30830;&#24230;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312; Scene Flow&#12289;KITTI 2015 &#21644; Middlebury 2014 &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02796v1 Announce Type: new  Abstract: In this paper, we introduce a novel Gaussian mixture based evidential learning solution for robust stereo matching. Diverging from previous evidential deep learning approaches that rely on a single Gaussian distribution, our framework posits that individual image data adheres to a mixture-of-Gaussian distribution in stereo matching. This assumption yields more precise pixel-level predictions and more accurately mirrors the real-world image distribution. By further employing the inverse-Gamma distribution as an intermediary prior for each mixture component, our probabilistic model achieves improved depth estimation compared to its counterpart with the single Gaussian and effectively captures the model uncertainty, which enables a strong cross-domain generation ability. We evaluated our method for stereo matching by training the model using the Scene Flow dataset and testing it on KITTI 2015 and Middlebury 2014. The experiment results cons
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30382;&#32932;&#30149;&#21464;&#20174;&#30382;&#32932;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#25260;&#21319;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#25910;&#38598;&#30340;Dataset&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#36328;&#22495;&#35782;&#21035;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02792</link><description>&lt;p&gt;
Lesion Elevation Prediction from Skin Images Improves Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02792
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30382;&#32932;&#30149;&#21464;&#20174;&#30382;&#32932;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#25260;&#21319;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#25910;&#38598;&#30340;Dataset&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#36328;&#22495;&#35782;&#21035;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02792v1 Announce Type: new  Abstract: While deep learning-based computer-aided diagnosis for skin lesion image analysis is approaching dermatologists' performance levels, there are several works showing that incorporating additional features such as shape priors, texture, color constancy, and illumination further improves the lesion diagnosis performance. In this work, we look at another clinically useful feature, skin lesion elevation, and investigate the feasibility of predicting and leveraging skin lesion elevation labels. Specifically, we use a deep learning model to predict image-level lesion elevation labels from 2D skin lesion images. We test the elevation prediction accuracy on the derm7pt dataset, and use the elevation prediction model to estimate elevation labels for images from five other datasets: ISIC 2016, 2017, and 2018 Challenge datasets, MSK, and DermoFit. We evaluate cross-domain generalization by using these estimated elevation labels as auxiliary inputs t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GazeXplain&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26469;&#39044;&#27979;&#35270;&#35273;&#25195;&#25551;&#36335;&#24452;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20256;&#32479;&#25195;&#25551;&#27169;&#22411;&#20165;&#39044;&#27979;&#35270;&#32447;&#36716;&#31227;&#32780;&#19981;&#25552;&#20379;&#35299;&#37322;&#30340;&#31354;&#30333;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#35270;&#32447;&#22266;&#23450;&#30340;&#20154;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#27880;&#24847;&#21147;-&#35821;&#35328;&#35299;&#30721;&#22120;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#25195;&#25551;&#36335;&#24452;&#21644;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#25972;&#21512;&#19968;&#31181;&#29420;&#29305;&#30340;&#35821;&#20041;&#23545;&#40784;&#26426;&#21046;&#26469;&#25552;&#39640;&#35299;&#37322;&#19982;&#35270;&#32447;&#22266;&#23450;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#21019;&#26032;&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#35270;&#35273;&#25195;&#25551;&#36335;&#24452;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02788</link><description>&lt;p&gt;
GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GazeXplain&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26469;&#39044;&#27979;&#35270;&#35273;&#25195;&#25551;&#36335;&#24452;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20256;&#32479;&#25195;&#25551;&#27169;&#22411;&#20165;&#39044;&#27979;&#35270;&#32447;&#36716;&#31227;&#32780;&#19981;&#25552;&#20379;&#35299;&#37322;&#30340;&#31354;&#30333;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#35270;&#32447;&#22266;&#23450;&#30340;&#20154;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#27880;&#24847;&#21147;-&#35821;&#35328;&#35299;&#30721;&#22120;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#25195;&#25551;&#36335;&#24452;&#21644;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#25972;&#21512;&#19968;&#31181;&#29420;&#29305;&#30340;&#35821;&#20041;&#23545;&#40784;&#26426;&#21046;&#26469;&#25552;&#39640;&#35299;&#37322;&#19982;&#35270;&#32447;&#22266;&#23450;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#21019;&#26032;&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#35270;&#35273;&#25195;&#25551;&#36335;&#24452;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02788v1 Announce Type: new  Abstract: While exploring visual scenes, humans' scanpaths are driven by their underlying attention processes. Understanding visual scanpaths is essential for various applications. Traditional scanpath models predict the where and when of gaze shifts without providing explanations, creating a gap in understanding the rationale behind fixations. To bridge this gap, we introduce GazeXplain, a novel study of visual scanpath prediction and explanation. This involves annotating natural-language explanations for fixations across eye-tracking datasets and proposing a general model with an attention-language decoder that jointly predicts scanpaths and generates explanations. It integrates a unique semantic alignment mechanism to enhance the consistency between fixations and explanations, alongside a cross-dataset co-training approach for generalization. These novelties present a comprehensive and adaptable solution for explainable human visual scanpath pr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;StyleSeg&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20998;&#21106;&#39118;&#26684;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#27880;&#32773;&#23545;&#24212;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#30382;&#32932;&#30149;&#21464;&#22270;&#20687;&#21450;&#20854;&#25513;&#33180;&#23545;&#20013;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#26368;&#22823;&#30340;&#22810;&#26631;&#27880;&#32773;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;ISIC-MultiAnnot&#65289;&#36827;&#34892;&#20102;&#24773;&#24863;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#27979;&#39118;&#26684;&#19982;&#26631;&#27880;&#32773;&#20559;&#22909;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02787</link><description>&lt;p&gt;
Segmentation Style Discovery: Application to Skin Lesion Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;StyleSeg&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20998;&#21106;&#39118;&#26684;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#27880;&#32773;&#23545;&#24212;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#30382;&#32932;&#30149;&#21464;&#22270;&#20687;&#21450;&#20854;&#25513;&#33180;&#23545;&#20013;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#26368;&#22823;&#30340;&#22810;&#26631;&#27880;&#32773;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;ISIC-MultiAnnot&#65289;&#36827;&#34892;&#20102;&#24773;&#24863;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#27979;&#39118;&#26684;&#19982;&#26631;&#27880;&#32773;&#20559;&#22909;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02787v1 Announce Type: new  Abstract: Variability in medical image segmentation, arising from annotator preferences, expertise, and their choice of tools, has been well documented. While the majority of multi-annotator segmentation approaches focus on modeling annotator-specific preferences, they require annotator-segmentation correspondence. In this work, we introduce the problem of segmentation style discovery, and propose StyleSeg, a segmentation method that learns plausible, diverse, and semantically consistent segmentation styles from a corpus of image-mask pairs without any knowledge of annotator correspondence. StyleSeg consistently outperforms competing methods on four publicly available skin lesion segmentation (SLS) datasets. We also curate ISIC-MultiAnnot, the largest multi-annotator SLS dataset with annotator correspondence, and our results show a strong alignment, using our newly proposed measure AS2, between the predicted styles and annotator preferences. The c
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#40065;&#26834;&#30340;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;LR-Net&#65292;&#36890;&#36807;&#26500;&#24314;&#36731;&#37327;&#32423;&#30340;&#29305;&#24449;&#25552;&#21462;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#31616;&#21333;&#30340;&#29305;&#24449;&#32454;&#21270;&#20256;&#36755;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#30446;&#26631;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#20449;&#24687;&#20132;&#27969;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#29305;&#24449;&#31934;&#32454;&#25552;&#21462;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36164;&#28304;&#28040;&#32791;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2408.02780</link><description>&lt;p&gt;
LR-Net: A Lightweight and Robust Network for Infrared Small Target Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#40065;&#26834;&#30340;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;LR-Net&#65292;&#36890;&#36807;&#26500;&#24314;&#36731;&#37327;&#32423;&#30340;&#29305;&#24449;&#25552;&#21462;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#31616;&#21333;&#30340;&#29305;&#24449;&#32454;&#21270;&#20256;&#36755;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#30446;&#26631;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#20449;&#24687;&#20132;&#27969;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#29305;&#24449;&#31934;&#32454;&#25552;&#21462;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36164;&#28304;&#28040;&#32791;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02780v1 Announce Type: new  Abstract: Limited by equipment limitations and the lack of target intrinsic features, existing infrared small target detection methods have difficulty meeting actual comprehensive performance requirements. Therefore, we propose an innovative lightweight and robust network (LR-Net), which abandons the complex structure and achieves an effective balance between detection accuracy and resource consumption. Specifically, to ensure the lightweight and robustness, on the one hand, we construct a lightweight feature extraction attention (LFEA) module, which can fully extract target features and strengthen information interaction across channels. On the other hand, we construct a simple refined feature transfer (RFT) module. Compared with direct cross-layer connections, the RFT module can improve the network's feature refinement extraction capability with little resource consumption. Meanwhile, to solve the problem of small target loss in high-level featu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21333;&#28857;&#30417;&#30563;&#30340;&#31934;&#28860;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#35780;&#20272;&#26631;&#31614;&#36827;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;TTA&#21644;CRF&#25216;&#26415;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21487;&#35843;&#28789;&#25935;&#24230;&#31574;&#30053;&#25552;&#39640;&#20102;&#26816;&#27979;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02773</link><description>&lt;p&gt;
Refined Infrared Small Target Detection Scheme with Single-Point Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02773
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21333;&#28857;&#30417;&#30563;&#30340;&#31934;&#28860;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#35780;&#20272;&#26631;&#31614;&#36827;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;TTA&#21644;CRF&#25216;&#26415;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21487;&#35843;&#28789;&#25935;&#24230;&#31574;&#30053;&#25552;&#39640;&#20102;&#26816;&#27979;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02773v1 Announce Type: new  Abstract: Recently, infrared small target detection with single-point supervision has attracted extensive attention. However, the detection accuracy of existing methods has difficulty meeting actual needs. Therefore, we propose an innovative refined infrared small target detection scheme with single-point supervision, which has excellent segmentation accuracy and detection rate. Specifically, we introduce label evolution with single point supervision (LESPS) framework and explore the performance of various excellent infrared small target detection networks based on this framework. Meanwhile, to improve the comprehensive performance, we construct a complete post-processing strategy. On the one hand, to improve the segmentation accuracy, we use a combination of test-time augmentation (TTA) and conditional random field (CRF) for post-processing. On the other hand, to improve the detection rate, we introduce an adjustable sensitivity (AS) strategy for
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Anticipation via Recognition and Reasoning (ARR)"&#30340;&#20840;&#26032;&#31471;&#21040;&#31471;&#35270;&#39057;&#24314;&#27169;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#20998;&#35299;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#20026;&#21160;&#20316;&#35782;&#21035;&#21644;&#24207;&#21015;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#19979;&#19968;&#27493;&#21160;&#20316;&#39044;&#27979;&#65288;NAP&#65289;&#26041;&#27861;&#65292;&#26377;&#25928;&#23398;&#20064;&#20102;&#19981;&#21516;&#21160;&#20316;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#26102;&#38388;&#32858;&#21512;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;ARR&#33021;&#22815;&#22312;&#23398;&#20064;&#21160;&#20316;&#20043;&#38388;&#30340;&#32479;&#35745;&#32852;&#31995;&#26041;&#38754;&#34920;&#29616;&#24471;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2408.02769</link><description>&lt;p&gt;
From Recognition to Prediction: Leveraging Sequence Reasoning for Action Anticipation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Anticipation via Recognition and Reasoning (ARR)"&#30340;&#20840;&#26032;&#31471;&#21040;&#31471;&#35270;&#39057;&#24314;&#27169;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#20998;&#35299;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#20026;&#21160;&#20316;&#35782;&#21035;&#21644;&#24207;&#21015;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#19979;&#19968;&#27493;&#21160;&#20316;&#39044;&#27979;&#65288;NAP&#65289;&#26041;&#27861;&#65292;&#26377;&#25928;&#23398;&#20064;&#20102;&#19981;&#21516;&#21160;&#20316;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#26102;&#38388;&#32858;&#21512;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;ARR&#33021;&#22815;&#22312;&#23398;&#20064;&#21160;&#20316;&#20043;&#38388;&#30340;&#32479;&#35745;&#32852;&#31995;&#26041;&#38754;&#34920;&#29616;&#24471;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02769v1 Announce Type: new  Abstract: The action anticipation task refers to predicting what action will happen based on observed videos, which requires the model to have a strong ability to summarize the present and then reason about the future. Experience and common sense suggest that there is a significant correlation between different actions, which provides valuable prior knowledge for the action anticipation task. However, previous methods have not effectively modeled this underlying statistical relationship. To address this issue, we propose a novel end-to-end video modeling architecture that utilizes attention mechanisms, named Anticipation via Recognition and Reasoning (ARR). ARR decomposes the action anticipation task into action recognition and sequence reasoning tasks, and effectively learns the statistical relationship between actions by next action prediction (NAP). In comparison to existing temporal aggregation strategies, ARR is able to extract more effective
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ConDL&#30340;&#28145;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#31264;&#23494;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#20840;&#21367;&#31215;&#27169;&#22411;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#31264;&#23494;&#29305;&#24449;&#22270;&#65292;&#27599;&#20010;&#20687;&#32032;&#37117;&#34987;&#36171;&#20104;&#20102;&#33021;&#22815;&#36328;&#22810;&#24352;&#22270;&#20687;&#21305;&#37197;&#30340;&#25551;&#36848;&#31526;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;&#21508;&#31181;&#25197;&#26354;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21253;&#25324;&#35270;&#35282;&#21464;&#21270;&#12289;&#20809;&#29031;&#24046;&#24322;&#12289;&#38452;&#24433;&#21644;&#21453;&#20809;&#20142;&#28857;&#31561;&#12290;&#35813;&#25991;&#31456;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#29305;&#24449;&#22270;&#23545;&#36825;&#20123;&#25197;&#26354;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21305;&#37197;&#32467;&#26524;&#30340;&#31283;&#22266;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#26816;&#27979;&#22120;&#65292;&#36825;&#26174;&#33879;&#21306;&#21035;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#22270;&#20687;&#21305;&#37197;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2408.02766</link><description>&lt;p&gt;
ConDL: Detector-Free Dense Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ConDL&#30340;&#28145;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#31264;&#23494;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#20840;&#21367;&#31215;&#27169;&#22411;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#31264;&#23494;&#29305;&#24449;&#22270;&#65292;&#27599;&#20010;&#20687;&#32032;&#37117;&#34987;&#36171;&#20104;&#20102;&#33021;&#22815;&#36328;&#22810;&#24352;&#22270;&#20687;&#21305;&#37197;&#30340;&#25551;&#36848;&#31526;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;&#21508;&#31181;&#25197;&#26354;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21253;&#25324;&#35270;&#35282;&#21464;&#21270;&#12289;&#20809;&#29031;&#24046;&#24322;&#12289;&#38452;&#24433;&#21644;&#21453;&#20809;&#20142;&#28857;&#31561;&#12290;&#35813;&#25991;&#31456;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#29305;&#24449;&#22270;&#23545;&#36825;&#20123;&#25197;&#26354;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21305;&#37197;&#32467;&#26524;&#30340;&#31283;&#22266;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#26816;&#27979;&#22120;&#65292;&#36825;&#26174;&#33879;&#21306;&#21035;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#22270;&#20687;&#21305;&#37197;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02766v1 Announce Type: new  Abstract: In this work, we introduce a deep-learning framework designed for estimating dense image correspondences. Our fully convolutional model generates dense feature maps for images, where each pixel is associated with a descriptor that can be matched across multiple images. Unlike previous methods, our model is trained on synthetic data that includes significant distortions, such as perspective changes, illumination variations, shadows, and specular highlights. Utilizing contrastive learning, our feature maps achieve greater invariance to these distortions, enabling robust matching. Notably, our method eliminates the need for a keypoint detector, setting it apart from many existing image-matching techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#21322;&#30417;&#30563;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#32531;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#24182;&#38477;&#20302;&#25968;&#25454;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#26631;&#31614;&#21644;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#24369;&#26631;&#31614;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#26631;&#27880;&#25104;&#26412;&#30340;&#21069;&#25552;&#19979;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22495;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02761</link><description>&lt;p&gt;
Dimensionality Reduction and Nearest Neighbors for Improving Out-of-Distribution Detection in Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#21322;&#30417;&#30563;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#32531;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#24182;&#38477;&#20302;&#25968;&#25454;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#26631;&#31614;&#21644;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#24369;&#26631;&#31614;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#26631;&#27880;&#25104;&#26412;&#30340;&#21069;&#25552;&#19979;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22495;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02761v1 Announce Type: new  Abstract: Clinically deployed deep learning-based segmentation models are known to fail on data outside of their training distributions. While clinicians review the segmentations, these models tend to perform well in most instances, which could exacerbate automation bias. Therefore, detecting out-of-distribution images at inference is critical to warn the clinicians that the model likely failed. This work applied the Mahalanobis distance (MD) post hoc to the bottleneck features of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted magnetic resonance imaging and computed tomography. By reducing the dimensions of the bottleneck features with either principal component analysis or uniform manifold approximation and projection, images the models failed on were detected with high performance and minimal computational load. In addition, this work explored a non-parametric alternative to the MD, a k-th nearest neighbors distance (
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#25968;&#25454;&#25366;&#25496;&#24037;&#20855;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#20013;&#35270;&#35273;&#20803;&#32032;&#30340;&#20856;&#22411;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#27010;&#25324;&#25968;&#25454;&#65292;&#24182;&#23545;&#20110;&#29305;&#23450;&#26631;&#31614;&#65288;&#22914;&#22320;&#29702;&#20301;&#32622;&#12289;&#26102;&#38388;&#25139;&#12289;&#26631;&#31614;&#25110;&#30142;&#30149;&#65289;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02752</link><description>&lt;p&gt;
Diffusion Models as Data Mining Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#25968;&#25454;&#25366;&#25496;&#24037;&#20855;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#20013;&#35270;&#35273;&#20803;&#32032;&#30340;&#20856;&#22411;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#27010;&#25324;&#25968;&#25454;&#65292;&#24182;&#23545;&#20110;&#29305;&#23450;&#26631;&#31614;&#65288;&#22914;&#22320;&#29702;&#20301;&#32622;&#12289;&#26102;&#38388;&#25139;&#12289;&#26631;&#31614;&#25110;&#30142;&#30149;&#65289;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02752v1 Announce Type: new  Abstract: This paper demonstrates how to use generative models trained for image synthesis as tools for visual data mining. Our insight is that since contemporary generative models learn an accurate representation of their training data, we can use them to summarize the data by mining for visual patterns. Concretely, we show that after finetuning conditional diffusion models to synthesize images from a specific dataset, we can use these models to define a typicality measure on that dataset. This measure assesses how typical visual elements are for different data labels, such as geographic location, time stamps, semantic labels, or even the presence of a disease. This analysis-by-synthesis approach to data mining has two key advantages. First, it scales much better than traditional correspondence-based approaches since it does not require explicitly comparing all pairs of visual elements. Second, while most previous works on visual data mining focu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21512;&#25104;&#30340;&#12289;&#19981;&#27844;&#38706;&#36523;&#20221;&#20449;&#24687;&#30340;&#32418;&#22806;&#22270;&#29255;&#30340;&#38544;&#31169;&#23433;&#20840;&#32418;&#22806;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#31526;&#21512;ISO/IEC 19794-6&#26631;&#20934;&#30340;&#32418;&#22806;&#22270;&#29255;&#65292;&#20197;&#21450;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#27169;&#25311;&#30495;&#23454;&#36523;&#20221;&#21644;&#26080;&#36523;&#20221;&#27844;&#38706;&#30340;&#21512;&#25104;&#26679;&#26412;&#26469;&#26816;&#27979;&#25915;&#20987;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#35780;&#20215;&#26102;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02750</link><description>&lt;p&gt;
Privacy-Safe Iris Presentation Attack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21512;&#25104;&#30340;&#12289;&#19981;&#27844;&#38706;&#36523;&#20221;&#20449;&#24687;&#30340;&#32418;&#22806;&#22270;&#29255;&#30340;&#38544;&#31169;&#23433;&#20840;&#32418;&#22806;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#31526;&#21512;ISO/IEC 19794-6&#26631;&#20934;&#30340;&#32418;&#22806;&#22270;&#29255;&#65292;&#20197;&#21450;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#27169;&#25311;&#30495;&#23454;&#36523;&#20221;&#21644;&#26080;&#36523;&#20221;&#27844;&#38706;&#30340;&#21512;&#25104;&#26679;&#26412;&#26469;&#26816;&#27979;&#25915;&#20987;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#35780;&#20215;&#26102;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02750v1 Announce Type: new  Abstract: This paper proposes a framework for a privacy-safe iris presentation attack detection (PAD) method, designed solely with synthetically-generated, identity-leakage-free iris images. Once trained, the method is evaluated in a classical way using state-of-the-art iris PAD benchmarks. We designed two generative models for the synthesis of ISO/IEC 19794-6-compliant iris images. The first model synthesizes bona fide-looking samples. To avoid ``identity leakage,'' the generated samples that accidentally matched those used in the model's training were excluded. The second model synthesizes images of irises with textured contact lenses and is conditioned by a given contact lens brand to have better control over textured contact lens appearance when forming the training set. Our experiments demonstrate that models trained solely on synthetic data achieve a lower but still reasonable performance when compared to solutions trained with iris images c
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MMIU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#22810;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;7&#31181;&#22810;&#22270;&#20687;&#20851;&#31995;&#12289;52&#20010;&#20219;&#21153;&#21644;11,000&#22810;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#22810;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;benchmark&#65292;&#25581;&#31034;&#20102;&#21253;&#25324;&#39030;&#32423;&#27169;&#22411;&#22312;&#20869;&#30340;&#22810;&#22270;&#20687;&#29702;&#35299;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#22810;&#22270;&#20687;&#29702;&#35299;&#21644;&#31354;&#38388;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2408.02718</link><description>&lt;p&gt;
MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MMIU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#22810;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;7&#31181;&#22810;&#22270;&#20687;&#20851;&#31995;&#12289;52&#20010;&#20219;&#21153;&#21644;11,000&#22810;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#22810;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;benchmark&#65292;&#25581;&#31034;&#20102;&#21253;&#25324;&#39030;&#32423;&#27169;&#22411;&#22312;&#20869;&#30340;&#22810;&#22270;&#20687;&#29702;&#35299;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#22810;&#22270;&#20687;&#29702;&#35299;&#21644;&#31354;&#38388;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02718v1 Announce Type: new  Abstract: The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Thro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;RCDM&#65288;Robust Conditional Diffusion Model&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#21160;&#24577;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#24182;&#26174;&#33879;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#30340;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25913;&#21892;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#22312;&#22788;&#29702;&#19981;&#24403;&#30340;&#29305;&#23450;&#36755;&#20837;&#26102;&#20986;&#29616;&#30340;&#32570;&#38519;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#26356;&#21152;&#36866;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.02710</link><description>&lt;p&gt;
RCDM: Enabling Robustness for Conditional Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;RCDM&#65288;Robust Conditional Diffusion Model&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#21160;&#24577;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#24182;&#26174;&#33879;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#30340;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25913;&#21892;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#22312;&#22788;&#29702;&#19981;&#24403;&#30340;&#29305;&#23450;&#36755;&#20837;&#26102;&#20986;&#29616;&#30340;&#32570;&#38519;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#26356;&#21152;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02710v1 Announce Type: cross  Abstract: The conditional diffusion model (CDM) enhances the standard diffusion model by providing more control, improving the quality and relevance of the outputs, and making the model adaptable to a wider range of complex tasks. However, inaccurate conditional inputs in the inverse process of CDM can easily lead to generating fixed errors in the neural network, which diminishes the adaptability of a well-trained model. The existing methods like data augmentation, adversarial training, robust optimization can improve the robustness, while they often face challenges such as high computational complexity, limited applicability to unknown perturbations, and increased training difficulty. In this paper, we propose a lightweight solution, the Robust Conditional Diffusion Model (RCDM), based on control theory to dynamically reduce the impact of noise and significantly enhance the model's robustness. RCDM leverages the collaborative interaction betwee
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25163;&#20889;&#31508;&#20132;&#20114;&#20998;&#21106;&#30340;&#21307;&#23398; hyperspectral &#22270;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29992;&#25143;&#30693;&#35782;&#32467;&#21512;&#20020;&#24202;&#27934;&#23519;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#21106; hyperspectral &#22270;&#20687;&#26102;&#38754;&#20020;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02708</link><description>&lt;p&gt;
Scribble-Based Interactive Segmentation of Medical Hyperspectral Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25163;&#20889;&#31508;&#20132;&#20114;&#20998;&#21106;&#30340;&#21307;&#23398; hyperspectral &#22270;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29992;&#25143;&#30693;&#35782;&#32467;&#21512;&#20020;&#24202;&#27934;&#23519;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#21106; hyperspectral &#22270;&#20687;&#26102;&#38754;&#20020;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02708v1 Announce Type: cross  Abstract: Hyperspectral imaging (HSI) is an advanced medical imaging modality that captures optical data across a broad spectral range, providing novel insights into the biochemical composition of tissues. HSI may enable precise differentiation between various tissue types and pathologies, making it particularly valuable for tumour detection, tissue classification, and disease diagnosis.   Deep learning-based segmentation methods have shown considerable advancements, offering automated and accurate results. However, these methods face challenges with HSI datasets due to limited annotated data and discrepancies from hardware and acquisition techniques~\cite{clancy2020surgical,studier2023heiporspectral}. Variability in clinical protocols also leads to different definitions of structure boundaries. Interactive segmentation methods, utilizing user knowledge and clinical insights, can overcome these issues and achieve precise segmentation results \ci
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;Compositional Physical Reasoning (ComPhy)&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26377;&#38480;&#30340;&#35270;&#39057;&#20869;&#23481;&#35753;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#23545;&#35937;&#30340;&#29289;&#29702;&#23646;&#24615;&#65288;&#22914;&#36136;&#37327;&#21644;&#30005;&#33655;&#65289;&#65292;&#24182;&#39044;&#27979;&#36825;&#20123;&#23646;&#24615;&#22312;&#20132;&#20114;&#20013;&#30340;&#21160;&#24577;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35780;&#20215;&#20102;&#27169;&#22411;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#33258;&#28982;&#30028;&#30340;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#22312;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.02687</link><description>&lt;p&gt;
Compositional Physical Reasoning of Objects and Events from Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;Compositional Physical Reasoning (ComPhy)&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26377;&#38480;&#30340;&#35270;&#39057;&#20869;&#23481;&#35753;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#23545;&#35937;&#30340;&#29289;&#29702;&#23646;&#24615;&#65288;&#22914;&#36136;&#37327;&#21644;&#30005;&#33655;&#65289;&#65292;&#24182;&#39044;&#27979;&#36825;&#20123;&#23646;&#24615;&#22312;&#20132;&#20114;&#20013;&#30340;&#21160;&#24577;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35780;&#20215;&#20102;&#27169;&#22411;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#33258;&#28982;&#30028;&#30340;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#22312;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02687v1 Announce Type: new  Abstract: Understanding and reasoning about objects' physical properties in the natural world is a fundamental challenge in artificial intelligence. While some properties like colors and shapes can be directly observed, others, such as mass and electric charge, are hidden from the objects' visual appearance. This paper addresses the unique challenge of inferring these hidden physical properties from objects' motion and interactions and predicting corresponding dynamics based on the inferred physical properties. We first introduce the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes limited videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions. Besides the synthetic videos from simulators, we also collect a real-wo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#25968;&#25454;&#24211;&#20013;&#22522;&#20110;&#30524;&#24213;&#22270;&#20687;&#30340;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23384;&#22312;&#30340;&#24040;&#22823;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#25972;&#20307;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#35780;&#20272;&#20013;&#24515;&#30340;&#20010;&#20307;&#20173;&#28982;&#38754;&#20020;&#26174;&#33879;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25581;&#31034;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#36807;&#31243;&#20013;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#32780;&#19988;&#23545;&#27604;&#20102;&#22810;&#31181;&#29616;&#26377;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38024;&#23545;&#19981;&#21516;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#32531;&#35299;&#26041;&#27861;&#25928;&#26524;&#24046;&#24322;&#29978;&#22823;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#23450;&#21046;&#21270;&#30340;&#32531;&#35299;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26368;&#32456;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32531;&#35299;&#25163;&#27573;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#25928;&#26524;&#26377;&#38480;&#65292;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#23545;&#20844;&#24179;&#24615;&#31639;&#27861;&#30340;&#39640;&#35201;&#27714;&#21644;&#32487;&#32493;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02676</link><description>&lt;p&gt;
On Biases in a UK Biobank-based Retinal Image Classification Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#25968;&#25454;&#24211;&#20013;&#22522;&#20110;&#30524;&#24213;&#22270;&#20687;&#30340;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23384;&#22312;&#30340;&#24040;&#22823;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#25972;&#20307;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#35780;&#20272;&#20013;&#24515;&#30340;&#20010;&#20307;&#20173;&#28982;&#38754;&#20020;&#26174;&#33879;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25581;&#31034;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#36807;&#31243;&#20013;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#32780;&#19988;&#23545;&#27604;&#20102;&#22810;&#31181;&#29616;&#26377;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38024;&#23545;&#19981;&#21516;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#32531;&#35299;&#26041;&#27861;&#25928;&#26524;&#24046;&#24322;&#29978;&#22823;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#23450;&#21046;&#21270;&#30340;&#32531;&#35299;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26368;&#32456;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32531;&#35299;&#25163;&#27573;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#25928;&#26524;&#26377;&#38480;&#65292;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#23545;&#20844;&#24179;&#24615;&#31639;&#27861;&#30340;&#39640;&#35201;&#27714;&#21644;&#32487;&#32493;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02676v1 Announce Type: cross  Abstract: Recent work has uncovered alarming disparities in the performance of machine learning models in healthcare. In this study, we explore whether such disparities are present in the UK Biobank fundus retinal images by training and evaluating a disease classification model on these images. We assess possible disparities across various population groups and find substantial differences despite strong overall performance of the model. In particular, we discover unfair performance for certain assessment centres, which is surprising given the rigorous data standardisation protocol. We compare how these differences emerge and apply a range of existing bias mitigation methods to each one. A key insight is that each disparity has unique properties and responds differently to the mitigation methods. We also find that these methods are largely unable to enhance fairness, highlighting the need for better bias mitigation methods tailored to the specif
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#25239;&#25915;&#20987;&#20013;&#24212;&#29992;&#24847;&#22270;&#28151;&#28102;&#25216;&#26415;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#23545;&#35937;&#37051;&#36817;&#28155;&#21152;&#38750;&#37325;&#21472;&#30340;&#25200;&#21160;&#23545;&#35937;&#26469;&#38544;&#34255;&#25915;&#20987;&#24847;&#22270;&#12290;&#30740;&#31350;&#22242;&#38431;&#36827;&#34892;&#20102;&#38543;&#26426;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;5&#31181;&#27969;&#34892;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;YOLOv3&#12289;SSD&#12289;RetinaNet&#12289;Faster R-CNN&#21644;Cascade R-CNN&#65289;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#25915;&#20987;&#26102;&#22343;&#23454;&#29616;&#20102;&#25104;&#21151;&#12290;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#25104;&#21151;&#22240;&#32032;&#65292;&#21253;&#25324;&#30446;&#26631;&#23545;&#35937;&#30340;&#21487;&#20449;&#24230;&#21644;&#25200;&#21160;&#23545;&#35937;&#30340;&#22823;&#23567;&#65292;&#24182;&#24314;&#35758;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#20123;&#22240;&#32032;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02674</link><description>&lt;p&gt;
On Feasibility of Intent Obfuscating Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#25239;&#25915;&#20987;&#20013;&#24212;&#29992;&#24847;&#22270;&#28151;&#28102;&#25216;&#26415;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#23545;&#35937;&#37051;&#36817;&#28155;&#21152;&#38750;&#37325;&#21472;&#30340;&#25200;&#21160;&#23545;&#35937;&#26469;&#38544;&#34255;&#25915;&#20987;&#24847;&#22270;&#12290;&#30740;&#31350;&#22242;&#38431;&#36827;&#34892;&#20102;&#38543;&#26426;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;5&#31181;&#27969;&#34892;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;YOLOv3&#12289;SSD&#12289;RetinaNet&#12289;Faster R-CNN&#21644;Cascade R-CNN&#65289;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#25915;&#20987;&#26102;&#22343;&#23454;&#29616;&#20102;&#25104;&#21151;&#12290;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#25104;&#21151;&#22240;&#32032;&#65292;&#21253;&#25324;&#30446;&#26631;&#23545;&#35937;&#30340;&#21487;&#20449;&#24230;&#21644;&#25200;&#21160;&#23545;&#35937;&#30340;&#22823;&#23567;&#65292;&#24182;&#24314;&#35758;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#20123;&#22240;&#32032;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02674v1 Announce Type: cross  Abstract: Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose incorporating intent obfuscation in generating adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors -- YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN -- using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attack
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EI-VLG&#30340;&#38271;&#26399;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#65288;VLG&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26469;&#27169;&#25311;&#20154;&#31867;&#32463;&#39564;&#65292;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#35270;&#39057;&#29255;&#27573;&#65292;&#22686;&#24378;&#20102;&#35270;&#39057;-&#35821;&#35328;&#20849;&#21516;&#34920;&#24449;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#38382;&#39064;&#20013;&#24573;&#30053;&#38750;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02336</link><description>&lt;p&gt;
Infusing Environmental Captions for Long-Form Video Language Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EI-VLG&#30340;&#38271;&#26399;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#65288;VLG&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26469;&#27169;&#25311;&#20154;&#31867;&#32463;&#39564;&#65292;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#35270;&#39057;&#29255;&#27573;&#65292;&#22686;&#24378;&#20102;&#35270;&#39057;-&#35821;&#35328;&#20849;&#21516;&#34920;&#24449;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#38382;&#39064;&#20013;&#24573;&#30053;&#38750;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#28304;&#25968;&#25454;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#28304;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#20511;&#21161;&#19981;&#30830;&#23450;&#24615;&#21644;&#28201;&#24230;&#35843;&#33410;&#65292;&#35813;&#25991;&#31456;&#23637;&#29616;&#20102;&#22312;&#32570;&#20047;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23545;&#35937;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.02209</link><description>&lt;p&gt;
Source-Free Domain-Invariant Performance Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#28304;&#25968;&#25454;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#28304;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#20511;&#21161;&#19981;&#30830;&#23450;&#24615;&#21644;&#28201;&#24230;&#35843;&#33410;&#65292;&#35813;&#25991;&#31456;&#23637;&#29616;&#20102;&#22312;&#32570;&#20047;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23545;&#35937;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02209v1 Announce Type: new  Abstract: Accurately estimating model performance poses a significant challenge, particularly in scenarios where the source and target domains follow different data distributions. Most existing performance prediction methods heavily rely on the source data in their estimation process, limiting their applicability in a more realistic setting where only the trained model is accessible. The few methods that do not require source data exhibit considerably inferior performance. In this work, we propose a source-free approach centred on uncertainty-based estimation, using a generative model for calibration in the absence of source data. We establish connections between our approach for unsupervised calibration and temperature scaling. We then employ a gradient-based strategy to evaluate the correctness of the calibrated predictions. Our experiments on benchmark object recognition datasets reveal that existing source-based methods fall short with limited
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#31995;&#32479;&#35780;&#20272;&#20102;&#38024;&#23545;&#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#25918;&#28304;&#25915;&#20987;&#26041;&#27861;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#23545;&#25915;&#20987;&#25928;&#26524;&#21644;&#30456;&#24212;&#32531;&#35299;&#25514;&#26045;&#29702;&#35299;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25351;&#20986;&#20102;&#26410;&#26469;&#22312;&#20445;&#25252;&#33258;&#21160;&#21270;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#23433;&#20840;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#22823;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.01934</link><description>&lt;p&gt;
A Survey and Evaluation of Adversarial Attacks for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#31995;&#32479;&#35780;&#20272;&#20102;&#38024;&#23545;&#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#25918;&#28304;&#25915;&#20987;&#26041;&#27861;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#23545;&#25915;&#20987;&#25928;&#26524;&#21644;&#30456;&#24212;&#32531;&#35299;&#25514;&#26045;&#29702;&#35299;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25351;&#20986;&#20102;&#26410;&#26469;&#22312;&#20445;&#25252;&#33258;&#21160;&#21270;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#23433;&#20840;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#22823;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01934v1 Announce Type: new  Abstract: Deep learning models excel in various computer vision tasks but are susceptible to adversarial examples-subtle perturbations in input data that lead to incorrect predictions. This vulnerability poses significant risks in safety-critical applications such as autonomous vehicles, security surveillance, and aircraft health monitoring. While numerous surveys focus on adversarial attacks in image classification, the literature on such attacks in object detection is limited. This paper offers a comprehensive taxonomy of adversarial attacks specific to object detection, reviews existing adversarial robustness evaluation metrics, and systematically assesses open-source attack methods and model robustness. Key observations are provided to enhance the understanding of attack effectiveness and corresponding countermeasures. Additionally, we identify crucial research challenges to guide future efforts in securing automated object detection systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FBSDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#39057;&#29575;&#24102;&#26367;&#25442;&#21040;&#25193;&#25955;&#29305;&#24449;&#20013;&#65292;&#29992;&#20110;&#39640;&#24230;&#21487;&#25511;&#30340;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32763;&#35793;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#22312;&#32447;&#20248;&#21270;&#65292;&#30452;&#25509;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2408.00998</link><description>&lt;p&gt;
FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FBSDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#39057;&#29575;&#24102;&#26367;&#25442;&#21040;&#25193;&#25955;&#29305;&#24449;&#20013;&#65292;&#29992;&#20110;&#39640;&#24230;&#21487;&#25511;&#30340;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32763;&#35793;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#22312;&#32447;&#20248;&#21270;&#65292;&#30452;&#25509;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00998v2 Announce Type: replace  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I gener
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;Segment Anything Model 2 (SAM 2)&#22312;2D&#21644;3D&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25193;&#23637;&#12290;&#36890;&#36807;&#23545;18&#31181;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;SAM 2&#22312;&#22810;&#24103;3D&#20998;&#21106;&#21644;&#21333;&#24103;2D&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;CT&#12289;MRI&#12289;PET&#31561;3D&#21644;X&#20809;&#12289;&#36229;&#22768;&#31561;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.00756</link><description>&lt;p&gt;
Segment anything model 2: an application to 2D and 3D medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;Segment Anything Model 2 (SAM 2)&#22312;2D&#21644;3D&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25193;&#23637;&#12290;&#36890;&#36807;&#23545;18&#31181;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;SAM 2&#22312;&#22810;&#24103;3D&#20998;&#21106;&#21644;&#21333;&#24103;2D&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;CT&#12289;MRI&#12289;PET&#31561;3D&#21644;X&#20809;&#12289;&#36229;&#22768;&#31561;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00756v2 Announce Type: replace  Abstract: Segment Anything Model (SAM) has gained significant attention because of its ability to segment varous objects in images given a prompt. The recently developed SAM 2 has extended this ability to video inputs. This opens an opportunity to apply SAM to 3D images, one of the fundamental tasks in the medical imaging field. In this paper, we extensively evaluate SAM 2's ability to segment both 2D and 3D medical images by first collecting 18 medical imaging datasets, including common 3D modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) as well as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of SAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are provided to one or multiple slice(s) selected from the volume, and (2) single-frame 2D segmentation, where prompts are provided to each slice. The former is only applicable to 3D modaliti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#22312;&#35786;&#26029;&#21644;&#35780;&#20272;&#24739;&#32773;&#32452;&#32455;&#26679;&#26412;&#26041;&#38754;&#65292;&#30149;&#29702;&#23398;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#25163;&#26415;&#21644;&#27963;&#26816;&#26679;&#26412;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#25972;&#20010;&#28369;&#29255;&#25195;&#25551;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#30149;&#29702;&#23398;AI&#25216;&#26415;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#20943;&#36731;&#20102;&#30149;&#29702;&#23398;&#23478;&#30340;&#36127;&#25285;&#65292;&#24182;&#20026;&#27835;&#30103;&#35745;&#21010;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#25991;&#31456;&#25552;&#21040;&#65292;&#19982;&#20256;&#32479;AI&#30456;&#27604;&#65292;&#26356;&#20934;&#30830;&#21644;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;AI&#27169;&#22411;&#65292;&#21363;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#26368;&#36817;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#25991;&#31456;&#25351;&#20986;&#65292;&#36825;&#31867;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#22312;&#30149;&#29702;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30142;&#30149;&#35786;&#26029;&#12289;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#12289;&#24739;&#32773;&#29983;&#23384;&#39044;&#21518;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#34920;&#36798;&#30740;&#31350;&#31561;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2407.21317</link><description>&lt;p&gt;
Pathology Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#22312;&#35786;&#26029;&#21644;&#35780;&#20272;&#24739;&#32773;&#32452;&#32455;&#26679;&#26412;&#26041;&#38754;&#65292;&#30149;&#29702;&#23398;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#25163;&#26415;&#21644;&#27963;&#26816;&#26679;&#26412;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#25972;&#20010;&#28369;&#29255;&#25195;&#25551;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#30149;&#29702;&#23398;AI&#25216;&#26415;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#20943;&#36731;&#20102;&#30149;&#29702;&#23398;&#23478;&#30340;&#36127;&#25285;&#65292;&#24182;&#20026;&#27835;&#30103;&#35745;&#21010;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#25991;&#31456;&#25552;&#21040;&#65292;&#19982;&#20256;&#32479;AI&#30456;&#27604;&#65292;&#26356;&#20934;&#30830;&#21644;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;AI&#27169;&#22411;&#65292;&#21363;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#26368;&#36817;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#25991;&#31456;&#25351;&#20986;&#65292;&#36825;&#31867;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#22312;&#30149;&#29702;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30142;&#30149;&#35786;&#26029;&#12289;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#12289;&#24739;&#32773;&#29983;&#23384;&#39044;&#21518;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#34920;&#36798;&#30740;&#31350;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21317v2 Announce Type: replace  Abstract: Pathology has played a crucial role in the diagnosis and evaluation of patient tissue samples obtained from surgeries and biopsies for many years. The advent of Whole Slide Scanners and the development of deep learning technologies have significantly advanced the field, leading to extensive research and development in pathology AI (Artificial Intelligence). These advancements have contributed to reducing the workload of pathologists and supporting decision-making in treatment plans. Recently, large-scale AI models known as Foundation Models (FMs), which are more accurate and applicable to a wide range of tasks compared to traditional AI, have emerged, and expanded their application scope in the healthcare field. Numerous FMs have been developed in pathology, and there are reported cases of their application in various tasks, such as disease diagnosis, rare cancer diagnosis, patient survival prognosis prediction, biomarker expression 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MMTrail&#26159;&#19968;&#20010;&#21547;&#22823;&#37327;&#39044;&#21578;&#29255;&#35270;&#39057;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35270;&#39057;-&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#35270;&#39057;&#37117;&#23545;&#24212;&#26377;&#35814;&#32454;&#30340;&#35270;&#35273;&#25551;&#36848;&#21644;&#22810;&#27169;&#24577;&#25551;&#36848;&#12290;&#36825;&#20123;&#25551;&#36848;&#28085;&#30422;&#20102;&#39044;&#21578;&#29255;&#30340;&#22810;&#31181;&#20027;&#39064;&#21644;&#31649;&#29702;&#65292;&#21253;&#25324;&#24773;&#33410;&#12289;&#35270;&#35273;&#24103;&#21644;&#32972;&#26223;&#38899;&#20048;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#38598;&#22312;&#25506;&#32034;&#36328;&#27169;&#24577;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2407.20962</link><description>&lt;p&gt;
MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MMTrail&#26159;&#19968;&#20010;&#21547;&#22823;&#37327;&#39044;&#21578;&#29255;&#35270;&#39057;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35270;&#39057;-&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#35270;&#39057;&#37117;&#23545;&#24212;&#26377;&#35814;&#32454;&#30340;&#35270;&#35273;&#25551;&#36848;&#21644;&#22810;&#27169;&#24577;&#25551;&#36848;&#12290;&#36825;&#20123;&#25551;&#36848;&#28085;&#30422;&#20102;&#39044;&#21578;&#29255;&#30340;&#22810;&#31181;&#20027;&#39064;&#21644;&#31649;&#29702;&#65292;&#21253;&#25324;&#24773;&#33410;&#12289;&#35270;&#35273;&#24103;&#21644;&#32972;&#26223;&#38899;&#20048;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#38598;&#22312;&#25506;&#32034;&#36328;&#27169;&#24577;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20962v2 Announce Type: replace  Abstract: Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions. Such ignorance results in the difficulty of multiple cross-modality studies. To fulfill this gap, we present MMTrail, a large-scale multi-modality video-language dataset incorporating more than 20M trailer clips with visual captions, and 2M high-quality clips with multimodal captions. Trailers preview full-length video works and integrate context, visual frames, and background music. In particular, the trailer has two main advantages: (1) the topics are diverse, and the content characters 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#25193;&#25955;&#36807;&#31243;&#35757;&#32451;CLIP&#27169;&#22411;&#20197;&#20811;&#26381;&#20854;&#35270;&#35273;&#30701;&#26495;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;DIVA&#65288;DIffusion model as a Visual Assistant for CLIP&#65289;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;CLIP&#30340;&#35270;&#35273;&#21161;&#25163;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;CLIP&#22312;&#35270;&#35273;&#24863;&#30693;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.20171</link><description>&lt;p&gt;
Diffusion Feedback Helps CLIP See Better
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#25193;&#25955;&#36807;&#31243;&#35757;&#32451;CLIP&#27169;&#22411;&#20197;&#20811;&#26381;&#20854;&#35270;&#35273;&#30701;&#26495;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;DIVA&#65288;DIffusion model as a Visual Assistant for CLIP&#65289;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;CLIP&#30340;&#35270;&#35273;&#21161;&#25163;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;CLIP&#22312;&#35270;&#35273;&#24863;&#30693;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20171v2 Announce Type: replace  Abstract: Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedb
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#22522;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22270;&#20687;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#35299;&#37322;&#28145;&#24230;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21516;&#26102;&#20248;&#21270;&#19968;&#20010;&#36873;&#25321;&#22120;&#21644;&#39044;&#27979;&#22120;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#37325;&#35201;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#35299;&#37322;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#21306;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#23436;&#20840;&#24615;&#21644;&#20114;&#38145;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#21152;&#24378;&#22823;&#19988;&#35299;&#37322;&#24615;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2407.19308</link><description>&lt;p&gt;
Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#22522;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22270;&#20687;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#35299;&#37322;&#28145;&#24230;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21516;&#26102;&#20248;&#21270;&#19968;&#20010;&#36873;&#25321;&#22120;&#21644;&#39044;&#27979;&#22120;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#37325;&#35201;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#35299;&#37322;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#21306;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#23436;&#20840;&#24615;&#21644;&#20114;&#38145;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#21152;&#24378;&#22823;&#19988;&#35299;&#37322;&#24615;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19308v2 Announce Type: replace  Abstract: As deep vision models' popularity rapidly increases, there is a growing emphasis on explanations for model predictions. The inherently explainable attribution method aims to enhance the understanding of model behavior by identifying the important regions in images that significantly contribute to predictions. It is achieved by cooperatively training a selector (generating an attribution map to identify important features) and a predictor (making predictions using the identified features). Despite many advancements, existing methods suffer from the incompleteness problem, where discriminative features are masked out, and the interlocking problem, where the non-optimized selector initially selects noise, causing the predictor to fit on this noise and perpetuate the cycle. To address these problems, we introduce a new objective that discourages the presence of discriminative features in the masked-out regions thus enhancing the comprehe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#30340;&#22810;&#35299;&#30721;&#22120;&#22330;&#26223;&#34920;&#31034;&#32593;&#32476;&#65288;MDSRN&#65289;&#65292;&#33021;&#22815;&#20026;&#36755;&#20837;&#22352;&#26631;&#29983;&#25104;&#22810;&#20010;&#21487;&#33021;&#39044;&#27979;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#22343;&#20540;&#20316;&#20026;&#22810;&#35299;&#30721;&#22120;&#38598;&#21512;&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#26041;&#24046;&#20316;&#20026;&#21487;&#20449;&#24230;&#20998;&#25968;&#65292;&#20174;&#32780;&#25903;&#25345;&#25512;&#29702;&#26102;&#39044;&#27979;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2407.19082</link><description>&lt;p&gt;
Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#30340;&#22810;&#35299;&#30721;&#22120;&#22330;&#26223;&#34920;&#31034;&#32593;&#32476;&#65288;MDSRN&#65289;&#65292;&#33021;&#22815;&#20026;&#36755;&#20837;&#22352;&#26631;&#29983;&#25104;&#22810;&#20010;&#21487;&#33021;&#39044;&#27979;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#22343;&#20540;&#20316;&#20026;&#22810;&#35299;&#30721;&#22120;&#38598;&#21512;&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#26041;&#24046;&#20316;&#20026;&#21487;&#20449;&#24230;&#20998;&#25968;&#65292;&#20174;&#32780;&#25903;&#25345;&#25512;&#29702;&#26102;&#39044;&#27979;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19082v2 Announce Type: replace-cross  Abstract: Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN) ensemble architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CityX&#30340;&#20840;&#26032;&#22810;&#27169;&#24335;&#21487;&#25511;&#30340;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#22810;&#31181;&#24067;&#23616;&#25351;&#20196;&#29983;&#25104;&#29616;&#23454;&#24863;&#21313;&#36275;&#12289;&#35268;&#27169;&#19981;&#21463;&#38480;&#30340;3D&#34394;&#25311;&#22478;&#24066;&#65292;&#24182;&#33021;&#23454;&#29616;&#23545;&#22478;&#24066;&#24067;&#23616;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2407.17572</link><description>&lt;p&gt;
CityX: Controllable Procedural Content Generation for Unbounded 3D Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17572
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CityX&#30340;&#20840;&#26032;&#22810;&#27169;&#24335;&#21487;&#25511;&#30340;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#22810;&#31181;&#24067;&#23616;&#25351;&#20196;&#29983;&#25104;&#29616;&#23454;&#24863;&#21313;&#36275;&#12289;&#35268;&#27169;&#19981;&#21463;&#38480;&#30340;3D&#34394;&#25311;&#22478;&#24066;&#65292;&#24182;&#33021;&#23454;&#29616;&#23545;&#22478;&#24066;&#24067;&#23616;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17572v3 Announce Type: replace  Abstract: Generating a realistic, large-scale 3D virtual city remains a complex challenge due to the involvement of numerous 3D assets, various city styles, and strict layout constraints. Existing approaches provide promising attempts at procedural content generation to create large-scale scenes using Blender agents. However, they face crucial issues such as difficulties in scaling up generation capability and achieving fine-grained control at the semantic layout level. To address these problems, we propose a novel multi-modal controllable procedural content generation method, named CityX, which enhances realistic, unbounded 3D city generation guided by multiple layout conditions, including OSM, semantic maps, and satellite images. Specifically, the proposed method contains a general protocol for integrating various PCG plugins and a multi-agent framework for transforming instructions into executable Blender actions. Through this effective fra
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;MMRA (Multi-Granularity and Multi-Image Relational Association) &#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#32500;&#24230;&#21644;&#22810;&#22270;&#20687;&#20851;&#31995;&#20851;&#32852;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#31934;&#24515;&#32534;&#21046;&#30340;1024&#20010;&#26679;&#26412;&#65292;&#25991;&#31456;&#25512;&#21160;&#20102;&#22270;&#20687;&#38388;&#20851;&#32852;&#20851;&#31995;&#30340;&#20219;&#21153;&#30740;&#31350;&#65292;&#36890;&#36807;11&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#23376;&#20219;&#21153;&#65292;&#22914;&#8220;&#20351;&#29992;&#30456;&#20284;&#24615;&#8221;&#21644;&#8220;&#23376;&#20107;&#20214;&#8221;&#65292;&#23545;&#24403;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2407.17379</link><description>&lt;p&gt;
MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image Relational Association Capabilities in Large Visual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;MMRA (Multi-Granularity and Multi-Image Relational Association) &#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#32500;&#24230;&#21644;&#22810;&#22270;&#20687;&#20851;&#31995;&#20851;&#32852;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#31934;&#24515;&#32534;&#21046;&#30340;1024&#20010;&#26679;&#26412;&#65292;&#25991;&#31456;&#25512;&#21160;&#20102;&#22270;&#20687;&#38388;&#20851;&#32852;&#20851;&#31995;&#30340;&#20219;&#21153;&#30740;&#31350;&#65292;&#36890;&#36807;11&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#23376;&#20219;&#21153;&#65292;&#22914;&#8220;&#20351;&#29992;&#30456;&#20284;&#24615;&#8221;&#21644;&#8220;&#23376;&#20107;&#20214;&#8221;&#65292;&#23545;&#24403;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17379v2 Announce Type: replace  Abstract: Given the remarkable success that large visual language models (LVLMs) have achieved in image perception tasks, the endeavor to make LVLMs perceive the world like humans is drawing increasing attention. Current multi-modal benchmarks primarily focus on facts or specific topic-related knowledge contained within individual images. However, they often overlook the associative relations between multiple images, which require the identification and analysis of similarities among entities or content present in different images. Therefore, we propose the multi-image relation association task and a meticulously curated Multi-granularity Multi-image Relational Association (MMRA) benchmark, comprising 1,024 samples. In order to systematically and comprehensively evaluate current LVLMs, we establish an associational relation system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent) at two granularity levels (i.e., image and 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#20849;&#23398;&#20064;&#65288;MMCL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#20849;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21482;&#20351;&#29992;&#31616;&#27905;&#30340;&#39592;&#39612;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36741;&#21161;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#30340;&#35757;&#32451;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#23545;&#20854;&#20182;&#35814;&#32454;&#36523;&#20307;&#20449;&#24687;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2407.15706</link><description>&lt;p&gt;
Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#20849;&#23398;&#20064;&#65288;MMCL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#20849;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21482;&#20351;&#29992;&#31616;&#27905;&#30340;&#39592;&#39612;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36741;&#21161;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#30340;&#35757;&#32451;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#23545;&#20854;&#20182;&#35814;&#32454;&#36523;&#20307;&#20449;&#24687;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15706v5 Announce Type: replace  Abstract: Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffX&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#29983;&#25104;&#30340;&#24067;&#23616;&#25351;&#23548;&#27169;&#22411;&#12290;DiffX&#27169;&#22411;&#22312;&#20849;&#20139; latent &#31354;&#38388;&#20013;&#25191;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Joint-Modality Embedder&#65288;JME&#65289;&#21644;&#29992;&#20110;&#38271;&#25551;&#36848;&#23884;&#20837;&#30340;Long-CLIP&#65292;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#24067;&#23616;&#21644;&#25991;&#26412;&#26465;&#20214;&#30340;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2407.15488</link><description>&lt;p&gt;
DiffX: Guide Your Layout to Cross-Modal Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffX&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#29983;&#25104;&#30340;&#24067;&#23616;&#25351;&#23548;&#27169;&#22411;&#12290;DiffX&#27169;&#22411;&#22312;&#20849;&#20139; latent &#31354;&#38388;&#20013;&#25191;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Joint-Modality Embedder&#65288;JME&#65289;&#21644;&#29992;&#20110;&#38271;&#25551;&#36848;&#23884;&#20837;&#30340;Long-CLIP&#65292;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#24067;&#23616;&#21644;&#25991;&#26412;&#26465;&#20214;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15488v3 Announce Type: replace  Abstract: Diffusion models have made significant strides in language-driven and layout-driven image generation. However, most diffusion models are limited to visible RGB image generation. In fact, human perception of the world is enriched by diverse viewpoints, such as chromatic contrast, thermal illumination, and depth information. In this paper, we introduce a novel diffusion model for general layout-guided cross-modal generation, called DiffX. Notably, DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space. Moreover, we introduce the Joint-Modality Embedder (JME) to enhance interaction between layout and text conditions by incorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is employed for long caption embedding for user instruction. To facilitate the user-instructed generative training, we construct the cro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36328;&#30456;&#20851;&#24615;&#25429;&#25417;&#29289;&#20307;&#26102;&#24207;&#20449;&#24687;&#30340;&#26032;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25506;&#27979;&#22120;&#19982;&#29305;&#24449;&#25552;&#21462;&#22120;&#20043;&#38388;&#30340;&#31454;&#20105;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36830;&#32493;&#24103;&#30340;&#28909;&#22270;&#19978;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#36816;&#21160;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;&#30452;&#25509;&#23558;&#37325;&#35782;&#21035;&#20219;&#21153;&#23884;&#20837;&#21040;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#38382;&#39064;&#65292;&#20351;&#25552;&#21462;&#30340;&#29305;&#24449;&#20855;&#26377;&#26356;&#39640;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36319;&#36394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.14086</link><description>&lt;p&gt;
Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.14086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36328;&#30456;&#20851;&#24615;&#25429;&#25417;&#29289;&#20307;&#26102;&#24207;&#20449;&#24687;&#30340;&#26032;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25506;&#27979;&#22120;&#19982;&#29305;&#24449;&#25552;&#21462;&#22120;&#20043;&#38388;&#30340;&#31454;&#20105;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36830;&#32493;&#24103;&#30340;&#28909;&#22270;&#19978;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#36816;&#21160;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;&#30452;&#25509;&#23558;&#37325;&#35782;&#21035;&#20219;&#21153;&#23884;&#20837;&#21040;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#38382;&#39064;&#65292;&#20351;&#25552;&#21462;&#30340;&#29305;&#24449;&#20855;&#26377;&#26356;&#39640;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36319;&#36394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.14086v2 Announce Type: replace  Abstract: Joint Detection and Embedding (JDE) trackers have demonstrated excellent performance in Multi-Object Tracking (MOT) tasks by incorporating the extraction of appearance features as auxiliary tasks through embedding Re-Identification task (ReID) into the detector, achieving a balance between inference speed and tracking performance. However, solving the competition between the detector and the feature extractor has always been a challenge. Meanwhile, the issue of directly embedding the ReID task into MOT has remained unresolved. The lack of high discriminability in appearance features results in their limited utility. In this paper, a new learning approach using cross-correlation to capture temporal information of objects is proposed. The feature extraction network is no longer trained solely on appearance features from each frame but learns richer motion features by utilizing feature heatmaps from consecutive frames, which addresses t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IMAGDressing-v1&#30340;&#31995;&#32479;&#65292;&#23427;&#20511;&#21161;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#34394;&#25311;&#35797;&#34915;&#25928;&#26524;&#65292;&#29305;&#21035;&#22320;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#35753;&#29992;&#25143;&#33258;&#30001;&#32534;&#36753;&#20154;&#29289;&#22270;&#20687;&#65292;&#24182;&#21487;&#26681;&#25454;&#38656;&#27714;&#28155;&#21152;&#25110;&#26356;&#25913;&#26381;&#35013;&#12289;&#33080;&#37096;&#12289;&#23039;&#21183;&#21644;&#32972;&#26223;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CAMI&#30340;&#32508;&#21512;&#24615;&#19968;&#33268;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20197;&#39564;&#35777;&#25152;&#29983;&#25104;&#22270;&#20687;&#19982;&#21442;&#32771;&#26381;&#35013;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#29420;&#29305;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;IMAGDressing-v1&#33021;&#22815;&#23558;&#26381;&#35013;&#30340;&#29305;&#24449;&#19982;&#22270;&#20687;&#30340;&#20854;&#20182;&#29305;&#24449;&#26377;&#25928;&#22320;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34394;&#25311;&#35797;&#34915;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2407.12705</link><description>&lt;p&gt;
IMAGDressing-v1: Customizable Virtual Dressing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.12705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IMAGDressing-v1&#30340;&#31995;&#32479;&#65292;&#23427;&#20511;&#21161;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#34394;&#25311;&#35797;&#34915;&#25928;&#26524;&#65292;&#29305;&#21035;&#22320;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#35753;&#29992;&#25143;&#33258;&#30001;&#32534;&#36753;&#20154;&#29289;&#22270;&#20687;&#65292;&#24182;&#21487;&#26681;&#25454;&#38656;&#27714;&#28155;&#21152;&#25110;&#26356;&#25913;&#26381;&#35013;&#12289;&#33080;&#37096;&#12289;&#23039;&#21183;&#21644;&#32972;&#26223;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CAMI&#30340;&#32508;&#21512;&#24615;&#19968;&#33268;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20197;&#39564;&#35777;&#25152;&#29983;&#25104;&#22270;&#20687;&#19982;&#21442;&#32771;&#26381;&#35013;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#29420;&#29305;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;IMAGDressing-v1&#33021;&#22815;&#23558;&#26381;&#35013;&#30340;&#29305;&#24449;&#19982;&#22270;&#20687;&#30340;&#20854;&#20182;&#29305;&#24449;&#26377;&#25928;&#22320;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34394;&#25311;&#35797;&#34915;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.12705v2 Announce Type: replace  Abstract: Latest advances have achieved realistic virtual try-on (VTON) through localized garment inpainting using latent diffusion models, significantly enhancing consumers' online shopping experience. However, existing VTON technologies neglect the need for merchants to showcase garments comprehensively, including flexible control over garments, optional faces, poses, and scenes. To address this issue, we define a virtual dressing (VD) task focused on generating freely editable human images with fixed garments and optional conditions. Meanwhile, we design a comprehensive affinity metric index (CAMI) to evaluate the consistency between generated images and reference garments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet that captures semantic features from CLIP and texture features from VAE. We present a hybrid attention module, including a frozen self-attention and a trainable cross-attention, to integrate garment feat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CCVA-FL&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21307;&#30103;&#24433;&#20687;&#39046;&#22495;&#20013;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#23567;&#21270;&#36328;&#23458;&#25143;&#31471;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20351;&#29992;Scalable Diffusion Models with Transformers&#65288;DiT&#65289;&#29983;&#25104;&#21453;&#26144;&#30446;&#26631;&#23458;&#25143;&#31471;&#22270;&#20687;&#25968;&#25454;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#34987;&#29992;&#20110;&#20849;&#20139;&#20197;&#24110;&#21161;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#36827;&#20837;&#30446;&#26631;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#24335;&#21307;&#30103;&#24433;&#20687;&#25968;&#25454;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CCVA-FL&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21307;&#30103;&#24433;&#20687;&#39046;&#22495;&#20013;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#23567;&#21270;&#36328;&#23458;&#25143;&#31471;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20351;&#29992;Scalable Diffusion Models with Transformers&#65288;DiT&#65289;&#29983;&#25104;&#21453;&#26144;&#30446;&#26631;&#23458;&#25143;&#31471;&#22270;&#20687;&#25968;&#25454;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#34987;&#29992;&#20110;&#20849;&#20139;&#20197;&#24110;&#21161;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#36827;&#20837;&#30446;&#26631;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#24335;&#21307;&#30103;&#24433;&#20687;&#25968;&#25454;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v4 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;VCHAR&#26694;&#26550;&#65292;&#19968;&#31181;&#22522;&#20110;variances&#39537;&#21160;&#30340;&#22797;&#26434;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#34920;&#31034;&#26469;&#26377;&#25928;&#35782;&#21035;&#21644;&#35299;&#37322;&#35270;&#39057;&#20013;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20256;&#32479;&#30340;&#26631;&#31614;&#21270;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2407.03291</link><description>&lt;p&gt;
VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.03291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;VCHAR&#26694;&#26550;&#65292;&#19968;&#31181;&#22522;&#20110;variances&#39537;&#21160;&#30340;&#22797;&#26434;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#34920;&#31034;&#26469;&#26377;&#25928;&#35782;&#21035;&#21644;&#35299;&#37322;&#35270;&#39057;&#20013;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20256;&#32479;&#30340;&#26631;&#31614;&#21270;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.03291v2 Announce Type: replace-cross  Abstract: Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evalu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;InterleavedBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#38543;&#24847;&#20132;&#32455;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#24037;&#20855;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24320;&#21457;&#20102;InterleavedEval&#65292;&#19968;&#20010;&#22522;&#20110;GPT-4o&#30340;&#24378;&#22823;&#26080;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2406.14643</link><description>&lt;p&gt;
Holistic Evaluation for Interleaved Text-and-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.14643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;InterleavedBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#38543;&#24847;&#20132;&#32455;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#24037;&#20855;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24320;&#21457;&#20102;InterleavedEval&#65292;&#19968;&#20010;&#22522;&#20110;GPT-4o&#30340;&#24378;&#22823;&#26080;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.14643v2 Announce Type: replace  Abstract: Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenECAD&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32534;&#36753;&#33021;&#21147;&#30340;3D CAD&#35774;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21046;&#36896;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2406.09913</link><description>&lt;p&gt;
OpenECAD: An Efficient Visual Language Model for Editable 3D-CAD Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.09913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenECAD&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32534;&#36753;&#33021;&#21147;&#30340;3D CAD&#35774;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21046;&#36896;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.09913v3 Announce Type: replace  Abstract: Computer-aided design (CAD) tools are utilized in the manufacturing industry for modeling everything from cups to spacecraft. These programs are complex to use and typically require years of training and experience to master. Structured and well-constrained 2D sketches and 3D constructions are crucial components of CAD modeling. A well-executed CAD model can be seamlessly integrated into the manufacturing process, thereby enhancing production efficiency. Deep generative models of 3D shapes and 3D object reconstruction models have garnered significant research interest. However, most of these models produce discrete forms of 3D objects that are not editable. Moreover, the few models based on CAD operations often have substantial input restrictions. In this work, we fine-tuned pre-trained models to create OpenECAD models (0.55B, 0.89B, 2.4B and 3.1B), leveraging the visual, logical, coding, and general capabilities of visual language m
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2406.04485</link><description>&lt;p&gt;
GenAI Arena: An Open Evaluation Platform for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04485v2 Announce Type: replace-cross  Abstract: Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source g
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#20687; Captioning &#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21487;&#33021;&#22240;&#33719;&#21462;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#32780;&#20135;&#29983;&#38169;&#35823;&#12290;&#36890;&#36807;&#20998;&#26512;SmallCap&#27169;&#22411;&#65292;&#20316;&#32773;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#39057;&#32321;&#20986;&#29616;&#22312;&#26816;&#32034;&#32467;&#26524;&#20013;&#30340;&#21333;&#35789;&#30340;&#24433;&#21709;&#65292;&#32780;&#36825;&#20123;&#21333;&#35789;&#20063;&#20542;&#21521;&#20110;&#34987;&#27169;&#22411;&#22797;&#21046;&#21040;&#29983;&#25104;&#30340;&#25551;&#36848;&#20013;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20174;&#26356;&#20855;&#22810;&#26679;&#24615;&#30340;&#26816;&#32034;&#32467;&#26524;&#20013;&#37319;&#26679;&#65292;&#26469;&#20943;&#23569;&#27169;&#22411;&#23398;&#20064;&#22797;&#21046;&#39640;&#39057;&#21333;&#35789;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#31867;&#20284;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2406.02265</link><description>&lt;p&gt;
Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.02265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#20687; Captioning &#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21487;&#33021;&#22240;&#33719;&#21462;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#32780;&#20135;&#29983;&#38169;&#35823;&#12290;&#36890;&#36807;&#20998;&#26512;SmallCap&#27169;&#22411;&#65292;&#20316;&#32773;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#39057;&#32321;&#20986;&#29616;&#22312;&#26816;&#32034;&#32467;&#26524;&#20013;&#30340;&#21333;&#35789;&#30340;&#24433;&#21709;&#65292;&#32780;&#36825;&#20123;&#21333;&#35789;&#20063;&#20542;&#21521;&#20110;&#34987;&#27169;&#22411;&#22797;&#21046;&#21040;&#29983;&#25104;&#30340;&#25551;&#36848;&#20013;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20174;&#26356;&#20855;&#22810;&#26679;&#24615;&#30340;&#26816;&#32034;&#32467;&#26524;&#20013;&#37319;&#26679;&#65292;&#26469;&#20943;&#23569;&#27169;&#22411;&#23398;&#20064;&#22797;&#21046;&#39640;&#39057;&#21333;&#35789;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#31867;&#20284;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.02265v3 Announce Type: replace  Abstract: Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;PT43D&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#29575;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#21253;&#21547;&#39640;&#24230;&#27169;&#31946;&#35270;&#35273;&#20449;&#24687;&#30340;RGB&#22270;&#20687;&#20013;&#29983;&#25104;3D&#24418;&#29366;&#12290;&#36890;&#36807;&#27169;&#25311;&#35757;&#32451;&#26679;&#26412;&#21644;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#36755;&#20837;&#22270;&#20687;&#20013;&#26368;&#37325;&#35201;&#30340;&#21306;&#22495;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#24212;&#23545;&#20102;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36974;&#25377;&#21644;&#35270;&#35282;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2405.11914</link><description>&lt;p&gt;
PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single Highly-Ambiguous RGB Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;PT43D&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#29575;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#21253;&#21547;&#39640;&#24230;&#27169;&#31946;&#35270;&#35273;&#20449;&#24687;&#30340;RGB&#22270;&#20687;&#20013;&#29983;&#25104;3D&#24418;&#29366;&#12290;&#36890;&#36807;&#27169;&#25311;&#35757;&#32451;&#26679;&#26412;&#21644;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#36755;&#20837;&#22270;&#20687;&#20013;&#26368;&#37325;&#35201;&#30340;&#21306;&#22495;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#24212;&#23545;&#20102;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36974;&#25377;&#21644;&#35270;&#35282;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11914v2 Announce Type: replace  Abstract: Generating 3D shapes from single RGB images is essential in various applications such as robotics. Current approaches typically target images containing clear and complete visual descriptions of the object, without considering common realistic cases where observations of objects that are largely occluded or truncated. We thus propose a transformer-based autoregressive model to generate the probabilistic distribution of 3D shapes conditioned on an RGB image containing potentially highly ambiguous observations of the object. To handle realistic scenarios such as occlusion or field-of-view truncation, we create simulated image-to-shape training pairs that enable improved fine-tuning for real-world scenarios. We then adopt cross-attention to effectively identify the most relevant region of interest from the input image for shape generation. This enables inference of sampled shapes with reasonable diversity and strong alignment with the i
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dual-Task Vision Transformer&#65288;DTViT&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26088;&#22312;&#33258;&#21160;&#24555;&#36895;&#20934;&#30830;&#22320;&#20998;&#31867;&#33041;CT&#22270;&#20687;&#20013;&#30340;&#20986;&#34880;&#24615;&#20013;&#39118;&#31867;&#22411;&#65292;&#24182;&#33021;&#26681;&#25454;&#20986;&#34880;&#37096;&#20301;&#30340;&#19981;&#21516;&#65288;&#22914;&#28145;&#37096;&#12289;&#33041;&#21494;&#19979;&#21644;&#33041;&#21494;&#20986;&#34880;&#65289;&#25552;&#20379;&#31934;&#30830;&#35786;&#26029;&#65292;&#20197;&#25903;&#25345;&#21450;&#26089;&#21046;&#23450;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2405.06814</link><description>&lt;p&gt;
Dual-Task Vision Transformer for Rapid and Accurate Intracerebral Hemorrhage CT Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.06814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dual-Task Vision Transformer&#65288;DTViT&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26088;&#22312;&#33258;&#21160;&#24555;&#36895;&#20934;&#30830;&#22320;&#20998;&#31867;&#33041;CT&#22270;&#20687;&#20013;&#30340;&#20986;&#34880;&#24615;&#20013;&#39118;&#31867;&#22411;&#65292;&#24182;&#33021;&#26681;&#25454;&#20986;&#34880;&#37096;&#20301;&#30340;&#19981;&#21516;&#65288;&#22914;&#28145;&#37096;&#12289;&#33041;&#21494;&#19979;&#21644;&#33041;&#21494;&#20986;&#34880;&#65289;&#25552;&#20379;&#31934;&#30830;&#35786;&#26029;&#65292;&#20197;&#25903;&#25345;&#21450;&#26089;&#21046;&#23450;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.06814v3 Announce Type: replace  Abstract: Intracerebral hemorrhage (ICH) is a severe and sudden medical condition caused by the rupture of blood vessels in the brain, leading to permanent damage to brain tissue and often resulting in functional disabilities or death in patients. Diagnosis and analysis of ICH typically rely on brain CT imaging. Given the urgency of ICH conditions, early treatment is crucial, necessitating rapid analysis of CT images to formulate tailored treatment plans. However, the complexity of ICH CT images and the frequent scarcity of specialist radiologists pose significant challenges. Therefore, we collect a dataset from the real world for ICH and normal classification and three types of ICH image classification based on the hemorrhage location, i.e., Deep, Subcortical, and Lobar. In addition, we propose a neural network structure, dual-task vision transformer (DTViT), for the automated classification and diagnosis of ICH images. The DTViT deploys the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#24067;&#26391;&#36816;&#21160;&#26725;&#27573;&#25193;&#25955;&#30340;&#24103;&#25554;&#20540;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#20445;&#35270;&#39057;&#24103;&#25554;&#20540;&#30340;&#36755;&#20986;&#19982;ground truth&#19968;&#33268;&#65292;&#23613;&#31649;Latent Diffusion Models&#22312;&#36816;&#34892;&#22810;&#20010;&#23454;&#20363;&#26102;&#20250;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2405.05953</link><description>&lt;p&gt;
Frame Interpolation with Consecutive Brownian Bridge Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.05953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#24067;&#26391;&#36816;&#21160;&#26725;&#27573;&#25193;&#25955;&#30340;&#24103;&#25554;&#20540;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#20445;&#35270;&#39057;&#24103;&#25554;&#20540;&#30340;&#36755;&#20986;&#19982;ground truth&#19968;&#33268;&#65292;&#23613;&#31649;Latent Diffusion Models&#22312;&#36816;&#34892;&#22810;&#20010;&#23454;&#20363;&#26102;&#20250;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.05953v4 Announce Type: replace  Abstract: Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, result
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoReX&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#22522;&#20110;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20197;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#65288;&#22914;&#29983;&#29289;&#23398;&#65289;&#25581;&#31034;&#21644;&#35780;&#20272;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#38480;&#21046;&#39044;&#27979;&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36827;&#34892;&#20102;&#22810;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;CoReX&#22312;&#35299;&#37322;&#21644;&#35780;&#20272;CNN&#27169;&#22411;&#20915;&#31574;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2405.01661</link><description>&lt;p&gt;
When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01661
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoReX&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#22522;&#20110;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20197;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#65288;&#22914;&#29983;&#29289;&#23398;&#65289;&#25581;&#31034;&#21644;&#35780;&#20272;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#38480;&#21046;&#39044;&#27979;&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36827;&#34892;&#20102;&#22810;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;CoReX&#22312;&#35299;&#37322;&#21644;&#35780;&#20272;CNN&#27169;&#22411;&#20915;&#31574;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#23545;&#40784;&#26469;&#25913;&#36827;&#24178;&#39044;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#23545;&#40784;&#27169;&#22359;&#65292;&#20316;&#32773;&#21457;&#29616;&#21487;&#20197;&#25913;&#21892;&#20154;&#31867;&#19987;&#23478;&#22312;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23545;&#27599;&#20010;&#22270;&#20687;&#30340;&#24178;&#39044;&#27425;&#25968;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2405.01531</link><description>&lt;p&gt;
Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#23545;&#40784;&#26469;&#25913;&#36827;&#24178;&#39044;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#23545;&#40784;&#27169;&#22359;&#65292;&#20316;&#32773;&#21457;&#29616;&#21487;&#20197;&#25913;&#21892;&#20154;&#31867;&#19987;&#23478;&#22312;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23545;&#27599;&#20010;&#22270;&#20687;&#30340;&#24178;&#39044;&#27425;&#25968;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01531v2 Announce Type: replace-cross  Abstract: Concept Bottleneck Models (CBMs) ground image classification on human-understandable concepts to allow for interpretable model decisions. Crucially, the CBM design inherently allows for human interventions, in which expert users are given the ability to modify potentially misaligned concept choices to influence the decision behavior of the model in an interpretable fashion. However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human feedback is expensive. In this paper, we find that this is noticeably driven by an independent treatment of concepts during intervention, wherein a change of one concept does not influence the use of other ones in the model's final decision. To address this issue, we introduce a trainable concept intervention realignment module, which leverages concept relations to realign concept ass
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimEndoGS&#30340;&#26032;&#22411;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#30340;&#25163;&#26415;&#22330;&#26223;&#27169;&#25311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#29289;&#29702;&#23884;&#20837;&#24335;3D&#39640;&#26031;&#20998;&#24067;&#26469;&#27169;&#25311;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#19977;&#32500;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#20013;&#33258;&#21160;&#23398;&#20064;&#25163;&#26415;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#24182;&#22312;&#20445;&#35777;&#22330;&#26223;&#20960;&#20309;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#28145;&#24230;&#30417;&#30563;&#21644;&#21508;&#21521;&#24322;&#24615;&#32422;&#26463;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#30495;&#23454;&#21644;&#21487;&#25193;&#22823;&#30340;&#27169;&#25311;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2405.00956</link><description>&lt;p&gt;
SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00956
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimEndoGS&#30340;&#26032;&#22411;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#30340;&#25163;&#26415;&#22330;&#26223;&#27169;&#25311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#29289;&#29702;&#23884;&#20837;&#24335;3D&#39640;&#26031;&#20998;&#24067;&#26469;&#27169;&#25311;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#19977;&#32500;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#20013;&#33258;&#21160;&#23398;&#20064;&#25163;&#26415;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#24182;&#22312;&#20445;&#35777;&#22330;&#26223;&#20960;&#20309;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#28145;&#24230;&#30417;&#30563;&#21644;&#21508;&#21521;&#24322;&#24615;&#32422;&#26463;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#30495;&#23454;&#21644;&#21487;&#25193;&#22823;&#30340;&#27169;&#25311;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00956v3 Announce Type: replace  Abstract: Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy r
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMM-PCQA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32473;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#25552;&#20379;&#25991;&#26412;&#30417;&#30563;&#26469;&#23454;&#29616;&#23545;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#30340;&#36741;&#21161;&#12290;&#36890;&#36807;&#23558;&#36136;&#37327;&#26631;&#31614;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#65292;LMMs&#33021;&#22815;&#20174;2D&#28857;&#20113;&#25237;&#24433;&#20013;&#25512;&#23548;&#20986;&#36136;&#37327;&#35780;&#20998;logits&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#21462;&#20102;&#32467;&#26500;&#29305;&#24449;&#26469;&#34917;&#20607;3D&#39046;&#22495;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#36136;&#37327;logits&#21644;&#32467;&#26500;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#20998;&#25968;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#29616;&#20102;&#23558;LMMs&#38598;&#25104;&#21040;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35780;&#20272;&#28857;&#20113;&#36136;&#37327;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.18203</link><description>&lt;p&gt;
LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMM-PCQA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32473;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#25552;&#20379;&#25991;&#26412;&#30417;&#30563;&#26469;&#23454;&#29616;&#23545;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#30340;&#36741;&#21161;&#12290;&#36890;&#36807;&#23558;&#36136;&#37327;&#26631;&#31614;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#65292;LMMs&#33021;&#22815;&#20174;2D&#28857;&#20113;&#25237;&#24433;&#20013;&#25512;&#23548;&#20986;&#36136;&#37327;&#35780;&#20998;logits&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#21462;&#20102;&#32467;&#26500;&#29305;&#24449;&#26469;&#34917;&#20607;3D&#39046;&#22495;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#36136;&#37327;logits&#21644;&#32467;&#26500;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#20998;&#25968;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#29616;&#20102;&#23558;LMMs&#38598;&#25104;&#21040;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35780;&#20272;&#28857;&#20113;&#36136;&#37327;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18203v2 Announce Type: replace  Abstract: Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model under
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafePaint&#30340;&#25239; forensic &#22270;&#20687;&#20462;&#34917;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22270;&#20687;&#20462;&#34917;&#30340;&#25239;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.18136</link><description>&lt;p&gt;
SafePaint: Anti-forensic Image Inpainting with Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafePaint&#30340;&#25239; forensic &#22270;&#20687;&#20462;&#34917;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22270;&#20687;&#20462;&#34917;&#30340;&#25239;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18136v2 Announce Type: replace  Abstract: Existing image inpainting methods have achieved remarkable accomplishments in generating visually appealing results, often accompanied by a trend toward creating more intricate structural textures. However, while these models excel at creating more realistic image content, they often leave noticeable traces of tampering, posing a significant threat to security. In this work, we take the anti-forensic capabilities into consideration, firstly proposing an end-to-end training framework for anti-forensic image inpainting named SafePaint. Specifically, we innovatively formulated image inpainting as two major tasks: semantically plausible content completion and region-wise optimization. The former is similar to current inpainting methods that aim to restore the missing regions of corrupted images. The latter, through domain adaptation, endeavors to reconcile the discrepancies between the inpainted region and the unaltered area to achieve a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MambaMOS&#30340;&#22522;&#20110;LiDAR&#30340;3D&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Time Clue Bootstrapping Embedding&#65288;TCBE&#65289;&#30340;&#26032;&#23884;&#20837;&#27169;&#22359;&#26469;&#22686;&#24378;&#28857;&#20113;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#32806;&#21512;&#65292;&#24182;&#32531;&#35299;&#20102;&#24573;&#30053;&#26102;&#31354;&#32447;&#32034;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;Motion-aware State Space Model&#65288;MSSM&#65289;&#65292;&#20197;&#35753;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#21516;&#19968;&#23545;&#35937;&#30340;&#31354;&#38388;&#29366;&#24577;&#65292;&#24378;&#35843;&#20102;&#31227;&#21160;&#23545;&#35937;&#30340;&#36816;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#22312;LiDAR&#28857;&#20113;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2404.12794</link><description>&lt;p&gt;
MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MambaMOS&#30340;&#22522;&#20110;LiDAR&#30340;3D&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Time Clue Bootstrapping Embedding&#65288;TCBE&#65289;&#30340;&#26032;&#23884;&#20837;&#27169;&#22359;&#26469;&#22686;&#24378;&#28857;&#20113;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#32806;&#21512;&#65292;&#24182;&#32531;&#35299;&#20102;&#24573;&#30053;&#26102;&#31354;&#32447;&#32034;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;Motion-aware State Space Model&#65288;MSSM&#65289;&#65292;&#20197;&#35753;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#21516;&#19968;&#23545;&#35937;&#30340;&#31354;&#38388;&#29366;&#24577;&#65292;&#24378;&#35843;&#20102;&#31227;&#21160;&#23545;&#35937;&#30340;&#36816;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#22312;LiDAR&#28857;&#20113;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12794v2 Announce Type: replace-cross  Abstract: LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment moving objects in point clouds of the current scan using motion information from previous scans. Despite the promising results achieved by previous MOS methods, several key issues, such as the weak coupling of temporal and spatial information, still need further study. In this paper, we propose a novel LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model, termed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue Bootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial information in point clouds and alleviate the issue of overlooked temporal clues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to endow the model with the capacity to understand the temporal correlations of the same object across different time steps. Specifically, MSSM emphasizes the motion states of the sa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoSA&#30340;&#29305;&#27530;&#38271;&#30701;&#26399;&#33539;&#22260;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#38024;&#23545;&#31471;&#21040;&#31471;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#65292;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#21644;&#22788;&#29702;&#26410;&#35009;&#21098;&#30340;&#35270;&#39057;&#12290;&#36825;&#31181;&#36866;&#37197;&#22120;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#65292;&#24182;&#36890;&#36807;&#22312;&#35270;&#39057; backbone &#30340;&#20013;&#38388;&#23618;&#19978;&#24341;&#20837;&#29305;&#27530;&#30340;&#38271;&#30701;&#26399;&#33539;&#22260;&#36866;&#37197;&#22120;&#65292;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#22823;&#37327; GPU &#20869;&#23384;&#28040;&#32791;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#36275;&#36857;&#12290;&#27492;&#22806;&#65292;LoSA &#36824;&#21253;&#25324;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#33539;&#22260;&#38376;&#25511;&#34701;&#21512;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#20123;&#36866;&#37197;&#22120;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#24207;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01282</link><description>&lt;p&gt;
LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoSA&#30340;&#29305;&#27530;&#38271;&#30701;&#26399;&#33539;&#22260;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#38024;&#23545;&#31471;&#21040;&#31471;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#65292;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#21644;&#22788;&#29702;&#26410;&#35009;&#21098;&#30340;&#35270;&#39057;&#12290;&#36825;&#31181;&#36866;&#37197;&#22120;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#65292;&#24182;&#36890;&#36807;&#22312;&#35270;&#39057; backbone &#30340;&#20013;&#38388;&#23618;&#19978;&#24341;&#20837;&#29305;&#27530;&#30340;&#38271;&#30701;&#26399;&#33539;&#22260;&#36866;&#37197;&#22120;&#65292;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#22823;&#37327; GPU &#20869;&#23384;&#28040;&#32791;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#36275;&#36857;&#12290;&#27492;&#22806;&#65292;LoSA &#36824;&#21253;&#25324;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#33539;&#22260;&#38376;&#25511;&#34701;&#21512;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#20123;&#36866;&#37197;&#22120;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#24207;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01282v2 Announce Type: replace  Abstract: Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Gated Fusion that strategically combines the output of these adapter
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#31354;&#38388;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;SPRIGHT&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;6&#30334;&#19975;&#22270;&#20687;&#30340;&#19987;&#38376;&#29992;&#20110;&#31354;&#38388;&#25551;&#36848;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;SPRIGHT&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;&#32422;0.25%&#65289;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;22%&#30340;&#31354;&#38388;&#20851;&#31995;&#25551;&#36848;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01197</link><description>&lt;p&gt;
Getting it Right: Improving Spatial Consistency in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#31354;&#38388;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;SPRIGHT&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;6&#30334;&#19975;&#22270;&#20687;&#30340;&#19987;&#38376;&#29992;&#20110;&#31354;&#38388;&#25551;&#36848;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;SPRIGHT&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;&#32422;0.25%&#65289;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;22%&#30340;&#31354;&#38388;&#20851;&#31995;&#25551;&#36848;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01197v2 Announce Type: replace  Abstract: One of the key shortcomings in current text-to-image (T2I) models is their inability to consistently generate images which faithfully follow the spatial relationships specified in the text prompt. In this paper, we offer a comprehensive investigation of this limitation, while also developing datasets and methods that support algorithmic solutions to improve spatial reasoning in T2I models. We find that spatial relationships are under-represented in the image descriptions found in current vision-language datasets. To alleviate this data bottleneck, we create SPRIGHT, the first spatially focused, large-scale dataset, by re-captioning 6 million images from 4 widely used vision datasets and through a 3-fold evaluation and analysis pipeline, show that SPRIGHT improves the proportion of spatial relationships in existing datasets. We show the efficacy of SPRIGHT data by showing that using only $\sim$0.25% of SPRIGHT results in a 22% improve
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#38024;&#23545; egocentric &#25163;&#37096;&#20114;&#21160;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#24378;&#35843;&#20102; 3D &#25163;&#37096;-&#29289;&#20307;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#34892;&#27492;&#31867;&#20219;&#21153;&#22312;&#26426;&#22120;&#20154;&#12289;AR/VR&#12289;&#21160;&#20316;&#35782;&#21035;&#21644;&#36816;&#21160;&#29983;&#25104;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#22522;&#20110;&#20960;&#20309;&#22270;&#24418;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#21253;&#22260;&#30418;&#22238;&#24402;&#26041;&#27861;&#65292;&#23545;&#20110;&#20462;&#27491;&#22240;&#22836;&#37096;&#36816;&#21160;&#20135;&#29983;&#30340;&#27169;&#31946;&#25928;&#24212;&#21644;&#35299;&#20915; egocentric &#30456;&#26426;&#29305;&#26377;&#30340;&#30072;&#21464;&#38382;&#39064;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#24403;&#21069;&#29366;&#24577;&#19979;&#26368;&#26032;&#22522;&#32447;&#21644;&#25552;&#20132;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#23545;&#22914;&#20309;&#26356;&#26377;&#25928;&#35299;&#20915;&#37325;&#26500;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25991;&#31456;&#36890;&#36807; HANDS23 &#25361;&#25112;&#35780;&#20272;&#20102;&#22312; ARCTIC &#21644; AssemblyHands &#25968;&#25454;&#38598;&#19978;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#39640;&#23481;&#37327; Transformer &#22312;&#23398;&#20064;&#22797;&#26434;&#25163;-&#29289;&#20307;&#20132;&#20114;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#19981;&#21516;&#39044;&#27979;&#26041;&#27861;&#23545;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#36825;&#19968;&#31995;&#21015;&#30340;&#20998;&#26512;&#65292;&#25991;&#31456;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#29305;&#21035;&#26159;&#22312;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21644; egocentric &#19977;&#32500;&#37325;&#24314;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;&#37325;&#26032;&#23457;&#35270;&#22522;&#20934;&#26041;&#27861;&#12289;&#31639;&#27861;&#21019;&#26032;&#21644;&#25216;&#26415;&#34701;&#21512;&#26041;&#38754;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.16428</link><description>&lt;p&gt;
Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#38024;&#23545; egocentric &#25163;&#37096;&#20114;&#21160;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#24378;&#35843;&#20102; 3D &#25163;&#37096;-&#29289;&#20307;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#34892;&#27492;&#31867;&#20219;&#21153;&#22312;&#26426;&#22120;&#20154;&#12289;AR/VR&#12289;&#21160;&#20316;&#35782;&#21035;&#21644;&#36816;&#21160;&#29983;&#25104;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#22522;&#20110;&#20960;&#20309;&#22270;&#24418;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#21253;&#22260;&#30418;&#22238;&#24402;&#26041;&#27861;&#65292;&#23545;&#20110;&#20462;&#27491;&#22240;&#22836;&#37096;&#36816;&#21160;&#20135;&#29983;&#30340;&#27169;&#31946;&#25928;&#24212;&#21644;&#35299;&#20915; egocentric &#30456;&#26426;&#29305;&#26377;&#30340;&#30072;&#21464;&#38382;&#39064;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#24403;&#21069;&#29366;&#24577;&#19979;&#26368;&#26032;&#22522;&#32447;&#21644;&#25552;&#20132;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#23545;&#22914;&#20309;&#26356;&#26377;&#25928;&#35299;&#20915;&#37325;&#26500;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25991;&#31456;&#36890;&#36807; HANDS23 &#25361;&#25112;&#35780;&#20272;&#20102;&#22312; ARCTIC &#21644; AssemblyHands &#25968;&#25454;&#38598;&#19978;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#39640;&#23481;&#37327; Transformer &#22312;&#23398;&#20064;&#22797;&#26434;&#25163;-&#29289;&#20307;&#20132;&#20114;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#19981;&#21516;&#39044;&#27979;&#26041;&#27861;&#23545;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#36825;&#19968;&#31995;&#21015;&#30340;&#20998;&#26512;&#65292;&#25991;&#31456;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#29305;&#21035;&#26159;&#22312;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21644; egocentric &#19977;&#32500;&#37325;&#24314;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;&#37325;&#26032;&#23457;&#35270;&#22522;&#20934;&#26041;&#27861;&#12289;&#31639;&#27861;&#21019;&#26032;&#21644;&#25216;&#26415;&#34701;&#21512;&#26041;&#38754;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16428v2 Announce Type: replace  Abstract: We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3Dunderstanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from differen
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CR3DT&#30340;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#25668;&#20687;&#22836;&#21644;&#38647;&#36798;&#25216;&#26415;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#36890;&#36807;&#23558;&#38647;&#36798;&#25968;&#25454;&#30340;&#21152;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CR3DT&#30340;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#25668;&#20687;&#22836;&#21644;&#38647;&#36798;&#25216;&#26415;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#36890;&#36807;&#23558;&#38647;&#36798;&#25968;&#25454;&#30340;&#21152;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v2 Announce Type: replace  Abstract: To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#22495;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#25512;&#24191;&#21040;&#26410;&#30693;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;Meta Domain Alignment Semantic Refinement&#65288;MDASR&#65289;&#30340;&#26032;&#26426;&#21046;&#65292;&#25991;&#20013;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#29305;&#24322;&#24615;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#30340;&#20887;&#20313;&#31867;&#35821;&#20041;&#20013;&#30340;&#38750;&#29305;&#23450;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#20174;&#24050;&#30693;&#39046;&#22495;&#21040;&#26410;&#30693;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.14362</link><description>&lt;p&gt;
Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#22495;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#25512;&#24191;&#21040;&#26410;&#30693;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;Meta Domain Alignment Semantic Refinement&#65288;MDASR&#65289;&#30340;&#26032;&#26426;&#21046;&#65292;&#25991;&#20013;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#29305;&#24322;&#24615;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#30340;&#20887;&#20313;&#31867;&#35821;&#20041;&#20013;&#30340;&#38750;&#29305;&#23450;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#20174;&#24050;&#30693;&#39046;&#22495;&#21040;&#26410;&#30693;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14362v3 Announce Type: replace  Abstract: Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes. However, existing GZSL is still limited to seen domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains. Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains. Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs), we present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic sem
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL-Net&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#20805;&#20998;&#25429;&#33719;&#20010;&#20307;&#22270;&#20687;&#22359;&#30340;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#27169;&#22359;&#26469;&#25552;&#21462;&#28145;&#23618;&#27425;&#30340;&#29305;&#24449;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#32500;&#20840;&#23616;&#21040;&#23616;&#37096;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#19968;&#27493;&#25366;&#25496;&#20010;&#20307;&#29305;&#24449;&#65292;&#20197;&#22312;&#39640;&#20809;&#35889;&#25968;&#25454;&#21305;&#37197;&#20013;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2403.11751</link><description>&lt;p&gt;
Relational Representation Learning Network for Cross-Spectral Image Patch Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11751
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL-Net&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#20805;&#20998;&#25429;&#33719;&#20010;&#20307;&#22270;&#20687;&#22359;&#30340;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#27169;&#22359;&#26469;&#25552;&#21462;&#28145;&#23618;&#27425;&#30340;&#29305;&#24449;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#32500;&#20840;&#23616;&#21040;&#23616;&#37096;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#19968;&#27493;&#25366;&#25496;&#20010;&#20307;&#29305;&#24449;&#65292;&#20197;&#22312;&#39640;&#20809;&#35889;&#25968;&#25454;&#21305;&#37197;&#20013;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11751v2 Announce Type: replace  Abstract: Recently, feature relation learning has drawn widespread attention in cross-spectral image patch matching. However, existing related research focuses on extracting diverse relations between image patch features and ignores sufficient intrinsic feature representations of individual image patches. Therefore, we propose an innovative relational representation learning idea that simultaneously focuses on sufficiently mining the intrinsic features of individual image patches and the relations between image patch features. Based on this, we construct a Relational Representation Learning Network (RRL-Net). Specifically, we innovatively construct an autoencoder to fully characterize the individual intrinsic features, and introduce a feature interaction learning (FIL) module to extract deep-level feature relations. To further fully mine individual intrinsic features, a lightweight multi-dimensional global-to-local attention (MGLA) module is c
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26631;&#31614;&#21435;&#22122;&#31574;&#30053;&#26469;&#35299;&#20915;skeleton-based&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#31232;&#30095;&#39592;&#26550;&#25968;&#25454;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09975</link><description>&lt;p&gt;
Skeleton-Based Human Action Recognition with Noisy Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26631;&#31614;&#21435;&#22122;&#31574;&#30053;&#26469;&#35299;&#20915;skeleton-based&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#31232;&#30095;&#39592;&#26550;&#25968;&#25454;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09975v2 Announce Type: replace-cross  Abstract: Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model's training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodolo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EchoTrack&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#38899;&#39057;&#25351;&#20195;&#22810;&#23545;&#35937;&#36319;&#36394;&#65288;AR-MOT&#65289;&#12290;EchoTrack&#36890;&#36807;&#21452;&#21521;&#39057;&#29575;&#22495;&#36328;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65288;Bi-FCFM&#65289;&#23454;&#29616;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#29305;&#24449;&#30340;&#21452;&#27969;&#31471;&#21040;&#31471;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21644;&#25991;&#26412;&#20381;&#36182;&#30340;&#22810;&#23545;&#35937;&#36319;&#36394;&#22312;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18302</link><description>&lt;p&gt;
EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EchoTrack&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#38899;&#39057;&#25351;&#20195;&#22810;&#23545;&#35937;&#36319;&#36394;&#65288;AR-MOT&#65289;&#12290;EchoTrack&#36890;&#36807;&#21452;&#21521;&#39057;&#29575;&#22495;&#36328;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65288;Bi-FCFM&#65289;&#23454;&#29616;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#29305;&#24449;&#30340;&#21452;&#27969;&#31471;&#21040;&#31471;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21644;&#25991;&#26412;&#20381;&#36182;&#30340;&#22810;&#23545;&#35937;&#36319;&#36394;&#22312;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18302v2 Announce Type: replace-cross  Abstract: This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30340;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMO&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#38899;&#39057;-&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#26080;&#38656;&#20013;&#38388;&#30340;3D&#27169;&#22411;&#25110;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#33021;&#22815;&#22686;&#24378;&#35828;&#35805;&#21644;&#21809;&#27468;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#29616;&#23454;&#24863;&#21644;&#34920;&#36798;&#21147;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#35270;&#39057;&#20013;&#24103;&#30340;&#36830;&#32493;&#24615;&#20197;&#21450;&#36523;&#20221;&#30340;&#19968;&#33268;&#24615;&#65292;&#29983;&#25104;&#20102;&#39640;&#24230;&#36924;&#30495;&#21644;&#33258;&#28982;&#30340;&#21160;&#30011;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMO&#22312;&#34920;&#36798;&#24615;&#21644;&#39046;&#20808;&#25216;&#26415;&#19978;&#22343;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.17485</link><description>&lt;p&gt;
EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMO&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#38899;&#39057;-&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#26080;&#38656;&#20013;&#38388;&#30340;3D&#27169;&#22411;&#25110;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#33021;&#22815;&#22686;&#24378;&#35828;&#35805;&#21644;&#21809;&#27468;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#29616;&#23454;&#24863;&#21644;&#34920;&#36798;&#21147;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#35270;&#39057;&#20013;&#24103;&#30340;&#36830;&#32493;&#24615;&#20197;&#21450;&#36523;&#20221;&#30340;&#19968;&#33268;&#24615;&#65292;&#29983;&#25104;&#20102;&#39640;&#24230;&#36924;&#30495;&#21644;&#33258;&#28982;&#30340;&#21160;&#30011;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMO&#22312;&#34920;&#36798;&#24615;&#21644;&#39046;&#20808;&#25216;&#26415;&#19978;&#22343;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17485v2 Announce Type: replace  Abstract: In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of express
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#29983;&#25104;&#27493;&#39588;&#36873;&#25321;&#19982;&#27979;&#37327;&#30456;&#21516;&#30340;&#26679;&#26412;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#20505;&#36873;&#26679;&#26412;&#25968;&#26469;&#22686;&#24378;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#20026;&#31934;&#20934;&#30340;&#22270;&#20687;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16907</link><description>&lt;p&gt;
Diffusion Posterior Proximal Sampling for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#29983;&#25104;&#27493;&#39588;&#36873;&#25321;&#19982;&#27979;&#37327;&#30456;&#21516;&#30340;&#26679;&#26412;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#20505;&#36873;&#26679;&#26412;&#25968;&#26469;&#22686;&#24378;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#20026;&#31934;&#20934;&#30340;&#22270;&#20687;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16907v2 Announce Type: replace-cross  Abstract: Diffusion models have demonstrated remarkable efficacy in generating high-quality samples. Existing diffusion-based image restoration algorithms exploit pre-trained diffusion models to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we present a refined paradigm for diffusion-based image restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. The number of candidate samples used for selection is adaptively determined based on the signal-to-noise ratio of the timestep. Additionally, we start the restoration process with an initialization combined with the meas
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#25991;&#39640;&#32771;&#30340;&#20840;&#26032;&#22810;&#27169;&#24577;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#35201;&#27714;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#22270;&#29255;&#12289;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#21644;&#20854;&#20182;10&#31181;&#31867;&#22411;&#22270;&#29255;&#30340;&#27491;&#30830;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#21516;&#26102;&#35201;&#27714;&#27169;&#22411;&#20855;&#26377;&#25512;&#29702;&#21644;&#22788;&#29702;&#39640;&#32423;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;GAOKAO-MM&#22522;&#20934;&#20013;&#65292;10&#20010;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#19981;&#20339;&#65292;&#23427;&#20204;&#30340;&#27491;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;LVLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#36739;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#25991;&#39640;&#32771;&#30340;&#20840;&#26032;&#22810;&#27169;&#24577;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#35201;&#27714;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#22270;&#29255;&#12289;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#21644;&#20854;&#20182;10&#31181;&#31867;&#22411;&#22270;&#29255;&#30340;&#27491;&#30830;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#21516;&#26102;&#35201;&#27714;&#27169;&#22411;&#20855;&#26377;&#25512;&#29702;&#21644;&#22788;&#29702;&#39640;&#32423;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;GAOKAO-MM&#22522;&#20934;&#20013;&#65292;10&#20010;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#19981;&#20339;&#65292;&#23427;&#20204;&#30340;&#27491;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;LVLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#36739;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v2 Announce Type: replace-cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs ha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#21270;&#37327;&#23376;&#28857;&#27979;&#37327;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#21019;&#26032;&#36129;&#29486;&#33021;&#22815;&#24110;&#21161;&#24555;&#36895;&#26377;&#25928;&#22320;&#23545;&#37327;&#23376;&#35745;&#31639;&#35774;&#22791;&#36827;&#34892;&#26356;&#31934;&#30830;&#30340;&#35843;&#26657;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#27979;&#37327;&#24471;&#21040;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#25351;&#23548;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#20174;&#32780;&#21152;&#36895;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
Automation of Quantum Dot Measurement Analysis via Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#21270;&#37327;&#23376;&#28857;&#27979;&#37327;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#21019;&#26032;&#36129;&#29486;&#33021;&#22815;&#24110;&#21161;&#24555;&#36895;&#26377;&#25928;&#22320;&#23545;&#37327;&#23376;&#35745;&#31639;&#35774;&#22791;&#36827;&#34892;&#26356;&#31934;&#30830;&#30340;&#35843;&#26657;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#27979;&#37327;&#24471;&#21040;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#25351;&#23548;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#20174;&#32780;&#21152;&#36895;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v4 Announce Type: replace  Abstract: The rapid development of quantum dot (QD) devices for quantum computing has necessitated more efficient and automated methods for device characterization and tuning. Many of the measurements acquired during the tuning process come in the form of images that need to be properly analyzed to guide the subsequent tuning steps. By design, features present in such images capture certain behaviors or states of the measured QD devices. When considered carefully, such features can aid the control and calibration of QD devices. An important example of such images are so-called \textit{triangle plots}, which visually represent current flow and reveal characteristics important for QD device calibration. While image-based classification tools, such as convolutional neural networks (CNNs), can be used to verify whether a given measurement is \textit{good} and thus warrants the initiation of the next phase of tuning, they do not provide any insight
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20351;&#29992;NeRF&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#25216;&#26415;&#36827;&#34892;&#26893;&#29289;&#19977;&#32500;&#20960;&#20309;&#37325;&#24314;&#65292;&#22312;&#23460;&#20869;&#22806;&#22810;&#31181;&#22797;&#26434;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25143;&#22806;&#22330;&#26223;&#20013;&#65292;&#20854;&#37325;&#24314;&#31934;&#24230;&#24050;&#36798;&#21040;74.6%&#65292;&#35777;&#23454;&#20102;NeRF&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;&#26089;&#20572;&#35757;&#32451;&#25216;&#24039;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20165;&#25439;&#22833;&#20102;7.4%&#30340;&#24179;&#22343;F1&#20998;&#25968;&#26469;&#20248;&#21270;&#20102;NeRF&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10344</link><description>&lt;p&gt;
Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry Reconstruction in Field Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20351;&#29992;NeRF&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#25216;&#26415;&#36827;&#34892;&#26893;&#29289;&#19977;&#32500;&#20960;&#20309;&#37325;&#24314;&#65292;&#22312;&#23460;&#20869;&#22806;&#22810;&#31181;&#22797;&#26434;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25143;&#22806;&#22330;&#26223;&#20013;&#65292;&#20854;&#37325;&#24314;&#31934;&#24230;&#24050;&#36798;&#21040;74.6%&#65292;&#35777;&#23454;&#20102;NeRF&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;&#26089;&#20572;&#35757;&#32451;&#25216;&#24039;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20165;&#25439;&#22833;&#20102;7.4%&#30340;&#24179;&#22343;F1&#20998;&#25968;&#26469;&#20248;&#21270;&#20102;NeRF&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10344v3 Announce Type: replace  Abstract: We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D reconstruction of plants in varied environments, from indoor settings to outdoor fields. Traditional methods usually fail to capture the complex geometric details of plants, which is crucial for phenotyping and breeding studies. We evaluate the reconstruction fidelity of NeRFs in three scenarios with increasing complexity and compare the results with the point cloud obtained using LiDAR as ground truth. In the most realistic field scenario, the NeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU, highlighting the efficacy of NeRFs for 3D reconstruction in challenging environments. Additionally, we propose an early stopping technique for NeRF training that almost halves the training time while achieving only a reduction of 7.4% in the average F1 score. This optimization process significantly enhances the speed and efficiency of 3D recon
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24341;&#20837;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#21644;&#25913;&#21892; multimodal &#27169;&#22411;&#22312;&#19981;&#21516;&#23545;&#35937;&#19982;&#39068;&#33394;&#21305;&#37197;&#26041;&#38754;&#30340;&#24615;&#33021;&#26469;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21457;&#23637;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#39064;&#30446;&#21644;&#22270;&#20687;&#20197;&#21450;&#20154;&#31867;&#21518;&#32493;&#30340;&#26657;&#23545;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;2,000&#32452;&#22270;&#29255;&#21644;&#21477;&#23376;&#65292;&#27599;&#32452;&#21253;&#21547;&#21407;&#22987;&#30340;&#22270;&#29255;&#21644;&#21477;&#23376;&#20197;&#21450;&#39068;&#33394;&#23545;&#35843;&#30340;&#37197;&#23545;&#65292;&#20174;&#32780;&#20026;&#27169;&#22411;&#22312;&#39068;&#33394;&#24863;&#30693;&#21644;&#29702;&#35299;&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;&#21363;&#20415;&#20351;&#29992;&#20102;&#35832;&#22914; GPT-4V &#21644; LLaVA &#31561;&#36739;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#26102;&#27169;&#22411;&#30340;&#34920;&#29616;&#20063;&#21482;&#33021;&#36798;&#21040;72%&#21644;42%&#65292;&#32780;&#19988;&#36890;&#36807;&#26356;&#39640;&#32423;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#23545;&#27604;&#24615;&#27169;&#22411;&#35832;&#22914; CLIP &#21644; SigLIP &#22312;&#22788;&#29702;&#36825;&#31867;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.04492</link><description>&lt;p&gt;
ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24341;&#20837;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#21644;&#25913;&#21892; multimodal &#27169;&#22411;&#22312;&#19981;&#21516;&#23545;&#35937;&#19982;&#39068;&#33394;&#21305;&#37197;&#26041;&#38754;&#30340;&#24615;&#33021;&#26469;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21457;&#23637;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#39064;&#30446;&#21644;&#22270;&#20687;&#20197;&#21450;&#20154;&#31867;&#21518;&#32493;&#30340;&#26657;&#23545;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;2,000&#32452;&#22270;&#29255;&#21644;&#21477;&#23376;&#65292;&#27599;&#32452;&#21253;&#21547;&#21407;&#22987;&#30340;&#22270;&#29255;&#21644;&#21477;&#23376;&#20197;&#21450;&#39068;&#33394;&#23545;&#35843;&#30340;&#37197;&#23545;&#65292;&#20174;&#32780;&#20026;&#27169;&#22411;&#22312;&#39068;&#33394;&#24863;&#30693;&#21644;&#29702;&#35299;&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;&#21363;&#20415;&#20351;&#29992;&#20102;&#35832;&#22914; GPT-4V &#21644; LLaVA &#31561;&#36739;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#26102;&#27169;&#22411;&#30340;&#34920;&#29616;&#20063;&#21482;&#33021;&#36798;&#21040;72%&#21644;42%&#65292;&#32780;&#19988;&#36890;&#36807;&#26356;&#39640;&#32423;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#23545;&#27604;&#24615;&#27169;&#22411;&#35832;&#22914; CLIP &#21644; SigLIP &#22312;&#22788;&#29702;&#36825;&#31867;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.04492v2 Announce Type: replace  Abstract: This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to cha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#35270;&#35273;&#20998;&#31867;&#22120;&#26102;&#24212;&#23545;&#22270;&#20687;&#25200;&#21160;&#24773;&#20917;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#20026;&#33322;&#31354;&#34892;&#19994;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#35748;&#35777;&#25552;&#20379;&#20102;&#26032;&#30340;&#25216;&#26415;&#36335;&#24452;&#65292;&#26088;&#22312;&#30830;&#20445;&#22312;&#20855;&#26377;&#39640;&#23433;&#20840;&#35201;&#27714;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#19981;&#20250;&#20986;&#29616;&#37325;&#22823;&#35823;&#21028;&#12290;</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#35270;&#35273;&#20998;&#31867;&#22120;&#26102;&#24212;&#23545;&#22270;&#20687;&#25200;&#21160;&#24773;&#20917;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#20026;&#33322;&#31354;&#34892;&#19994;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#35748;&#35777;&#25552;&#20379;&#20102;&#26032;&#30340;&#25216;&#26415;&#36335;&#24452;&#65292;&#26088;&#22312;&#30830;&#20445;&#22312;&#20855;&#26377;&#39640;&#23433;&#20840;&#35201;&#27714;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#19981;&#20250;&#20986;&#29616;&#37325;&#22823;&#35823;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.00035v4 Announce Type: replace  Abstract: As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we there
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PathoDuet&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;H&amp;E&#21644;IHC&#26579;&#33394;&#30340;&#30149;&#29702;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29305;&#23450;&#20110;&#30149;&#29702;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#22914;&#19981;&#21516;&#20493;&#29575;&#30340;&#22270;&#20687;&#21644;&#19981;&#21516;&#26579;&#33394;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#8212;&#8212;&#36328;&#23610;&#24230;&#23450;&#20301;&#21644;&#36328;&#26579;&#33394;&#36716;&#25442;&#65292;&#30740;&#31350;&#32773;&#33021;&#22815;&#22312;H&amp;E&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#26377;&#25928;&#36801;&#31227;&#21040;IHC&#22270;&#20687;&#20219;&#21153;&#19978;&#12290;</title><link>https://arxiv.org/abs/2312.09894</link><description>&lt;p&gt;
PathoDuet: Foundation Models for Pathological Slide Analysis of H&amp;E and IHC Stains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PathoDuet&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;H&amp;E&#21644;IHC&#26579;&#33394;&#30340;&#30149;&#29702;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29305;&#23450;&#20110;&#30149;&#29702;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#22914;&#19981;&#21516;&#20493;&#29575;&#30340;&#22270;&#20687;&#21644;&#19981;&#21516;&#26579;&#33394;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#8212;&#8212;&#36328;&#23610;&#24230;&#23450;&#20301;&#21644;&#36328;&#26579;&#33394;&#36716;&#25442;&#65292;&#30740;&#31350;&#32773;&#33021;&#22815;&#22312;H&amp;E&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#26377;&#25928;&#36801;&#31227;&#21040;IHC&#22270;&#20687;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09894v2 Announce Type: replace  Abstract: Large amounts of digitized histopathological data display a promising future for developing pathological foundation models via self-supervised learning methods. Foundation models pretrained with these methods serve as a good basis for downstream tasks. However, the gap between natural and histopathological images hinders the direct application of existing methods. In this work, we present PathoDuet, a series of pretrained models on histopathological images, and a new self-supervised learning framework in histopathology. The framework is featured by a newly-introduced pretext token and later task raisers to explicitly utilize certain relations between images, like multiple magnifications and multiple stains. Based on this, two pretext tasks, cross-scale positioning and cross-stain transferring, are designed to pretrain the model on Hematoxylin and Eosin (H&amp;E) images and transfer the model to immunohistochemistry (IHC) images, respecti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#21435;&#23398;&#20064;&#65288;Deep Unlearning&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#20381;&#36182;&#26799;&#24230;&#20449;&#24687;&#22320;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#35299;&#20915;&#31867;&#21035;&#36951;&#24536;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#38656;&#37325;&#35757;&#27169;&#22411;&#21363;&#21487;&#20026;&#27599;&#20010;&#21024;&#38500;&#35831;&#27714;&#36827;&#34892;&#31934;&#30830;&#30340;&#31867;&#21035;&#26356;&#26032;&#65292;&#22312;&#19981;&#36829;&#32972;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.00761</link><description>&lt;p&gt;
Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#21435;&#23398;&#20064;&#65288;Deep Unlearning&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#20381;&#36182;&#26799;&#24230;&#20449;&#24687;&#22320;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#35299;&#20915;&#31867;&#21035;&#36951;&#24536;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#38656;&#37325;&#35757;&#27169;&#22411;&#21363;&#21487;&#20026;&#27599;&#20010;&#21024;&#38500;&#35831;&#27714;&#36827;&#34892;&#31934;&#30830;&#30340;&#31867;&#21035;&#26356;&#26032;&#65292;&#22312;&#19981;&#36829;&#32972;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00761v4 Announce Type: replace-cross  Abstract: Machine unlearning is a prominent and challenging field, driven by regulatory demands for user data deletion and heightened privacy awareness. Existing approaches involve retraining model or multiple finetuning steps for each deletion request, often constrained by computational limits and restricted data access. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate specific classes from the learned model. Our algorithm first estimates the Retain and the Forget Spaces using Singular Value Decomposition on the layerwise activations for a small subset of samples from the retain and unlearn classes, respectively. We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space. Finally, we obtain the unlearned model by updating the weights to suppress the class discriminatory features from the activation spaces. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;InceptionHuman&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#25351;&#20196;&#36755;&#20837;&#65288;&#22914;&#25991;&#26412;&#12289;&#23039;&#24577;&#12289;&#36793;&#32536;&#12289;&#20998;&#21106;&#22270;&#31561;&#65289;&#65292;&#33021;&#22815;&#25511;&#21046;&#29983;&#25104;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;3D&#20154;&#31867;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#19981;&#20855;&#26377;&#29420;&#29305;&#29305;&#24449;&#12289;&#19981;&#33258;&#28982;&#30340;&#38452;&#24433;&#12289;&#19981;&#33258;&#28982;&#30340;&#23039;&#21183;/&#26381;&#39280;&#12289;&#26377;&#38480;&#30340;&#35270;&#35282;&#31561;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#32454;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#20004;&#20010;&#26032;&#39062;&#27169;&#22359;&#8212;&#8212;&#36845;&#20195;&#23039;&#24577;&#24863;&#30693;&#31934;&#20462;&#65288;IPAR&#65289;&#21644;&#28176;&#36827;&#24335;&#22686;&#24378;&#37325;&#24314;&#65288;PAR&#65289;&#65292;&#23454;&#29616;&#20102;3D&#20154;&#31867;&#27169;&#22411;&#30340;&#31283;&#23450;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2311.16499</link><description>&lt;p&gt;
InceptionHuman: Controllable Prompt-to-NeRF for Photorealistic 3D Human Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;InceptionHuman&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#25351;&#20196;&#36755;&#20837;&#65288;&#22914;&#25991;&#26412;&#12289;&#23039;&#24577;&#12289;&#36793;&#32536;&#12289;&#20998;&#21106;&#22270;&#31561;&#65289;&#65292;&#33021;&#22815;&#25511;&#21046;&#29983;&#25104;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;3D&#20154;&#31867;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#19981;&#20855;&#26377;&#29420;&#29305;&#29305;&#24449;&#12289;&#19981;&#33258;&#28982;&#30340;&#38452;&#24433;&#12289;&#19981;&#33258;&#28982;&#30340;&#23039;&#21183;/&#26381;&#39280;&#12289;&#26377;&#38480;&#30340;&#35270;&#35282;&#31561;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#32454;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#20004;&#20010;&#26032;&#39062;&#27169;&#22359;&#8212;&#8212;&#36845;&#20195;&#23039;&#24577;&#24863;&#30693;&#31934;&#20462;&#65288;IPAR&#65289;&#21644;&#28176;&#36827;&#24335;&#22686;&#24378;&#37325;&#24314;&#65288;PAR&#65289;&#65292;&#23454;&#29616;&#20102;3D&#20154;&#31867;&#27169;&#22411;&#30340;&#31283;&#23450;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16499v2 Announce Type: replace  Abstract: This paper presents InceptionHuman, a prompt-to-NeRF framework that allows easy control via a combination of prompts in different modalities (e.g., text, poses, edge, segmentation map, etc) as inputs to generate photorealistic 3D humans. While many works have focused on generating 3D human models, they suffer one or more of the following: lack of distinctive features, unnatural shading/shadows, unnatural poses/clothes, limited views, etc. InceptionHuman achieves consistent 3D human generation within a progressively refined NeRF space with two novel modules, Iterative Pose-Aware Refinement (IPAR) and Progressive-Augmented Reconstruction (PAR). IPAR iteratively refines the diffusion-generated images and synthesizes high-quality 3D-aware views considering the close-pose RGB values. PAR employs a pretrained diffusion prior to augment the generated synthetic views and adds regularization for view-independent appearance. Overall, the synth
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31283;&#23450;&#24615;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#21319;&#31070;&#32463;ODE&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.15890</link><description>&lt;p&gt;
Stability-Informed Initialization of Neural Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31283;&#23450;&#24615;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#21319;&#31070;&#32463;ODE&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15890v3 Announce Type: replace-cross  Abstract: This paper addresses the training of Neural Ordinary Differential Equations (neural ODEs), and in particular explores the interplay between numerical integration techniques, stability regions, step size, and initialization techniques. It is shown how the choice of integration technique implicitly regularizes the learned model, and how the solver's corresponding stability region affects training and prediction performance. From this analysis, a stability-informed parameter initialization technique is introduced. The effectiveness of the initialization method is displayed across several learning benchmarks and industrial applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSB-Pose&#30340;&#30701;&#22522;&#32447;&#21452;&#30446;&#19977;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#31435;&#20307;&#22810;&#36793;&#24418;&#29305;&#24449;&#21644;&#31435;&#20307;&#20849;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22359;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#30001;&#20110;&#22522;&#32447;&#32553;&#30701;&#32780;&#23548;&#33268;&#30340;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;2D&#20851;&#38190;&#28857;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#21644;&#25668;&#20687;&#22836;&#20043;&#38388;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.14242</link><description>&lt;p&gt;
RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with Occlusion Handling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSB-Pose&#30340;&#30701;&#22522;&#32447;&#21452;&#30446;&#19977;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#31435;&#20307;&#22810;&#36793;&#24418;&#29305;&#24449;&#21644;&#31435;&#20307;&#20849;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22359;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#30001;&#20110;&#22522;&#32447;&#32553;&#30701;&#32780;&#23548;&#33268;&#30340;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;2D&#20851;&#38190;&#28857;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#21644;&#25668;&#20687;&#22836;&#20043;&#38388;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14242v2 Announce Type: replace  Abstract: In the domain of 3D Human Pose Estimation, which finds widespread daily applications, the requirement for convenient acquisition equipment continues to grow. To satisfy this demand, we set our sights on a short-baseline binocular setting that offers both portability and a geometric measurement property that radically mitigates depth ambiguity. However, as the binocular baseline shortens, two serious challenges emerge: first, the robustness of 3D reconstruction against 2D errors deteriorates; and second, occlusion reoccurs due to the limited visual differences between two views. To address the first challenge, we propose the Stereo Co-Keypoints Estimation module to improve the view consistency of 2D keypoints and enhance the 3D robustness. In this module, the disparity is utilized to represent the correspondence of binocular 2D points and the Stereo Volume Feature is introduced to contain binocular features across different disparitie
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#24418;&#22330;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#25968;&#25454;&#30340;&#21487;&#37325;&#23450;&#21521;&#32534;&#36753;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#20869;&#23481;&#30340;&#21516;&#26102;&#24378;&#21270;&#20302;&#20449;&#24687;&#20869;&#23481;&#30340;&#21306;&#22495;&#21464;&#24418;&#65292;&#36866;&#29992;&#20110;&#22270;&#20687;&#12289;3D&#22330;&#26223;&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#21644; polygon &#32593;&#26684;&#31561;&#22810;&#31181;&#35270;&#35273;&#25968;&#25454;&#26684;&#24335;&#65292;&#30456;&#27604;&#20197;&#24448;&#26041;&#27861;&#22312;&#20869;&#23481;&#33258;&#36866;&#24212;&#37325;&#22609;&#26041;&#38754;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>https://arxiv.org/abs/2311.13297</link><description>&lt;p&gt;
Retargeting Visual Data with Deformation Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#24418;&#22330;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#25968;&#25454;&#30340;&#21487;&#37325;&#23450;&#21521;&#32534;&#36753;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#20869;&#23481;&#30340;&#21516;&#26102;&#24378;&#21270;&#20302;&#20449;&#24687;&#20869;&#23481;&#30340;&#21306;&#22495;&#21464;&#24418;&#65292;&#36866;&#29992;&#20110;&#22270;&#20687;&#12289;3D&#22330;&#26223;&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#21644; polygon &#32593;&#26684;&#31561;&#22810;&#31181;&#35270;&#35273;&#25968;&#25454;&#26684;&#24335;&#65292;&#30456;&#27604;&#20197;&#24448;&#26041;&#27861;&#22312;&#20869;&#23481;&#33258;&#36866;&#24212;&#37325;&#22609;&#26041;&#38754;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13297v2 Announce Type: replace  Abstract: Seam carving is an image editing method that enable content-aware resizing, including operations like removing objects. However, the seam-finding strategy based on dynamic programming or graph-cut limits its applications to broader visual data formats and degrees of freedom for editing. Our observation is that describing the editing and retargeting of images more generally by a displacement field yields a generalisation of content-aware deformations. We propose to learn a deformation with a neural network that keeps the output plausible while trying to deform it only in places with low information content. This technique applies to different kinds of visual data, including images, 3D scenes given as neural radiance fields, or even polygon meshes. Experiments conducted on different visual data show that our method achieves better content-aware retargeting compared to previous methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#19977;&#32500;&#29615;&#22659;&#37325;&#24314;&#21644;&#24494;&#37325;&#21147;&#26465;&#20214;&#19979;&#21464;&#21270;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30001;&#39134;&#34892;&#30340;&#26426;&#22120;&#20154;&#26469;&#32500;&#25252;&#22826;&#31354;&#22522;&#22320;&#12290;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#22270;&#20687;&#21644;&#28145;&#24230;&#20449;&#24687;&#37325;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#21478;&#19968;&#20010;&#26426;&#22120;&#20154;&#23450;&#26399;&#26816;&#26597;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#36866;&#29992;&#20110;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2311.02558</link><description>&lt;p&gt;
Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#19977;&#32500;&#29615;&#22659;&#37325;&#24314;&#21644;&#24494;&#37325;&#21147;&#26465;&#20214;&#19979;&#21464;&#21270;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30001;&#39134;&#34892;&#30340;&#26426;&#22120;&#20154;&#26469;&#32500;&#25252;&#22826;&#31354;&#22522;&#22320;&#12290;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#22270;&#20687;&#21644;&#28145;&#24230;&#20449;&#24687;&#37325;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#21478;&#19968;&#20010;&#26426;&#22120;&#20154;&#23450;&#26399;&#26816;&#26597;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#36866;&#29992;&#20110;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02558v3 Announce Type: replace  Abstract: Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction sys
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;Agent&#21327;&#20316;&#29615;&#22659;&#20013;&#35299;&#20915;&#30001;&#20110;&#35270;&#35282;&#21464;&#21270;&#36896;&#25104;&#30340;&#35270;&#35273;&#23450;&#20301;&#38382;&#39064;&#12290;&#30740;&#31350;&#24037;&#20316;&#36890;&#36807;&#23545;&#27604;&#29616;&#26377;&#26041;&#27861;&#21644;&#25552;&#20986;&#30340;&#22810;&#20010;&#22522;&#32447;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#26088;&#22312;&#36873;&#25321;&#22312;&#29305;&#23450;&#20301;&#32622;&#36827;&#34892;&#26368;&#20339;&#30340;&#35270;&#35282;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2310.02650</link><description>&lt;p&gt;
Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;Agent&#21327;&#20316;&#29615;&#22659;&#20013;&#35299;&#20915;&#30001;&#20110;&#35270;&#35282;&#21464;&#21270;&#36896;&#25104;&#30340;&#35270;&#35273;&#23450;&#20301;&#38382;&#39064;&#12290;&#30740;&#31350;&#24037;&#20316;&#36890;&#36807;&#23545;&#27604;&#29616;&#26377;&#26041;&#27861;&#21644;&#25552;&#20986;&#30340;&#22810;&#20010;&#22522;&#32447;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#26088;&#22312;&#36873;&#25321;&#22312;&#29305;&#23450;&#20301;&#32622;&#36827;&#34892;&#26368;&#20339;&#30340;&#35270;&#35282;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02650v3 Announce Type: replace  Abstract: Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#20165;&#20381;&#36182;&#26597;&#35810;&#28857;&#26631;&#27880;&#30340;&#24369; supervision&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#20165;&#38656;&#23569;&#37327;&#30340;&#26597;&#35810;&#28857;&#26631;&#27880;&#21363;&#21487;&#36827;&#34892;&#21355;&#26143;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39640;&#31934;&#24230;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2309.05490</link><description>&lt;p&gt;
Learning Semantic Segmentation with Query Points Supervision on Aerial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#20165;&#20381;&#36182;&#26597;&#35810;&#28857;&#26631;&#27880;&#30340;&#24369; supervision&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#20165;&#38656;&#23569;&#37327;&#30340;&#26597;&#35810;&#28857;&#26631;&#27880;&#21363;&#21487;&#36827;&#34892;&#21355;&#26143;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39640;&#31934;&#24230;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05490v2 Announce Type: replace  Abstract: Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models supervised with imag
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;Adv3D&#65292;&#19968;&#31181;&#20351;&#29992;Neural Radiance Fields&#65288;NeRF&#65289;&#29983;&#25104;3D&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;3D&#29289;&#20307;&#26816;&#27979;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#37051;&#23545;&#35937;&#22312;&#35757;&#32451;&#38598;&#20013;&#39044;&#27979;&#30340;&#20449;&#24515;&#65292;Adv3D&#21033;&#29992;NeRF&#30340;3D&#20934;&#30830;&#24230;&#21644;&#36924;&#30495;&#22806;&#35266;&#29305;&#24615;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#19978;&#21487;&#23454;&#29616;&#30340;&#23545;&#25239;&#24615;&#23041;&#32961;&#65292;&#23637;&#31034;&#20102;&#22312;&#28210;&#26579;NeRF&#26102;&#23545;3D&#26816;&#27979;&#22120;&#30340;&#26174;&#33879;&#24615;&#33021;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2309.01351</link><description>&lt;p&gt;
Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in Driving Scenarios with NeRF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;Adv3D&#65292;&#19968;&#31181;&#20351;&#29992;Neural Radiance Fields&#65288;NeRF&#65289;&#29983;&#25104;3D&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;3D&#29289;&#20307;&#26816;&#27979;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#37051;&#23545;&#35937;&#22312;&#35757;&#32451;&#38598;&#20013;&#39044;&#27979;&#30340;&#20449;&#24515;&#65292;Adv3D&#21033;&#29992;NeRF&#30340;3D&#20934;&#30830;&#24230;&#21644;&#36924;&#30495;&#22806;&#35266;&#29305;&#24615;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#19978;&#21487;&#23454;&#29616;&#30340;&#23545;&#25239;&#24615;&#23041;&#32961;&#65292;&#23637;&#31034;&#20102;&#22312;&#28210;&#26579;NeRF&#26102;&#23545;3D&#26816;&#27979;&#22120;&#30340;&#26174;&#33879;&#24615;&#33021;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01351v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adver
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#34701;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#25163;&#24037;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#65292;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#26448;&#26009;&#26465;&#20214;&#19979;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2308.10015</link><description>&lt;p&gt;
DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#34701;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#25163;&#24037;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#65292;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#26448;&#26009;&#26465;&#20214;&#19979;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10015v2 Announce Type: replace  Abstract: Automatic fingerprint recognition systems suffer from the threat of presentation attacks due to their wide range of deployment in areas including national borders and commercial applications. A presentation attack can be performed by creating a spoof of a user's fingerprint with or without their consent. This paper presents a dynamic ensemble of deep CNN and handcrafted features to detect presentation attacks in known-material and unknown-material protocols of the livness detection competition. The proposed presentation attack detection model, in this way, utilizes the capabilities of both deep CNN and handcrafted features techniques and exhibits better performance than their individual performances. The proposed method is validated using benchmark databases from the Liveness Detection Competition in 2015, 2017, and 2019, yielding overall accuracy of 96.10\%, 96.49\%, and 94.99\% on them, respectively. The proposed method outperforms
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FS-CDIS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#19987;&#38376;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#20110;&#20855;&#26377;&#39640;&#24230;&#30456;&#20284;&#32972;&#26223;&#30340;&#38544;&#36523;&#23545;&#35937;&#65288;&#22914;&#37326;&#29983;&#21160;&#29289;&#65289;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2304.07444</link><description>&lt;p&gt;
The Art of Camouflage: Few-Shot Learning for Animal Detection and Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.07444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FS-CDIS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#19987;&#38376;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#20110;&#20855;&#26377;&#39640;&#24230;&#30456;&#20284;&#32972;&#26223;&#30340;&#38544;&#36523;&#23545;&#35937;&#65288;&#22914;&#37326;&#29983;&#21160;&#29289;&#65289;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.07444v4 Announce Type: replace  Abstract: Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data on concealed objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. As camouflaged instances are challenging to recognize due to their similarity compared to the surroundings, we guide our models to obtain camouflaged features that highly distinguish the instances from the background. In this work, we propose FS-CDIS, a framework to efficiently detect and segment camouflaged instances via two loss functions contributing to the training process. Firstly, the instance triplet loss with the characteristic of differentiating the anchor, which is the mean of all camouflaged foreground points, and the background poi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#22270;&#20687;&#26816;&#27979;&#21644;&#37327;&#21270;Giardia&#21644;Cryptosporidium&#65288;oo&#65289;&#22218;&#27663;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26174;&#24494;&#38236;&#26816;&#27979;&#25104;&#26412;&#39640;&#12289;&#19981;&#26131;&#25658;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#25216;&#26415;&#20154;&#21592;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2304.05339</link><description>&lt;p&gt;
Deep-learning Assisted Detection and Quantification of (oo)cysts of Giardia and Cryptosporidium on Smartphone Microscopy Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.05339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#22270;&#20687;&#26816;&#27979;&#21644;&#37327;&#21270;Giardia&#21644;Cryptosporidium&#65288;oo&#65289;&#22218;&#27663;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26174;&#24494;&#38236;&#26816;&#27979;&#25104;&#26412;&#39640;&#12289;&#19981;&#26131;&#25658;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#25216;&#26415;&#20154;&#21592;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.05339v2 Announce Type: replace-cross  Abstract: The consumption of microbial-contaminated food and water is responsible for the deaths of millions of people annually. Smartphone-based microscopy systems are portable, low-cost, and more accessible alternatives for the detection of Giardia and Cryptosporidium than traditional brightfield microscopes. However, the images from smartphone microscopes are noisier and require manual cyst identification by trained technicians, usually unavailable in resource-limited settings. Automatic detection of (oo)cysts using deep-learning-based object detection could offer a solution for this limitation. We evaluate the performance of four state-of-the-art object detectors to detect (oo)cysts of Giardia and Cryptosporidium on a custom dataset that includes both smartphone and brightfield microscopic images from vegetable samples. Faster RCNN, RetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer (Deformable DETR) deep-l
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#19979;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#20272;&#35745;&#65288;AMGE&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;MuscleMap&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#36816;&#21160;&#35270;&#39057;&#21644;&#22810;&#31181;&#29289;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#30340;&#26631;&#27880;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#19968;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#24037;&#20316;&#30340;&#35270;&#39057;&#20998;&#26512;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#20307;&#32946;&#21644;&#24247;&#22797;&#21307;&#30103;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2303.00952</link><description>&lt;p&gt;
Towards Activated Muscle Group Estimation in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#19979;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#20272;&#35745;&#65288;AMGE&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;MuscleMap&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#36816;&#21160;&#35270;&#39057;&#21644;&#22810;&#31181;&#29289;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#30340;&#26631;&#27880;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#19968;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#24037;&#20316;&#30340;&#35270;&#39057;&#20998;&#26512;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#20307;&#32946;&#21644;&#24247;&#22797;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00952v5 Announce Type: replace-cross  Abstract: In this paper, we tackle the new task of video-based Activated Muscle Group Estimation (AMGE) aiming at identifying active muscle regions during physical activity in the wild. To this intent, we provide the MuscleMap dataset featuring &gt;15K video clips with 135 different activities and 20 labeled muscle groups. This dataset opens the vistas to multiple video-based applications in sports and rehabilitation medicine under flexible environment constraints. The proposed MuscleMap dataset is constructed with YouTube videos, specifically targeting High-Intensity Interval Training (HIIT) physical exercise in the wild. To make the AMGE model applicable in real-life situations, it is crucial to ensure that the model can generalize well to numerous types of physical activities not present during training and involving new combinations of activated muscles. To achieve this, our benchmark also covers an evaluation setting where the model is
&lt;/p&gt;</description></item></channel></rss>