<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.CV.xml</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;Latent-INR&#65292;&#23427;&#33021;&#22815;&#20351;&#24471;&#35270;&#39057;&#38544;&#24335;&#34920;&#31034;&#65288;INR&#65289;&#19981;&#20165;&#20855;&#26377;&#39640;&#25928;&#30340;&#21387;&#32553;&#29305;&#24615;&#65292;&#36824;&#33021;&#22815;&#20256;&#36798;&#20986;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#35270;&#39057;&#21387;&#32553;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#35270;&#39057;&#26816;&#32034;&#31561;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.02672</link><description>&lt;p&gt;
Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;Latent-INR&#65292;&#23427;&#33021;&#22815;&#20351;&#24471;&#35270;&#39057;&#38544;&#24335;&#34920;&#31034;&#65288;INR&#65289;&#19981;&#20165;&#20855;&#26377;&#39640;&#25928;&#30340;&#21387;&#32553;&#29305;&#24615;&#65292;&#36824;&#33021;&#22815;&#20256;&#36798;&#20986;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#35270;&#39057;&#21387;&#32553;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#35270;&#39057;&#26816;&#32034;&#31561;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02672v1 Announce Type: new  Abstract: Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes. With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality. However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval. This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression. To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR. We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lumina-mGPT&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#28789;&#27963;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#21644;&#22270;&#20687;&#28151;&#21512;&#30340;&#24207;&#21015;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#39640;&#36136;&#22270;&#20687;&#25991;&#26412;&#23545;&#36827;&#34892;&#28176;&#36827;&#24335;&#30417;&#30563;&#24494;&#35843;&#21644;&#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#65292;Lumina-mGPT&#33021;&#22815;&#22312;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#20855;&#26377;&#39640;&#32654;&#23398;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2408.02657</link><description>&lt;p&gt;
Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lumina-mGPT&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#28789;&#27963;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#21644;&#22270;&#20687;&#28151;&#21512;&#30340;&#24207;&#21015;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#39640;&#36136;&#22270;&#20687;&#25991;&#26412;&#23545;&#36827;&#34892;&#28176;&#36827;&#24335;&#30417;&#30563;&#24494;&#35843;&#21644;&#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#65292;Lumina-mGPT&#33021;&#22815;&#22312;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#20855;&#26377;&#39640;&#32654;&#23398;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02657v1 Announce Type: new  Abstract: We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general m
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#20351;&#29992;&#20302;&#20559;&#24046;&#20266;&#38543;&#26426;&#25968;&#24207;&#21015;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21021;&#22987;&#21270;&#20013;&#65292;&#23545;&#26631;&#20934;&#27491;&#21017;&#21270;&#22120;&#21644;&#35745;&#31639;&#24211;&#20013;&#30340;&#24120;&#35265;&#21021;&#22987;&#21270;&#26041;&#27861;&#65288;&#22914;Glorot&#12289;He&#12289;Lecun&#65289;&#36827;&#34892;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#20248;&#21270;&#27169;&#22411;&#30340;&#21021;&#22987;&#26465;&#20214;&#20174;&#32780;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02654</link><description>&lt;p&gt;
On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#20351;&#29992;&#20302;&#20559;&#24046;&#20266;&#38543;&#26426;&#25968;&#24207;&#21015;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21021;&#22987;&#21270;&#20013;&#65292;&#23545;&#26631;&#20934;&#27491;&#21017;&#21270;&#22120;&#21644;&#35745;&#31639;&#24211;&#20013;&#30340;&#24120;&#35265;&#21021;&#22987;&#21270;&#26041;&#27861;&#65288;&#22914;Glorot&#12289;He&#12289;Lecun&#65289;&#36827;&#34892;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#20248;&#21270;&#27169;&#22411;&#30340;&#21021;&#22987;&#26465;&#20214;&#20174;&#32780;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02654v1 Announce Type: cross  Abstract: The effectiveness of training neural networks directly impacts computational costs, resource allocation, and model development timelines in machine learning applications. An optimizer's ability to train the model adequately (in terms of trained model performance) depends on the model's initial weights. Model weight initialization schemes use pseudorandom number generators (PRNGs) as a source of randomness.   We investigate whether substituting PRNGs for low-discrepancy quasirandom number generators (QRNGs) -- namely Sobol' sequences -- as a source of randomness for initializers can improve model performance. We examine Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random Normal
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#36816;&#29992;Segment Anything Model 2&#65288;SAM 2&#65289;&#36827;&#34892;3D&#21307;&#23398;&#22270;&#20687;&#30340;&#20840;&#33258;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#35270;&#39057;&#24103;&#30340;2D&#20999;&#29255;&#36827;&#34892;&#22788;&#29702;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21322;&#33258;&#21160;&#36880;&#23618;&#26631;&#27880;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2408.02635</link><description>&lt;p&gt;
Interactive 3D Medical Image Segmentation with SAM 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#36816;&#29992;Segment Anything Model 2&#65288;SAM 2&#65289;&#36827;&#34892;3D&#21307;&#23398;&#22270;&#20687;&#30340;&#20840;&#33258;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#35270;&#39057;&#24103;&#30340;2D&#20999;&#29255;&#36827;&#34892;&#22788;&#29702;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21322;&#33258;&#21160;&#36880;&#23618;&#26631;&#27880;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02635v1 Announce Type: new  Abstract: Interactive medical image segmentation (IMIS) has shown significant potential in enhancing segmentation accuracy by integrating iterative feedback from medical professionals. However, the limited availability of enough 3D medical data restricts the generalization and robustness of most IMIS methods. The Segment Anything Model (SAM), though effective for 2D images, requires expensive semi-auto slice-by-slice annotations for 3D medical images. In this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM model trained on videos, for 3D medical image segmentation. By treating sequential 2D slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations from a single frame to the entire 3D volume. We propose a practical pipeline for using SAM 2 in 3D medical image segmentation and present key findings highlighting its efficiency and potential for further optimization. Concretely, numeric
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VidGen-1M&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#23545;&#36136;&#37327;&#19981;&#20339;&#12289;&#26631;&#31614;&#36136;&#37327;&#24046;&#12289;&#35270;&#39057;&#36136;&#37327;&#20302;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#35757;&#32451;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2408.02629</link><description>&lt;p&gt;
VidGen-1M: A Large-Scale Dataset for Text-to-video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VidGen-1M&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#23545;&#36136;&#37327;&#19981;&#20339;&#12289;&#26631;&#31614;&#36136;&#37327;&#24046;&#12289;&#35270;&#39057;&#36136;&#37327;&#20302;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#35757;&#32451;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02629v1 Announce Type: new  Abstract: The quality of video-text pairs fundamentally determines the upper bound of text-to-video models. Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution. The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data. As a result, there is a lack of appropriate training datasets for text-to-video models. To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models. Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency. When used to train the video generation model, this dataset has led to experimental results that surpass those obta
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;YOWOv3&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#20154;&#31867;&#21160;&#20316;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#30456;&#36739;&#20110;YOWOv2&#29256;&#26412;&#65292;YOWOv3&#22312;&#30456;&#21516;&#30340;&#20004;&#22823;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#21644;&#35745;&#31639;&#37327;&#26174;&#33879;&#20943;&#23569;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02623</link><description>&lt;p&gt;
YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;YOWOv3&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#20154;&#31867;&#21160;&#20316;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#30456;&#36739;&#20110;YOWOv2&#29256;&#26412;&#65292;YOWOv3&#22312;&#30456;&#21516;&#30340;&#20004;&#22823;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#21644;&#35745;&#31639;&#37327;&#26174;&#33879;&#20943;&#23569;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02623v1 Announce Type: new  Abstract: In this paper, we propose a new framework called YOWOv3, which is an improved version of YOWOv2, designed specifically for the task of Human Action Detection and Recognition. This framework is designed to facilitate extensive experimentation with different configurations and supports easy customization of various components within the model, reducing efforts required for understanding and modifying the code. YOWOv3 demonstrates its superior performance compared to YOWOv2 on two widely used datasets for Human Action Detection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor model YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model - YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33% and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that YOWOv3 significantly reduces the number
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#20445;&#30495;&#24230;&#25193;&#25955;&#27169;&#22411;LaMamba-Diff&#65292;&#37319;&#29992;&#23616;&#37096;&#27880;&#24847;&#21147;&#21644;Mamba&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;Mamba&#30340;&#20248;&#28857;&#65292;&#33021;&#22815;&#22312;&#19981;&#25439;&#22833;&#23616;&#37096;&#32454;&#33410;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#36755;&#20837;&#65292;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#35270;&#35273;&#29983;&#25104;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2408.02615</link><description>&lt;p&gt;
LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#20445;&#30495;&#24230;&#25193;&#25955;&#27169;&#22411;LaMamba-Diff&#65292;&#37319;&#29992;&#23616;&#37096;&#27880;&#24847;&#21147;&#21644;Mamba&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;Mamba&#30340;&#20248;&#28857;&#65292;&#33021;&#22815;&#22312;&#19981;&#25439;&#22833;&#23616;&#37096;&#32454;&#33410;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#36755;&#20837;&#65292;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#35270;&#35273;&#29983;&#25104;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36890;&#36807;&#22788;&#29702;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#25551;&#36848;&#24615;&#22270;&#20687;&#25551;&#36848;&#30340;&#36755;&#20837;&#19977;&#20803;&#32452;&#65292;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#12290;&#36890;&#36807;&#25429;&#25417;&#25991;&#26412;&#19982;&#35270;&#35273;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#35773;&#21050;&#12290;</title><link>https://arxiv.org/abs/2408.02595</link><description>&lt;p&gt;
Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02595
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36890;&#36807;&#22788;&#29702;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#25551;&#36848;&#24615;&#22270;&#20687;&#25551;&#36848;&#30340;&#36755;&#20837;&#19977;&#20803;&#32452;&#65292;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#12290;&#36890;&#36807;&#25429;&#25417;&#25991;&#26412;&#19982;&#35270;&#35273;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02595v1 Announce Type: new  Abstract: Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primar
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#34701;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#25104;&#21151;&#25581;&#31034;&#20102;&#21477;&#23376;&#12289;&#22270;&#20687;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#65292;&#20026;&#31038;&#20132;&#23186;&#20307;&#20013;&#24773;&#24863;&#30340;&#34920;&#36798;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2408.02571</link><description>&lt;p&gt;
Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#34701;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#25104;&#21151;&#25581;&#31034;&#20102;&#21477;&#23376;&#12289;&#22270;&#20687;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#65292;&#20026;&#31038;&#20132;&#23186;&#20307;&#20013;&#24773;&#24863;&#30340;&#34920;&#36798;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02571v1 Announce Type: new  Abstract: The emoticons are symbolic representations that generally accompany the textual content to visually enhance or summarize the true intention of a written message. Although widely utilized in the realm of social media, the core semantics of these emoticons have not been extensively explored based on multiple modalities. Incorporating textual and visual information within a single message develops an advanced way of conveying information. Hence, this research aims to analyze the relationship among sentences, visuals, and emoticons. For an orderly exposition, this paper initially provides a detailed examination of the various techniques for extracting multimodal features, emphasizing the pros and cons of each method. Through conducting a comprehensive examination of several multimodal algorithms, with specific emphasis on the fusion approaches, we have proposed a novel contrastive learning based multimodal architecture. The proposed model em
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMCSL&#65288;Cross-Modality Clustering-based Self-Labeling&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#29305;&#24449;&#31354;&#38388;&#30340;&#36328;&#27169;&#24577;&#32858;&#31867;&#21644;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#23567;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02568</link><description>&lt;p&gt;
Cross-Modality Clustering-based Self-Labeling for Multimodal Data Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMCSL&#65288;Cross-Modality Clustering-based Self-Labeling&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#29305;&#24449;&#31354;&#38388;&#30340;&#36328;&#27169;&#24577;&#32858;&#31867;&#21644;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#23567;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02568v1 Announce Type: cross  Abstract: Technological advances facilitate the ability to acquire multimodal data, posing a challenge for recognition systems while also providing an opportunity to use the heterogeneous nature of the information to increase the generalization capability of models. An often overlooked issue is the cost of the labeling process, which is typically high due to the need for a significant investment in time and money associated with human experts. Existing semi-supervised learning methods often focus on operating in the feature space created by the fusion of available modalities, neglecting the potential for cross-utilizing complementary information available in each modality. To address this problem, we propose Cross-Modality Clustering-based Self-Labeling (CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances belonging to each modality in the deep feature space and then propagates known labels within the resulting clusters. Next
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HQOD&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#20195;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#30340;&#20219;&#21153;&#19981;&#21644;&#35856;&#38382;&#39064;&#65292;&#21363;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#38388;&#30340;&#36136;&#37327;&#19981;&#19968;&#33268;&#24615;&#12290;&#25991;&#31456;&#36890;&#36807;&#24341;&#20837;&#20219;&#21153;&#30456;&#20851;&#25439;&#22833;&#21644;&#21644;&#35856;IoU&#26469;&#40723;&#21169;&#26816;&#27979;&#22120;&#22312;&#37327;&#21270;&#33258;&#36866;&#24212;&#35757;&#32451;&#26399;&#38388;&#20248;&#20808;&#25913;&#21892;&#37027;&#20123;&#20219;&#21153;&#19981;&#21644;&#35856;&#24230;&#36739;&#20302;&#30340;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02561</link><description>&lt;p&gt;
HQOD: Harmonious Quantization for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HQOD&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#20195;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#30340;&#20219;&#21153;&#19981;&#21644;&#35856;&#38382;&#39064;&#65292;&#21363;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#38388;&#30340;&#36136;&#37327;&#19981;&#19968;&#33268;&#24615;&#12290;&#25991;&#31456;&#36890;&#36807;&#24341;&#20837;&#20219;&#21153;&#30456;&#20851;&#25439;&#22833;&#21644;&#21644;&#35856;IoU&#26469;&#40723;&#21169;&#26816;&#27979;&#22120;&#22312;&#37327;&#21270;&#33258;&#36866;&#24212;&#35757;&#32451;&#26399;&#38388;&#20248;&#20808;&#25913;&#21892;&#37027;&#20123;&#20219;&#21153;&#19981;&#21644;&#35856;&#24230;&#36739;&#20302;&#30340;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02561v1 Announce Type: new  Abstract: Task inharmony problem commonly occurs in modern object detectors, leading to inconsistent qualities between classification and regression tasks. The predicted boxes with high classification scores but poor localization positions or low classification scores but accurate localization positions will worsen the performance of detectors after Non-Maximum Suppression. Furthermore, when object detectors collaborate with Quantization-Aware Training (QAT), we observe that the task inharmony problem will be further exacerbated, which is considered one of the main causes of the performance degradation of quantized detectors. To tackle this issue, we propose the Harmonious Quantization for Object Detection (HQOD) framework, which consists of two components. Firstly, we propose a task-correlated loss to encourage detectors to focus on improving samples with lower task harmony quality during QAT. Secondly, a harmonious Intersection over Union (IoU) 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25512;&#20986;&#20102;MeshAnything V2&#65292;&#36825;&#26159;&#19968;&#39033;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#30456;&#37051;&#32593;&#26684;tokenization&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;MeshAnything V2&#22312;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#36825;&#31181;&#25913;&#36827;&#24402;&#21151;&#20110;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;tokenization&#26041;&#27861;&#8212;&#8212;Adjacent Mesh Tokenization&#65288;AMT&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#21487;&#33021;&#30340;&#24773;&#20917;&#21482;&#20351;&#29992;&#21333;&#20010;&#39030;&#28857;&#32780;&#19981;&#26159;&#19977;&#20010;&#39030;&#28857;&#26469;&#34920;&#31034;&#27599;&#20010;&#38754;&#65292;&#20174;&#32780;&#24179;&#22343;&#24773;&#20917;&#19979;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#20102;&#19968;&#21322;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;token&#24207;&#21015;&#26356;&#21152;&#32039;&#20945;&#21644;&#32467;&#26500;&#21270;&#65292;&#20174;&#32780;&#23545;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#20135;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;AMT&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02555</link><description>&lt;p&gt;
MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25512;&#20986;&#20102;MeshAnything V2&#65292;&#36825;&#26159;&#19968;&#39033;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#30456;&#37051;&#32593;&#26684;tokenization&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;MeshAnything V2&#22312;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#36825;&#31181;&#25913;&#36827;&#24402;&#21151;&#20110;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;tokenization&#26041;&#27861;&#8212;&#8212;Adjacent Mesh Tokenization&#65288;AMT&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#21487;&#33021;&#30340;&#24773;&#20917;&#21482;&#20351;&#29992;&#21333;&#20010;&#39030;&#28857;&#32780;&#19981;&#26159;&#19977;&#20010;&#39030;&#28857;&#26469;&#34920;&#31034;&#27599;&#20010;&#38754;&#65292;&#20174;&#32780;&#24179;&#22343;&#24773;&#20917;&#19979;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#20102;&#19968;&#21322;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;token&#24207;&#21015;&#26356;&#21152;&#32039;&#20945;&#21644;&#32467;&#26500;&#21270;&#65292;&#20174;&#32780;&#23545;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#20135;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;AMT&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02555v1 Announce Type: new  Abstract: We introduce MeshAnything V2, an autoregressive transformer that generates Artist-Created Meshes (AM) aligned to given shapes. It can be integrated with various 3D asset production pipelines to achieve high-quality, highly controllable AM generation. MeshAnything V2 surpasses previous methods in both efficiency and performance using models of the same size. These improvements are due to our newly proposed mesh tokenization method: Adjacent Mesh Tokenization (AMT). Different from previous methods that represent each face with three vertices, AMT uses a single vertex whenever possible. Compared to previous methods, AMT requires about half the token sequence length to represent the same mesh in average. Furthermore, the token sequences from AMT are more compact and well-structured, fundamentally benefiting AM generation. Our extensive experiments show that AMT significantly improves the efficiency and performance of AM generation. Project P
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;pore&#20301;&#32622;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#21161;in-situ&#30417;&#27979;&#25968;&#25454;&#26469;&#35757;&#32451;&#19981;&#21516;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;pore&#20301;&#32622;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#32780;&#20026;Laser Powder Bed Fusion&#24037;&#33402;&#30340;&#36136;&#37327;&#30417;&#25511;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02507</link><description>&lt;p&gt;
Estimating Pore Location of PBF-LB/M Processes with Segmentation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02507
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;pore&#20301;&#32622;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#21161;in-situ&#30417;&#27979;&#25968;&#25454;&#26469;&#35757;&#32451;&#19981;&#21516;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;pore&#20301;&#32622;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#32780;&#20026;Laser Powder Bed Fusion&#24037;&#33402;&#30340;&#36136;&#37327;&#30417;&#25511;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02507v1 Announce Type: new  Abstract: Reliably manufacturing defect free products is still an open challenge for Laser Powder Bed Fusion processes. Particularly, pores that occur frequently have a negative impact on mechanical properties like fatigue performance. Therefore, an accurate localisation of pores is mandatory for quality assurance, but requires time-consuming post-processing steps like computer tomography scans. Although existing solutions using in-situ monitoring data can detect pore occurrence within a layer, they are limited in their localisation precision. Therefore, we propose a pore localisation approach that estimates their position within a single layer using a Gaussian kernel density estimation. This allows segmentation models to learn the correlation between in-situ monitoring data and the derived probability distribution of pore occurrence. Within our experiments, we compare the prediction performance of different segmentation models depending on machin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#19981;&#23436;&#20840;&#28023;&#39532;&#21453;&#36716;&#65288;IHI&#65289;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#22235;&#20010;&#35299;&#21078;&#23398;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#23545;&#28023;&#39532;&#21453;&#36716;&#31243;&#24230;&#36827;&#34892;&#35780;&#20998;&#65292;&#20174;&#32780;&#20943;&#36731;&#25163;&#21160;&#35780;&#20272;&#30340;&#32321;&#29712;&#24615;&#65292;&#25552;&#39640;&#22823;&#35268;&#27169;&#30740;&#31350;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02496</link><description>&lt;p&gt;
Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#19981;&#23436;&#20840;&#28023;&#39532;&#21453;&#36716;&#65288;IHI&#65289;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#22235;&#20010;&#35299;&#21078;&#23398;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#23545;&#28023;&#39532;&#21453;&#36716;&#31243;&#24230;&#36827;&#34892;&#35780;&#20998;&#65292;&#20174;&#32780;&#20943;&#36731;&#25163;&#21160;&#35780;&#20272;&#30340;&#32321;&#29712;&#24615;&#65292;&#25552;&#39640;&#22823;&#35268;&#27169;&#30740;&#31350;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02496v1 Announce Type: cross  Abstract: Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal malrotation, is an atypical anatomical pattern of the hippocampus found in about 20% of the general population. IHI can be visually assessed on coronal slices of T1 weighted MR images, using a composite score that combines four anatomical criteria. IHI has been associated with several brain disorders (epilepsy, schizophrenia). However, these studies were based on small samples. Furthermore, the factors (genetic or environmental) that contribute to the genesis of IHI are largely unknown. Large-scale studies are thus needed to further understand IHI and their potential relationships to neurological and psychiatric disorders. However, visual evaluation is long and tedious, justifying the need for an automatic method. In this paper, we propose, for the first time, to automatically rate IHI. We proceed by predicting four anatomical criteria, which are then summed up to for
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyperSpaceX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810; hyperspherical &#31354;&#38388;&#20013;&#30340;&#35282;&#24230;&#21644;&#21322;&#24452;&#32500;&#24230;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20998;&#31867;&#20219;&#21153;&#20013;&#29305;&#24449;&#30340;&#21306;&#20998;&#24230;&#12290;&#36825;&#39033;&#21019;&#26032;&#37319;&#29992;&#20102;&#22522;&#20110;&#35282;&#24230;&#36317;&#31163;&#65288;DistArc&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#32467;&#21512;&#20102;&#19977;&#20010;&#29305;&#24449;&#24067;&#23616;&#25104;&#20998;&#65292;&#23454;&#29616;&#20102;&#31867;&#20869;&#32467;&#21512;&#21644;&#31867;&#38388;&#20998;&#31163;&#30340;&#25928;&#26524;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39044;&#27979;&#24230;&#37327;&#26469;&#35780;&#20272;&#29305;&#24449;&#30340;&#20195;&#34920;&#24615;&#65292;&#20174;&#32780;&#20026;&#29305;&#24449;&#34920;&#31034;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2408.02494</link><description>&lt;p&gt;
HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02494
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyperSpaceX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810; hyperspherical &#31354;&#38388;&#20013;&#30340;&#35282;&#24230;&#21644;&#21322;&#24452;&#32500;&#24230;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20998;&#31867;&#20219;&#21153;&#20013;&#29305;&#24449;&#30340;&#21306;&#20998;&#24230;&#12290;&#36825;&#39033;&#21019;&#26032;&#37319;&#29992;&#20102;&#22522;&#20110;&#35282;&#24230;&#36317;&#31163;&#65288;DistArc&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#32467;&#21512;&#20102;&#19977;&#20010;&#29305;&#24449;&#24067;&#23616;&#25104;&#20998;&#65292;&#23454;&#29616;&#20102;&#31867;&#20869;&#32467;&#21512;&#21644;&#31867;&#38388;&#20998;&#31163;&#30340;&#25928;&#26524;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39044;&#27979;&#24230;&#37327;&#26469;&#35780;&#20272;&#29305;&#24449;&#30340;&#20195;&#34920;&#24615;&#65292;&#20174;&#32780;&#20026;&#29305;&#24449;&#34920;&#31034;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02494v1 Announce Type: new  Abstract: Traditional deep learning models rely on methods such as softmax cross-entropy and ArcFace loss for tasks like classification and face recognition. These methods mainly explore angular features in a hyperspherical space, often resulting in entangled inter-class features due to dense angular data across many classes. In this paper, a new field of feature exploration is proposed known as HyperSpaceX which enhances class discrimination by exploring both angular and radial dimensions in multi-hyperspherical spaces, facilitated by a novel DistArc loss. The proposed DistArc loss encompasses three feature arrangement components: two angular and one radial, enforcing intra-class binding and inter-class separation in multi-radial arrangement, improving feature discriminability. Evaluation of HyperSpaceX framework for the novel representation utilizes a proposed predictive measure that accounts for both angular and radial elements, providing a mor
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMMP&#30340;&#38646; shot &#20154;&#31867;&#23545;&#35937;&#20132;&#20114;&#65288;HOI&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#22686;&#24378;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#22914;CLIP&#22312;HOI&#26816;&#27979;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#29420;&#31435;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25552;&#31034;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#65292;&#20854;&#20013;&#35270;&#35273;&#25552;&#31034;&#29992;&#20110;&#20132;&#20114;&#24615;&#24863;&#30693;&#19979;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#35821;&#35328;&#25552;&#31034;&#29992;&#20110;&#21487;&#27867;&#21270;&#30340;&#20114;&#21160;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#31456;&#36824;&#25552;&#20986;&#23398;&#20064;&#19981;&#21516;&#23618;&#27425;&#30340;&#21069;&#32622;&#30693;&#35782;&#65292;&#23558;&#20854;&#34701;&#20837;&#26465;&#20214;&#35270;&#35273;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#19981;&#21487;&#35265;&#20132;&#20114;&#31867;&#21035;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02484</link><description>&lt;p&gt;
Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMMP&#30340;&#38646; shot &#20154;&#31867;&#23545;&#35937;&#20132;&#20114;&#65288;HOI&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#22686;&#24378;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#22914;CLIP&#22312;HOI&#26816;&#27979;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#29420;&#31435;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25552;&#31034;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#65292;&#20854;&#20013;&#35270;&#35273;&#25552;&#31034;&#29992;&#20110;&#20132;&#20114;&#24615;&#24863;&#30693;&#19979;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#35821;&#35328;&#25552;&#31034;&#29992;&#20110;&#21487;&#27867;&#21270;&#30340;&#20114;&#21160;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#31456;&#36824;&#25552;&#20986;&#23398;&#20064;&#19981;&#21516;&#23618;&#27425;&#30340;&#21069;&#32622;&#30693;&#35782;&#65292;&#23558;&#20854;&#34701;&#20837;&#26465;&#20214;&#35270;&#35273;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#19981;&#21487;&#35265;&#20132;&#20114;&#31867;&#21035;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02484v1 Announce Type: new  Abstract: Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier topic due to its capability to detect HOIs beyond a predefined set of categories. This task entails not only identifying the interactiveness of human-object pairs and localizing them but also recognizing both seen and unseen interaction categories. In this paper, we introduce a novel framework for zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP. This approach enhances the generalization of large foundation models, such as CLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning methods, we propose learning decoupled vision and language prompts for interactiveness-aware visual feature extraction and generalizable interaction classification, respectively. Specifically, we integrate prior knowledge of different granularity into conditional vision prompts, including an input-conditioned instance prior and a global spatia
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#27010;&#36848;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#32531;&#35299;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#35813;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#32531;&#35299;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#26399;&#21019;&#24314;&#26356;&#21152;&#20844;&#27491;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#35273;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2408.02464</link><description>&lt;p&gt;
Fairness and Bias Mitigation in Computer Vision: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#27010;&#36848;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#32531;&#35299;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#35813;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#32531;&#35299;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#26399;&#21019;&#24314;&#26356;&#21152;&#20844;&#27491;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#35273;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02464v1 Announce Type: new  Abstract: Computer vision systems have witnessed rapid progress over the past two decades due to multiple advances in the field. As these systems are increasingly being deployed in high-stakes real-world applications, there is a dire need to ensure that they do not propagate or amplify any discriminatory tendencies in historical or human-curated data or inadvertently learn biases from spurious correlations. This paper presents a comprehensive survey on fairness that summarizes and sheds light on ongoing trends and successes in the context of computer vision. The topics we discuss include 1) The origin and technical definitions of fairness drawn from the wider fair machine learning literature and adjacent disciplines. 2) Work that sought to discover and analyze biases in computer vision systems. 3) A summary of methods proposed to mitigate bias in computer vision systems in recent years. 4) A comprehensive summary of resources and datasets produced
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25506;&#35752;&#20102;AI&#22312;&#33258;&#21160;&#21270;&#22788;&#29702;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;CMR&#65289;&#20998;&#21106;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#25581;&#31034;&#20102;&#31181;&#26063;&#20559;&#35265;&#20135;&#29983;&#30340;&#21407;&#22240;&#65292;&#26088;&#22312;&#23547;&#25214;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02462</link><description>&lt;p&gt;
An investigation into the causes of race bias in AI-based cine CMR segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25506;&#35752;&#20102;AI&#22312;&#33258;&#21160;&#21270;&#22788;&#29702;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;CMR&#65289;&#20998;&#21106;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#25581;&#31034;&#20102;&#31181;&#26063;&#20559;&#35265;&#20135;&#29983;&#30340;&#21407;&#22240;&#65292;&#26088;&#22312;&#23547;&#25214;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02462v1 Announce Type: cross  Abstract: Artificial intelligence (AI) methods are being used increasingly for the automated segmentation of cine cardiac magnetic resonance (CMR) imaging. However, these methods have been shown to be subject to race bias, i.e. they exhibit different levels of performance for different races depending on the (im)balance of the data used to train the AI model. In this paper we investigate the source of this bias, seeking to understand its root cause(s) so that it can be effectively mitigated. We perform a series of classification and segmentation experiments on short-axis cine CMR images acquired from Black and White subjects from the UK Biobank and apply AI interpretability methods to understand the results. In the classification experiments, we found that race can be predicted with high accuracy from the images alone, but less accurately from ground truth segmentations, suggesting that the distributional shift between races, which is often the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;X&#23556;&#32447;&#34928;&#20943;&#27169;&#22411;&#65292;&#22312;2D&#38136;&#20214;&#31881;&#26411;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23454;&#29616;&#20102;&#23545;&#23380;&#32570;&#38519;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;11.4%&#30340;F1&#20998;&#25968;&#65292;&#23637;&#29616;&#20102;&#22312;&#33258;&#21160;&#21270;&#21046;&#36896;&#36807;&#31243;&#20013;&#36827;&#34892;&#23454;&#26102;&#23380;&#32570;&#38519;&#26816;&#27979;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02427</link><description>&lt;p&gt;
Attenuation-adjusted deep learning of pore defects in 2D radiographs of additive manufacturing powders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;X&#23556;&#32447;&#34928;&#20943;&#27169;&#22411;&#65292;&#22312;2D&#38136;&#20214;&#31881;&#26411;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23454;&#29616;&#20102;&#23545;&#23380;&#32570;&#38519;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;11.4%&#30340;F1&#20998;&#25968;&#65292;&#23637;&#29616;&#20102;&#22312;&#33258;&#21160;&#21270;&#21046;&#36896;&#36807;&#31243;&#20013;&#36827;&#34892;&#23454;&#26102;&#23380;&#32570;&#38519;&#26816;&#27979;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02427v1 Announce Type: new  Abstract: The presence of gas pores in metal feedstock powder for additive manufacturing greatly affects the final AM product. Since current porosity analysis often involves lengthy X-ray computed tomography (XCT) scans with a full rotation around the sample, motivation exists to explore methods that allow for high throughput -- possibly enabling in-line porosity analysis during manufacturing. Through labelling pore pixels on single 2D radiographs of powders, this work seeks to simulate such future efficient setups. High segmentation accuracy is achieved by combining a model of X-ray attenuation through particles with a variant of the widely applied UNet architecture; notably, F1-score increases by $11.4\%$ compared to the baseline UNet. The proposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2) making tight particle cutouts, and 3) subtracting an ideal particle without pores generated from a distance map inspired by Lamber
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPT+&#30340;&#21442;&#25968;&#21644;&#20869;&#23384;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#28040;&#32791;&#65292;&#30456;&#27604;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2408.02426</link><description>&lt;p&gt;
FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPT+&#30340;&#21442;&#25968;&#21644;&#20869;&#23384;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#28040;&#32791;&#65292;&#30456;&#27604;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02426v1 Announce Type: new  Abstract: The success of large-scale pre-trained models has established fine-tuning as a standard method for achieving significant improvements in downstream tasks. However, fine-tuning the entire parameter set of a pre-trained model is costly. Parameter-efficient transfer learning (PETL) has recently emerged as a cost-effective alternative for adapting pre-trained models to downstream tasks. Despite its advantages, the increasing model size and input resolution present challenges for PETL, as the training memory consumption is not reduced as effectively as the parameter usage. In this paper, we introduce Fine-grained Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical image classification, which significantly reduces memory consumption compared to other PETL methods. FPT+ performs transfer learning by training a lightweight side network and accessing pre-trained knowledge from a large pre-trained model (LPM) through fine
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FE-Adapter&#30340;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28304;&#33258;&#22270;&#20687;&#30340;&#24773;&#24863;&#20998;&#31867;&#22120;&#39640;&#25928;&#22320;&#36866;&#37197;&#21040;&#35270;&#39057;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02421</link><description>&lt;p&gt;
FE-Adapter: Adapting Image-based Emotion Classifiers to Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FE-Adapter&#30340;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28304;&#33258;&#22270;&#20687;&#30340;&#24773;&#24863;&#20998;&#31867;&#22120;&#39640;&#25928;&#22320;&#36866;&#37197;&#21040;&#35270;&#39057;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02421v1 Announce Type: new  Abstract: Utilizing large pre-trained models for specific tasks has yielded impressive results. However, fully fine-tuning these increasingly large models is becoming prohibitively resource-intensive. This has led to a focus on more parameter-efficient transfer learning, primarily within the same modality. But this approach has limitations, particularly in video understanding where suitable pre-trained models are less common. Addressing this, our study introduces a novel cross-modality transfer learning approach from images to videos, which we call parameter-efficient image-to-video transfer learning. We present the Facial-Emotion Adapter (FE-Adapter), designed for efficient fine-tuning in video tasks. This adapter allows pre-trained image models, which traditionally lack temporal processing capabilities, to analyze dynamic video content efficiently. Notably, it uses about 15 times fewer parameters than previous methods, while improving accuracy. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCGF&#30340;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992; denoising diffusion &#27169;&#22411;&#32467;&#21512;&#20102;&#22270;&#20687;&#26080;&#25439;&#24674;&#22797;&#21644;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#33021;&#22815;&#26377;&#25928;&#36866;&#37197;&#21644;&#35782;&#21035;&#26410;&#30693;&#30340;&#26497;&#31471;&#22825;&#27668;&#29616;&#35937;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;&#30340;&#24674;&#22797;&#27169;&#22359;&#26469;&#28165;&#38500;&#24433;&#21709;&#23450;&#20301;&#30340;&#22825;&#27668;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992; EVA-02 &#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02408</link><description>&lt;p&gt;
Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCGF&#30340;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992; denoising diffusion &#27169;&#22411;&#32467;&#21512;&#20102;&#22270;&#20687;&#26080;&#25439;&#24674;&#22797;&#21644;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#33021;&#22815;&#26377;&#25928;&#36866;&#37197;&#21644;&#35782;&#21035;&#26410;&#30693;&#30340;&#26497;&#31471;&#22825;&#27668;&#29616;&#35937;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;&#30340;&#24674;&#22797;&#27169;&#22359;&#26469;&#28165;&#38500;&#24433;&#21709;&#23450;&#20301;&#30340;&#22825;&#27668;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992; EVA-02 &#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02408v1 Announce Type: new  Abstract: Cross-view geo-localization in GNSS-denied environments aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery. Recent research shows that learning discriminative image representations under specific weather conditions can significantly enhance performance. However, the frequent occurrence of unseen extreme weather conditions hinders progress. This paper introduces MCGF, a Multi-weather Cross-view Geo-localization Framework designed to dynamically adapt to unseen weather conditions. MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. For image restoration, MCGF incorporates a shared encoder and a lightweight restoration module to help the backbone eliminate weather-specific information. For geo-localization, MCGF uses EVA-02 as a backbone for feature extraction, with cross-entropy loss for
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#22330;&#34920;&#31034;&#30340;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36805;&#36895;&#22788;&#29702;&#26059;&#36716;&#65292;&#36866;&#29992;&#20110;&#21152;&#36895;3D&#22270;&#20687;&#65288;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#65289;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2408.02398</link><description>&lt;p&gt;
Tensorial template matching for fast cross-correlation with rotations and its application for tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#22330;&#34920;&#31034;&#30340;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36805;&#36895;&#22788;&#29702;&#26059;&#36716;&#65292;&#36866;&#29992;&#20110;&#21152;&#36895;3D&#22270;&#20687;&#65288;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#65289;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02398v1 Announce Type: new  Abstract: Object detection is a main task in computer vision. Template matching is the reference method for detecting objects with arbitrary templates. However, template matching computational complexity depends on the rotation accuracy, being a limiting factor for large 3D images (tomograms). Here, we implement a new algorithm called tensorial template matching, based on a mathematical framework that represents all rotations of a template with a tensor field. Contrary to standard template matching, the computational complexity of the presented algorithm is independent of the rotation accuracy. Using both, synthetic and real data from tomography, we demonstrate that tensorial template matching is much faster than template matching and has the potential to improve its accuracy
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#27880;&#20876;&#20195;&#29702;&#65288;CMR-Agent&#65289;&#65292;&#35813;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36880;&#27493;&#35843;&#25972;&#30456;&#26426;&#23039;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02394</link><description>&lt;p&gt;
CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#27880;&#20876;&#20195;&#29702;&#65288;CMR-Agent&#65289;&#65292;&#35813;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36880;&#27493;&#35843;&#25972;&#30456;&#26426;&#23039;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02394v1 Announce Type: cross  Abstract: Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMR-Agent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaFreeI2P&#30340;&#26080;&#21305;&#37197;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20027;&#21160;&#26816;&#32034;&#30456;&#26426;&#23039;&#24577;&#26469;&#35299;&#20915;&#20004;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#27169;&#24335;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#27604;&#28857;&#20113;&#21644;&#26597;&#35810;&#22270;&#20687;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;SE(3)&#31354;&#38388;&#20013;&#31934;&#30830;&#22320;&#30830;&#23450;&#30456;&#26426;&#20301;&#32622;&#12290;&#36890;&#36807;&#37319;&#26679;&#19968;&#31995;&#21015;&#20505;&#36873;&#30456;&#26426;&#23039;&#24577;&#24182;&#26500;&#24314;&#25104;&#26412;&#20307;&#31215;&#65292;&#25104;&#26412;&#20307;&#31215;&#21033;&#29992;&#36328;&#27169;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#24182;&#19988;&#36890;&#36807;&#30456;&#20284;&#24615;&#35780;&#20272;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#30456;&#26426;&#23039;&#24577;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02392</link><description>&lt;p&gt;
MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm with Active Camera Pose Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaFreeI2P&#30340;&#26080;&#21305;&#37197;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20027;&#21160;&#26816;&#32034;&#30456;&#26426;&#23039;&#24577;&#26469;&#35299;&#20915;&#20004;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#27169;&#24335;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#27604;&#28857;&#20113;&#21644;&#26597;&#35810;&#22270;&#20687;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;SE(3)&#31354;&#38388;&#20013;&#31934;&#30830;&#22320;&#30830;&#23450;&#30456;&#26426;&#20301;&#32622;&#12290;&#36890;&#36807;&#37319;&#26679;&#19968;&#31995;&#21015;&#20505;&#36873;&#30456;&#26426;&#23039;&#24577;&#24182;&#26500;&#24314;&#25104;&#26412;&#20307;&#31215;&#65292;&#25104;&#26412;&#20307;&#31215;&#21033;&#29992;&#36328;&#27169;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#24182;&#19988;&#36890;&#36807;&#30456;&#20284;&#24615;&#35780;&#20272;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#30456;&#26426;&#23039;&#24577;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02392v1 Announce Type: new  Abstract: Image-to-point cloud registration seeks to estimate their relative camera pose, which remains an open question due to the data modality gaps. The recent matching-based methods tend to tackle this by building 2D-3D correspondences. In this paper, we reveal the information loss inherent in these methods and propose a matching-free paradigm, named MaFreeI2P. Our key insight is to actively retrieve the camera pose in SE(3) space by contrasting the geometric features between the point cloud and the query image. To achieve this, we first sample a set of candidate camera poses and construct their cost volume using the cross-modal features. Superior to matching, cost volume can preserve more information and its feature similarity implicitly reflects the confidence level of the sampled poses. Afterwards, we employ a convolutional network to adaptively formulate a similarity assessment function, where the input cost volume is further improved by f
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20132;&#21449;&#20266;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#21355;&#26143;&#22270;&#20687;&#19978;&#26631;&#35760;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#19978;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22303;&#22320;&#20351;&#29992;&#21644;&#35206;&#30422;&#22330;&#26223;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25991;&#31456;&#23581;&#35797;&#35299;&#20915;&#29616;&#26377;&#20132;&#21449;&#20266;&#30417;&#30563;&#25216;&#24039;&#22312;&#35299;&#20915;&#22478;&#24066;&#21644;&#36164;&#28304;&#35268;&#21010;&#20013;&#20851;&#38190;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;&#22312;&#21360;&#24230;&#19981;&#21516;&#22320;&#21306;&#20351;&#29992;&#30340;&#22810;&#26679;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#30340;LULC&#65288;Land Use and Land Cover&#65289;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2408.02382</link><description>&lt;p&gt;
Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02382
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20132;&#21449;&#20266;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#21355;&#26143;&#22270;&#20687;&#19978;&#26631;&#35760;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#19978;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22303;&#22320;&#20351;&#29992;&#21644;&#35206;&#30422;&#22330;&#26223;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25991;&#31456;&#23581;&#35797;&#35299;&#20915;&#29616;&#26377;&#20132;&#21449;&#20266;&#30417;&#30563;&#25216;&#24039;&#22312;&#35299;&#20915;&#22478;&#24066;&#21644;&#36164;&#28304;&#35268;&#21010;&#20013;&#20851;&#38190;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;&#22312;&#21360;&#24230;&#19981;&#21516;&#22320;&#21306;&#20351;&#29992;&#30340;&#22810;&#26679;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#30340;LULC&#65288;Land Use and Land Cover&#65289;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02382v1 Announce Type: new  Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource planning and is one of the key elements in developing smart and sustainable cities. This study introduces a semi-supervised segmentation model for LULC prediction using high-resolution satellite images with a huge diversity in data distributions in different areas from the country of India. Our approach ensures a robust generalization across different types of buildings, roads, trees, and water bodies within these distinct areas. We propose a modified Cross Pseudo Supervision framework to train image segmentation models on sparsely labelled data. The proposed framework addresses the limitations of the popular "Cross Pseudo Supervision" technique for semi-supervised learning. Specifically, it tackles the challenges of training segmentation models on noisy satellite image data with sparse and inaccurate labels. This comprehensive approach enhances the accuracy and utili
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#20197;NPU-ASLP&#21629;&#21517;&#30340;&#29992;&#20110;CNVSRC 2024&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#30340;&#25152;&#26377;&#22235;&#20010;&#36712;&#36947;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#21253;&#25324;&#21333;&#19968;&#35828;&#35805;&#32773;VSR&#20219;&#21153;&#21644;&#22810;&#35828;&#35805;&#32773;VSR&#20219;&#21153;&#30340;&#22266;&#23450;&#21644;&#24320;&#25918;&#36712;&#36947;&#12290;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#30784;&#32447;&#19978;&#30340;&#21767;&#21160;&#25552;&#21462;&#22120;&#20135;&#29983;&#20102;&#22810;&#23610;&#24230;&#35270;&#39057;&#25968;&#25454;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#21508;&#31181;&#22686;&#24378;&#25216;&#26415;&#34987;&#24212;&#29992;&#65292;&#21253;&#25324;&#36895;&#24230;&#20559;&#31227;&#12289;&#38543;&#26426;&#26059;&#36716;&#12289;&#27700;&#24179;&#32763;&#36716;&#21644;&#39068;&#33394;&#21464;&#25442;&#12290;VSR&#27169;&#22411;&#37319;&#29992;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#24102;&#26377;&#32852;&#21512;CTC/&#27880;&#24847;&#21147;&#25439;&#22833;&#65292;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;ResNet3D&#35270;&#35273;&#21069;&#31471;&#12289;E-Branchformer&#32534;&#30721;&#22120;&#21644;&#21452;&#26041;&#21521;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#31995;&#32479;&#22312;&#21333;&#19968;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#24320;&#25918;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;30.47%&#30340;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#22312;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#24320;&#25918;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;34.30%&#30340;CER&#65292;&#22312;&#21333;&#19968;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#22266;&#23450;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;32.51%&#30340;CER&#65292;&#22312;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#22266;&#23450;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;35.28%&#30340;CER&#65292;&#20174;&#32780;&#22312;&#35813;&#25361;&#25112;&#20013;&#36194;&#24471;&#20102;&#31532;&#20108;&#21517;&#65292;&#35777;&#26126;&#20102;&#20854;&#21019;&#26032;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02369</link><description>&lt;p&gt;
The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02369
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#20197;NPU-ASLP&#21629;&#21517;&#30340;&#29992;&#20110;CNVSRC 2024&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#30340;&#25152;&#26377;&#22235;&#20010;&#36712;&#36947;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#21253;&#25324;&#21333;&#19968;&#35828;&#35805;&#32773;VSR&#20219;&#21153;&#21644;&#22810;&#35828;&#35805;&#32773;VSR&#20219;&#21153;&#30340;&#22266;&#23450;&#21644;&#24320;&#25918;&#36712;&#36947;&#12290;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#30784;&#32447;&#19978;&#30340;&#21767;&#21160;&#25552;&#21462;&#22120;&#20135;&#29983;&#20102;&#22810;&#23610;&#24230;&#35270;&#39057;&#25968;&#25454;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#21508;&#31181;&#22686;&#24378;&#25216;&#26415;&#34987;&#24212;&#29992;&#65292;&#21253;&#25324;&#36895;&#24230;&#20559;&#31227;&#12289;&#38543;&#26426;&#26059;&#36716;&#12289;&#27700;&#24179;&#32763;&#36716;&#21644;&#39068;&#33394;&#21464;&#25442;&#12290;VSR&#27169;&#22411;&#37319;&#29992;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#24102;&#26377;&#32852;&#21512;CTC/&#27880;&#24847;&#21147;&#25439;&#22833;&#65292;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;ResNet3D&#35270;&#35273;&#21069;&#31471;&#12289;E-Branchformer&#32534;&#30721;&#22120;&#21644;&#21452;&#26041;&#21521;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#31995;&#32479;&#22312;&#21333;&#19968;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#24320;&#25918;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;30.47%&#30340;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#22312;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#24320;&#25918;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;34.30%&#30340;CER&#65292;&#22312;&#21333;&#19968;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#22266;&#23450;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;32.51%&#30340;CER&#65292;&#22312;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;&#22266;&#23450;&#36712;&#36947;&#19978;&#21462;&#24471;&#20102;35.28%&#30340;CER&#65292;&#20174;&#32780;&#22312;&#35813;&#25361;&#25112;&#20013;&#36194;&#24471;&#20102;&#31532;&#20108;&#21517;&#65292;&#35777;&#26126;&#20102;&#20854;&#21019;&#26032;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#21517;&#20026;StoDIP&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#26080;ground-truth&#25968;&#25454;&#30340;DIP&#25216;&#26415;&#25193;&#23637;&#33267;3D MRF&#25104;&#20687;&#65292;&#35299;&#20915;&#20102;3D&#25104;&#20687;&#20013;&#30340;&#25361;&#25112;&#24182;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02367</link><description>&lt;p&gt;
StoDIP: Efficient 3D MRF image reconstruction with deep image priors and stochastic iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#21517;&#20026;StoDIP&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#26080;ground-truth&#25968;&#25454;&#30340;DIP&#25216;&#26415;&#25193;&#23637;&#33267;3D MRF&#25104;&#20687;&#65292;&#35299;&#20915;&#20102;3D&#25104;&#20687;&#20013;&#30340;&#25361;&#25112;&#24182;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02367v1 Announce Type: cross  Abstract: Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to quantitative MRI for multiparametric tissue mapping. The reconstruction of quantitative maps requires tailored algorithms for removing aliasing artefacts from the compressed sampled MRF acquisitions. Within approaches found in the literature, many focus solely on two-dimensional (2D) image reconstruction, neglecting the extension to volumetric (3D) scans despite their higher relevance and clinical value. A reason for this is that transitioning to 3D imaging without appropriate mitigations presents significant challenges, including increased computational cost and storage requirements, and the need for large amount of ground-truth (artefact-free) data for training. To address these issues, we introduce StoDIP, a new algorithm that extends the ground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging. StoDIP employs memory-efficient stochastic updates
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#38416;&#36848;&#20102;&#22320;&#29699;&#31995;&#32479;&#25968;&#25454;&#31435;&#26041;&#20307;&#65288;ESDCs&#65289;&#22312;&#25512;&#36827;&#22320;&#29699;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#32452;&#32455;&#25104;&#26131;&#29992;&#19988;&#23545;&#29992;&#25143;&#21451;&#22909;&#19988;&#38656;&#35201;&#36739;&#23569;&#25216;&#26415;&#25968;&#25454;&#22788;&#29702;&#30693;&#35782;&#30340;&#20998;&#26512;&#26684;&#24335;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#25968;&#25454;&#22788;&#29702;&#30340;&#36127;&#25285;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#25552;&#21040;&#20102;&#22312;ESDC&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#25216;&#26415;&#25361;&#25112;&#21644;&#29305;&#23450;&#39046;&#22495;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#20123;&#25361;&#25112;&#23545;&#23454;&#29616;&#25968;&#25454;&#38598;&#28508;&#21147;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2408.02348</link><description>&lt;p&gt;
Earth System Data Cubes: Avenues for advancing Earth system research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#38416;&#36848;&#20102;&#22320;&#29699;&#31995;&#32479;&#25968;&#25454;&#31435;&#26041;&#20307;&#65288;ESDCs&#65289;&#22312;&#25512;&#36827;&#22320;&#29699;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#32452;&#32455;&#25104;&#26131;&#29992;&#19988;&#23545;&#29992;&#25143;&#21451;&#22909;&#19988;&#38656;&#35201;&#36739;&#23569;&#25216;&#26415;&#25968;&#25454;&#22788;&#29702;&#30693;&#35782;&#30340;&#20998;&#26512;&#26684;&#24335;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#25968;&#25454;&#22788;&#29702;&#30340;&#36127;&#25285;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#25552;&#21040;&#20102;&#22312;ESDC&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#25216;&#26415;&#25361;&#25112;&#21644;&#29305;&#23450;&#39046;&#22495;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#20123;&#25361;&#25112;&#23545;&#23454;&#29616;&#25968;&#25454;&#38598;&#28508;&#21147;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02348v1 Announce Type: new  Abstract: Recent advancements in Earth system science have been marked by the exponential increase in the availability of diverse, multivariate datasets characterised by moderate to high spatio-temporal resolutions. Earth System Data Cubes (ESDCs) have emerged as one suitable solution for transforming this flood of data into a simple yet robust data structure. ESDCs achieve this by organising data into an analysis-ready format aligned with a spatio-temporal grid, facilitating user-friendly analysis and diminishing the need for extensive technical data processing knowledge. Despite these significant benefits, the completion of the entire ESDC life cycle remains a challenging task. Obstacles are not only of a technical nature but also relate to domain-specific problems in Earth system research. There exist barriers to realising the full potential of data collections in light of novel cloud-based technologies, particularly in curating data tailored f
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EI-VLG&#30340;&#38271;&#26399;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#65288;VLG&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26469;&#27169;&#25311;&#20154;&#31867;&#32463;&#39564;&#65292;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#35270;&#39057;&#29255;&#27573;&#65292;&#22686;&#24378;&#20102;&#35270;&#39057;-&#35821;&#35328;&#20849;&#21516;&#34920;&#24449;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#38382;&#39064;&#20013;&#24573;&#30053;&#38750;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02336</link><description>&lt;p&gt;
Infusing Environmental Captions for Long-Form Video Language Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EI-VLG&#30340;&#38271;&#26399;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#65288;VLG&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26469;&#27169;&#25311;&#20154;&#31867;&#32463;&#39564;&#65292;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#35270;&#39057;&#29255;&#27573;&#65292;&#22686;&#24378;&#20102;&#35270;&#39057;-&#35821;&#35328;&#20849;&#21516;&#34920;&#24449;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#35821;&#35328;&#22320;&#38754;&#38382;&#39064;&#20013;&#24573;&#30053;&#38750;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#25104;&#26412;&#36229;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CNN&#36716;&#21270;&#20026;&#22810;&#20998;&#25903;&#32467;&#26500;&#24182;&#37319;&#29992;&#19981;&#21516;&#25968;&#30446;&#32452;&#30340;&#20998;&#32452;&#21367;&#31215;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#20998;&#25903;&#38388;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#65292;&#21363;&#20351;&#22312;&#32500;&#25345;&#21407;&#22987;&#21333;&#19968;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#65292;&#20063;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02307</link><description>&lt;p&gt;
Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped Convolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02307
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#25104;&#26412;&#36229;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CNN&#36716;&#21270;&#20026;&#22810;&#20998;&#25903;&#32467;&#26500;&#24182;&#37319;&#29992;&#19981;&#21516;&#25968;&#30446;&#32452;&#30340;&#20998;&#32452;&#21367;&#31215;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#20998;&#25903;&#38388;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#65292;&#21363;&#20351;&#22312;&#32500;&#25345;&#21407;&#22987;&#21333;&#19968;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#65292;&#20063;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02307v1 Announce Type: new  Abstract: Recent advancements in low-cost ensemble learning have demonstrated improved efficiency for image classification. However, the existing low-cost ensemble methods show relatively lower accuracy compared to conventional ensemble learning. In this paper, we propose a new low-cost ensemble learning, which can simultaneously achieve high efficiency and classification performance. A CNN is transformed into a multi-branch structure without introduction of additional components, which maintains the computational complexity as that of the original single model and also enhances diversity among the branches' outputs via sufficient separation between different pathways of the branches. In addition, we propose a new strategy that applies grouped convolution in the branches with different numbers of groups in different branches, which boosts the diversity of the branches' outputs. For training, we employ knowledge distillation using the ensemble of t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MoNFAP&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;FUP&#27169;&#22359;&#21644;MNM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#33080;&#25805;&#32437;&#22270;&#20687;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;FUP&#27169;&#22359;&#23558;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#34701;&#21512;&#65292;&#32780;MNM&#27169;&#22359;&#21017;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#30452;&#25509;&#23545;&#25805;&#32437;&#22270;&#20687;&#36827;&#34892;&#23398;&#20064;&#65292;MoNFAP&#22312;&#26816;&#27979;&#21644;&#23450;&#20301;&#24615;&#33021;&#19978;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02306</link><description>&lt;p&gt;
Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face Manipulation Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MoNFAP&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;FUP&#27169;&#22359;&#21644;MNM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#33080;&#25805;&#32437;&#22270;&#20687;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;FUP&#27169;&#22359;&#23558;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#34701;&#21512;&#65292;&#32780;MNM&#27169;&#22359;&#21017;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#30452;&#25509;&#23545;&#25805;&#32437;&#22270;&#20687;&#36827;&#34892;&#23398;&#20064;&#65292;MoNFAP&#22312;&#26816;&#27979;&#21644;&#23450;&#20301;&#24615;&#33021;&#19978;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02306v1 Announce Type: new  Abstract: With the advancement of face manipulation technology, forgery images in multi-face scenarios are gradually becoming a more complex and realistic challenge. Despite this, detection and localization methods for such multi-face manipulations remain underdeveloped. Traditional manipulation localization methods either indirectly derive detection results from localization masks, resulting in limited detection performance, or employ a naive two-branch structure to simultaneously obtain detection and localization results, which cannot effectively benefit the localization capability due to limited interaction between two tasks. This paper proposes a new framework, namely MoNFAP, specifically tailored for multi-face manipulation detection and localization. The MoNFAP primarily introduces two novel modules: the Forgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module (MNM). The FUP integrates detection and localization tasks us
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Network Fission Ensembles&#65288;NFE&#65289;&#30340;&#20302;&#25104;&#26412;&#33258;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#25913;&#36896;&#65292;&#20351;&#20854;&#21253;&#21547;&#22810;&#20010;&#36755;&#20986;&#36335;&#24452;&#65292;&#20174;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#21363;&#21487;&#23454;&#29616;&#38598;&#25104;&#23398;&#20064;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#38598;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.02301</link><description>&lt;p&gt;
Network Fission Ensembles for Low-Cost Self-Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Network Fission Ensembles&#65288;NFE&#65289;&#30340;&#20302;&#25104;&#26412;&#33258;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#25913;&#36896;&#65292;&#20351;&#20854;&#21253;&#21547;&#22810;&#20010;&#36755;&#20986;&#36335;&#24452;&#65292;&#20174;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#21363;&#21487;&#23454;&#29616;&#38598;&#25104;&#23398;&#20064;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#38598;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02301v1 Announce Type: new  Abstract: Recent ensemble learning methods for image classification have been shown to improve classification accuracy with low extra cost. However, they still require multiple trained models for ensemble inference, which eventually becomes a significant burden when the model size increases. In this paper, we propose a low-cost ensemble learning and inference, called Network Fission Ensembles (NFE), by converting a conventional network itself into a multi-exit structure. Starting from a given initial network, we first prune some of the weights to reduce the training burden. We then group the remaining weights into several sets and create multiple auxiliary paths using each set to construct multi-exits. We call this process Network Fission. Through this, multiple outputs can be obtained from a single network, which enables ensemble learning. Since this process simply changes the existing network structure to multi-exits without using additional net
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20855;&#26377;&#32930;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#30830;&#20445;&#20854;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#29289;&#20307;&#25628;&#32034;&#31561;&#20219;&#21153;&#26102;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2408.02297</link><description>&lt;p&gt;
Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20855;&#26377;&#32930;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#30830;&#20445;&#20854;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#29289;&#20307;&#25628;&#32034;&#31561;&#20219;&#21153;&#26102;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02297v1 Announce Type: new  Abstract: Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through calibrated perception probabilities and uncertainty across aggregation and found decisions, thereby adapting the models for sequential tasks. The resulting methods can be directly integrated with pretrained models across a wide family of existing search approaches at no additional training cost. We perform extensive evaluations of aggregation methods across both different semantic perception models and policies, confirming the importance of calib
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;SelfGeo&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#20154;&#31867;&#27880;&#37322;&#21363;&#21487;&#20174;&#20219;&#24847;&#28857;&#20113;&#25968;&#25454;&#20013;&#20272;&#35745;&#38750;&#21018;&#24615;&#29289;&#20307;3D keypoints&#65292;&#24182;&#30830;&#20445;&#36825;&#20123;&#20851;&#38190;&#28857;&#33021;&#38543;&#30528;&#29289;&#20307;&#30340;&#24418;&#24577;&#21464;&#21270;&#20445;&#25345;&#35821;&#20041;&#21644;&#20960;&#20309;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23454;&#29616;&#20851;&#38190;&#28857;&#19982;&#20854;&#20182;&#28857;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#20960;&#20309;&#36317;&#31163;&#65292;&#36825;&#23545;&#20110;&#38750;&#21018;&#24615;&#29289;&#20307;&#20013;&#20851;&#38190;&#28857;&#30340;&#31283;&#23450;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2408.02291</link><description>&lt;p&gt;
SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints on Deformable Shapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;SelfGeo&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#20154;&#31867;&#27880;&#37322;&#21363;&#21487;&#20174;&#20219;&#24847;&#28857;&#20113;&#25968;&#25454;&#20013;&#20272;&#35745;&#38750;&#21018;&#24615;&#29289;&#20307;3D keypoints&#65292;&#24182;&#30830;&#20445;&#36825;&#20123;&#20851;&#38190;&#28857;&#33021;&#38543;&#30528;&#29289;&#20307;&#30340;&#24418;&#24577;&#21464;&#21270;&#20445;&#25345;&#35821;&#20041;&#21644;&#20960;&#20309;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23454;&#29616;&#20851;&#38190;&#28857;&#19982;&#20854;&#20182;&#28857;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#20960;&#20309;&#36317;&#31163;&#65292;&#36825;&#23545;&#20110;&#38750;&#21018;&#24615;&#29289;&#20307;&#20013;&#20851;&#38190;&#28857;&#30340;&#31283;&#23450;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02291v1 Announce Type: new  Abstract: Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex task, even more challenging when an object shape is deforming. As keypoints should be semantically and geometrically consistent across all the 3D frames - each keypoint should be anchored to a specific part of the deforming shape irrespective of intrinsic and extrinsic motion. This paper presents, "SelfGeo", a self-supervised method that computes persistent 3D keypoints of non-rigid objects from arbitrary PCDs without the need of human annotations. The gist of SelfGeo is to estimate keypoints between frames that respect invariant properties of deforming bodies. Our main contribution is to enforce that keypoints deform along with the shape while keeping constant geodesic distances among them. This principle is then propagated to the design of a set of losses which minimization let emerge repeatable keypoints in specific semantic locations of the non-rigid shape.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#34701;&#21512;&#21021;&#22987;&#28909;&#22270;&#20013;&#30340;&#23616;&#37096;&#20851;&#33410;&#20449;&#24687;&#21644;&#35270;&#39057;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#36816;&#21160;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23039;&#24577;&#20272;&#35745;&#24615;&#33021;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02285</link><description>&lt;p&gt;
Joint-Motion Mutual Learning for Pose Estimation in Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#34701;&#21512;&#21021;&#22987;&#28909;&#22270;&#20013;&#30340;&#23616;&#37096;&#20851;&#33410;&#20449;&#24687;&#21644;&#35270;&#39057;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#36816;&#21160;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23039;&#24577;&#20272;&#35745;&#24615;&#33021;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02285v1 Announce Type: new  Abstract: Human pose estimation in videos has long been a compelling yet challenging task within the realm of computer vision. Nevertheless, this task remains difficult because of the complex video scenes, such as video defocus and self-occlusion. Recent methods strive to integrate multi-frame visual features generated by a backbone network for pose estimation. However, they often ignore the useful joint information encoded in the initial heatmap, which is a by-product of the backbone generation. Comparatively, methods that attempt to refine the initial heatmap fail to consider any spatio-temporal motion features. As a result, the performance of existing methods for pose estimation falls short due to the lack of ability to leverage both local joint (heatmap) information and global motion (feature) dynamics.   To address this problem, we propose a novel joint-motion mutual learning framework for pose estimation, which effectively concentrates on bo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#31934;&#28860;&#35270;&#39057;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#26469;&#21516;&#26102;&#20248;&#21270;&#22270;&#20687;&#23545;&#40784;&#21644;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;CRVD&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#22270;&#26469;&#36991;&#20813;&#22312;&#22122;&#22768;&#36739;&#36731;&#30340;&#35270;&#39057;&#24103;&#19978;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24179;&#22343;&#20943;&#23569;&#20102;25%&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.02284</link><description>&lt;p&gt;
Cascading Refinement Video Denoising with Uncertainty Adaptivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#31934;&#28860;&#35270;&#39057;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#26469;&#21516;&#26102;&#20248;&#21270;&#22270;&#20687;&#23545;&#40784;&#21644;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;CRVD&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#22270;&#26469;&#36991;&#20813;&#22312;&#22122;&#22768;&#36739;&#36731;&#30340;&#35270;&#39057;&#24103;&#19978;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24179;&#22343;&#20943;&#23569;&#20102;25%&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02284v1 Announce Type: new  Abstract: Accurate alignment is crucial for video denoising. However, estimating alignment in noisy environments is challenging. This paper introduces a cascading refinement video denoising method that can refine alignment and restore images simultaneously. Better alignment enables restoration of more detailed information in each frame. Furthermore, better image quality leads to better alignment. This method has achieved SOTA performance by a large margin on the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an uncertainty map was created after each iteration. Because of this, redundant computation on the easily restored videos was avoided. By applying this method, the entire computation was reduced by 25% on average.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20960;&#20309;&#20195;&#25968;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;3D&#22330;&#26223;&#30340;&#21487;&#25511;&#32534;&#36753;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#20219;&#21153;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20309;&#20195;&#25968;&#20316;&#20026;&#27491;&#24335;&#30340;&#35821;&#35328;&#26469;&#31934;&#30830;&#24314;&#27169;&#31354;&#38388;&#21464;&#25442;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;Shot&#23398;&#20064;&#33021;&#21147;&#26469;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;CGA&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#25805;&#20316;&#32773;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;3D&#22330;&#26223;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#65292;&#26080;&#38656;&#20381;&#36182;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#19987;&#38376;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#39033;&#25216;&#26415;&#20026;3D&#22330;&#26223;&#30340;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#20307;&#39564;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.02275</link><description>&lt;p&gt;
Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20960;&#20309;&#20195;&#25968;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;3D&#22330;&#26223;&#30340;&#21487;&#25511;&#32534;&#36753;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#20219;&#21153;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20309;&#20195;&#25968;&#20316;&#20026;&#27491;&#24335;&#30340;&#35821;&#35328;&#26469;&#31934;&#30830;&#24314;&#27169;&#31354;&#38388;&#21464;&#25442;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;Shot&#23398;&#20064;&#33021;&#21147;&#26469;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;CGA&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#25805;&#20316;&#32773;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;3D&#22330;&#26223;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#65292;&#26080;&#38656;&#20381;&#36182;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#19987;&#38376;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#39033;&#25216;&#26415;&#20026;3D&#22330;&#26223;&#30340;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#20307;&#39564;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02275v1 Announce Type: new  Abstract: This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, shenlong ensures compatibility with existing
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COM Kitchens&#30340;&#26410;&#32463;&#32534;&#36753;&#30340;&#20463;&#30640;&#35270;&#35282;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#22522;&#20934;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#19981;&#21152;&#20462;&#39280;&#30340;&#20463;&#30640;&#35270;&#35282;&#35270;&#39057;&#65292;&#20854;&#20013;&#21442;&#19982;&#32773;&#26681;&#25454;&#32473;&#23450;&#30340;&#39135;&#35889;&#36827;&#34892;&#39135;&#29289;&#20934;&#22791;&#12290;&#36890;&#36807;&#20351;&#29992;&#29616;&#20195;&#23485;&#35282;&#26234;&#33021;&#25163;&#26426;&#38236;&#22836;&#35206;&#30422;&#20174;&#27700;&#27133;&#21040;&#28809;&#28790;&#30340;&#28921;&#39274;&#21488;&#38754;&#65292;&#23454;&#29616;&#20102;&#27963;&#21160;&#30340;&#39640;&#25928;&#25429;&#25417;&#65292;&#26080;&#38656;&#30452;&#25509;&#21327;&#21161;&#12290;&#36890;&#36807;&#20998;&#21457;&#26234;&#33021;&#25163;&#26426;&#32473;&#21442;&#19982;&#32773;&#65292;&#25105;&#20204;&#25628;&#38598;&#21040;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#39057;&#21040;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;Online Recipe Understanding&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#29702;&#35299;&#39135;&#35889;&#20013;&#30340;&#25351;&#20196;&#12290;</title><link>https://arxiv.org/abs/2408.02272</link><description>&lt;p&gt;
COM Kitchens: An Unedited Overhead-view Video Dataset as a Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COM Kitchens&#30340;&#26410;&#32463;&#32534;&#36753;&#30340;&#20463;&#30640;&#35270;&#35282;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#22522;&#20934;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#19981;&#21152;&#20462;&#39280;&#30340;&#20463;&#30640;&#35270;&#35282;&#35270;&#39057;&#65292;&#20854;&#20013;&#21442;&#19982;&#32773;&#26681;&#25454;&#32473;&#23450;&#30340;&#39135;&#35889;&#36827;&#34892;&#39135;&#29289;&#20934;&#22791;&#12290;&#36890;&#36807;&#20351;&#29992;&#29616;&#20195;&#23485;&#35282;&#26234;&#33021;&#25163;&#26426;&#38236;&#22836;&#35206;&#30422;&#20174;&#27700;&#27133;&#21040;&#28809;&#28790;&#30340;&#28921;&#39274;&#21488;&#38754;&#65292;&#23454;&#29616;&#20102;&#27963;&#21160;&#30340;&#39640;&#25928;&#25429;&#25417;&#65292;&#26080;&#38656;&#30452;&#25509;&#21327;&#21161;&#12290;&#36890;&#36807;&#20998;&#21457;&#26234;&#33021;&#25163;&#26426;&#32473;&#21442;&#19982;&#32773;&#65292;&#25105;&#20204;&#25628;&#38598;&#21040;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#39057;&#21040;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;Online Recipe Understanding&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#29702;&#35299;&#39135;&#35889;&#20013;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02272v1 Announce Type: new  Abstract: Procedural video understanding is gaining attention in the vision and language community. Deep learning-based video analysis requires extensive data. Consequently, existing works often use web videos as training resources, making it challenging to query instructional contents from raw video observations. To address this issue, we propose a new dataset, COM Kitchens. The dataset consists of unedited overhead-view videos captured by smartphones, in which participants performed food preparation based on given recipes. Fixed-viewpoint video datasets often lack environmental diversity due to high camera setup costs. We used modern wide-angle smartphone lenses to cover cooking counters from sink to cooktop in an overhead view, capturing activity without in-person assistance. With this setup, we collected a diverse dataset by distributing smartphones to participants. With this dataset, we propose the novel video-to-text retrieval task Online Re
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"OpenCBM"&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#30340;&#21464;&#20307;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;CLIP&#20013;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#21407;&#22411;&#22522;&#30784;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#27010;&#24565;&#12290;&#36825;&#19968;&#26041;&#27861;&#20801;&#35768;&#22810;&#31181;&#24320;&#25918;&#27010;&#24565;&#22312;&#20915;&#23450;&#36807;&#31243;&#20013;&#36215;&#20316;&#29992;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#24565;&#38598;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02265</link><description>&lt;p&gt;
Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"OpenCBM"&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#30340;&#21464;&#20307;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;CLIP&#20013;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#21407;&#22411;&#22522;&#30784;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#27010;&#24565;&#12290;&#36825;&#19968;&#26041;&#27861;&#20801;&#35768;&#22810;&#31181;&#24320;&#25918;&#27010;&#24565;&#22312;&#20915;&#23450;&#36807;&#31243;&#20013;&#36215;&#20316;&#29992;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#24565;&#38598;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02265v1 Announce Type: new  Abstract: The concept bottleneck model (CBM) is an interpretable-by-design framework that makes decisions by first predicting a set of interpretable concepts, and then predicting the class label based on the given concepts. Existing CBMs are trained with a fixed set of concepts (concepts are either annotated by the dataset or queried from language models). However, this closed-world assumption is unrealistic in practice, as users may wonder about the role of any desired concept in decision-making after the model is deployed. Inspired by the large success of recent vision-language pre-trained models such as CLIP in zero-shot classification, we propose "OpenCBM" to equip the CBM with open vocabulary concepts via: (1) Aligning the feature space of a trainable image feature extractor with that of a CLIP's image encoder via a prototype based feature alignment; (2) Simultaneously training an image classifier on the downstream dataset; (3) Reconstructing
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VoxelTrack&#30340;3D&#28857;&#20113;&#23545;&#35937;&#36861;&#36394;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#21270;&#20026;3D&#20307;&#32032;&#24182;&#20351;&#29992;&#31232;&#30095;&#21367;&#31215;&#22359;&#25552;&#21462;&#20854;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#31934;&#30830;&#32780;&#40065;&#26834;&#30340;3D&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#23545;&#35937;&#30340;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2408.02263</link><description>&lt;p&gt;
VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VoxelTrack&#30340;3D&#28857;&#20113;&#23545;&#35937;&#36861;&#36394;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#21270;&#20026;3D&#20307;&#32032;&#24182;&#20351;&#29992;&#31232;&#30095;&#21367;&#31215;&#22359;&#25552;&#21462;&#20854;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#31934;&#30830;&#32780;&#40065;&#26834;&#30340;3D&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#23545;&#35937;&#30340;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02263v1 Announce Type: new  Abstract: Current LiDAR point cloud-based 3D single object tracking (SOT) methods typically rely on point-based representation network. Despite demonstrated success, such networks suffer from some fundamental problems: 1) It contains pooling operation to cope with inherently disordered point clouds, hindering the capture of 3D spatial information that is useful for tracking, a regression task. 2) The adopted set abstraction operation hardly handles density-inconsistent point clouds, also preventing 3D spatial information from being modeled. To solve these problems, we introduce a novel tracking framework, termed VoxelTrack. By voxelizing inherently disordered point clouds into 3D voxels and extracting their features via sparse convolution blocks, VoxelTrack effectively models precise and robust 3D spatial information, thereby guiding accurate position prediction for tracked objects. Moreover, VoxelTrack incorporates a dual-stream encoder with cros
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CSI&#65288;Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using Vision Language Models&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#35821;&#20041;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#28304;&#30446;&#26631;&#31867;&#23384;&#22312;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#22320;&#25191;&#34892;&#36328;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#30340;&#36328;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#21644;VLMs&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;CSI&#33021;&#22815;&#20026;&#30446;&#26631;&#22495;&#30340;&#26032;&#31867;&#21035;&#36827;&#34892;&#37325;&#26032;&#26631;&#27880;&#12290;</title><link>https://arxiv.org/abs/2408.02261</link><description>&lt;p&gt;
Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CSI&#65288;Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using Vision Language Models&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#35821;&#20041;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#28304;&#30446;&#26631;&#31867;&#23384;&#22312;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#22320;&#25191;&#34892;&#36328;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#30340;&#36328;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#21644;VLMs&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;CSI&#33021;&#22815;&#20026;&#30446;&#26631;&#22495;&#30340;&#26032;&#31867;&#21035;&#36827;&#34892;&#37325;&#26032;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02261v1 Announce Type: new  Abstract: The challenge of semantic segmentation in Unsupervised Domain Adaptation (UDA) emerges not only from domain shifts between source and target images but also from discrepancies in class taxonomies across domains. Traditional UDA research assumes consistent taxonomy between the source and target domains, thereby limiting their ability to recognize and adapt to the taxonomy of the target domain. This paper introduces a novel approach, Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using Vision Language Models (CSI), which effectively performs domain-adaptive semantic segmentation even in situations of source-target class mismatches. CSI leverages the semantic generalization potential of Visual Language Models (VLMs) to create synergy with previous UDA methods. It leverages segment reasoning obtained through traditional UDA methods, combined with the rich semantic knowledge embedded in VLMs, to relabel new classes in the target 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#20108;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#39640;&#32500;&#25968;&#25454;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#20803;&#32032;&#22312;&#19981;&#21516;&#21608;&#26399;&#20013;&#30340;&#20851;&#31995;&#26469;&#25913;&#36827;&#20256;&#32479;&#30340;&#21333;&#21608;&#26399;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20449;&#24687;&#20256;&#25773;&#21644;&#21608;&#26399;&#32467;&#26500;&#30340;&#35268;&#21017;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#20013;&#38388;&#32858;&#31867;&#30340;&#25968;&#37327;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.02250</link><description>&lt;p&gt;
Hierarchical Clustering using Reversible Binary Cellular Automata for High-Dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#20108;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#39640;&#32500;&#25968;&#25454;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#20803;&#32032;&#22312;&#19981;&#21516;&#21608;&#26399;&#20013;&#30340;&#20851;&#31995;&#26469;&#25913;&#36827;&#20256;&#32479;&#30340;&#21333;&#21608;&#26399;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20449;&#24687;&#20256;&#25773;&#21644;&#21608;&#26399;&#32467;&#26500;&#30340;&#35268;&#21017;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#20013;&#38388;&#32858;&#31867;&#30340;&#25968;&#37327;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02250v1 Announce Type: cross  Abstract: This work proposes a hierarchical clustering algorithm for high-dimensional datasets using the cyclic space of reversible finite cellular automata. In cellular automaton (CA) based clustering, if two objects belong to the same cycle, they are closely related and considered as part of the same cluster. However, if a high-dimensional dataset is clustered using the cycles of one CA, closely related objects may belong to different cycles. This paper identifies the relationship between objects in two different cycles based on the median of all elements in each cycle so that they can be grouped in the next stage. Further, to minimize the number of intermediate clusters which in turn reduces the computational cost, a rule selection strategy is taken to find the best rules based on information propagation and cycle structure. After encoding the dataset using frequency-based encoding such that the consecutive data elements maintain a minimum ha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Curriculum Learning&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;masked autoencoder&#21644;&#21435;&#22122;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#21319;RGB-D&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#39318;&#20808;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#20132;&#21449;&#27169;&#24577;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#21033;&#29992;&#31532;&#19968;&#20010;&#38454;&#27573;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#29305;&#23450;&#30340;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;masked autoencoder&#21644;&#21435;&#22122;&#25216;&#26415;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#21644;&#25968;&#25454;&#23454;&#20363;&#26469;&#36880;&#28176;&#25351;&#23548;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02245</link><description>&lt;p&gt;
Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Curriculum Learning&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;masked autoencoder&#21644;&#21435;&#22122;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#21319;RGB-D&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#39318;&#20808;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#20132;&#21449;&#27169;&#24577;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#21033;&#29992;&#31532;&#19968;&#20010;&#38454;&#27573;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#29305;&#23450;&#30340;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;masked autoencoder&#21644;&#21435;&#22122;&#25216;&#26415;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#21644;&#25968;&#25454;&#23454;&#20363;&#26469;&#36880;&#28176;&#25351;&#23548;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;vision-language&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#29992;&#20110;&#25705;&#25176;&#36710;&#12289;&#20056;&#23458;&#21644;&#22836;&#30420;&#30340;&#26816;&#27979;&#12289;&#20998;&#31867;&#21450;&#20851;&#32852;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;vision-language&#27169;&#22411;OwLv2&#26469;&#25552;&#39640;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#26816;&#27979;&#21644;&#20998;&#31867;&#25705;&#25176;&#36710;&#21644;&#22836;&#30420;&#20351;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02244</link><description>&lt;p&gt;
Evaluating Vision-Language Models for Zero-Shot Detection, Classification, and Association of Motorcycles, Passengers, and Helmets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;vision-language&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#29992;&#20110;&#25705;&#25176;&#36710;&#12289;&#20056;&#23458;&#21644;&#22836;&#30420;&#30340;&#26816;&#27979;&#12289;&#20998;&#31867;&#21450;&#20851;&#32852;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;vision-language&#27169;&#22411;OwLv2&#26469;&#25552;&#39640;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#26816;&#27979;&#21644;&#20998;&#31867;&#25705;&#25176;&#36710;&#21644;&#22836;&#30420;&#20351;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02244v1 Announce Type: new  Abstract: Motorcycle accidents pose significant risks, particularly when riders and passengers do not wear helmets. This study evaluates the efficacy of an advanced vision-language foundation model, OWLv2, in detecting and classifying various helmet-wearing statuses of motorcycle occupants using video data. We extend the dataset provided by the CVPR AI City Challenge and employ a cascaded model approach for detection and classification tasks, integrating OWLv2 and CNN models. The results highlight the potential of zero-shot learning to address challenges arising from incomplete and biased training datasets, demonstrating the usage of such models in detecting motorcycles, helmet usage, and occupant positions under varied conditions. We have achieved an average precision of 0.5324 for helmet detection and provided precision-recall curves detailing the detection and classification performance. Despite limitations such as low-resolution data and poor 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;REVISION&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;3D&#28210;&#26579; pipeline &#29983;&#25104;&#20934;&#30830;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;REVISION&#25903;&#25345;&#22810;&#31181;3D&#36164;&#20135;&#12289;&#31354;&#38388;&#20851;&#31995;&#21644;&#35270;&#35282;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#22312;&#39044;&#27979;&#31354;&#38388;&#20851;&#31995;&#26102;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;VISOR&#21644;T2I-CompBench&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2408.02231</link><description>&lt;p&gt;
REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;REVISION&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;3D&#28210;&#26579; pipeline &#29983;&#25104;&#20934;&#30830;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;REVISION&#25903;&#25345;&#22810;&#31181;3D&#36164;&#20135;&#12289;&#31354;&#38388;&#20851;&#31995;&#21644;&#35270;&#35282;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#22312;&#39044;&#27979;&#31354;&#38388;&#20851;&#31995;&#26102;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;VISOR&#21644;T2I-CompBench&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02231v1 Announce Type: new  Abstract: Text-to-Image (T2I) and multimodal large language models (MLLMs) have been adopted in solutions for several computer vision and multimodal learning tasks. However, it has been found that such vision-language models lack the ability to correctly reason over spatial relationships. To tackle this shortcoming, we develop the REVISION framework which improves spatial fidelity in vision-language models. REVISION is a 3D rendering based pipeline that generates spatially accurate synthetic images, given a textual prompt. REVISION is an extendable framework, which currently supports 100+ 3D assets, 11 spatial relationships, all with diverse camera perspectives and backgrounds. Leveraging images from REVISION as additional guidance in a training-free manner consistently improves the spatial consistency of T2I models across all spatial relationships, achieving competitive performance on the VISOR and T2I-CompBench benchmarks. We also design RevQA, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCreate&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#25512;&#21160;&#29983;&#25104;&#22270;&#20687;&#30340;&#23884;&#20837;&#36828;&#31163;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#65292;&#25913;&#36827;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#19982;&#21019;&#36896;&#21147;&#65292;&#24182;&#26377;&#25928;&#38450;&#27490;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#22797;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;FSCG-8&#30340;&#23569;&#26679;&#26412;&#21019;&#36896;&#24615;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20843;&#20010;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ProCreate&#30340;&#26377;&#25928;&#24615;&#12290;ProCreate&#30340;&#25104;&#21151;&#23637;&#31034;&#20102;&#20854;&#22312;&#38450;&#27490;&#25991;&#26412;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#22797;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;</title><link>https://arxiv.org/abs/2408.02226</link><description>&lt;p&gt;
ProCreate, Don\'t Reproduce! Propulsive Energy Diffusion for Creative Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCreate&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#25512;&#21160;&#29983;&#25104;&#22270;&#20687;&#30340;&#23884;&#20837;&#36828;&#31163;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#65292;&#25913;&#36827;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#19982;&#21019;&#36896;&#21147;&#65292;&#24182;&#26377;&#25928;&#38450;&#27490;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#22797;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;FSCG-8&#30340;&#23569;&#26679;&#26412;&#21019;&#36896;&#24615;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20843;&#20010;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ProCreate&#30340;&#26377;&#25928;&#24615;&#12290;ProCreate&#30340;&#25104;&#21151;&#23637;&#31034;&#20102;&#20854;&#22312;&#38450;&#27490;&#25991;&#26412;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#22797;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02226v1 Announce Type: new  Abstract: In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts. Code and FSCG-8 are available at https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The project page is available at https://procreate-diffusion.github.io.
&lt;/p&gt;</description></item><item><title>&#21407;&#25991;&#26631;&#39064;&#65306;&#29992;&#20110;RGBT&#36319;&#36394;&#30340;&#36328;&#35843;&#21046;&#27880;&#24847;&#21464;&#21387;&#22120;
&#25688;&#35201;&#65306;&#29616;&#26377;&#22522;&#20110;Transformer&#30340;RGBT&#36319;&#36394;&#22120;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26469;&#25552;&#21462;&#21333;&#27169;&#24577;&#29305;&#24449;&#24182;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#24573;&#35270;&#20102;&#20998;&#25903;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#36825;&#31181;&#24573;&#35270;&#20250;&#23548;&#33268;&#19981;&#26126;&#30830;&#30340;&#21644;&#19981;&#24688;&#24403;&#30340;&#30456;&#20851;&#24615;&#26435;&#37325;&#35745;&#31639;&#12290;&#36825;&#19981;&#20165;&#38480;&#21046;&#20102;&#21333;&#27169;&#24577;&#29305;&#24449;&#30340;&#34920;&#29616;&#65292;&#20063;&#20260;&#23475;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#35843;&#21046;&#27880;&#24847;&#21464;&#21387;&#22120;&#65288;CAFormer&#65289;&#26041;&#27861;&#65292;&#23427;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#25191;&#34892;&#21333;&#27169;&#24577;&#33258;&#25105;&#30456;&#20851;&#24615;&#35745;&#31639;&#12289;&#21452;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#35745;&#31639;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#65292;&#29992;&#20110;RGBT&#36319;&#36394;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#29420;&#31435;&#29983;&#25104;</title><link>https://arxiv.org/abs/2408.02222</link><description>&lt;p&gt;
Cross-modulated Attention Transformer for RGBT Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02222
&lt;/p&gt;
&lt;p&gt;
&#21407;&#25991;&#26631;&#39064;&#65306;&#29992;&#20110;RGBT&#36319;&#36394;&#30340;&#36328;&#35843;&#21046;&#27880;&#24847;&#21464;&#21387;&#22120;
&#25688;&#35201;&#65306;&#29616;&#26377;&#22522;&#20110;Transformer&#30340;RGBT&#36319;&#36394;&#22120;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26469;&#25552;&#21462;&#21333;&#27169;&#24577;&#29305;&#24449;&#24182;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#24573;&#35270;&#20102;&#20998;&#25903;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#36825;&#31181;&#24573;&#35270;&#20250;&#23548;&#33268;&#19981;&#26126;&#30830;&#30340;&#21644;&#19981;&#24688;&#24403;&#30340;&#30456;&#20851;&#24615;&#26435;&#37325;&#35745;&#31639;&#12290;&#36825;&#19981;&#20165;&#38480;&#21046;&#20102;&#21333;&#27169;&#24577;&#29305;&#24449;&#30340;&#34920;&#29616;&#65292;&#20063;&#20260;&#23475;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#35843;&#21046;&#27880;&#24847;&#21464;&#21387;&#22120;&#65288;CAFormer&#65289;&#26041;&#27861;&#65292;&#23427;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#25191;&#34892;&#21333;&#27169;&#24577;&#33258;&#25105;&#30456;&#20851;&#24615;&#35745;&#31639;&#12289;&#21452;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#35745;&#31639;&#21644;&#27169;&#26495;&#25628;&#32034;&#30456;&#20851;&#24615;&#35745;&#31639;&#65292;&#29992;&#20110;RGBT&#36319;&#36394;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#29420;&#31435;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02222v1 Announce Type: new  Abstract: Existing Transformer-based RGBT trackers achieve remarkable performance benefits by leveraging self-attention to extract uni-modal features and cross-attention to enhance multi-modal feature interaction and template-search correlation computation. Nevertheless, the independent search-template correlation calculations ignore the consistency between branches, which can result in ambiguous and inappropriate correlation weights. It not only limits the intra-modal feature representation, but also harms the robustness of cross-attention for multi-modal feature interaction and search-template correlation computation. To address these issues, we propose a novel approach called Cross-modulated Attention Transformer (CAFormer), which performs intra-modality self-correlation, inter-modality feature interaction, and search-template correlation computation in a unified attention model, for RGBT tracking. In particular, we first independently generate
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#31934;&#32454;&#31890;&#24230;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#24320;&#21457;&#20102;AI&#27169;&#22411;&#20197;&#20256;&#36798;&#31867;&#20284;&#20110;&#20154;&#31867;&#19987;&#23478;&#30340;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#33719;&#21462;&#30340;&#31934;&#32454;&#31890;&#24230;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;AI&#27169;&#22411;&#22312;&#27491;&#26679;&#26412;&#20869;&#37096;&#32570;&#20047;&#24046;&#24322;&#24615;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02214</link><description>&lt;p&gt;
More Than Positive and Negative: Communicating Fine Granularity in Medical Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02214
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#31934;&#32454;&#31890;&#24230;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#24320;&#21457;&#20102;AI&#27169;&#22411;&#20197;&#20256;&#36798;&#31867;&#20284;&#20110;&#20154;&#31867;&#19987;&#23478;&#30340;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#33719;&#21462;&#30340;&#31934;&#32454;&#31890;&#24230;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;AI&#27169;&#22411;&#22312;&#27491;&#26679;&#26412;&#20869;&#37096;&#32570;&#20047;&#24046;&#24322;&#24615;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02214v1 Announce Type: new  Abstract: With the advance of deep learning, much progress has been made in building powerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR) analysis. Most existing AI models are trained to be a binary classifier with the aim of distinguishing positive and negative cases. However, a large gap exists between the simple binary setting and complicated real-world medical scenarios. In this work, we reinvestigate the problem of automatic radiology diagnosis. We first observe that there is considerable diversity among cases within the positive class, which means simply classifying them as positive loses many important details. This motivates us to build AI models that can communicate fine-grained knowledge from medical images like human experts. To this end, we first propose a new benchmark on fine granularity learning from medical images. Specifically, we devise a division rule based on medical knowledge to divide positive cases i
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExoViP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#39564;&#35777;&#27169;&#22359;&#20316;&#20026;&#8220;&#22806;&#37096;&#39592;&#26550;&#8221;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#32534;&#31243;&#26041;&#26696;&#65292;&#20197;&#32416;&#27491;&#35745;&#21010;&#21644;&#25191;&#34892;&#38454;&#27573;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#39564;&#35777;&#26469;&#25552;&#21319;&#35270;&#35273;&#20219;&#21153;&#25191;&#34892;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02210</link><description>&lt;p&gt;
ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExoViP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#39564;&#35777;&#27169;&#22359;&#20316;&#20026;&#8220;&#22806;&#37096;&#39592;&#26550;&#8221;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#32534;&#31243;&#26041;&#26696;&#65292;&#20197;&#32416;&#27491;&#35745;&#21010;&#21644;&#25191;&#34892;&#38454;&#27573;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#39564;&#35777;&#26469;&#25552;&#21319;&#35270;&#35273;&#20219;&#21153;&#25191;&#34892;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02210v1 Announce Type: new  Abstract: Compositional visual reasoning methods, which translate a complex query into a structured composition of feasible visual tasks, have exhibited a strong potential in complicated multi-modal tasks. Empowered by recent advances in large language models (LLMs), this multi-modal challenge has been brought to a new stage by treating LLMs as few-shot/zero-shot planners, i.e., vision-language (VL) programming. Such methods, despite their numerous merits, suffer from challenges due to LLM planning mistakes or inaccuracy of visual execution modules, lagging behind the non-compositional models. In this work, we devise a "plug-and-play" method, ExoViP, to correct errors in both the planning and execution stages through introspective verification. We employ verification modules as "exoskeletons" to enhance current VL programming schemes. Specifically, our proposed verification module utilizes a mixture of three sub-verifiers to validate predictions a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#28304;&#25968;&#25454;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#28304;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#20511;&#21161;&#19981;&#30830;&#23450;&#24615;&#21644;&#28201;&#24230;&#35843;&#33410;&#65292;&#35813;&#25991;&#31456;&#23637;&#29616;&#20102;&#22312;&#32570;&#20047;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23545;&#35937;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.02209</link><description>&lt;p&gt;
Source-Free Domain-Invariant Performance Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#28304;&#25968;&#25454;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#28304;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#20511;&#21161;&#19981;&#30830;&#23450;&#24615;&#21644;&#28201;&#24230;&#35843;&#33410;&#65292;&#35813;&#25991;&#31456;&#23637;&#29616;&#20102;&#22312;&#32570;&#20047;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23545;&#35937;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02209v1 Announce Type: new  Abstract: Accurately estimating model performance poses a significant challenge, particularly in scenarios where the source and target domains follow different data distributions. Most existing performance prediction methods heavily rely on the source data in their estimation process, limiting their applicability in a more realistic setting where only the trained model is accessible. The few methods that do not require source data exhibit considerably inferior performance. In this work, we propose a source-free approach centred on uncertainty-based estimation, using a generative model for calibration in the absence of source data. We establish connections between our approach for unsupervised calibration and temperature scaling. We then employ a gradient-based strategy to evaluate the correctness of the calibrated predictions. Our experiments on benchmark object recognition datasets reveal that existing source-based methods fall short with limited
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#32467;&#21512;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#30693;&#35782;&#36801;&#31227;&#31574;&#30053;&#22823;&#24133;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#24046;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#38388;&#35757;&#32451;&#26102;&#30340;&#23384;&#20648;&#38656;&#27714;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02192</link><description>&lt;p&gt;
Unsupervised Domain Adaption Harnessing Vision-Language Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#32467;&#21512;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#30693;&#35782;&#36801;&#31227;&#31574;&#30053;&#22823;&#24133;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#24046;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#38388;&#35757;&#32451;&#26102;&#30340;&#23384;&#20648;&#38656;&#27714;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02192v1 Announce Type: new  Abstract: This paper addresses two vital challenges in Unsupervised Domain Adaptation (UDA) with a focus on harnessing the power of Vision-Language Pre-training (VLP) models. Firstly, UDA has primarily relied on ImageNet pre-trained models. However, the potential of VLP models in UDA remains largely unexplored. The rich representation of VLP models holds significant promise for enhancing UDA tasks. To address this, we propose a novel method called Cross-Modal Knowledge Distillation (CMKD), leveraging VLP models as teacher models to guide the learning process in the target domain, resulting in state-of-the-art performance. Secondly, current UDA paradigms involve training separate models for each task, leading to significant storage overhead and impractical model deployment as the number of transfer tasks grows. To overcome this challenge, we introduce Residual Sparse Training (RST) exploiting the benefits conferred by VLP's extensive pre-training, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dense Feature Interaction Network&#65288;DeFI-Net&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22270;&#20687;&#20462;&#34917;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#29305;&#24449;&#37329;&#23383;&#22612;&#26550;&#26500;&#65292;DeFI-Net&#33021;&#22815;&#25429;&#25417;&#21644;&#25918;&#22823;&#19981;&#21516;&#38454;&#27573;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#23610;&#24230;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#20102;&#23545;&#22270;&#20687;&#20462;&#34917;&#26816;&#27979;&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21161;&#20110;&#25581;&#31034;&#22270;&#20687;&#20462;&#34917;&#20013;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#65292;&#21363;&#20351;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#21644;&#23610;&#24230;&#30446;&#26631;&#26102;&#65292;&#20063;&#33021;&#20943;&#23569;&#35823;&#25253;&#24182;&#25552;&#39640;&#36793;&#32536;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02191</link><description>&lt;p&gt;
Dense Feature Interaction Network for Image Inpainting Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dense Feature Interaction Network&#65288;DeFI-Net&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22270;&#20687;&#20462;&#34917;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#29305;&#24449;&#37329;&#23383;&#22612;&#26550;&#26500;&#65292;DeFI-Net&#33021;&#22815;&#25429;&#25417;&#21644;&#25918;&#22823;&#19981;&#21516;&#38454;&#27573;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#23610;&#24230;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#20102;&#23545;&#22270;&#20687;&#20462;&#34917;&#26816;&#27979;&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21161;&#20110;&#25581;&#31034;&#22270;&#20687;&#20462;&#34917;&#20013;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#65292;&#21363;&#20351;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#21644;&#23610;&#24230;&#30446;&#26631;&#26102;&#65292;&#20063;&#33021;&#20943;&#23569;&#35823;&#25253;&#24182;&#25552;&#39640;&#36793;&#32536;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02191v1 Announce Type: new  Abstract: Image inpainting, which is the task of filling in missing areas in an image, is a common image editing technique. Inpainting can be used to conceal or alter image contents in malicious manipulation of images, driving the need for research in image inpainting detection. Existing methods mostly rely on a basic encoder-decoder structure, which often results in a high number of false positives or misses the inpainted regions, especially when dealing with targets of varying semantics and scales. Additionally, the absence of an effective approach to capture boundary artifacts leads to less accurate edge localization. In this paper, we describe a new method for inpainting detection based on a Dense Feature Interaction Network (DeFI-Net). DeFI-Net uses a novel feature pyramid architecture to capture and amplify multi-scale representations across various stages, thereby improving the detection of image inpainting by better revealing feature-level
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;AssemAI&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#21046;&#36896;&#31649;&#36947;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#22522;&#30784;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#12290;&#31995;&#32479;&#25104;&#21151;&#21019;&#24314;&#20102;&#38024;&#23545;&#24037;&#19994;&#28779;&#31661;&#32452;&#35013;&#31649;&#36947;&#30340;&#29305;&#23450;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#23450;&#20041;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;YOLO-FF&#65292;&#19987;&#38376;&#29992;&#20110;&#21046;&#36896;&#19994;&#32452;&#35013;&#29615;&#22659;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22788;&#29702;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#24615;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22270;&#20687;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#31616;&#21333;CNN&#26041;&#27861;&#21644;&#33258;&#23450;&#20041;Visual Transformer&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#19987;&#19994;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02181</link><description>&lt;p&gt;
AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02181
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;AssemAI&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#21046;&#36896;&#31649;&#36947;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#22522;&#30784;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#12290;&#31995;&#32479;&#25104;&#21151;&#21019;&#24314;&#20102;&#38024;&#23545;&#24037;&#19994;&#28779;&#31661;&#32452;&#35013;&#31649;&#36947;&#30340;&#29305;&#23450;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#23450;&#20041;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;YOLO-FF&#65292;&#19987;&#38376;&#29992;&#20110;&#21046;&#36896;&#19994;&#32452;&#35013;&#29615;&#22659;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22788;&#29702;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#24615;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22270;&#20687;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#31616;&#21333;CNN&#26041;&#27861;&#21644;&#33258;&#23450;&#20041;Visual Transformer&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#19987;&#19994;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02181v1 Announce Type: new  Abstract: Anomaly detection in manufacturing pipelines remains a critical challenge, intensified by the complexity and variability of industrial environments. This paper introduces AssemAI, an interpretable image-based anomaly detection system tailored for smart manufacturing pipelines. Our primary contributions include the creation of a tailored image dataset and the development of a custom object detection model, YOLO-FF, designed explicitly for anomaly detection in manufacturing assembly environments. Utilizing the preprocessed image dataset derived from an industry-focused rocket assembly pipeline, we address the challenge of imbalanced image data and demonstrate the importance of image-based methods in anomaly detection. The proposed approach leverages domain knowledge in data preparation, model development and reasoning. We compare our method against several baselines, including simple CNN and custom Visual Transformer (ViT) models, showcasi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#30830;&#20445;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#20844;&#24179;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#20010;&#20154;&#36164;&#26009;&#26631;&#27880;&#21644;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23545;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#20102;&#37325;&#26032;&#24605;&#32771;&#65292;&#24182;&#37325;&#26032;&#35745;&#31639;&#20102;&#20808;&#21069;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20026;&#26410;&#26469;&#30340;&#24773;&#24863;&#35782;&#21035;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#21152;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2408.02164</link><description>&lt;p&gt;
Rethinking Affect Analysis: A Protocol for Ensuring Fairness and Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#30830;&#20445;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#20844;&#24179;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#20010;&#20154;&#36164;&#26009;&#26631;&#27880;&#21644;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23545;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#20102;&#37325;&#26032;&#24605;&#32771;&#65292;&#24182;&#37325;&#26032;&#35745;&#31639;&#20102;&#20808;&#21069;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20026;&#26410;&#26469;&#30340;&#24773;&#24863;&#35782;&#21035;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#21152;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02164v1 Announce Type: new  Abstract: Evaluating affect analysis methods presents challenges due to inconsistencies in database partitioning and evaluation protocols, leading to unfair and biased results. Previous studies claim continuous performance improvements, but our findings challenge such assertions. Using these insights, we propose a unified protocol for database partitioning that ensures fairness and comparability. We provide detailed demographic annotations (in terms of race, gender and age), evaluation metrics, and a common framework for expression recognition, action unit detection and valence-arousal estimation. We also rerun the methods with the new protocol and introduce a new leaderboards to encourage future research in affect recognition with a fairer comparison. Our annotations, code, and pre-trained models are available on \hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PanoFree&#30340;&#26080;&#39035;&#24494;&#35843;&#30340;&#20840;&#23616;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#36328;&#35270;&#22270;&#33258;&#25105;&#25351;&#23548;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#35270;&#22270;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#25191;&#34892;&#35270;&#22270;&#20043;&#38388;&#30340;&#21464;&#25442;&#21644;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#32047;&#31215;&#35823;&#24046;&#24102;&#26469;&#30340;&#19981;&#19968;&#33268;&#21644;&#20266;&#20687;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#36328;&#35270;&#22270;&#24847;&#35782;&#21644;&#25913;&#36827;&#25554;&#20540;&#21644;&#21464;&#25442;&#36807;&#31243;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#31216;&#30340;&#21452;&#21521;&#24341;&#23548;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.02157</link><description>&lt;p&gt;
PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PanoFree&#30340;&#26080;&#39035;&#24494;&#35843;&#30340;&#20840;&#23616;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#36328;&#35270;&#22270;&#33258;&#25105;&#25351;&#23548;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#35270;&#22270;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#25191;&#34892;&#35270;&#22270;&#20043;&#38388;&#30340;&#21464;&#25442;&#21644;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#32047;&#31215;&#35823;&#24046;&#24102;&#26469;&#30340;&#19981;&#19968;&#33268;&#21644;&#20266;&#20687;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#36328;&#35270;&#22270;&#24847;&#35782;&#21644;&#25913;&#36827;&#25554;&#20540;&#21644;&#21464;&#25442;&#36807;&#31243;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#31216;&#30340;&#21452;&#21521;&#24341;&#23548;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02157v1 Announce Type: new  Abstract: Immersive scene generation, notably panorama creation, benefits significantly from the adaptation of large pre-trained text-to-image (T2I) models for multi-view image generation. Due to the high cost of acquiring multi-view images, tuning-free generation is preferred. However, existing methods are either limited to simple correspondences or require extensive fine-tuning to capture complex ones. We present PanoFree, a novel method for tuning-free multi-view image generation that supports an extensive array of correspondences. PanoFree sequentially generates multi-view images using iterative warping and inpainting, addressing the key issues of inconsistency and artifacts from error accumulation without the need for fine-tuning. It improves error accumulation by enhancing cross-view awareness and refines the warping and inpainting processes via cross-view guidance, risky area estimation and erasing, and symmetric bidirectional guided genera
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#35270;&#39057;&#20998;&#26512;&#25163;&#27573;&#65292;&#35814;&#32454;&#25506;&#35752;&#20102;&#36275;&#29699;&#27604;&#36187;&#23545;&#20307;&#32946;&#22330;&#21608;&#36793;&#20132;&#36890;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#22312;&#27604;&#36187;&#26085;&#19982;&#38750;&#27604;&#36187;&#26085;&#20043;&#38388;&#30340;&#26174;&#33879;&#20132;&#36890;&#27169;&#24335;&#24046;&#24322;&#65292;&#20026;&#20132;&#36890;&#31649;&#29702;&#20154;&#21592;&#25552;&#20379;&#20102;&#39044;&#27979;&#21644;&#31649;&#29702;&#20132;&#36890;&#27969;&#37327;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.02146</link><description>&lt;p&gt;
Video-based Pedestrian and Vehicle Traffic Analysis During Football Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#35270;&#39057;&#20998;&#26512;&#25163;&#27573;&#65292;&#35814;&#32454;&#25506;&#35752;&#20102;&#36275;&#29699;&#27604;&#36187;&#23545;&#20307;&#32946;&#22330;&#21608;&#36793;&#20132;&#36890;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#22312;&#27604;&#36187;&#26085;&#19982;&#38750;&#27604;&#36187;&#26085;&#20043;&#38388;&#30340;&#26174;&#33879;&#20132;&#36890;&#27169;&#24335;&#24046;&#24322;&#65292;&#20026;&#20132;&#36890;&#31649;&#29702;&#20154;&#21592;&#25552;&#20379;&#20102;&#39044;&#27979;&#21644;&#31649;&#29702;&#20132;&#36890;&#27969;&#37327;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02146v1 Announce Type: new  Abstract: This paper utilizes video analytics to study pedestrian and vehicle traffic behavior, focusing on analyzing traffic patterns during football gamedays. The University of Florida (UF) hosts six to seven home football games on Saturdays during the college football season, attracting significant pedestrian activity. Through video analytics, this study provides valuable insights into the impact of these events on traffic volumes and safety at intersections. Comparing pedestrian and vehicle activities on gamedays versus non-gamedays reveals differing patterns. For example, pedestrian volume substantially increases during gamedays, which is positively correlated with the probability of the away team winning. This correlation is likely because fans of the home team enjoy watching difficult games. Win probabilities as an early predictor of pedestrian volumes at intersections can be a tool to help traffic professionals anticipate traffic managemen
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#22686;&#24378;&#30340;GAN&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#39640;&#25928;&#25552;&#21462;&#40657;&#31665;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#23545;&#35270;&#39057;&#20998;&#31867;&#30340;&#27169;&#22411;&#24179;&#22343;&#25552;&#21319;&#25928;&#26524;&#39640;&#36798;26.11%&#65292;&#24182;&#22312;&#19968;&#20123;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22823;33.36%&#30340;&#25552;&#21319;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#36827;&#40657;&#31665;&#27169;&#22411;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02140</link><description>&lt;p&gt;
VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#22686;&#24378;&#30340;GAN&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#39640;&#25928;&#25552;&#21462;&#40657;&#31665;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#23545;&#35270;&#39057;&#20998;&#31867;&#30340;&#27169;&#22411;&#24179;&#22343;&#25552;&#21319;&#25928;&#26524;&#39640;&#36798;26.11%&#65292;&#24182;&#22312;&#19968;&#20123;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22823;33.36%&#30340;&#25552;&#21319;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#36827;&#40657;&#31665;&#27169;&#22411;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RICA^2&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#25972;&#21512;&#35780;&#20998;&#35268;&#21017;&#24182;&#32771;&#34385;&#21040;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#12290;RICA^2&#36890;&#36807;&#22312;&#26681;&#25454;&#35780;&#20998;&#35268;&#21017;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#19978;&#23884;&#20837;&#21160;&#20316;&#27493;&#39588;&#65292;&#26377;&#25928;&#22320;&#37327;&#21270;&#20102;&#21160;&#20316;&#25191;&#34892;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02138</link><description>&lt;p&gt;
RICA^2: Rubric-Informed, Calibrated Assessment of Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RICA^2&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#25972;&#21512;&#35780;&#20998;&#35268;&#21017;&#24182;&#32771;&#34385;&#21040;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#12290;RICA^2&#36890;&#36807;&#22312;&#26681;&#25454;&#35780;&#20998;&#35268;&#21017;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#19978;&#23884;&#20837;&#21160;&#20316;&#27493;&#39588;&#65292;&#26377;&#25928;&#22320;&#37327;&#21270;&#20102;&#21160;&#20316;&#25191;&#34892;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02138v1 Announce Type: new  Abstract: The ability to quantify how well an action is carried out, also known as action quality assessment (AQA), has attracted recent interest in the vision community. Unfortunately, prior methods often ignore the score rubric used by human experts and fall short of quantifying the uncertainty of the model prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model that integrates score rubric and accounts for prediction uncertainty for AQA. Central to our method lies in stochastic embeddings of action steps, defined on a graph structure that encodes the score rubric. The embeddings spread probabilistic density in the latent space and allow our method to represent model uncertainty. The graph encodes the scoring criteria, based on which the quality scores can be decoded. We demonstrate that our method establishes new state of the art on public benchmarks, including FineDiving, MTL-AQA, and JIGSAWS, with superior performance in
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;-&#32034;&#27494;&#22827;&#31995;&#21015;&#26469;&#34920;&#31034;&#21644;&#35782;&#21035;&#31526;&#21495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#27604;&#20808;&#21069;&#20351;&#29992;&#30340;&#21202;&#35753;&#24503;-&#32034;&#27494;&#22827;&#31995;&#21015;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2408.02135</link><description>&lt;p&gt;
A First Look at Chebyshev-Sobolev Series for Digital Ink
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;-&#32034;&#27494;&#22827;&#31995;&#21015;&#26469;&#34920;&#31034;&#21644;&#35782;&#21035;&#31526;&#21495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#27604;&#20808;&#21069;&#20351;&#29992;&#30340;&#21202;&#35753;&#24503;-&#32034;&#27494;&#22827;&#31995;&#21015;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02135v1 Announce Type: new  Abstract: Considering digital ink as plane curves provides a valuable framework for various applications, including signature verification, note-taking, and mathematical handwriting recognition. These plane curves can be obtained as parameterized pairs of approximating truncated series (x(s), y(s)) determined by sampled points. Earlier work has found that representing these truncated series (polynomials) in a Legendre or Legendre-Sobolev basis has a number of desirable properties. These include compact data representation, meaningful clustering of like symbols in the vector space of polynomial coefficients, linear separability of classes in this space, and highly efficient calculation of variation between curves. In this work, we take a first step at examining the use of Chebyshev-Sobolev series for symbol recognition. The early indication is that this representation may be superior to Legendre-Sobolev representation for some purposes.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Foveation-based Explanations (FovEx)&#65292;&#19968;&#31181;&#21463;&#20154;&#31867;&#35270;&#35273;&#21551;&#21457;&#30340;XAI&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#32467;&#21512;&#29983;&#29289;&#21551;&#21457;&#24335;&#25200;&#21160;&#21644;&#26799;&#24230;&#22522;&#35270;&#35273;&#25506;&#32034;&#65292;&#20026;Vision Transformers&#21644;Convolutional Neural Networks&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02123</link><description>&lt;p&gt;
FovEx: Human-inspired Explanations for Vision Transformers and Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Foveation-based Explanations (FovEx)&#65292;&#19968;&#31181;&#21463;&#20154;&#31867;&#35270;&#35273;&#21551;&#21457;&#30340;XAI&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#32467;&#21512;&#29983;&#29289;&#21551;&#21457;&#24335;&#25200;&#21160;&#21644;&#26799;&#24230;&#22522;&#35270;&#35273;&#25506;&#32034;&#65292;&#20026;Vision Transformers&#21644;Convolutional Neural Networks&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02123v1 Announce Type: new  Abstract: Explainability in artificial intelligence (XAI) remains a crucial aspect for fostering trust and understanding in machine learning models. Current visual explanation techniques, such as gradient-based or class-activation-based methods, often exhibit a strong dependence on specific model architectures. Conversely, perturbation-based methods, despite being model-agnostic, are computationally expensive as they require evaluating models on a large number of forward passes. In this work, we introduce Foveation-based Explanations (FovEx), a novel XAI method inspired by human vision. FovEx seamlessly integrates biologically inspired perturbations by iteratively creating foveated renderings of the image and combines them with gradient-based visual explorations to determine locations of interest efficiently. These locations are selected to maximize the performance of the model to be explained with respect to the downstream task and then combined 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AvatarPose&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#20154;&#21270;&#30340;&#38544;&#21464;&#37327;&#31070;&#32463;&#35282;&#33394;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#65292;&#20174;&#31232;&#30095;&#30340;&#22810;&#35270;&#22270;&#35270;&#39057;&#20013;&#23454;&#29616;&#20102;&#23545;&#32039;&#23494;&#20132;&#27969;&#30340;&#22810;&#20154;3D&#23039;&#24577;&#20272;&#35745;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#32469;&#36807;&#20102;&#20381;&#36182;&#20110;&#31895;&#30053;&#30340;2D&#20851;&#33410;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#30452;&#25509;&#20248;&#21270;3D&#23039;&#24577;&#65292;&#22522;&#20110;&#39068;&#33394;&#21644;&#36718;&#24275;&#28210;&#26579;&#25439;&#22833;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#32039;&#23494;&#20132;&#27969;&#20013;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#22240;&#37325;&#21472;&#23039;&#24577;&#23548;&#33268;&#30340;&#38556;&#30861;&#29289;&#21076;&#38500;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#37325;&#21472;&#24418;&#29366;&#21306;&#22495;&#19978;&#24212;&#29992;&#30896;&#25758;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2408.02110</link><description>&lt;p&gt;
AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AvatarPose&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#20154;&#21270;&#30340;&#38544;&#21464;&#37327;&#31070;&#32463;&#35282;&#33394;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#65292;&#20174;&#31232;&#30095;&#30340;&#22810;&#35270;&#22270;&#35270;&#39057;&#20013;&#23454;&#29616;&#20102;&#23545;&#32039;&#23494;&#20132;&#27969;&#30340;&#22810;&#20154;3D&#23039;&#24577;&#20272;&#35745;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#32469;&#36807;&#20102;&#20381;&#36182;&#20110;&#31895;&#30053;&#30340;2D&#20851;&#33410;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#30452;&#25509;&#20248;&#21270;3D&#23039;&#24577;&#65292;&#22522;&#20110;&#39068;&#33394;&#21644;&#36718;&#24275;&#28210;&#26579;&#25439;&#22833;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#32039;&#23494;&#20132;&#27969;&#20013;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#22240;&#37325;&#21472;&#23039;&#24577;&#23548;&#33268;&#30340;&#38556;&#30861;&#29289;&#21076;&#38500;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#37325;&#21472;&#24418;&#29366;&#21306;&#22495;&#19978;&#24212;&#29992;&#30896;&#25758;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02110v1 Announce Type: new  Abstract: Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Radiance Fields&#32534;&#36753;&#31649;&#32447;&#65292;&#23427;&#36890;&#36807;&#20165;&#23545;&#21333;&#19968;&#21442;&#32771;&#22270;&#20687;&#25191;&#34892;&#20869;&#23481;&#22635;&#20805;&#65292;&#28982;&#21518;&#22312;&#28145;&#24230;&#20449;&#24687;&#22522;&#30784;&#19978;&#23558;&#20854;&#25237;&#24433;&#21040;&#22810;&#20010;&#35270;&#22270;&#20013;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20869;&#23481;&#21024;&#38500;&#30340;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#35270;&#22270;&#20013;&#20986;&#29616;&#30340;&#27495;&#20041;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#32771;&#34385;&#21040;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25237;&#24433;&#20551;&#35774;&#21644;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#32534;&#36753;&#21518;&#22330;&#26223;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02100</link><description>&lt;p&gt;
View-consistent Object Removal in Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Radiance Fields&#32534;&#36753;&#31649;&#32447;&#65292;&#23427;&#36890;&#36807;&#20165;&#23545;&#21333;&#19968;&#21442;&#32771;&#22270;&#20687;&#25191;&#34892;&#20869;&#23481;&#22635;&#20805;&#65292;&#28982;&#21518;&#22312;&#28145;&#24230;&#20449;&#24687;&#22522;&#30784;&#19978;&#23558;&#20854;&#25237;&#24433;&#21040;&#22810;&#20010;&#35270;&#22270;&#20013;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20869;&#23481;&#21024;&#38500;&#30340;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#35270;&#22270;&#20013;&#20986;&#29616;&#30340;&#27495;&#20041;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#32771;&#34385;&#21040;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25237;&#24433;&#20551;&#35774;&#21644;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#32534;&#36753;&#21518;&#22330;&#26223;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02100v1 Announce Type: new  Abstract: Radiance Fields (RFs) have emerged as a crucial technology for 3D scene representation, enabling the synthesis of novel views with remarkable realism. However, as RFs become more widely used, the need for effective editing techniques that maintain coherence across different perspectives becomes evident. Current methods primarily depend on per-frame 2D image inpainting, which often fails to maintain consistency across views, thus compromising the realism of edited RF scenes. In this work, we introduce a novel RF editing pipeline that significantly enhances consistency by requiring the inpainting of only a single reference image. This image is then projected across multiple views using a depth-based approach, effectively reducing the inconsistencies observed with per-frame inpainting. However, projections typically assume photometric consistency across views, which is often impractical in real-world settings. To accommodate realistic varia
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21382;&#21490;&#21160;&#20316;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#22797;&#29616;&#36807;&#21435;&#24207;&#21015;&#21644;&#26410;&#26469;&#24207;&#21015;&#30340;&#24341;&#23548;&#22797;&#29616;&#65292;&#26377;&#25928;&#25552;&#21319;3D&#39592;&#39612;&#21160;&#20316;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20307;&#29616;&#22312;&#20854;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#27169;&#20223;&#36807;&#21435;&#21160;&#20316;&#27169;&#24335;&#26469;&#22686;&#24378;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#36827;&#34892;&#26410;&#26469;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#36890;&#36807;&#19968;&#20010;&#29992;&#20110;&#20851;&#27880;&#26174;&#33879;&#36816;&#21160;&#20851;&#33410;&#30340;&#21152;&#26435;&#31574;&#30053;&#65292;&#25991;&#31456;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#21644;&#28508;&#22312;&#30340;&#36807;&#21435;&#19982;&#26410;&#26469;&#20851;&#31995;&#30340;&#25429;&#33719;&#65292;&#36825;&#23545;&#20110;&#20165;&#20381;&#36182;&#21382;&#21490;&#21160;&#20316;&#25968;&#25454;&#30340;&#21160;&#20316;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2408.02091</link><description>&lt;p&gt;
Past Movements-Guided Motion Representation Learning for Human Motion Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21382;&#21490;&#21160;&#20316;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#22797;&#29616;&#36807;&#21435;&#24207;&#21015;&#21644;&#26410;&#26469;&#24207;&#21015;&#30340;&#24341;&#23548;&#22797;&#29616;&#65292;&#26377;&#25928;&#25552;&#21319;3D&#39592;&#39612;&#21160;&#20316;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20307;&#29616;&#22312;&#20854;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#27169;&#20223;&#36807;&#21435;&#21160;&#20316;&#27169;&#24335;&#26469;&#22686;&#24378;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#36827;&#34892;&#26410;&#26469;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#36890;&#36807;&#19968;&#20010;&#29992;&#20110;&#20851;&#27880;&#26174;&#33879;&#36816;&#21160;&#20851;&#33410;&#30340;&#21152;&#26435;&#31574;&#30053;&#65292;&#25991;&#31456;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#21644;&#28508;&#22312;&#30340;&#36807;&#21435;&#19982;&#26410;&#26469;&#20851;&#31995;&#30340;&#25429;&#33719;&#65292;&#36825;&#23545;&#20110;&#20165;&#20381;&#36182;&#21382;&#21490;&#21160;&#20316;&#25968;&#25454;&#30340;&#21160;&#20316;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02091v1 Announce Type: new  Abstract: Human motion prediction based on 3D skeleton is a significant challenge in computer vision, primarily focusing on the effective representation of motion. In this paper, we propose a self-supervised learning framework designed to enhance motion representation. This framework consists of two stages: first, the network is pretrained through the self-reconstruction of past sequences, and the guided reconstruction of future sequences based on past movements. We design a velocity-based mask strategy to focus on the joints with large-scale moving. Subsequently, the pretrained network undergoes finetuning for specific tasks. Self-reconstruction, guided by patterns of past motion, substantially improves the model's ability to represent the spatiotemporal relationships among joints but also captures the latent relationships between past and future sequences. This capability is crucial for motion prediction tasks that solely depend on historical mo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCBEV-KAN&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#21253;&#25324;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#65289;&#26469;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;BEV&#65288;&#40479;&#30640;&#35270;&#22270;&#65289;&#21644;Transformer&#26550;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#25968;&#25454;&#28304;&#38388;&#30340;&#25972;&#21512;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#26816;&#27979;&#31867;&#21035;&#19978;&#65292;RCBEV-KAN&#27169;&#22411;&#22312;Mean Distance AP&#12289;ND Score&#21644;Evaluation Time&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#34920;&#26126;&#20854;&#26816;&#27979;&#24615;&#33021;&#22823;&#24133;&#39046;&#20808;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02088</link><description>&lt;p&gt;
KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for autonomous driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCBEV-KAN&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#21253;&#25324;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#65289;&#26469;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;BEV&#65288;&#40479;&#30640;&#35270;&#22270;&#65289;&#21644;Transformer&#26550;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#25968;&#25454;&#28304;&#38388;&#30340;&#25972;&#21512;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#26816;&#27979;&#31867;&#21035;&#19978;&#65292;RCBEV-KAN&#27169;&#22411;&#22312;Mean Distance AP&#12289;ND Score&#21644;Evaluation Time&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#34920;&#26126;&#20854;&#26816;&#27979;&#24615;&#33021;&#22823;&#24133;&#39046;&#20808;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02088v1 Announce Type: new  Abstract: Accurate 3D object detection in autonomous driving is critical yet challenging due to occlusions, varying object scales, and complex urban environments. This paper introduces the RCBEV-KAN algorithm, a pioneering method designed to enhance 3D object detection by fusing multimodal sensor data from cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View (BEV)-based approach, utilizing a Transformer architecture, significantly boosts detection precision and efficiency by seamlessly integrating diverse data sources, improving spatial relationship handling, and optimizing computational processes. Experimental results show that the RCBEV-KAN model demonstrates superior performance across most detection categories, achieving higher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score (0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8% faster). These results indicate that RCBEV-KAN is 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#22810;&#35270;&#22270;&#22270;&#20687;&#20013;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#20808;&#39564;&#26469;&#25913;&#36827;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#25216;&#26415;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#36974;&#25377;&#21644;&#38750;&#29702;&#24819;&#28459;&#21453;&#23556;&#34920;&#38754;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02079</link><description>&lt;p&gt;
Improving Neural Surface Reconstruction with Feature Priors from Multi-View Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#22810;&#35270;&#22270;&#22270;&#20687;&#20013;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#20808;&#39564;&#26469;&#25913;&#36827;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#25216;&#26415;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#36974;&#25377;&#21644;&#38750;&#29702;&#24819;&#28459;&#21453;&#23556;&#34920;&#38754;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02079v1 Announce Type: new  Abstract: Recent advancements in Neural Surface Reconstruction (NSR) have significantly improved multi-view reconstruction when coupled with volume rendering. However, relying solely on photometric consistency in image space falls short of addressing complexities posed by real-world data, including occlusions and non-Lambertian surfaces. To tackle these challenges, we propose an investigation into feature-level consistent loss, aiming to harness valuable feature priors from diverse pretext visual tasks and overcome current limitations. It is crucial to note the existing gap in determining the most effective pretext visual task for enhancing NSR. In this study, we comprehensively explore multi-view feature priors from seven pretext visual tasks, comprising thirteen methods. Our main goal is to strengthen NSR training by considering a wide range of possibilities. Additionally, we examine the impact of varying feature resolutions and evaluate both pi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDFaceNet&#30340;&#22522;&#20110;&#28508;&#20239;&#25193;&#25955;&#27169;&#22411;&#30340;&#33080;&#37096;&#26367;&#25442;&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#20998;&#21106;&#21644;&#38754;&#37096;&#35782;&#21035;&#27169;&#22359;&#36827;&#34892;&#26465;&#20214;&#21435;&#22122;&#36807;&#31243;&#65292;&#24182;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#21033;&#29992;&#29420;&#29305;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#26041;&#21521;&#24615;&#24341;&#23548;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#20266;&#36896;&#29983;&#25104;&#30340;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02078</link><description>&lt;p&gt;
LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDFaceNet&#30340;&#22522;&#20110;&#28508;&#20239;&#25193;&#25955;&#27169;&#22411;&#30340;&#33080;&#37096;&#26367;&#25442;&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#20998;&#21106;&#21644;&#38754;&#37096;&#35782;&#21035;&#27169;&#22359;&#36827;&#34892;&#26465;&#20214;&#21435;&#22122;&#36807;&#31243;&#65292;&#24182;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#21033;&#29992;&#29420;&#29305;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#26041;&#21521;&#24615;&#24341;&#23548;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#20266;&#36896;&#29983;&#25104;&#30340;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02078v1 Announce Type: new  Abstract: Over the past decade, there has been tremendous progress in the domain of synthetic media generation. This is mainly due to the powerful methods based on generative adversarial networks (GANs). Very recently, diffusion probabilistic models, which are inspired by non-equilibrium thermodynamics, have taken the spotlight. In the realm of image generation, diffusion models (DMs) have exhibited remarkable proficiency in producing both realistic and heterogeneous imagery through their stochastic sampling procedure. This paper proposes a novel facial swapping module, termed as LDFaceNet (Latent Diffusion based Face Swapping Network), which is based on a guided latent diffusion model that utilizes facial segmentation and facial recognition modules for a conditioned denoising process. The model employs a unique loss function to offer directional guidance to the diffusion process. Notably, LDFaceNet can incorporate supplementary facial guidance fo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#23398;&#20064;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;FDiff-Fusion&#65292;&#25972;&#21512;&#20102;&#20256;&#32479;&#30340;U-Net&#32593;&#32476;&#21644;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20197;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#22270;&#20687;&#20687;&#32032;&#32423;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02075</link><description>&lt;p&gt;
FDiff-Fusion:Denoising diffusion fusion network based on fuzzy learning for 3D medical image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#23398;&#20064;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;FDiff-Fusion&#65292;&#25972;&#21512;&#20102;&#20256;&#32479;&#30340;U-Net&#32593;&#32476;&#21644;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20197;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#22270;&#20687;&#20687;&#32032;&#32423;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02075v1 Announce Type: new  Abstract: In recent years, the denoising diffusion model has achieved remarkable success in image segmentation modeling. With its powerful nonlinear modeling capabilities and superior generalization performance, denoising diffusion models have gradually been applied to medical image segmentation tasks, bringing new perspectives and methods to this field. However, existing methods overlook the uncertainty of segmentation boundaries and the fuzziness of regions, resulting in the instability and inaccuracy of the segmentation results. To solve this problem, a denoising diffusion fusion network based on fuzzy learning for 3D medical image segmentation (FDiff-Fusion) is proposed in this paper. By integrating the denoising diffusion model into the classical U-Net network, this model can effectively extract rich semantic information from input medical images, thus providing excellent pixel-level representation for medical image segmentation. ... Finally,
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-GAN&#65289;&#19982;&#22534;&#21472;&#26102;&#29627;&#29827;&#32593;&#32476;&#65288;SHGN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#20998;&#21106;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#22686;&#24378;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22797;&#26434;&#25104;&#20687;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;L1&#21644;L2&#37325;&#24314;&#25439;&#22833;&#65292;&#24182;&#36741;&#20197;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20197;&#22312;&#20869;&#22312;&#34880;&#31649;&#36229;&#22768;&#65288;IVUS&#65289;&#25104;&#20687;&#20013;&#31934;&#32454;&#21270;&#20998;&#21106;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#22914;&#32452;&#32455;&#36793;&#30028;&#21644;&#34880;&#31649;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20381;&#36182;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#35813;&#31639;&#27861;&#22312;&#26631;&#20934;&#21307;&#23398;&#22270;&#20687;&#24211;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02074</link><description>&lt;p&gt;
Applying Conditional Generative Adversarial Networks for Imaging Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-GAN&#65289;&#19982;&#22534;&#21472;&#26102;&#29627;&#29827;&#32593;&#32476;&#65288;SHGN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#20998;&#21106;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#22686;&#24378;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22797;&#26434;&#25104;&#20687;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;L1&#21644;L2&#37325;&#24314;&#25439;&#22833;&#65292;&#24182;&#36741;&#20197;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20197;&#22312;&#20869;&#22312;&#34880;&#31649;&#36229;&#22768;&#65288;IVUS&#65289;&#25104;&#20687;&#20013;&#31934;&#32454;&#21270;&#20998;&#21106;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#22914;&#32452;&#32455;&#36793;&#30028;&#21644;&#34880;&#31649;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20381;&#36182;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#35813;&#31639;&#27861;&#22312;&#26631;&#20934;&#21307;&#23398;&#22270;&#20687;&#24211;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02074v1 Announce Type: cross  Abstract: This study introduces an innovative application of Conditional Generative Adversarial Networks (C-GAN) integrated with Stacked Hourglass Networks (SHGN) aimed at enhancing image segmentation, particularly in the challenging environment of medical imaging. We address the problem of overfitting, common in deep learning models applied to complex imaging datasets, by augmenting data through rotation and scaling. A hybrid loss function combining L1 and L2 reconstruction losses, enriched with adversarial training, is introduced to refine segmentation processes in intravascular ultrasound (IVUS) imaging. Our approach is unique in its capacity to accurately delineate distinct regions within medical images, such as tissue boundaries and vascular structures, without extensive reliance on domain-specific knowledge. The algorithm was evaluated using a standard medical image library, showing superior performance metrics compared to existing methods
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#37319;&#29992;&#19968;&#31181;&#32467;&#21512;CNN-Transformer&#21644;&#33014;&#22218;&#32593;&#32476;&#30340;&#26696;&#20363;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20799;&#31461;&#21457;&#32946;&#36831;&#32531;&#36827;&#34892;&#26089;&#26399;&#31579;&#26597;&#65292;&#20197;&#26399;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#26089;&#26399;&#24178;&#39044;&#23558;&#26174;&#33879;&#20943;&#23569;&#21307;&#30103;&#36164;&#28304;&#28010;&#36153;&#21644;&#31038;&#20250;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.02073</link><description>&lt;p&gt;
Case-based reasoning approach for diagnostic screening of children with developmental delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#37319;&#29992;&#19968;&#31181;&#32467;&#21512;CNN-Transformer&#21644;&#33014;&#22218;&#32593;&#32476;&#30340;&#26696;&#20363;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20799;&#31461;&#21457;&#32946;&#36831;&#32531;&#36827;&#34892;&#26089;&#26399;&#31579;&#26597;&#65292;&#20197;&#26399;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#26089;&#26399;&#24178;&#39044;&#23558;&#26174;&#33879;&#20943;&#23569;&#21307;&#30103;&#36164;&#28304;&#28010;&#36153;&#21644;&#31038;&#20250;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02073v1 Announce Type: new  Abstract: According to the World Health Organization, the population of children with developmental delays constitutes approximately 6% to 9% of the total population. Based on the number of newborns in Huaibei, Anhui Province, China, in 2023 (94,420), it is estimated that there are about 7,500 cases (suspected cases of developmental delays) of suspicious cases annually. Early identification and appropriate early intervention for these children can significantly reduce the wastage of medical resources and societal costs. International research indicates that the optimal period for intervention in children with developmental delays is before the age of six, with the golden treatment period being before three and a half years of age. Studies have shown that children with developmental delays who receive early intervention exhibit significant improvement in symptoms; some may even fully recover. This research adopts a hybrid model combining a CNN-Tran
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20572;&#36710;&#22330;&#20840;&#31243;&#35268;&#21010;&#31995;&#32479;&#65288;ParkingE2E&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#22270;&#20687;&#21040;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23436;&#25104;&#22797;&#26434;&#20572;&#36710;&#22330;&#26223;&#19979;&#30340;&#20219;&#21153;&#65292;&#20026;&#26234;&#33021;&#39550;&#39542;&#36710;&#36742;&#30340;&#33258;&#20027;&#27850;&#36710;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02061</link><description>&lt;p&gt;
ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20572;&#36710;&#22330;&#20840;&#31243;&#35268;&#21010;&#31995;&#32479;&#65288;ParkingE2E&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#22270;&#20687;&#21040;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23436;&#25104;&#22797;&#26434;&#20572;&#36710;&#22330;&#26223;&#19979;&#30340;&#20219;&#21153;&#65292;&#20026;&#26234;&#33021;&#39550;&#39542;&#36710;&#36742;&#30340;&#33258;&#20027;&#27850;&#36710;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02061v1 Announce Type: cross  Abstract: Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conducted extensive experiments in real-world scenarios, and the resul
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#23567;&#24517;&#35201;&#26080;&#22122;&#38899;&#27493;&#39588;&#65292;&#20174;&#32780;&#26174;&#33879;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#24182;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02054</link><description>&lt;p&gt;
Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02054
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#23567;&#24517;&#35201;&#26080;&#22122;&#38899;&#27493;&#39588;&#65292;&#20174;&#32780;&#26174;&#33879;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#24182;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02054v1 Announce Type: new  Abstract: In this paper, we introduce an innovative NLP model specifically fine-tuned to determine the minimal number of denoising steps required for any given text prompt. This advanced model serves as a real-time tool that recommends the ideal denoise steps for generating high-quality images efficiently. It is designed to work seamlessly with the Diffusion model, ensuring that images are produced with superior quality in the shortest possible time. Although our explanation focuses on the DDIM scheduler, the methodology is adaptable and can be applied to various other schedulers like Euler, Euler Ancestral, Heun, DPM2 Karras, UniPC, and more. This model allows our customers to conserve costly computing resources by executing the fewest necessary denoising steps to achieve optimal quality in the produced images.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PanicleNeRF&#30340;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#27700;&#31291;&#31319;&#29366;&#33457;&#24207;&#30000;&#38388;&#34920;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#29031;&#29255;&#21363;&#21487;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22823;&#22411;&#27169;&#22411;Segment Anything Model&#65288;SAM&#65289;&#21644;&#23567;&#22411;&#27169;&#22411;You Only Look Once version 8&#65288;YOLOv8&#65289;&#23545;&#27700;&#31291;&#31319;&#29366;&#33457;&#24207;&#22270;&#20687;&#36827;&#34892;&#39640;&#31934;&#24230;&#20998;&#21106;&#65292;&#24182;&#29992;NeRF&#25216;&#26415;&#36827;&#34892;3D&#37325;&#24314;&#65292;&#26368;&#32456;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#25552;&#21462;&#31319;&#29366;&#33457;&#24207;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PanicleNeRF&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;2D&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#39044;&#27979;&#20934;&#30830;&#29575;&#19982;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#21563;&#21512;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02053</link><description>&lt;p&gt;
PanicleNeRF: low-cost, high-precision in-field phenotypingof rice panicles with smartphone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02053
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PanicleNeRF&#30340;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#27700;&#31291;&#31319;&#29366;&#33457;&#24207;&#30000;&#38388;&#34920;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#29031;&#29255;&#21363;&#21487;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22823;&#22411;&#27169;&#22411;Segment Anything Model&#65288;SAM&#65289;&#21644;&#23567;&#22411;&#27169;&#22411;You Only Look Once version 8&#65288;YOLOv8&#65289;&#23545;&#27700;&#31291;&#31319;&#29366;&#33457;&#24207;&#22270;&#20687;&#36827;&#34892;&#39640;&#31934;&#24230;&#20998;&#21106;&#65292;&#24182;&#29992;NeRF&#25216;&#26415;&#36827;&#34892;3D&#37325;&#24314;&#65292;&#26368;&#32456;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#25552;&#21462;&#31319;&#29366;&#33457;&#24207;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PanicleNeRF&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;2D&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#39044;&#27979;&#20934;&#30830;&#29575;&#19982;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#21563;&#21512;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02053v1 Announce Type: new  Abstract: The rice panicle traits significantly influence grain yield, making them a primary target for rice phenotyping studies. However, most existing techniques are limited to controlled indoor environments and difficult to capture the rice panicle traits under natural growth conditions. Here, we developed PanicleNeRF, a novel method that enables high-precision and low-cost reconstruction of rice panicle three-dimensional (3D) models in the field using smartphone. The proposed method combined the large model Segment Anything Model (SAM) and the small model You Only Look Once version 8 (YOLOv8) to achieve high-precision segmentation of rice panicle images. The NeRF technique was then employed for 3D reconstruction using the images with 2D segmentation. Finally, the resulting point clouds are processed to successfully extract panicle traits. The results show that PanicleNeRF effectively addressed the 2D image segmentation task, achieving a mean F
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Enhanced Outlier Logit&#65288;EOL&#65289;&#30340; novel transductive inference technique&#65292;&#36890;&#36807;InfoMax&#21407;&#29702;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#30693;&#31867;&#21035;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#22411;&#30340;&#26657;&#20934;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#27491;&#20363;&#21644;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2408.02052</link><description>&lt;p&gt;
EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier Logits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Enhanced Outlier Logit&#65288;EOL&#65289;&#30340; novel transductive inference technique&#65292;&#36890;&#36807;InfoMax&#21407;&#29702;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#30693;&#31867;&#21035;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#22411;&#30340;&#26657;&#20934;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#27491;&#20363;&#21644;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02052v1 Announce Type: new  Abstract: In Few-Shot Learning (FSL), models are trained to recognise unseen objects from a query set, given a few labelled examples from a support set. In standard FSL, models are evaluated on query instances sampled from the same class distribution of the support set. In this work, we explore the more nuanced and practical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard FSL, OSFSL incorporates unknown classes into the query set, thereby requiring the model not only to classify known classes but also to identify outliers. Building on the groundwork laid by previous studies, we define a novel transductive inference technique that leverages the InfoMax principle to exploit the unlabelled query set. We called our approach the Enhanced Outlier Logit (EOL) method. EOL refines class prototype representations through model calibration, effectively balancing the inlier-outlier ratio. This calibration enhances pseudo-label accuracy for
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HVTrack&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28857; clouds&#20855;&#26377;&#39640;&#26102;&#21464;&#25968;&#25454;&#20013;&#30340;3D&#21333;&#23545;&#35937;&#36319;&#36394;&#65292;&#36890;&#36807;&#19977;&#20010;&#26032;&#22411;&#32452;&#20214;&#24212;&#23545;&#39640;&#26102;&#21464;&#30340;&#25361;&#25112;&#65306;&#30456;&#23545;&#23039;&#24577;&#20851;&#27880;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#26102;&#21464;&#25968;&#25454;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02049</link><description>&lt;p&gt;
3D Single-object Tracking in Point Clouds with High Temporal Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HVTrack&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28857; clouds&#20855;&#26377;&#39640;&#26102;&#21464;&#25968;&#25454;&#20013;&#30340;3D&#21333;&#23545;&#35937;&#36319;&#36394;&#65292;&#36890;&#36807;&#19977;&#20010;&#26032;&#22411;&#32452;&#20214;&#24212;&#23545;&#39640;&#26102;&#21464;&#30340;&#25361;&#25112;&#65306;&#30456;&#23545;&#23039;&#24577;&#20851;&#27880;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#26102;&#21464;&#25968;&#25454;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02049v1 Announce Type: new  Abstract: The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#23618;&#39057;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#36229;&#22768;&#25104;&#20687;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#30340;Transformer&#29305;&#24449;&#21644;&#35889;&#22270;&#29702;&#35770;&#65292;&#33021;&#22815;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26356;&#21152;&#30452;&#35266;&#21644;&#26131;&#20110;&#29702;&#35299;&#30340;&#32452;&#32455;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2408.02043</link><description>&lt;p&gt;
Deep Spectral Methods for Unsupervised Ultrasound Image Interpretation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#23618;&#39057;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#36229;&#22768;&#25104;&#20687;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#30340;Transformer&#29305;&#24449;&#21644;&#35889;&#22270;&#29702;&#35770;&#65292;&#33021;&#22815;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26356;&#21152;&#30452;&#35266;&#21644;&#26131;&#20110;&#29702;&#35299;&#30340;&#32452;&#32455;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02043v1 Announce Type: new  Abstract: Ultrasound imaging is challenging to interpret due to non-uniform intensities, low contrast, and inherent artifacts, necessitating extensive training for non-specialists. Advanced representation with clear tissue structure separation could greatly assist clinicians in mapping underlying anatomy and distinguishing between tissue layers. Decomposing an image into semantically meaningful segments is mainly achieved using supervised segmentation algorithms. Unsupervised methods are beneficial, as acquiring large labeled datasets is difficult and costly, but despite their advantages, they still need to be explored in ultrasound. This paper proposes a novel unsupervised deep learning strategy tailored to ultrasound to obtain easily interpretable tissue separations. We integrate key concepts from unsupervised deep spectral methods, which combine spectral graph theory with deep learning methods. We utilize self-supervised transformer features fo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20687;&#32032;&#32423;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#21450;&#22810;&#22836;&#22495;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#23545;&#20110;&#39046;&#22495;&#26080;&#20851;&#30340;&#20687;&#32032;&#32423;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#30446;&#26631;&#21306;&#22495;&#28608;&#27963;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#31934;&#24230;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02039</link><description>&lt;p&gt;
Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20687;&#32032;&#32423;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#21450;&#22810;&#22836;&#22495;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#23545;&#20110;&#39046;&#22495;&#26080;&#20851;&#30340;&#20687;&#32032;&#32423;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#30446;&#26631;&#21306;&#22495;&#28608;&#27963;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#31934;&#24230;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02039v1 Announce Type: new  Abstract: Recent attention has been devoted to the pursuit of learning semantic segmentation models exclusively from image tags, a paradigm known as image-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts adopt the Class Activation Maps (CAMs) as priors to mine object regions yet observe the imbalanced activation issue, where only the most discriminative object parts are located. In this paper, we argue that the distribution discrepancy between the discriminative and the non-discriminative parts of objects prevents the model from producing complete and precise pseudo masks as ground truths. For this purpose, we propose a Pixel-Level Domain Adaptation (PLDA) method to encourage the model in learning pixel-wise domain-invariant features. Specifically, a multi-head domain classifier trained adversarially with the feature extraction is introduced to promote the emergence of pixel features that are invariant with respect to the sh
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEGO&#30340;&#23616;&#37096;&#26174;&#24335;&#21644;&#20840;&#23616;&#39034;&#24207;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#25968;&#25454;&#38598;&#20998;&#24067;&#24046;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02036</link><description>&lt;p&gt;
LEGO: Self-Supervised Representation Learning for Scene Text Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEGO&#30340;&#23616;&#37096;&#26174;&#24335;&#21644;&#20840;&#23616;&#39034;&#24207;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#25968;&#25454;&#38598;&#20998;&#24067;&#24046;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02036v1 Announce Type: new  Abstract: In recent years, significant progress has been made in scene text recognition by data-driven methods. However, due to the scarcity of annotated real-world data, the training of these methods predominantly relies on synthetic data. The distribution gap between synthetic and real data constrains the further performance improvement of these methods in real-world applications. To tackle this problem, a highly promising approach is to utilize massive amounts of unlabeled real data for self-supervised training, which has been widely proven effective in many NLP and CV tasks. Nevertheless, generic self-supervised methods are unsuitable for scene text images due to their sequential nature. To address this issue, we propose a Local Explicit and Global Order-aware self-supervised representation learning method (LEGO) that accounts for the characteristics of scene text images. Inspired by the human cognitive process of learning words, which involve
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mini-Monkey&#30340;&#36731;&#37327;&#32423;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#35009;&#21098;&#31574;&#30053;&#65288;MSAC&#65289;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#23545;&#29289;&#20307;&#20998;&#21106;&#30340;&#38382;&#39064;&#12290;MSAC&#33021;&#22815;&#20174;&#19981;&#21516;&#23610;&#24230;&#20013;&#36873;&#25321;&#38750;&#20998;&#21106;&#30340;&#29289;&#20307;&#65292;&#32780;Scale Compression Mechanism&#65288;SCM&#65289;&#21017;&#26377;&#25928;&#20943;&#23569;&#20102;MSAC&#24341;&#20837;&#30340;&#35745;&#31639; overhead&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#23545;&#23567;&#29289;&#20307;&#25110;&#38750;&#35268;&#21017;&#24418;&#29366;&#29289;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02034</link><description>&lt;p&gt;
Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive Cropping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mini-Monkey&#30340;&#36731;&#37327;&#32423;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#35009;&#21098;&#31574;&#30053;&#65288;MSAC&#65289;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#23545;&#29289;&#20307;&#20998;&#21106;&#30340;&#38382;&#39064;&#12290;MSAC&#33021;&#22815;&#20174;&#19981;&#21516;&#23610;&#24230;&#20013;&#36873;&#25321;&#38750;&#20998;&#21106;&#30340;&#29289;&#20307;&#65292;&#32780;Scale Compression Mechanism&#65288;SCM&#65289;&#21017;&#26377;&#25928;&#20943;&#23569;&#20102;MSAC&#24341;&#20837;&#30340;&#35745;&#31639; overhead&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#23545;&#23567;&#29289;&#20307;&#25110;&#38750;&#35268;&#21017;&#24418;&#29366;&#29289;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02034v1 Announce Type: new  Abstract: Recently, there has been significant interest in enhancing the capability of multimodal large language models (MLLMs) to process high-resolution images. Most existing methods focus on adopting a cropping strategy to improve the ability of multimodal large language models to understand image details. However, this cropping operation inevitably causes the segmentation of objects and connected areas, which impairs the MLLM's ability to recognize small or irregularly shaped objects or text. This issue is particularly evident in lightweight MLLMs. Addressing this issue, we propose Mini-Monkey, a lightweight MLLM that incorporates a plug-and-play method called multi-scale adaptive crop strategy (MSAC). Mini-Monkey adaptively generates multi-scale representations, allowing it to select non-segmented objects from various scales. To mitigate the computational overhead introduced by MSAC, we propose a Scale Compression Mechanism (SCM), which effec
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#35270;&#21548;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#21547;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#34892;&#21160;&#20316;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#31867;&#22411;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26041;&#27861;&#22312;&#30495;&#23454;&#20844;&#20849;&#22330;&#21512;&#27169;&#25311;&#22330;&#26223;&#20013;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#19968;&#20010;&#21253;&#21547;54&#20010;&#23454;&#38469;&#35270;&#39057;&#30340;&#22330;&#26223;&#20013;&#25104;&#21151;&#35782;&#21035;&#20102;&#26292;&#21147;&#21644;&#38750;&#26292;&#21147;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2408.02033</link><description>&lt;p&gt;
Enhancing Human Action Recognition and Violence Detection Through Deep Learning Audiovisual Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#35270;&#21548;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#21547;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#34892;&#21160;&#20316;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#31867;&#22411;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26041;&#27861;&#22312;&#30495;&#23454;&#20844;&#20849;&#22330;&#21512;&#27169;&#25311;&#22330;&#26223;&#20013;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#19968;&#20010;&#21253;&#21547;54&#20010;&#23454;&#38469;&#35270;&#39057;&#30340;&#22330;&#26223;&#20013;&#25104;&#21151;&#35782;&#21035;&#20102;&#26292;&#21147;&#21644;&#38750;&#26292;&#21147;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02033v1 Announce Type: new  Abstract: This paper proposes a hybrid fusion-based deep learning approach based on two different modalities, audio and video, to improve human activity recognition and violence detection in public places. To take advantage of audiovisual fusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning (HFBDL) are used and compared. Since the objective is to detect and recognize human violence in public places, Real-life violence situation (RLVS) dataset is expanded and used. Simulating results of HFBDL show 96.67\% accuracy on validation data, which is more accurate than the other state-of-the-art methods on this dataset. To showcase our model's ability in real-world scenarios, another dataset of 54 sounded videos of both violent and non-violent situations was recorded. The model could successfully detect 52 out of 54 videos correctly. The proposed method shows a promising performance on real scenarios. Thus, it can be used for hum
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Self-Introspective Decoding (SID)&#26041;&#27861;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#25105;&#23457;&#35270;&#65292;&#25104;&#21151;&#20943;&#36731;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#19981;&#31526;&#21512;&#30495;&#23454;&#35821;&#22659;&#30340;&#25991;&#26412;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#65289;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#25551;&#36848;&#20934;&#30830;&#24615;&#65292;&#20419;&#36827;&#20102;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;</title><link>https://arxiv.org/abs/2408.02032</link><description>&lt;p&gt;
Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Self-Introspective Decoding (SID)&#26041;&#27861;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#25105;&#23457;&#35270;&#65292;&#25104;&#21151;&#20943;&#36731;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#19981;&#31526;&#21512;&#30495;&#23454;&#35821;&#22659;&#30340;&#25991;&#26412;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#65289;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#25551;&#36848;&#20934;&#30830;&#24615;&#65292;&#20419;&#36827;&#20102;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02032v1 Announce Type: new  Abstract: While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the `hallucination' problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing auxiliary analysis networks, which inevitable incur additional costs. Another approach, known as contrastive decoding, induces hallucinations by manually disturbing the vision or instruction raw inputs and mitigates them by contrasting the outputs of the disturbed and original LVLMs. However, these approaches rely on empirical holistic input disturbances and double the inference cost. To avoid these issues, we propose a simple yet effective method named Self-Introspective Decoding (SID). Our empirical investigation reveals that pretrained LVLMs ca
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;EffiDiffAct&#65292;&#19968;&#20010;&#23545;&#35270;&#39057;&#20998;&#26512;&#20013;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#26377;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#20316;&#36793;&#30028;&#30340;&#30830;&#23450;&#24615;&#21644;&#22788;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38271;&#24207;&#21015;&#35270;&#39057;&#29305;&#24449;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23454;&#26102;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02024</link><description>&lt;p&gt;
Faster Diffusion Action Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;EffiDiffAct&#65292;&#19968;&#20010;&#23545;&#35270;&#39057;&#20998;&#26512;&#20013;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#26377;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#20316;&#36793;&#30028;&#30340;&#30830;&#23450;&#24615;&#21644;&#22788;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38271;&#24207;&#21015;&#35270;&#39057;&#29305;&#24449;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23454;&#26102;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02024v1 Announce Type: new  Abstract: Temporal Action Segmentation (TAS) is an essential task in video analysis, aiming to segment and classify continuous frames into distinct action segments. However, the ambiguous boundaries between actions pose a significant challenge for high-precision segmentation. Recent advances in diffusion models have demonstrated substantial success in TAS tasks due to their stable training process and high-quality generation capabilities. However, the heavy sampling steps required by diffusion models pose a substantial computational burden, limiting their practicality in real-time applications. Additionally, most related works utilize Transformer-based encoder architectures. Although these architectures excel at capturing long-range dependencies, they incur high computational costs and face feature-smoothing issues when processing long video sequences. To address these challenges, we propose EffiDiffAct, an efficient and high-performance TAS algor
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#20307;&#21270;&#22810;&#26102;&#38388;&#23610;&#24230;MRI&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;auto&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#22522;&#20110;&#24180;&#40836;&#12289;&#30142;&#30149;&#29366;&#24577;&#21644;&#21333;&#20010;&#24739;&#32773;&#21382;&#21490;&#25195;&#25551;&#25968;&#25454;&#30340;&#26410;&#26469;MRI&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#30340;&#36830;&#32493;&#25104;&#20687;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#37319;&#26679;&#20197;&#29983;&#25104;&#26410;&#26469;&#35299;&#21078;&#23398;&#21464;&#21270;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;10&#24180;&#21518;&#30340;MRI&#22270;&#20687;&#12290;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#25351;&#26631;&#26174;&#31034;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02018</link><description>&lt;p&gt;
Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#20307;&#21270;&#22810;&#26102;&#38388;&#23610;&#24230;MRI&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;auto&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#22522;&#20110;&#24180;&#40836;&#12289;&#30142;&#30149;&#29366;&#24577;&#21644;&#21333;&#20010;&#24739;&#32773;&#21382;&#21490;&#25195;&#25551;&#25968;&#25454;&#30340;&#26410;&#26469;MRI&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#30340;&#36830;&#32493;&#25104;&#20687;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#37319;&#26679;&#20197;&#29983;&#25104;&#26410;&#26469;&#35299;&#21078;&#23398;&#21464;&#21270;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;10&#24180;&#21518;&#30340;MRI&#22270;&#20687;&#12290;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#25351;&#26631;&#26174;&#31034;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02018v1 Announce Type: new  Abstract: Neurodegeneration as measured through magnetic resonance imaging (MRI) is recognized as a potential biomarker for diagnosing Alzheimer's disease (AD), but is generally considered less specific than amyloid or tau based biomarkers. Due to a large amount of variability in brain anatomy between different individuals, we hypothesize that leveraging MRI time series can help improve specificity, by treating each patient as their own baseline. Here we turn to conditional variational autoencoders to generate individualized MRI predictions given the subject's age, disease status and one previous scan. Using serial imaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a novel architecture to build a latent space distribution which can be sampled from to generate future predictions of changing anatomy. This enables us to extrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated the model on a held-out set fr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#34913;&#33258;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29305;&#24449;&#22349;&#32553;&#38382;&#39064;&#65292;&#24182;&#20026;&#22270;&#20687;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2408.02014</link><description>&lt;p&gt;
Unsupervised Representation Learning by Balanced Self Attention Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#34913;&#33258;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29305;&#24449;&#22349;&#32553;&#38382;&#39064;&#65292;&#24182;&#20026;&#22270;&#20687;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02014v1 Announce Type: new  Abstract: Many leading self-supervised methods for unsupervised representation learning, in particular those for embedding image features, are built on variants of the instance discrimination task, whose optimization is known to be prone to instabilities that can lead to feature collapse. Different techniques have been devised to circumvent this issue, including the use of negative pairs with different contrastive losses, the use of external memory banks, and breaking of symmetry by using separate encoding networks with possibly different structures. Our method, termed BAM, rather than directly matching features of different views (augmentations) of input images, is based on matching their self-attention vectors, which are the distributions of similarities to the entire set of augmented images of a batch. We obtain rich representations and avoid feature collapse by minimizing a loss that matches these distributions to their globally balanced and e
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#24555;&#36895;&#35780;&#20272;&#32925;&#22806;&#20260;&#24739;&#32773;&#30340;&#30149;&#24773;&#65292;&#26088;&#22312;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#20943;&#23569;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2408.02012</link><description>&lt;p&gt;
Decision Support System to triage of liver trauma
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02012
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#24555;&#36895;&#35780;&#20272;&#32925;&#22806;&#20260;&#24739;&#32773;&#30340;&#30149;&#24773;&#65292;&#26088;&#22312;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#20943;&#23569;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02012v1 Announce Type: cross  Abstract: Trauma significantly impacts global health, accounting for over 5 million deaths annually, which is comparable to mortality rates from diseases such as tuberculosis, AIDS, and malaria. In Iran, the financial repercussions of road traffic accidents represent approximately 2% of the nation's Gross National Product each year. Bleeding is the leading cause of mortality in trauma patients within the first 24 hours following an injury, making rapid diagnosis and assessment of severity crucial. Trauma patients require comprehensive scans of all organs, generating a large volume of data. Evaluating CT images for the entire body is time-consuming and requires significant expertise, underscoring the need for efficient time management in diagnosis. Efficient diagnostic processes can significantly reduce treatment costs and decrease the likelihood of secondary complications. In this context, the development of a reliable Decision Support System (D
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaCBM&#30340;&#36866;&#24212;&#24615;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#29575;&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#27010;&#24565;&#29942;&#39048;&#26694;&#26550;&#30340;&#20960;&#20309;&#34920;&#31034;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#20174;&#36890;&#29992;&#25968;&#25454;&#21040;&#29305;&#23450;&#35786;&#26029;&#20219;&#21153;&#30340;&#36716;&#31227;&#23398;&#20064;&#38656;&#27714;&#12290;&#36890;&#36807;&#36825;&#31687;&#25991;&#31456;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#30340;&#21442;&#25968;&#35843;&#25972;&#21644;&#25968;&#25454;&#36866;&#24212;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#30103;&#22270;&#20687;&#35786;&#26029;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.02001</link><description>&lt;p&gt;
AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaCBM&#30340;&#36866;&#24212;&#24615;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#29575;&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#27010;&#24565;&#29942;&#39048;&#26694;&#26550;&#30340;&#20960;&#20309;&#34920;&#31034;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#20174;&#36890;&#29992;&#25968;&#25454;&#21040;&#29305;&#23450;&#35786;&#26029;&#20219;&#21153;&#30340;&#36716;&#31227;&#23398;&#20064;&#38656;&#27714;&#12290;&#36890;&#36807;&#36825;&#31687;&#25991;&#31456;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#30340;&#21442;&#25968;&#35843;&#25972;&#21644;&#25968;&#25454;&#36866;&#24212;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#30103;&#22270;&#20687;&#35786;&#26029;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02001v1 Announce Type: new  Abstract: The integration of vision-language models such as CLIP and Concept Bottleneck Models (CBMs) offers a promising approach to explaining deep neural network (DNN) decisions using concepts understandable by humans, addressing the black-box concern of DNNs. While CLIP provides both explainability and zero-shot classification capability, its pre-training on generic image and text data may limit its classification accuracy and applicability to medical image diagnostic tasks, creating a transfer learning problem. To maintain explainability and address transfer learning needs, CBM methods commonly design post-processing modules after the bottleneck module. However, this way has been ineffective. This paper takes an unconventional approach by re-examining the CBM framework through the lens of its geometrical representation as a simple linear classification system. The analysis uncovers that post-CBM fine-tuning modules merely rescale and shift the
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;SAM&#21644;Detic&#25216;&#26415;&#21435;&#38500;&#32972;&#26223;&#22122;&#38899;&#65292;&#20165;&#20445;&#30041;&#30446;&#26631;&#29289;&#20307;&#30340;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#65292;&#20026;&#31934;&#32454;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#26500;&#24314;&#20102;&#21069;&#26223;-&#20165;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#27169;&#22411;&#23545;&#32972;&#26223;&#20449;&#24687;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#20026;&#25968;&#25454;&#38598;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#25299;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2408.01998</link><description>&lt;p&gt;
What Happens Without Background? Constructing Foreground-Only Data for Fine-Grained Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;SAM&#21644;Detic&#25216;&#26415;&#21435;&#38500;&#32972;&#26223;&#22122;&#38899;&#65292;&#20165;&#20445;&#30041;&#30446;&#26631;&#29289;&#20307;&#30340;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#65292;&#20026;&#31934;&#32454;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#26500;&#24314;&#20102;&#21069;&#26223;-&#20165;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#27169;&#22411;&#23545;&#32972;&#26223;&#20449;&#24687;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#20026;&#25968;&#25454;&#38598;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#25299;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01998v1 Announce Type: new  Abstract: Fine-grained recognition, a pivotal task in visual signal processing, aims to distinguish between similar subclasses based on discriminative information present in samples. However, prevailing methods often erroneously focus on background areas, neglecting the capture of genuinely effective discriminative information from the subject, thus impeding practical application. To facilitate research into the impact of background noise on models and enhance their ability to concentrate on the subject's discriminative features, we propose an engineered pipeline that leverages the capabilities of SAM and Detic to create fine-grained datasets with only foreground subjects, devoid of background. Extensive cross-experiments validate this approach as a preprocessing step prior to training, enhancing algorithmic performance and holding potential for further modal expansion of the data.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeMansia&#30340;&#26550;&#26500;&#65292;&#23427;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;token&#26631;&#35760;&#25216;&#26415;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#20256;&#32479;&#30340;transformer&#26550;&#26500;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01986</link><description>&lt;p&gt;
DeMansia: Mamba Never Forgets Any Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01986
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeMansia&#30340;&#26550;&#26500;&#65292;&#23427;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;token&#26631;&#35760;&#25216;&#26415;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#20256;&#32479;&#30340;transformer&#26550;&#26500;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01986v1 Announce Type: new  Abstract: This paper examines the mathematical foundations of transformer architectures, highlighting their limitations particularly in handling long sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM), and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia integrates state space models with token labeling techniques to enhance performance in image classification tasks, efficiently addressing the computational challenges posed by traditional transformers. The architecture, benchmark, and comparisons with contemporary models demonstrate DeMansia's effectiveness. The implementation of this paper is available on GitHub at https://github.com/catalpaaa/DeMansia
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26597;&#35810;&#22522;&#40657;&#30418;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;AdvQDet&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#27604;&#25552;&#23581;&#25216;&#24039;&#21160;&#24577;&#35843;&#25972;&#25552;&#31034;&#65292;&#26377;&#25928;&#25429;&#33719;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#22312;&#26816;&#27979;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#25928;&#29575;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01978</link><description>&lt;p&gt;
AdvQDet: Detecting Query-Based Adversarial Attacks with Adversarial Contrastive Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26597;&#35810;&#22522;&#40657;&#30418;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;AdvQDet&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#27604;&#25552;&#23581;&#25216;&#24039;&#21160;&#24577;&#35843;&#25972;&#25552;&#31034;&#65292;&#26377;&#25928;&#25429;&#33719;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#22312;&#26816;&#27979;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#25928;&#29575;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01978v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks even under a black-box setting where the adversary can only query the model. Particularly, query-based black-box adversarial attacks estimate adversarial gradients based on the returned probability vectors of the target model for a sequence of queries. During this process, the queries made to the target model are intermediate adversarial examples crafted at the previous attack step, which share high similarities in the pixel space. Motivated by this observation, stateful detection methods have been proposed to detect and reject query-based attacks. While demonstrating promising results, these methods either have been evaded by more advanced attacks or suffer from low efficiency in terms of the number of shots (queries) required to detect different attacks. Arguably, the key challenge here is to assign high similarity scores for any two intermediate adversarial 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Label Augmentation&#65288;LA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#26679;&#26412;&#21644;&#24120;&#35265;&#38169;&#35823;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#33021;&#21147;&#65292;&#20026;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#38477;&#20302;&#20102;&#35823;&#21028;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01977</link><description>&lt;p&gt;
Label Augmentation for Neural Networks Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Label Augmentation&#65288;LA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#26679;&#26412;&#21644;&#24120;&#35265;&#38169;&#35823;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#33021;&#21147;&#65292;&#20026;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#38477;&#20302;&#20102;&#35823;&#21028;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01977v1 Announce Type: new  Abstract: Out-of-distribution generalization can be categorized into two types: common perturbations arising from natural variations in the real world and adversarial perturbations that are intentionally crafted to deceive neural networks. While deep neural networks excel in accuracy under the assumption of identical distributions between training and test data, they often encounter out-of-distribution scenarios resulting in a significant decline in accuracy. Data augmentation methods can effectively enhance robustness against common corruptions, but they typically fall short in improving robustness against adversarial perturbations. In this study, we develop Label Augmentation (LA), which enhances robustness against both common and intentional perturbations and improves uncertainty estimation. Our findings indicate a Clean error rate improvement of up to 23.29% when employing LA in comparisons to the baseline. Additionally, it enhances robustness
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SSHD-Net&#30340;&#21333;&#19968;&#28857;&#30417;&#30563;&#39640;&#20998;&#36776;&#29575;&#21160;&#24577;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#28857;&#30417;&#30563;&#36798;&#21040;&#20102;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#36798;&#21040;&#30340;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;SSHD-Net&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#20132;&#21449;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65288;HCEM&#65289;&#21644;&#21160;&#24577;&#22352;&#26631;&#34701;&#21512;&#27169;&#22359;&#65288;DCFM&#65289;&#65292;&#23454;&#29616;&#20102;&#29983;&#29289;&#21521;&#29305;&#24449;&#20132;&#20114;&#21644;&#39640;&#20998;&#36776;&#29575;&#28145;&#23618;&#32418;&#22806;&#23567;&#30446;&#26631;&#20449;&#24687;&#30340;&#32500;&#25252;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01976</link><description>&lt;p&gt;
Single-Point Supervised High-Resolution Dynamic Network for Infrared Small Target Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01976
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SSHD-Net&#30340;&#21333;&#19968;&#28857;&#30417;&#30563;&#39640;&#20998;&#36776;&#29575;&#21160;&#24577;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#28857;&#30417;&#30563;&#36798;&#21040;&#20102;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#36798;&#21040;&#30340;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;SSHD-Net&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#20132;&#21449;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65288;HCEM&#65289;&#21644;&#21160;&#24577;&#22352;&#26631;&#34701;&#21512;&#27169;&#22359;&#65288;DCFM&#65289;&#65292;&#23454;&#29616;&#20102;&#29983;&#29289;&#21521;&#29305;&#24449;&#20132;&#20114;&#21644;&#39640;&#20998;&#36776;&#29575;&#28145;&#23618;&#32418;&#22806;&#23567;&#30446;&#26631;&#20449;&#24687;&#30340;&#32500;&#25252;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01976v1 Announce Type: new  Abstract: Infrared small target detection (IRSTD) tasks are extremely challenging for two main reasons: 1) it is difficult to obtain accurate labelling information that is critical to existing methods, and 2) infrared (IR) small target information is easily lost in deep networks. To address these issues, we propose a single-point supervised high-resolution dynamic network (SSHD-Net). In contrast to existing methods, we achieve state-of-the-art (SOTA) detection performance using only single-point supervision. Specifically, we first design a high-resolution cross-feature extraction module (HCEM), that achieves bi-directional feature interaction through stepped feature cascade channels (SFCC). It balances network depth and feature resolution to maintain deep IR small-target information. Secondly, the effective integration of global and local features is achieved through the dynamic coordinate fusion module (DCFM), which enhances the anti-interference
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SR-CIS&#30340;&#33258;&#25105;&#21453;&#24605;&#22686;&#37327;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#35760;&#24518;&#21644;&#25512;&#29702;&#30340;&#35299;&#32806;&#27169;&#22359;&#65292;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#30340;&#24555;&#36895;&#23398;&#20064;&#21644;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#31995;&#32479;&#30001;&#19968;&#20010;&#24555;&#36895;&#25512;&#29702;&#30340;&#23567;&#22411;&#27169;&#22359;&#21644;&#19968;&#20010;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#22411;&#27169;&#22359;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;Low-Rank Adaptive&#26426;&#21046;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#30340;&#30693;&#35782;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2408.01970</link><description>&lt;p&gt;
SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SR-CIS&#30340;&#33258;&#25105;&#21453;&#24605;&#22686;&#37327;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#35760;&#24518;&#21644;&#25512;&#29702;&#30340;&#35299;&#32806;&#27169;&#22359;&#65292;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#30340;&#24555;&#36895;&#23398;&#20064;&#21644;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#31995;&#32479;&#30001;&#19968;&#20010;&#24555;&#36895;&#25512;&#29702;&#30340;&#23567;&#22411;&#27169;&#22359;&#21644;&#19968;&#20010;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#22411;&#27169;&#22359;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;Low-Rank Adaptive&#26426;&#21046;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#30340;&#30693;&#35782;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnomalySD&#30340;&#22522;&#20110;Stable Diffusion&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#22810;&#31867;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#22823;&#37327;&#27491;&#24120;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#33021;&#22815;&#23545;&#19981;&#21516;&#23545;&#35937;&#28789;&#27963;&#36866;&#29992;&#30340;&#26816;&#27979;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2408.01960</link><description>&lt;p&gt;
AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnomalySD&#30340;&#22522;&#20110;Stable Diffusion&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#22810;&#31867;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#22823;&#37327;&#27491;&#24120;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#33021;&#22815;&#23545;&#19981;&#21516;&#23545;&#35937;&#28789;&#27963;&#36866;&#29992;&#30340;&#26816;&#27979;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01960v1 Announce Type: new  Abstract: Anomaly detection is a critical task in industrial manufacturing, aiming to identify defective parts of products. Most industrial anomaly detection methods assume the availability of sufficient normal data for training. This assumption may not hold true due to the cost of labeling or data privacy policies. Additionally, mainstream methods require training bespoke models for different objects, which incurs heavy costs and lacks flexibility in practice. To address these issues, we seek help from Stable Diffusion (SD) model due to its capability of zero/few-shot inpainting, which can be leveraged to inpaint anomalous regions as normal. In this paper, a few-shot multi-class anomaly detection framework that adopts Stable Diffusion model is proposed, named AnomalySD. To adapt SD to anomaly detection task, we design different hierarchical text descriptions and the foreground mask mechanism for fine-tuning SD. In the inference stage, to accurate
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#22823;&#26102;&#65292;&#26356;&#33021;&#21453;&#26144;&#31038;&#20250;&#20013;&#30340;&#20154;&#33080;&#35780;&#20215;&#20559;&#35265;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#31038;&#20250;&#20013;&#26222;&#36941;&#30340;&#20559;&#35265;&#31243;&#24230;&#19982;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#31243;&#24230;&#27491;&#30456;&#20851;&#12290;&#36825;&#34920;&#26126;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#23398;&#20064;&#20154;&#31867;&#8220;&#21487;&#35266;&#23519;&#8221;&#21644;&#8220;&#19981;&#21487;&#35266;&#23519;&#8221;&#29305;&#24449;&#35780;&#20215;&#26102;&#65292;&#26356;&#23481;&#26131;&#22797;&#21046;&#31038;&#20250;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2408.01959</link><description>&lt;p&gt;
Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#22823;&#26102;&#65292;&#26356;&#33021;&#21453;&#26144;&#31038;&#20250;&#20013;&#30340;&#20154;&#33080;&#35780;&#20215;&#20559;&#35265;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#31038;&#20250;&#20013;&#26222;&#36941;&#30340;&#20559;&#35265;&#31243;&#24230;&#19982;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#31243;&#24230;&#27491;&#30456;&#20851;&#12290;&#36825;&#34920;&#26126;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#23398;&#20064;&#20154;&#31867;&#8220;&#21487;&#35266;&#23519;&#8221;&#21644;&#8220;&#19981;&#21487;&#35266;&#23519;&#8221;&#29305;&#24449;&#35780;&#20215;&#26102;&#65292;&#26356;&#23481;&#26131;&#22797;&#21046;&#31038;&#20250;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EqvAfford&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20445;&#35777;&#28857;&#32423; affordance&#23398;&#20064;&#20013;&#30340;equivariance&#65292;&#20026;&#19979;&#28216;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#23039;&#24577;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01953</link><description>&lt;p&gt;
EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EqvAfford&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20445;&#35777;&#28857;&#32423; affordance&#23398;&#20064;&#20013;&#30340;equivariance&#65292;&#20026;&#19979;&#28216;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#23039;&#24577;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01953v1 Announce Type: new  Abstract: Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CACE-Net&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21452;&#21521;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#38899;&#39057;&#20449;&#21495;&#21644;&#35270;&#39057;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114; guided &#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#24577;&#38388;&#25968;&#25454;&#30340;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#22686;&#24378;&#20102;&#32972;&#26223;&#20107;&#20214;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#36776;&#30456;&#20284;&#22330;&#26223;&#20013;&#30340;&#32972;&#26223;&#20107;&#20214;&#12290;CACE-Net&#22312;&#38899;&#39057;&#35270;&#35273;&#20107;&#20214;&#23450;&#20301;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01952</link><description>&lt;p&gt;
CACE-Net: Co-guidance Attention and Contrastive Enhancement for Effective Audio-Visual Event Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CACE-Net&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21452;&#21521;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#38899;&#39057;&#20449;&#21495;&#21644;&#35270;&#39057;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114; guided &#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#24577;&#38388;&#25968;&#25454;&#30340;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#22686;&#24378;&#20102;&#32972;&#26223;&#20107;&#20214;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#36776;&#30456;&#20284;&#22330;&#26223;&#20013;&#30340;&#32972;&#26223;&#20107;&#20214;&#12290;CACE-Net&#22312;&#38899;&#39057;&#35270;&#35273;&#20107;&#20214;&#23450;&#20301;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01952v1 Announce Type: new  Abstract: The audio-visual event localization task requires identifying concurrent visual and auditory events from unconstrained videos within a network model, locating them, and classifying their category. The efficient extraction and integration of audio and visual modal information have always been challenging in this field. In this paper, we introduce CACE-Net, which differs from most existing methods that solely use audio signals to guide visual information. We propose an audio-visual co-guidance attention mechanism that allows for adaptive bi-directional cross-modal attentional guidance between audio and visual information, thus reducing inconsistencies between modalities. Moreover, we have observed that existing methods have difficulty distinguishing between similar background and event and lack the fine-grained features for event classification. Consequently, we employ background-event contrast enhancement to increase the discrimination of
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Masked Angle-Aware Autoencoder (MA3E)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#35282;&#24230;&#30340;&#26059;&#36716;&#22270;&#20687;&#35757;&#32451;&#22270;&#29255;&#65292;&#24182;&#20351;&#29992;Optimal Transport&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#37325;&#26500;&#30340;&#22270;&#20687;&#65292;&#26377;&#25928;&#23398;&#20064;&#20102;&#26059;&#36716;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#35299;&#20915;&#36828;&#31243; sensing&#22270;&#20687;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01946</link><description>&lt;p&gt;
Masked Angle-Aware Autoencoder for Remote Sensing Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Masked Angle-Aware Autoencoder (MA3E)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#35282;&#24230;&#30340;&#26059;&#36716;&#22270;&#20687;&#35757;&#32451;&#22270;&#29255;&#65292;&#24182;&#20351;&#29992;Optimal Transport&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#37325;&#26500;&#30340;&#22270;&#20687;&#65292;&#26377;&#25928;&#23398;&#20064;&#20102;&#26059;&#36716;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#35299;&#20915;&#36828;&#31243; sensing&#22270;&#20687;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01946v1 Announce Type: new  Abstract: To overcome the inherent domain gap between remote sensing (RS) images and natural images, some self-supervised representation learning methods have made promising progress. However, they have overlooked the diverse angles present in RS objects. This paper proposes the Masked Angle-Aware Autoencoder (MA3E) to perceive and learn angles during pre-training. We design a \textit{scaling center crop} operation to create the rotated crop with random orientation on each original image, introducing the explicit angle variation. MA3E inputs this composite image while reconstruct the original image, aiming to effectively learn rotation-invariant representations by restoring the angle variation introduced on the rotated crop. To avoid biases caused by directly reconstructing the rotated crop, we propose an Optimal Transport (OT) loss that automatically assigns similar original image patches to each rotated crop patch for reconstruction. MA3E demons
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#35266;&#27979;&#19981;&#30830;&#23450;&#24615;&#21516;&#26102;&#20272;&#35745;&#23039;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;PnP&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.01945</link><description>&lt;p&gt;
Generalized Maximum Likelihood Estimation for Perspective-n-Point Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#35266;&#27979;&#19981;&#30830;&#23450;&#24615;&#21516;&#26102;&#20272;&#35745;&#23039;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;PnP&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01945v1 Announce Type: cross  Abstract: The Perspective-n-Point (PnP) problem has been widely studied in the literature and applied in various vision-based pose estimation scenarios. However, existing methods ignore the anisotropy uncertainty of observations, as demonstrated in several real-world datasets in this paper. This oversight may lead to suboptimal and inaccurate estimation, particularly in the presence of noisy observations. To this end, we propose a generalized maximum likelihood PnP solver, named GMLPnP, that minimizes the determinant criterion by iterating the GLS procedure to estimate the pose and uncertainty simultaneously. Further, the proposed method is decoupled from the camera model. Results of synthetic and real experiments show that our method achieves better accuracy in common pose estimation scenarios, GMLPnP improves rotation/translation accuracy by 4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best baseline. It is more ac
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RobNODDI&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#36830;&#32493;&#34920;&#31034;&#23454;&#29616;&#20102;NODDI&#21442;&#25968;&#30340;&#40065;&#26834;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#39564;&#35777;&#20102;&#29616;&#26377;&#20027;&#27969;&#26041;&#27861;&#22312;&#27979;&#35797;&#25193;&#25955;&#26041;&#21521;&#19982;&#35757;&#32451;&#26041;&#21521;&#19981;&#21305;&#37197;&#26102;&#30340;&#21442;&#25968;&#20272;&#35745;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#40065;&#26834;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01944</link><description>&lt;p&gt;
RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under Continuous Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RobNODDI&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#36830;&#32493;&#34920;&#31034;&#23454;&#29616;&#20102;NODDI&#21442;&#25968;&#30340;&#40065;&#26834;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#39564;&#35777;&#20102;&#29616;&#26377;&#20027;&#27969;&#26041;&#27861;&#22312;&#27979;&#35797;&#25193;&#25955;&#26041;&#21521;&#19982;&#35757;&#32451;&#26041;&#21521;&#19981;&#21305;&#37197;&#26102;&#30340;&#21442;&#25968;&#20272;&#35745;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#40065;&#26834;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01944v1 Announce Type: new  Abstract: Neurite Orientation Dispersion and Density Imaging (NODDI) is an important imaging technology used to evaluate the microstructure of brain tissue, which is of great significance for the discovery and treatment of various neurological diseases. Current deep learning-based methods perform parameter estimation through diffusion magnetic resonance imaging (dMRI) with a small number of diffusion gradients. These methods speed up parameter estimation and improve accuracy. However, the diffusion directions used by most existing deep learning models during testing needs to be strictly consistent with the diffusion directions during training. This results in poor generalization and robustness of deep learning models in dMRI parameter estimation. In this work, we verify for the first time that the parameter estimation performance of current mainstream methods will significantly decrease when the testing diffusion directions and the training diffus
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#35270;&#35273; grounding &#33021;&#21147;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#25351;&#20196;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#36890;&#36807;&#35270;&#35273; grounding&#65292;&#20316;&#32773;&#33719;&#24471;&#20102;&#19968;&#20010;&#19982;&#25351;&#20196;&#20013;&#25351;&#31034;&#30340;&#30446;&#26631;&#29289;&#20307;&#30456;&#23545;&#24212;&#30340;&#23545;&#35937;&#23454;&#20307;&#32622;&#20449;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;VLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;RL&#20013;&#65306;&#19968;&#26159;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#22270;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35937;&#23450;&#21521;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#20195;&#29702;&#26397;&#21521;&#30446;&#26631;&#29289;&#20307;&#65307;&#20108;&#26159;&#20351;&#29992;&#32622;&#20449;&#22270;&#20026;&#20195;&#29702;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;&#35821;&#35328;&#23884;&#20837;&#26356;&#21152;&#32479;&#19968;&#21644;&#21487;&#35775;&#38382;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#23545;&#27604;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#21644;&#25351;&#20196;&#12290;</title><link>https://arxiv.org/abs/2408.01942</link><description>&lt;p&gt;
Visual Grounding for Object-Level Generalization in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01942
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#35270;&#35273; grounding &#33021;&#21147;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#25351;&#20196;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#36890;&#36807;&#35270;&#35273; grounding&#65292;&#20316;&#32773;&#33719;&#24471;&#20102;&#19968;&#20010;&#19982;&#25351;&#20196;&#20013;&#25351;&#31034;&#30340;&#30446;&#26631;&#29289;&#20307;&#30456;&#23545;&#24212;&#30340;&#23545;&#35937;&#23454;&#20307;&#32622;&#20449;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;VLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;RL&#20013;&#65306;&#19968;&#26159;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#22270;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35937;&#23450;&#21521;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#20195;&#29702;&#26397;&#21521;&#30446;&#26631;&#29289;&#20307;&#65307;&#20108;&#26159;&#20351;&#29992;&#32622;&#20449;&#22270;&#20026;&#20195;&#29702;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;&#35821;&#35328;&#23884;&#20837;&#26356;&#21152;&#32479;&#19968;&#21644;&#21487;&#35775;&#38382;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#23545;&#27604;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#21644;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01942v1 Announce Type: cross  Abstract: Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#31995;&#32479;&#35780;&#20272;&#20102;&#38024;&#23545;&#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#25918;&#28304;&#25915;&#20987;&#26041;&#27861;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#23545;&#25915;&#20987;&#25928;&#26524;&#21644;&#30456;&#24212;&#32531;&#35299;&#25514;&#26045;&#29702;&#35299;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25351;&#20986;&#20102;&#26410;&#26469;&#22312;&#20445;&#25252;&#33258;&#21160;&#21270;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#23433;&#20840;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#22823;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.01934</link><description>&lt;p&gt;
A Survey and Evaluation of Adversarial Attacks for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#31995;&#32479;&#35780;&#20272;&#20102;&#38024;&#23545;&#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#25918;&#28304;&#25915;&#20987;&#26041;&#27861;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#23545;&#25915;&#20987;&#25928;&#26524;&#21644;&#30456;&#24212;&#32531;&#35299;&#25514;&#26045;&#29702;&#35299;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25351;&#20986;&#20102;&#26410;&#26469;&#22312;&#20445;&#25252;&#33258;&#21160;&#21270;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#23433;&#20840;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#22823;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01934v1 Announce Type: new  Abstract: Deep learning models excel in various computer vision tasks but are susceptible to adversarial examples-subtle perturbations in input data that lead to incorrect predictions. This vulnerability poses significant risks in safety-critical applications such as autonomous vehicles, security surveillance, and aircraft health monitoring. While numerous surveys focus on adversarial attacks in image classification, the literature on such attacks in object detection is limited. This paper offers a comprehensive taxonomy of adversarial attacks specific to object detection, reviews existing adversarial robustness evaluation metrics, and systematically assesses open-source attack methods and model robustness. Key observations are provided to enhance the understanding of attack effectiveness and corresponding countermeasures. Additionally, we identify crucial research challenges to guide future efforts in securing automated object detection systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#25918;&#22823;&#20449;&#24687;&#30340;&#22788;&#29702;&#65292;&#20197;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#20013;&#35299;&#20915;&#20083;&#33146;&#30284;H&amp;E&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#35780;&#20272;HER2&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#25918;&#22823;&#20449;&#24687;&#22788;&#29702;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25991;&#31456;&#30340;&#21019;&#26032;&#22312;&#20110;&#25552;&#39640;&#20102;&#20174;H&amp;E&#26579;&#33394;&#21040;IHC&#26579;&#33394;&#32763;&#35793;&#20219;&#21153;&#30340;&#31934;&#30830;&#24230;&#65292;&#36825;&#23545;&#20110;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#20083;&#33146;&#30284;&#27835;&#30103;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.01929</link><description>&lt;p&gt;
Advancing H&amp;E-to-IHC Stain Translation in Breast Cancer: A Multi-Magnification and Attention-Based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#25918;&#22823;&#20449;&#24687;&#30340;&#22788;&#29702;&#65292;&#20197;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#20013;&#35299;&#20915;&#20083;&#33146;&#30284;H&amp;E&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#35780;&#20272;HER2&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#25918;&#22823;&#20449;&#24687;&#22788;&#29702;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25991;&#31456;&#30340;&#21019;&#26032;&#22312;&#20110;&#25552;&#39640;&#20102;&#20174;H&amp;E&#26579;&#33394;&#21040;IHC&#26579;&#33394;&#32763;&#35793;&#20219;&#21153;&#30340;&#31934;&#30830;&#24230;&#65292;&#36825;&#23545;&#20110;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#20083;&#33146;&#30284;&#27835;&#30103;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01929v1 Announce Type: cross  Abstract: Breast cancer presents a significant healthcare challenge globally, demanding precise diagnostics and effective treatment strategies, where histopathological examination of Hematoxylin and Eosin (H&amp;E) stained tissue sections plays a central role. Despite its importance, evaluating specific biomarkers like Human Epidermal Growth Factor Receptor 2 (HER2) for personalized treatment remains constrained by the resource-intensive nature of Immunohistochemistry (IHC). Recent strides in deep learning, particularly in image-to-image translation, offer promise in synthesizing IHC-HER2 slides from H\&amp;E stained slides. However, existing methodologies encounter challenges, including managing multiple magnifications in pathology images and insufficient focus on crucial information during translation. To address these issues, we propose a novel model integrating attention mechanisms and multi-magnification information processing. Our model employs a 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#28508;&#29305;&#24449;&#20998;&#24067;&#20248;&#21270;&#30340;&#22270;&#20687; clustering &#31639;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102; clustering &#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01920</link><description>&lt;p&gt;
Self-Supervised Pretrained Models and Latent Feature Distribution Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#28508;&#29305;&#24449;&#20998;&#24067;&#20248;&#21270;&#30340;&#22270;&#20687; clustering &#31639;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102; clustering &#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01920v1 Announce Type: cross  Abstract: In the face of complex natural images, existing deep clustering algorithms fall significantly short in terms of clustering accuracy when compared to supervised classification methods, making them less practical. This paper introduces an image clustering algorithm based on self-supervised pretrained models and latent feature distribution optimization, substantially enhancing clustering performance. It is found that: (1) For complex natural images, we effectively enhance the discriminative power of latent features by leveraging self-supervised pretrained models and their fine-tuning, resulting in improved clustering performance. (2) In the latent feature space, by searching for k-nearest neighbor images for each training sample and shortening the distance between the training sample and its nearest neighbor, the discriminative power of latent features can be further enhanced, and clustering performance can be improved. (3) In the latent 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;CAF-YOLO&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21367;&#31215;&#25805;&#20316;&#65292;&#22686;&#24378;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20013;&#35782;&#21035;&#23567;&#35268;&#27169;&#30149;&#28790;&#30340;&#33021;&#21147;&#12290;CAF-YOLO&#20462;&#25913;&#20102;YOLOv8&#65292;&#20351;&#29992;ACFM&#27169;&#22359;&#35299;&#20915;&#20102;&#20256;&#32479;&#21367;&#31215;&#32593;&#32476;&#38590;&#20197;&#22788;&#29702;&#36828;&#36317;&#31163;&#20449;&#24687;&#20132;&#20114;&#30340;&#32570;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#30340;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#34880;&#28082;&#21644;&#32954;&#30149;&#30149;&#29702;&#20013;&#30340;&#24494;&#23567;&#30149;&#21464;&#26377;&#20934;&#30830;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01897</link><description>&lt;p&gt;
CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in Biomedical Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;CAF-YOLO&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21367;&#31215;&#25805;&#20316;&#65292;&#22686;&#24378;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20013;&#35782;&#21035;&#23567;&#35268;&#27169;&#30149;&#28790;&#30340;&#33021;&#21147;&#12290;CAF-YOLO&#20462;&#25913;&#20102;YOLOv8&#65292;&#20351;&#29992;ACFM&#27169;&#22359;&#35299;&#20915;&#20102;&#20256;&#32479;&#21367;&#31215;&#32593;&#32476;&#38590;&#20197;&#22788;&#29702;&#36828;&#36317;&#31163;&#20449;&#24687;&#20132;&#20114;&#30340;&#32570;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#30340;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#34880;&#28082;&#21644;&#32954;&#30149;&#30149;&#29702;&#20013;&#30340;&#24494;&#23567;&#30149;&#21464;&#26377;&#20934;&#30830;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01897v1 Announce Type: new  Abstract: Object detection is of paramount importance in biomedical image analysis, particularly for lesion identification. While current methodologies are proficient in identifying and pinpointing lesions, they often lack the precision needed to detect minute biomedical entities (e.g., abnormal cells, lung nodules smaller than 3 mm), which are critical in blood and lung pathology. To address this challenge, we propose CAF-YOLO, based on the YOLOv8 architecture, a nimble yet robust method for medical object detection that leverages the strengths of convolutional neural networks (CNNs) and transformers. To overcome the limitation of convolutional kernels, which have a constrained capacity to interact with distant information, we introduce an attention and convolution fusion module (ACFM). This module enhances the modeling of both global and local features, enabling the capture of long-term feature dependencies and spatial autocorrelation. Additiona
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#30340;&#36741;&#21161;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;&#33394;&#30450;&#24739;&#32773;&#36890;&#36807;&#25968;&#23383;&#21270;&#25163;&#27573;&#35782;&#21035;&#21644;&#21629;&#21517;&#21407;&#26412;&#38590;&#20197;&#21306;&#20998;&#30340;&#39068;&#33394;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;AR&#30028;&#38754;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#28369;&#21160;&#25805;&#20316;&#26469;&#25913;&#21464;&#39068;&#33394;&#24863;&#30693;&#65292;&#20174;&#32780;&#22312;&#26356;&#22797;&#26434;&#30340;&#33394;&#24425;&#31354;&#38388;&#20013;&#36827;&#34892;&#39068;&#33394;&#30340;&#36776;&#35782;&#19982;&#30830;&#35748;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#20026;&#33394;&#30450;&#24739;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#39068;&#33394;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01895</link><description>&lt;p&gt;
Computational Trichromacy Reconstruction: Empowering the Color-Vision Deficient to Recognize Colors Using Augmented Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#30340;&#36741;&#21161;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;&#33394;&#30450;&#24739;&#32773;&#36890;&#36807;&#25968;&#23383;&#21270;&#25163;&#27573;&#35782;&#21035;&#21644;&#21629;&#21517;&#21407;&#26412;&#38590;&#20197;&#21306;&#20998;&#30340;&#39068;&#33394;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;AR&#30028;&#38754;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#28369;&#21160;&#25805;&#20316;&#26469;&#25913;&#21464;&#39068;&#33394;&#24863;&#30693;&#65292;&#20174;&#32780;&#22312;&#26356;&#22797;&#26434;&#30340;&#33394;&#24425;&#31354;&#38388;&#20013;&#36827;&#34892;&#39068;&#33394;&#30340;&#36776;&#35782;&#19982;&#30830;&#35748;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#20026;&#33394;&#30450;&#24739;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#39068;&#33394;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01895v1 Announce Type: cross  Abstract: We propose an assistive technology that helps individuals with Color Vision Deficiencies (CVD) to recognize/name colors. A dichromat's color perception is a reduced two-dimensional (2D) subset of a normal trichromat's three dimensional color (3D) perception, leading to confusion when visual stimuli that appear identical to the dichromat are referred to by different color names. Using our proposed system, CVD individuals can interactively induce distinct perceptual changes to originally confusing colors via a computational color space transformation. By combining their original 2D precepts for colors with the discriminative changes, a three dimensional color space is reconstructed, where the dichromat can learn to resolve color name confusions and accurately recognize colors. Our system is implemented as an Augmented Reality (AR) interface on smartphones, where users interactively control the rotation through swipe gestures and observe 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FBINeRF&#30340;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#29305;&#24449;&#23548;&#21521;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#36866;&#24212;&#30340;GRUs&#65288;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#26102;&#24207;&#20449;&#24687;&#30340;fisheye&#30456;&#26426;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#30456;&#26426;&#23039;&#24577;&#35843;&#25972;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#28145;&#24230;&#21021;&#22987;&#21270;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#28145;&#24230;&#20449;&#24687;&#38169;&#35823;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#35270;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;fisheye&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#24433;&#20687;&#22330;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#20854;&#20182;&#26041;&#27861;&#22240;&#25237;&#24433;&#20809;&#26463;&#36317;&#31163;&#25439;&#22833;&#24341;&#36215;&#30340;&#22270;&#20687;&#36136;&#37327;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01878</link><description>&lt;p&gt;
FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and Fisheye Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FBINeRF&#30340;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#29305;&#24449;&#23548;&#21521;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#36866;&#24212;&#30340;GRUs&#65288;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#26102;&#24207;&#20449;&#24687;&#30340;fisheye&#30456;&#26426;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#30456;&#26426;&#23039;&#24577;&#35843;&#25972;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#28145;&#24230;&#21021;&#22987;&#21270;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#28145;&#24230;&#20449;&#24687;&#38169;&#35823;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#35270;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;fisheye&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#24433;&#20687;&#22330;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#20854;&#20182;&#26041;&#27861;&#22240;&#25237;&#24433;&#20809;&#26463;&#36317;&#31163;&#25439;&#22833;&#24341;&#36215;&#30340;&#22270;&#20687;&#36136;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01878v1 Announce Type: new  Abstract: Previous studies aiming to optimize and bundle-adjust camera poses using Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated impressive capabilities in 3D scene reconstruction. However, these approaches have been designed for pinhole-camera pose optimization and do not perform well under radial image distortions such as those in fisheye cameras. Furthermore, inaccurate depth initialization in DBARF results in erroneous geometric information affecting the overall convergence and quality of results. In this paper, we propose adaptive GRUs with a flexible bundle-adjustment method adapted to radial distortions and incorporate feature-based recurrent neural networks to generate continuous novel views from fisheye datasets. Other NeRF methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray distance loss for distorted pose refinement, causing severe artifacts, long rendering time, and are difficult to u
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22312;&#38646; Shot ObjectNav &#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#19982;&#20855;&#26377;&#19968;&#23450;&#20840;&#23616;&#35270;&#37326;&#30340;&#31354;&#20013;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#30340;&#20027;&#20307;&#20195;&#29702;&#65292;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#36890;&#20449;&#65288;GC&#65289;&#30340;&#24773;&#20917;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#36825;&#31181;&#36890;&#20449;&#26041;&#24335;&#33021;&#22815;&#24110;&#21161;&#20027;&#20307;&#20195;&#29702;&#22312;&#27809;&#26377;&#29615;&#22659;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23547;&#25214;&#30446;&#26631;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20998;&#26512;&#20102;&#36825;&#31181;&#20132;&#20114;&#20013;&#20986;&#29616;&#30340;&#24187;&#24819;&#21644;&#21512;&#20316;&#34892;&#20026;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#24187;&#24819;&#34892;&#20026;&#65292;&#36825;&#25552;&#39640;&#20102;&#20027;&#20307;&#20195;&#29702;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01877</link><description>&lt;p&gt;
Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22312;&#38646; Shot ObjectNav &#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#19982;&#20855;&#26377;&#19968;&#23450;&#20840;&#23616;&#35270;&#37326;&#30340;&#31354;&#20013;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#30340;&#20027;&#20307;&#20195;&#29702;&#65292;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#36890;&#20449;&#65288;GC&#65289;&#30340;&#24773;&#20917;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#36825;&#31181;&#36890;&#20449;&#26041;&#24335;&#33021;&#22815;&#24110;&#21161;&#20027;&#20307;&#20195;&#29702;&#22312;&#27809;&#26377;&#29615;&#22659;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23547;&#25214;&#30446;&#26631;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20998;&#26512;&#20102;&#36825;&#31181;&#20132;&#20114;&#20013;&#20986;&#29616;&#30340;&#24187;&#24819;&#21644;&#21512;&#20316;&#34892;&#20026;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#24187;&#24819;&#34892;&#20026;&#65292;&#36825;&#25552;&#39640;&#20102;&#20027;&#20307;&#20195;&#29702;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01877v1 Announce Type: new  Abstract: In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a target object specified by a natural language label without any environment-specific fine-tuning. This is challenging, given the limited view of a ground agent and its independent exploratory behavior. To address these issues, we consider an assistive overhead agent with a bounded global view alongside the ground agent and present two coordinated navigation schemes for judicious exploration. We establish the influence of the Generative Communication (GC) between the embodied agents equipped with Vision-Language Models (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in the ground agent's ability to find the target object in comparison with an unassisted setup in simulation. We further analyze the GC for unique traits quantifying the presence of hallucination and cooperation. In particular, we identify a unique trait of "preemptive hallucinat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20869;&#20998;&#24067;&#25968;&#25454;&#20316;&#20026;&#27491;&#20363;&#30340;&#21487;&#38752; semi-supervised &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#31867;&#20998;&#24067;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23384;&#22312; out-of-distribution (OOD) &#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25991;&#31456;&#26368;&#22823;&#21270;&#20869;&#20998;&#24067;&#25968;&#25454;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#24182;&#36890;&#36807;&#21487;&#21464;&#31995;&#25968;&#30340;&#26102;&#38388;&#34920;&#26377;&#25928;&#22320;&#32858;&#21512;&#30456;&#24212;&#30340;&#36127;&#26679;&#12290;</title><link>https://arxiv.org/abs/2408.01872</link><description>&lt;p&gt;
Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20869;&#20998;&#24067;&#25968;&#25454;&#20316;&#20026;&#27491;&#20363;&#30340;&#21487;&#38752; semi-supervised &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#31867;&#20998;&#24067;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23384;&#22312; out-of-distribution (OOD) &#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25991;&#31456;&#26368;&#22823;&#21270;&#20869;&#20998;&#24067;&#25968;&#25454;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#24182;&#36890;&#36807;&#21487;&#21464;&#31995;&#25968;&#30340;&#26102;&#38388;&#34920;&#26377;&#25928;&#22320;&#32858;&#21512;&#30456;&#24212;&#30340;&#36127;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gershgorin&#22278;&#30424;&#23545;&#40784;&#30340;&#24555;&#36895;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#37325;&#22797;&#24615;&#31227;&#21160;&#35774;&#22791;&#35270;&#39057;&#30340;&#32447;&#24615;&#26102;&#38388;&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#35270;&#39057;&#24207;&#21015;&#36716;&#25442;&#20026;&#20855;&#26377;$M$&#27493;&#36335;&#24452;&#30340;&#22270;&#65292;&#20197;&#21450;&#38543;&#21518;&#23558;&#35813;&#22270;&#23637;&#24320;&#20026;&#21333;&#27493;&#36335;&#24452;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35270;&#39057;&#25688;&#35201;&#25552;&#21462;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#35270;&#39057;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#20851;&#38190;&#24103;&#26469;&#27010;&#25324;&#25972;&#20010;&#35270;&#39057;&#12290;</title><link>https://arxiv.org/abs/2408.01859</link><description>&lt;p&gt;
Graph Unfolding and Sampling for Transitory Video Summarization via Gershgorin Disc Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gershgorin&#22278;&#30424;&#23545;&#40784;&#30340;&#24555;&#36895;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#37325;&#22797;&#24615;&#31227;&#21160;&#35774;&#22791;&#35270;&#39057;&#30340;&#32447;&#24615;&#26102;&#38388;&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#35270;&#39057;&#24207;&#21015;&#36716;&#25442;&#20026;&#20855;&#26377;$M$&#27493;&#36335;&#24452;&#30340;&#22270;&#65292;&#20197;&#21450;&#38543;&#21518;&#23558;&#35813;&#22270;&#23637;&#24320;&#20026;&#21333;&#27493;&#36335;&#24452;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35270;&#39057;&#25688;&#35201;&#25552;&#21462;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#35270;&#39057;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#20851;&#38190;&#24103;&#26469;&#27010;&#25324;&#25972;&#20010;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01859v1 Announce Type: new  Abstract: User-generated videos (UGVs) uploaded from mobile phones to social media sites like YouTube and TikTok are short and non-repetitive. We summarize a transitory UGV into several keyframes in linear time via fast graph sampling based on Gershgorin disc alignment (GDA). Specifically, we first model a sequence of $N$ frames in a UGV as an $M$-hop path graph $\mathcal{G}^o$ for $M \ll N$, where the similarity between two frames within $M$ time instants is encoded as a positive edge based on feature similarity. Towards efficient sampling, we then "unfold" $\mathcal{G}^o$ to a $1$-hop path graph $\mathcal{G}$, specified by a generalized graph Laplacian matrix $\mathcal{L}$, via one of two graph unfolding procedures with provable performance bounds. We show that maximizing the smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of a coefficient matrix $\mathbf{B} = \textit{diag}\left(\mathbf{h}\right) + \mu \mathcal{L}$, where $\mathbf{h}$ is the bi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#30340;&#21487;&#35265;&#20809;&#21040;&#32418;&#22806;&#22270;&#20687;&#30340;&#32763;&#35793;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#65292;&#32531;&#35299;&#21487;&#35265;&#20809;&#19982;&#32418;&#22806;&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#21253;&#25324;&#23545;&#35937;&#26816;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#38598;&#25104;&#36229;&#20998;&#36776;&#29575;&#27493;&#39588;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#31934;&#24230;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2408.01843</link><description>&lt;p&gt;
Supervised Image Translation from Visible to Infrared Domain for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#30340;&#21487;&#35265;&#20809;&#21040;&#32418;&#22806;&#22270;&#20687;&#30340;&#32763;&#35793;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#65292;&#32531;&#35299;&#21487;&#35265;&#20809;&#19982;&#32418;&#22806;&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#21253;&#25324;&#23545;&#35937;&#26816;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#38598;&#25104;&#36229;&#20998;&#36776;&#29575;&#27493;&#39588;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#31934;&#24230;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01843v1 Announce Type: new  Abstract: This study aims to learn a translation from visible to infrared imagery, bridging the domain gap between the two modalities so as to improve accuracy on downstream tasks including object detection. Previous approaches attempt to perform bi-domain feature fusion through iterative optimization or end-to-end deep convolutional networks. However, we pose the problem as similar to that of image translation, adopting a two-stage training strategy with a Generative Adversarial Network and an object detection model. The translation model learns a conversion that preserves the structural detail of visible images while preserving the texture and other characteristics of infrared images. Images so generated are used to train standard object detection frameworks including Yolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating a super-resolution step into our pipeline to further improve model accuracy, and achieve an improvem
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E$^3$NeRF&#30340;&#12289;&#22522;&#20110;&#20107;&#20214;&#22686;&#24378;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#27169;&#31946;&#22270;&#20687;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;3D&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;RGB&#22270;&#20687;&#19982;&#20107;&#20214;&#27969;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;&#28210;&#26579;&#25439;&#22833;&#21644;&#20107;&#20214;&#28210;&#26579;&#25439;&#22833;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#20107;&#20214;&#27969;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;&#32593;&#32476;&#36890;&#36807;&#27169;&#25311;&#23454;&#38469;&#27169;&#31946;&#36807;&#31243;&#21644;&#20107;&#20214;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#20013;&#30456;&#26426;&#23039;&#24577;&#20272;&#31639;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2408.01840</link><description>&lt;p&gt;
E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E$^3$NeRF&#30340;&#12289;&#22522;&#20110;&#20107;&#20214;&#22686;&#24378;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#27169;&#31946;&#22270;&#20687;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;3D&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;RGB&#22270;&#20687;&#19982;&#20107;&#20214;&#27969;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;&#28210;&#26579;&#25439;&#22833;&#21644;&#20107;&#20214;&#28210;&#26579;&#25439;&#22833;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#20107;&#20214;&#27969;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;&#32593;&#32476;&#36890;&#36807;&#27169;&#25311;&#23454;&#38469;&#27169;&#31946;&#36807;&#31243;&#21644;&#20107;&#20214;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#20013;&#30456;&#26426;&#23039;&#24577;&#20272;&#31639;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01840v1 Announce Type: new  Abstract: Neural Radiance Fields (NeRF) achieve impressive rendering performance by learning volumetric 3D representation from several images of different views. However, it is difficult to reconstruct a sharp NeRF from blurry input as it often occurs in the wild. To solve this problem, we propose a novel Efficient Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and event streams. To effectively introduce event streams into the neural volumetric representation learning process, we propose an event-enhanced blur rendering loss and an event rendering loss, which guide the network via modeling the real blur process and event generation process, respectively. Specifically, we leverage spatial-temporal information from the event stream to evenly distribute learning attention over temporal blur while simultaneously focusing on blurry texture through the spatial attention. Moreover, a camera pose estimation framework for real-w
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TS-SAM&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#31934;&#32454;&#35843;&#33410;&#26041;&#27861;&#26469;&#25913;&#21892;SAM&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27969;SAM&#65288;TS-SAM&#65289;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32467;&#21512;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#20391;&#36866;&#37197;&#22120;&#21644;&#19968;&#31181;&#22810;&#23610;&#24230;&#32454;&#21270;&#27169;&#22359;&#65292;&#20197;&#27492;&#25972;&#21512;&#24378;&#22823;&#30340;&#29305;&#24449;&#24182;&#19982;&#20391;&#32593;&#32476;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20445;&#25345;&#32454;&#33410;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TS-SAM&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;SAM-Adapter&#21644;SSOM&#65292;&#24182;&#25509;&#36817;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.01835</link><description>&lt;p&gt;
TS-SAM: Fine-Tuning Segment-Anything Model for Downstream Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TS-SAM&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#31934;&#32454;&#35843;&#33410;&#26041;&#27861;&#26469;&#25913;&#21892;SAM&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27969;SAM&#65288;TS-SAM&#65289;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32467;&#21512;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#20391;&#36866;&#37197;&#22120;&#21644;&#19968;&#31181;&#22810;&#23610;&#24230;&#32454;&#21270;&#27169;&#22359;&#65292;&#20197;&#27492;&#25972;&#21512;&#24378;&#22823;&#30340;&#29305;&#24449;&#24182;&#19982;&#20391;&#32593;&#32476;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20445;&#25345;&#32454;&#33410;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TS-SAM&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;SAM-Adapter&#21644;SSOM&#65292;&#24182;&#25509;&#36817;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01835v1 Announce Type: new  Abstract: Adapter based fine-tuning has been studied for improving the performance of SAM on downstream tasks. However, there is still a significant performance gap between fine-tuned SAMs and domain-specific models. To reduce the gap, we propose Two-Stream SAM (TS-SAM). On the one hand, inspired by the side network in Parameter-Efficient Fine-Tuning (PEFT), we designed a lightweight Convolutional Side Adapter (CSA), which integrates the powerful features from SAM into side network training for comprehensive feature fusion. On the other hand, in line with the characteristics of segmentation tasks, we designed Multi-scale Refinement Module (MRM) and Feature Fusion Decoder (FFD) to keep both the detailed and semantic features. Extensive experiments on ten public datasets from three tasks demonstrate that TS-SAM not only significantly outperforms the recently proposed SAM-Adapter and SSOM, but achieves competitive performance with the SOTA domain-spe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20840;&#26032;&#31574;&#30053;&#65292;&#29992;&#20110;&#28040;&#38500;&#28304;&#33258;&#26377;&#38480;&#39057;&#24102;&#25391;&#21160;&#28304;&#30340;&#25391;&#21160;&#25968;&#25454;&#20013;&#30340;&#29615;&#29366;&#25928;&#24212;&#65292;&#25552;&#21319;&#20102;&#39318;&#20010;&#20449;&#21495;&#38388;&#26029;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01831</link><description>&lt;p&gt;
A Deep CNN Model for Ringing Effect Attenuation of Vibroseis Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20840;&#26032;&#31574;&#30053;&#65292;&#29992;&#20110;&#28040;&#38500;&#28304;&#33258;&#26377;&#38480;&#39057;&#24102;&#25391;&#21160;&#28304;&#30340;&#25391;&#21160;&#25968;&#25454;&#20013;&#30340;&#29615;&#29366;&#25928;&#24212;&#65292;&#25552;&#21319;&#20102;&#39318;&#20010;&#20449;&#21495;&#38388;&#26029;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01831v1 Announce Type: new  Abstract: In the field of exploration geophysics, seismic vibrator is one of the widely used seismic sources to acquire seismic data, which is usually named vibroseis. "Ringing effect" is a common problem in vibroseis data processing due to the limited frequency bandwidth of the vibrator, which degrades the performance of first-break picking. In this paper, we proposed a novel deringing model for vibroseis data using deep convolutional neural network (CNN). In this model we use end-to-end training strategy to obtain the deringed data directly, and skip connections to improve model training process and preserve the details of vibroseis data. For real vibroseis deringing task we synthesize training data and corresponding labels from real vibroseis data and utilize them to train the deep CNN model. Experiments are conducted both on synthetic data and real vibroseis data. The experiment results show that deep CNN model can attenuate the ringing effect
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ST-SACLF&#27169;&#22411;&#65292;&#36890;&#36807;Style Transfer&#32467;&#21512;AdaIN&#29983;&#25104;&#25968;&#25454;&#65292;&#32467;&#21512;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21160;&#24577;&#26679;&#26412;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#33402;&#26415;&#20316;&#21697;&#30340;&#39640;&#31934;&#24230;&#20998;&#31867;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01827</link><description>&lt;p&gt;
ST-SACLF: Style Transfer Informed Self-Attention Classifier for Bias-Aware Painting Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ST-SACLF&#27169;&#22411;&#65292;&#36890;&#36807;Style Transfer&#32467;&#21512;AdaIN&#29983;&#25104;&#25968;&#25454;&#65292;&#32467;&#21512;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21160;&#24577;&#26679;&#26412;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#33402;&#26415;&#20316;&#21697;&#30340;&#39640;&#31934;&#24230;&#20998;&#31867;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01827v1 Announce Type: new  Abstract: Painting classification plays a vital role in organizing, finding, and suggesting artwork for digital and classic art galleries. Existing methods struggle with adapting knowledge from the real world to artistic images during training, leading to poor performance when dealing with different datasets. Our innovation lies in addressing these challenges through a two-step process. First, we generate more data using Style Transfer with Adaptive Instance Normalization (AdaIN), bridging the gap between diverse styles. Then, our classifier gains a boost with feature-map adaptive spatial attention modules, improving its understanding of artistic details. Moreover, we tackle the problem of imbalanced class representation by dynamically adjusting augmented samples. Through a dual-stage process involving careful hyperparameter search and model fine-tuning, we achieve an impressive 87.24\% accuracy using the ResNet-50 backbone over 40 training epochs
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GLDiTalker&#30340;3D&#35821;&#38899;&#39537;&#21160;&#38754;&#37096;&#21160;&#30011;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#28508;&#20239;&#25193;&#25955;&#36716;&#35793;&#22120;&#65288;Graph Latent Diffusion Transformer&#65289;&#33021;&#22815;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#38750;&#21475;&#35821;&#38754;&#37096;&#34920;&#24773;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#38454;&#27573;&#20998;&#21035;&#20351;&#29992;VQ-VAE&#23545;&#38754;&#37096;&#21160;&#20316;&#32593;&#26684;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#24182;&#23558;&#22122;&#22768;&#36880;&#32423;&#28155;&#21152;&#25110;&#21435;&#38500;&#21040;&#38754;&#37096;&#36816;&#21160;&#29305;&#24449;&#20013;&#65292;GLDiTalker&#33021;&#22815;&#22312;&#20445;&#25345;&#36328;&#27169;&#24577;&#26144;&#23556;&#38750;&#30830;&#23450;&#24615;&#29305;&#24449;&#30340;&#21516;&#26102;&#22686;&#21152;&#38754;&#37096;&#34920;&#24773;&#30340;&#38750;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01826</link><description>&lt;p&gt;
GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GLDiTalker&#30340;3D&#35821;&#38899;&#39537;&#21160;&#38754;&#37096;&#21160;&#30011;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#28508;&#20239;&#25193;&#25955;&#36716;&#35793;&#22120;&#65288;Graph Latent Diffusion Transformer&#65289;&#33021;&#22815;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#38750;&#21475;&#35821;&#38754;&#37096;&#34920;&#24773;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#38454;&#27573;&#20998;&#21035;&#20351;&#29992;VQ-VAE&#23545;&#38754;&#37096;&#21160;&#20316;&#32593;&#26684;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#24182;&#23558;&#22122;&#22768;&#36880;&#32423;&#28155;&#21152;&#25110;&#21435;&#38500;&#21040;&#38754;&#37096;&#36816;&#21160;&#29305;&#24449;&#20013;&#65292;GLDiTalker&#33021;&#22815;&#22312;&#20445;&#25345;&#36328;&#27169;&#24577;&#26144;&#23556;&#38750;&#30830;&#23450;&#24615;&#29305;&#24449;&#30340;&#21516;&#26102;&#22686;&#21152;&#38754;&#37096;&#34920;&#24773;&#30340;&#38750;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;SkyDiffusion&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#40479;&#30640;&#35270;&#22270;&#65288;BEV&#65289;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#34903;&#36947;&#35270;&#22270;&#22270;&#20687;&#21040;&#21355;&#26143;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#36328;&#35270;&#35282;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#22495;&#24046;&#36317;&#22823;&#21644;&#20869;&#23481;&#37325;&#32452;&#38590;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#25104;&#26144;&#23556;&#26102;&#21516;&#26102;&#25340;&#25509;&#22810;&#24352;&#34903;&#36947;&#35270;&#22270;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2408.01812</link><description>&lt;p&gt;
SkyDiffusion: Street-to-Satellite Image Synthesis with Diffusion Models and BEV Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;SkyDiffusion&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#40479;&#30640;&#35270;&#22270;&#65288;BEV&#65289;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#34903;&#36947;&#35270;&#22270;&#22270;&#20687;&#21040;&#21355;&#26143;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#36328;&#35270;&#35282;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#22495;&#24046;&#36317;&#22823;&#21644;&#20869;&#23481;&#37325;&#32452;&#38590;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#25104;&#26144;&#23556;&#26102;&#21516;&#26102;&#25340;&#25509;&#22810;&#24352;&#34903;&#36947;&#35270;&#22270;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01812v1 Announce Type: new  Abstract: Street-to-satellite image synthesis focuses on generating realistic satellite images from corresponding ground street-view images while maintaining a consistent content layout, similar to looking down from the sky. The significant differences in perspectives create a substantial domain gap between the views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing satellite images from street-view images, leveraging diffusion models and Bird's Eye View (BEV) paradigm. First, we design a Curved-BEV method to transform street-view images to the satellite view, reformulating the challenging cross-domain image synthesis task into a conditional generation problem. Curved-BEV also includes a "Multi-to-One" mapping strategy for combining multiple street-view images within the same satellite coverage area, effectively solving the occlusion iss
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MiniCPM-V&#31995;&#21015;MLLMs&#23454;&#29616;&#20102;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#30340;&#39640;&#25928;&#37096;&#32626;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21644;&#26657;&#20934;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#65292;&#20351;&#20854;&#22312;&#31227;&#21160;&#12289;&#31163;&#32447;&#12289;&#33021;&#28304;&#25935;&#24863;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;GPT-4V-1106&#12289;Gemini Pro&#21644;Claud&#30456;&#27604;&#26102;&#65292;MiniCPM-Llama3-V 2.5&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20247;&#12290;</title><link>https://arxiv.org/abs/2408.01800</link><description>&lt;p&gt;
MiniCPM-V: A GPT-4V Level MLLM on Your Phone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MiniCPM-V&#31995;&#21015;MLLMs&#23454;&#29616;&#20102;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#30340;&#39640;&#25928;&#37096;&#32626;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21644;&#26657;&#20934;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#65292;&#20351;&#20854;&#22312;&#31227;&#21160;&#12289;&#31163;&#32447;&#12289;&#33021;&#28304;&#25935;&#24863;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;GPT-4V-1106&#12289;Gemini Pro&#21644;Claud&#30456;&#27604;&#26102;&#65292;MiniCPM-Llama3-V 2.5&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01800v1 Announce Type: new  Abstract: The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain preventing MLLMs from being practical in real-world applications. The most notable challenge comes from the huge cost of running an MLLM with a massive number of parameters and extensive computation. As a result, most MLLMs need to be deployed on high-performing cloud servers, which greatly limits their application scopes such as mobile, offline, energy-sensitive, and privacy-protective scenarios. In this work, we present MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By integrating the latest MLLM techniques in architecture, pretraining and alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1) Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claud
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NuLite&#30340;&#26032;&#22411;&#36731;&#37327;&#32423;&#19988;&#24555;&#36895;&#30340;&#27169;&#22411;&#65292;&#19987;&#20026;H\&amp;E&#26579;&#33394;&#20999;&#29255;&#20013;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#38382;&#39064;&#35774;&#35745;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;Fast-ViT&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#19982;SOTA&#27169;&#22411;CellViT&#30456;&#24403;&#30340;Panoptic&#36136;&#37327;&#26816;&#27979;&#24615;&#33021;&#12290;&#20196;&#20154;&#30633;&#30446;&#30340;&#26159;&#65292;NuLite&#30340;&#26368;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;GFlops&#19978;&#20998;&#21035;&#21482;&#26377;&#20854;1/40&#21644;1/8&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#37096;&#32626;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01797</link><description>&lt;p&gt;
NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NuLite&#30340;&#26032;&#22411;&#36731;&#37327;&#32423;&#19988;&#24555;&#36895;&#30340;&#27169;&#22411;&#65292;&#19987;&#20026;H\&amp;E&#26579;&#33394;&#20999;&#29255;&#20013;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#38382;&#39064;&#35774;&#35745;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;Fast-ViT&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#19982;SOTA&#27169;&#22411;CellViT&#30456;&#24403;&#30340;Panoptic&#36136;&#37327;&#26816;&#27979;&#24615;&#33021;&#12290;&#20196;&#20154;&#30633;&#30446;&#30340;&#26159;&#65292;NuLite&#30340;&#26368;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;GFlops&#19978;&#20998;&#21035;&#21482;&#26377;&#20854;1/40&#21644;1/8&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#37096;&#32626;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01797v1 Announce Type: cross  Abstract: In pathology, accurate and efficient analysis of Hematoxylin and Eosin (H\&amp;E) slides is crucial for timely and effective cancer diagnosis. Although many deep learning solutions for nuclei instance segmentation and classification exist in the literature, they often entail high computational costs and resource requirements, thus limiting their practical usage in medical applications. To address this issue, we introduce a novel convolutional neural network, NuLite, a U-Net-like architecture designed explicitly on Fast-ViT, a state-of-the-art (SOTA) lightweight CNN. We obtained three versions of our model, NuLite-S, NuLite-M, and NuLite-H, trained on the PanNuke dataset. The experimental results prove that our models equal CellViT (SOTA) in terms of panoptic quality and detection. However, our lightest model, NuLite-S, is 40 times smaller in terms of parameters and about 8 times smaller in terms of GFlops, while our heaviest model is 17 ti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDA&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#26469;&#39044;&#27979;&#39550;&#39542;&#21592;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#26088;&#22312;&#36890;&#36807;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#21306;&#22495;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01774</link><description>&lt;p&gt;
STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDA&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#26469;&#39044;&#27979;&#39550;&#39542;&#21592;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#26088;&#22312;&#36890;&#36807;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#21306;&#22495;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01774v1 Announce Type: new  Abstract: Accurate behavior prediction for vehicles is essential but challenging for autonomous driving. Most existing studies show satisfying performance under regular scenarios, but most neglected safety-critical scenarios. In this study, a spatio-temporal dual-encoder network named STDA for safety-critical scenarios was developed. Considering the exceptional capabilities of human drivers in terms of situational awareness and comprehending risks, driver attention was incorporated into STDA to facilitate swift identification of the critical regions, which is expected to improve both performance and interpretability. STDA contains four parts: the driver attention prediction module, which predicts driver attention; the fusion module designed to fuse the features between driver attention and raw images; the temporary encoder module used to enhance the capability to interpret dynamic scenes; and the behavior prediction module to predict the behavior.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#32593;&#32476;&#21442;&#25968;&#32422;&#26463;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#23884;&#20837;&#31354;&#38388;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#23884;&#20837;&#20013;&#23637;&#31034;&#20102; MNIST&#12289;Fashion MNIST &#21644; CIFAR-10 &#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#25903;&#25345;&#24320;&#25918;&#24335;&#35782;&#21035;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#35299;&#37322;&#24615;&#31561;&#39640;&#32423;&#25216;&#26415;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.01767</link><description>&lt;p&gt;
Comparison of Embedded Spaces for Deep Learning Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#32593;&#32476;&#21442;&#25968;&#32422;&#26463;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#23884;&#20837;&#31354;&#38388;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#23884;&#20837;&#20013;&#23637;&#31034;&#20102; MNIST&#12289;Fashion MNIST &#21644; CIFAR-10 &#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#25903;&#25345;&#24320;&#25918;&#24335;&#35782;&#21035;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#35299;&#37322;&#24615;&#31561;&#39640;&#32423;&#25216;&#26415;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01767v1 Announce Type: cross  Abstract: Embedded spaces are a key feature in deep learning. Good embedded spaces represent the data well to support classification and advanced techniques such as open-set recognition, few-short learning and explainability. This paper presents a compact overview of different techniques to design embedded spaces for classification. It compares different loss functions and constraints on the network parameters with respect to the achievable geometric structure of the embedded space. The techniques are demonstrated with two and three-dimensional embeddings for the MNIST, Fashion MNIST and CIFAR-10 datasets, allowing visual inspection of the embedded spaces.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiFuser&#30340;&#20840;&#26032;&#22810;&#27169;&#24577;&#34701;&#21512;Transformer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21644;&#21033;&#29992;&#22810;&#27169;&#24577;&#36710;&#20869;&#35270;&#39057;&#20013;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#25552;&#39640;&#20102;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01766</link><description>&lt;p&gt;
MultiFuser: Multimodal Fusion Transformer for Enhanced Driver Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiFuser&#30340;&#20840;&#26032;&#22810;&#27169;&#24577;&#34701;&#21512;Transformer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21644;&#21033;&#29992;&#22810;&#27169;&#24577;&#36710;&#20869;&#35270;&#39057;&#20013;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#25552;&#39640;&#20102;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01766v1 Announce Type: new  Abstract: Driver action recognition, aiming to accurately identify drivers' behaviours, is crucial for enhancing driver-vehicle interactions and ensuring driving safety. Unlike general action recognition, drivers' environments are often challenging, being gloomy and dark, and with the development of sensors, various cameras such as IR and depth cameras have emerged for analyzing drivers' behaviors. Therefore, in this paper, we propose a novel multimodal fusion transformer, named MultiFuser, which identifies cross-modal interrelations and interactions among multimodal car cabin videos and adaptively integrates different modalities for improved representations. Specifically, MultiFuser comprises layers of Bi-decomposed Modules to model spatiotemporal features, with a modality synthesizer for multimodal features integration. Each Bi-decomposed Module includes a Modal Expertise ViT block for extracting modality-specific features and a Patch-wise Adapt
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#8212;&#8212;&#29992;&#20110;&#31934;&#30830;&#35782;&#21035;&#21644;&#35786;&#26029;&#27700;&#31291;&#21494;&#29255;&#30142;&#30149;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#26377;&#25928;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#19988;&#36890;&#36807;&#28155;&#21152;&#20840;&#36830;&#25509;&#23618;&#21644;&#20351;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.01752</link><description>&lt;p&gt;
Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#8212;&#8212;&#29992;&#20110;&#31934;&#30830;&#35782;&#21035;&#21644;&#35786;&#26029;&#27700;&#31291;&#21494;&#29255;&#30142;&#30149;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#26377;&#25928;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#19988;&#36890;&#36807;&#28155;&#21152;&#20840;&#36830;&#25509;&#23618;&#21644;&#20351;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01752v1 Announce Type: new  Abstract: Rice plays a vital role as a primary food source for over half of the world's population, and its production is critical for global food security. Nevertheless, rice cultivation is frequently affected by various diseases that can severely decrease yield and quality. Therefore, early and accurate detection of rice diseases is necessary to prevent their spread and minimize crop losses. In this research, we explore three mobile-compatible CNN architectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice leaf disease classification. These models are selected due to their compatibility with mobile devices, as they demand less computational power and memory compared to other CNN models. To enhance the performance of the three models, we added two fully connected layers separated by a dropout layer. We used early stop creation to prevent the model from being overfiting. The results of the study showed that the best performance wa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Domain Penalisation&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#23545;&#35937;&#26816;&#27979;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#26694;&#26550;&#36890;&#36807;&#20026;&#22810;&#20010;&#28304;&#22495;&#25968;&#25454;&#20998;&#37197;&#24809;&#32602;&#26435;&#37325;&#65292;&#24182;&#26681;&#25454;&#26816;&#27979;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#22495;&#19978;&#30340;&#24615;&#33021;&#26356;&#26032;&#36825;&#20123;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#25928;&#24179;&#34913;&#24182;&#20248;&#20808;&#32771;&#34385;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#30340;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#27169;&#22411;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#19978;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01746</link><description>&lt;p&gt;
Domain penalisation for improved Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Domain Penalisation&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#23545;&#35937;&#26816;&#27979;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#26694;&#26550;&#36890;&#36807;&#20026;&#22810;&#20010;&#28304;&#22495;&#25968;&#25454;&#20998;&#37197;&#24809;&#32602;&#26435;&#37325;&#65292;&#24182;&#26681;&#25454;&#26816;&#27979;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#22495;&#19978;&#30340;&#24615;&#33021;&#26356;&#26032;&#36825;&#20123;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#25928;&#24179;&#34913;&#24182;&#20248;&#20808;&#32771;&#34385;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#30340;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#27169;&#22411;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#19978;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01746v1 Announce Type: new  Abstract: In the field of object detection, domain generalisation (DG) aims to ensure robust performance across diverse and unseen target domains by learning the robust domain-invariant features corresponding to the objects of interest across multiple source domains. While there are many approaches established for performing DG for the task of classification, there has been a very little focus on object detection. In this paper, we propose a domain penalisation (DP) framework for the task of object detection, where the data is assumed to be sampled from multiple source domains and tested on completely unseen test domains. We assign penalisation weights to each domain, with the values updated based on the detection networks performance on the respective source domains. By prioritising the domains that needs more attention, our approach effectively balances the training process. We evaluate our solution on the GWHD 2021 dataset, a component of the W
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;LAM3D&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#20110;Pyramid Vision Transformer v2 (PVTv2)&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23545;Monocular 3D Object Detection&#20219;&#21153;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#22312;KITTI 3D Object Detection Benchmark&#19978;&#39564;&#35777;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.01739</link><description>&lt;p&gt;
LAM3D: Leveraging Attention for Monocular 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;LAM3D&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#20110;Pyramid Vision Transformer v2 (PVTv2)&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23545;Monocular 3D Object Detection&#20219;&#21153;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#22312;KITTI 3D Object Detection Benchmark&#19978;&#39564;&#35777;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01739v1 Announce Type: new  Abstract: Since the introduction of the self-attention mechanism and the adoption of the Transformer architecture for Computer Vision tasks, the Vision Transformer-based architectures gained a lot of popularity in the field, being used for tasks such as image classification, object detection and image segmentation. However, efficiently leveraging the attention mechanism in vision transformers for the Monocular 3D Object Detection task remains an open question. In this paper, we present LAM3D, a framework that Leverages self-Attention mechanism for Monocular 3D object Detection. To do so, the proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as feature extraction backbone and 2D/3D detection machinery. We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods. Moreover, due to the usage of sel
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#38899;&#39057;&#20449;&#24687;&#21644;&#39044;&#29983;&#25104;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21767;&#24418;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#22836;&#35270;&#39057;&#12290;</title><link>https://arxiv.org/abs/2408.01732</link><description>&lt;p&gt;
Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#38899;&#39057;&#20449;&#24687;&#21644;&#39044;&#29983;&#25104;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21767;&#24418;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#22836;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23545;&#20351;&#29992;&#23039;&#21183;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#25216;&#26415;&#22312;&#34394;&#25311;&#29616;&#23454;&#39046;&#22495;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35843;&#26597;19&#31687;&#30456;&#20851;&#30740;&#31350;&#35770;&#25991;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#12289;&#20998;&#31867;&#31639;&#27861;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#20934;&#30830;&#24615;&#36825;&#19968;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#24471;&#20986;&#65292;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#23039;&#21183;&#20272;&#35745;&#39046;&#22495;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#25913;&#36827;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2408.01728</link><description>&lt;p&gt;
Survey on Emotion Recognition through Posture Detection and the possibility of its application in Virtual Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23545;&#20351;&#29992;&#23039;&#21183;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#25216;&#26415;&#22312;&#34394;&#25311;&#29616;&#23454;&#39046;&#22495;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35843;&#26597;19&#31687;&#30456;&#20851;&#30740;&#31350;&#35770;&#25991;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#12289;&#20998;&#31867;&#31639;&#27861;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#20934;&#30830;&#24615;&#36825;&#19968;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#24471;&#20986;&#65292;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#23039;&#21183;&#20272;&#35745;&#39046;&#22495;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#25913;&#36827;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01728v1 Announce Type: new  Abstract: A survey is presented focused on using pose estimation techniques in Emotional recognition using various technologies normal cameras, and depth cameras for real-time, and the potential use of VR and inputs including images, videos, and 3-dimensional poses described in vector space. We discussed 19 research papers collected from selected journals and databases highlighting their methodology, classification algorithm, and the used datasets that relate to emotion recognition and pose estimation. A benchmark has been made according to their accuracy as it was the most common performance measurement metric used. We concluded that the multimodal Approaches overall made the best accuracy and then we mentioned futuristic concerns that can improve the development of this research topic.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#22270;&#20687;&#25551;&#36848;&#35780;&#20215;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#22270;&#29255;&#24182;&#25552;&#20379;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#35780;&#20215;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20215;&#24037;&#20855;&#19982;&#20154;&#24037;&#35780;&#20215;&#20043;&#38388;&#24369;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01723</link><description>&lt;p&gt;
A Novel Evaluation Framework for Image2Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#22270;&#20687;&#25551;&#36848;&#35780;&#20215;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#22270;&#29255;&#24182;&#25552;&#20379;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#35780;&#20215;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20215;&#24037;&#20855;&#19982;&#20154;&#24037;&#35780;&#20215;&#20043;&#38388;&#24369;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01723v1 Announce Type: new  Abstract: Evaluating the quality of automatically generated image descriptions is challenging, requiring metrics that capture various aspects such as grammaticality, coverage, correctness, and truthfulness. While human evaluation offers valuable insights, its cost and time-consuming nature pose limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr aim to bridge this gap but often show weak correlations with human judgment. We address this challenge by introducing a novel evaluation framework rooted in a modern large language model (LLM), such as GPT-4 or Gemini, capable of image generation. In our proposed framework, we begin by feeding an input image into a designated image captioning model, chosen for evaluation, to generate a textual description. Using this description, an LLM then creates a new image. By extracting features from both the original and LLM-created images, we measure their similarity using a designated simil
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#35780;&#20272;&#20102;&#20960;&#31181;&#24320;&#28304;&#30340;&#35270;&#35273;&#24815;&#24615;SLAM&#31995;&#32479;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#22238;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22238;&#29615;&#38381;&#21512;&#23545;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#20855;&#26377;&#26174;&#33879;&#25928;&#24212;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#19981;&#21516;&#22270;&#20687;&#25293;&#25668;&#39057;&#29575;&#23545;&#23450;&#20301;&#31934;&#24230;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#25991;&#31456;&#25552;&#20986;&#30340;&#36825;&#20123;&#35780;&#20272;&#23545;&#20110;&#23558;&#36825;&#20123;SLAM&#31995;&#32479;&#24212;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2408.01716</link><description>&lt;p&gt;
Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the Benefits and Computational Costs of Loop Closing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01716
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#35780;&#20272;&#20102;&#20960;&#31181;&#24320;&#28304;&#30340;&#35270;&#35273;&#24815;&#24615;SLAM&#31995;&#32479;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#22238;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22238;&#29615;&#38381;&#21512;&#23545;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#20855;&#26377;&#26174;&#33879;&#25928;&#24212;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#19981;&#21516;&#22270;&#20687;&#25293;&#25668;&#39057;&#29575;&#23545;&#23450;&#20301;&#31934;&#24230;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#25991;&#31456;&#25552;&#20986;&#30340;&#36825;&#20123;&#35780;&#20272;&#23545;&#20110;&#23558;&#36825;&#20123;SLAM&#31995;&#32479;&#24212;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01716v1 Announce Type: new  Abstract: Simultaneous Localization and Mapping (SLAM) is essential for mobile robotics, enabling autonomous navigation in dynamic, unstructured outdoor environments without relying on external positioning systems. In agricultural applications, where environmental conditions can be particularly challenging due to variable lighting or weather conditions, Visual-Inertial SLAM has emerged as a potential solution. This paper benchmarks several open-source Visual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS, Kimera, and SVO Pro, to evaluate their performance in agricultural settings. We focus on the impact of loop closing on localization accuracy and computational demands, providing a comprehensive analysis of these systems' effectiveness in real-world environments and especially their application to embedded systems in agricultural robotics. Our contributions further include an assessment of varying frame rates on localization acc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20540;&#36793;&#32536;&#22270;&#20687;&#20013;&#20132;&#28857;&#12289;&#25509;&#28857;&#21644;&#20854;&#23427;&#32467;&#26500;&#30340;&#19968;&#33324;&#19988;&#30452;&#35266;&#30340;&#27169;&#31946;&#24230;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36793;&#32536;&#36861;&#36394;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36793;&#32536;&#30340;&#26377;&#24207;&#20687;&#32032;&#24207;&#21015;&#65292;&#26088;&#22312;&#20026;&#35832;&#22914;&#36793;&#30028;&#32447;&#21010;&#20998;&#12289;&#23545;&#35937;&#35782;&#21035;&#12289;&#25299;&#25169;&#20998;&#26512;&#21644;&#20854;&#23427;&#20219;&#21153;&#25552;&#20379;&#19968;&#31181;&#36890;&#29992;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#31616;&#21333;&#30452;&#35266;&#30340;&#21407;&#21017;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20266;&#20195;&#30721;&#26469;&#25551;&#36848;&#21644;&#23454;&#29616;&#21518;&#32493;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#26377;&#21161;&#20110;&#35299;&#20915;&#25509;&#28857;&#22788;&#30340;&#36793;&#32536;&#36830;&#25509;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#20805;&#30340;&#36793;&#32536;&#22320;&#22270;&#65292;&#30456;&#37051;&#30340;&#36793;&#32536;&#21487;&#20197;&#36890;&#36807;&#24555;&#36895;&#23616;&#37096;&#25628;&#32034;&#25805;&#20316;&#24471;&#21040;&#35775;&#38382;&#65292;&#36793;&#32536;&#36861;&#36394;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#36882;&#24402;&#25216;&#26415;&#65292;&#20351;&#24471;&#32534;&#30721;&#31616;&#27905;&#12290;&#36890;&#36807;&#19982;&#30456;&#20851;&#26041;&#27861;&#30340;&#23545;&#27604;&#20998;&#26512;&#21644;&#20248;&#21270;&#32467;&#26524;&#30340;&#23637;&#31034;&#65292;&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2408.01712</link><description>&lt;p&gt;
A General Ambiguity Model for Binary Edge Images with Edge Tracing and its Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20540;&#36793;&#32536;&#22270;&#20687;&#20013;&#20132;&#28857;&#12289;&#25509;&#28857;&#21644;&#20854;&#23427;&#32467;&#26500;&#30340;&#19968;&#33324;&#19988;&#30452;&#35266;&#30340;&#27169;&#31946;&#24230;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36793;&#32536;&#36861;&#36394;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36793;&#32536;&#30340;&#26377;&#24207;&#20687;&#32032;&#24207;&#21015;&#65292;&#26088;&#22312;&#20026;&#35832;&#22914;&#36793;&#30028;&#32447;&#21010;&#20998;&#12289;&#23545;&#35937;&#35782;&#21035;&#12289;&#25299;&#25169;&#20998;&#26512;&#21644;&#20854;&#23427;&#20219;&#21153;&#25552;&#20379;&#19968;&#31181;&#36890;&#29992;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#31616;&#21333;&#30452;&#35266;&#30340;&#21407;&#21017;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20266;&#20195;&#30721;&#26469;&#25551;&#36848;&#21644;&#23454;&#29616;&#21518;&#32493;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#26377;&#21161;&#20110;&#35299;&#20915;&#25509;&#28857;&#22788;&#30340;&#36793;&#32536;&#36830;&#25509;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#20805;&#30340;&#36793;&#32536;&#22320;&#22270;&#65292;&#30456;&#37051;&#30340;&#36793;&#32536;&#21487;&#20197;&#36890;&#36807;&#24555;&#36895;&#23616;&#37096;&#25628;&#32034;&#25805;&#20316;&#24471;&#21040;&#35775;&#38382;&#65292;&#36793;&#32536;&#36861;&#36394;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#36882;&#24402;&#25216;&#26415;&#65292;&#20351;&#24471;&#32534;&#30721;&#31616;&#27905;&#12290;&#36890;&#36807;&#19982;&#30456;&#20851;&#26041;&#27861;&#30340;&#23545;&#27604;&#20998;&#26512;&#21644;&#20248;&#21270;&#32467;&#26524;&#30340;&#23637;&#31034;&#65292;&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01712v1 Announce Type: new  Abstract: We present a general and intuitive ambiguity model for intersections, junctions and other structures in binary edge images. The model is combined with edge tracing, where edges are ordered sequences of connected pixels. The objective is to provide a versatile preprocessing method for tasks such as figure-ground segmentation, object recognition, topological analysis, etc. By using only a small set of straightforward principles, the results are intuitive to describe. This helps to implement subsequent processing steps, such as resolving ambiguous edge connections at junctions. By using an augmented edge map, neighboring edges can be directly accessed using quick local search operations. The edge tracing uses recursion, which leads to compact programming code. We explain our algorithm using pseudocode, compare it with related methods, and show how simple modular postprocessing steps can be used to optimize the results. The complete algorith
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;AVESFormer&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30340;&#25439;&#32791;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;AVESFormer&#36890;&#36807;&#25913;&#36827;&#25552;&#31034;&#36755;&#20837;&#29983;&#25104;&#22120;&#21644;ELF&#35299;&#30721;&#22120;&#26469;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#25913;&#36827;&#19981;&#20165;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36824;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#25928;&#30340;&#21516;&#26102;&#23454;&#29616;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2408.01708</link><description>&lt;p&gt;
AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;AVESFormer&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30340;&#25439;&#32791;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;AVESFormer&#36890;&#36807;&#25913;&#36827;&#25552;&#31034;&#36755;&#20837;&#29983;&#25104;&#22120;&#21644;ELF&#35299;&#30721;&#22120;&#26469;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#25913;&#36827;&#19981;&#20165;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36824;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#25928;&#30340;&#21516;&#26102;&#23454;&#29616;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01708v1 Announce Type: new  Abstract: Recently, transformer-based models have demonstrated remarkable performance on audio-visual segmentation (AVS) tasks. However, their expensive computational cost makes real-time inference impractical. By characterizing attention maps of the network, we identify two key obstacles in AVS models: 1) attention dissipation, corresponding to the over-concentrated attention weights by Softmax within restricted frames, and 2) inefficient, burdensome transformer decoder, caused by narrow focus patterns in early stages. In this paper, we introduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation transformer that achieves fast, efficient and light-weight simultaneously. Our model leverages an efficient prompt query generator to correct the behaviour of cross-attention. Additionally, we propose ELF decoder to bring greater efficiency by facilitating convolutions suitable for local features to reduce computational burdens. Extensiv
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19979;&#28216;&#36716;&#31227;&#25915;&#20987;&#8221;&#65288;DTA&#65289;&#30340;&#20840;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#32423;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21363;&#20351;&#26159;&#21516;&#19968;&#20010;&#36755;&#20837;&#22270;&#29255;&#65292;&#20063;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#29983;&#25104;&#19981;&#21516;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#27492;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01705</link><description>&lt;p&gt;
Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19979;&#28216;&#36716;&#31227;&#25915;&#20987;&#8221;&#65288;DTA&#65289;&#30340;&#20840;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#32423;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21363;&#20351;&#26159;&#21516;&#19968;&#20010;&#36755;&#20837;&#22270;&#29255;&#65292;&#20063;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#29983;&#25104;&#19981;&#21516;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#27492;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01705v1 Announce Type: new  Abstract: With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on \emph{sample-wise} transfer attacks and propose a novel attack method termed \emph{Downstream Transfer Attack (DTA)}. For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of the
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Signal-SGN&#30340;&#31361;&#35302;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#36890;&#36807;&#23398;&#20064;&#26102;&#39057;&#21160;&#24577;&#26469;&#35782;&#21035;&#39592;&#39612;&#21160;&#20316;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#37319;&#29992;&#26102;&#24207;&#29305;&#24449;&#20316;&#20026;&#31361;&#35302;&#35302;&#21457;&#27493;&#38271;&#22788;&#29702;&#39592;&#39612;&#24207;&#21015;&#65292;&#24182;&#23558;&#29305;&#24449;&#35270;&#20026;&#31163;&#25955;&#30340;&#38543;&#26426;&#20449;&#21495;&#12290;&#35813;&#32593;&#32476;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;1D&#31361;&#35302;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;1D-SGN&#65289;&#21644;&#19968;&#20010;&#39057;&#29575;&#31361;&#35302;&#21367;&#31215;&#32593;&#32476;&#65288;FSN&#65289;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#36739;&#20302;&#33021;&#32791;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#39592;&#39612;&#21160;&#20316;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#21160;&#20316;&#35782;&#21035;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01701</link><description>&lt;p&gt;
Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Signal-SGN&#30340;&#31361;&#35302;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#36890;&#36807;&#23398;&#20064;&#26102;&#39057;&#21160;&#24577;&#26469;&#35782;&#21035;&#39592;&#39612;&#21160;&#20316;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#37319;&#29992;&#26102;&#24207;&#29305;&#24449;&#20316;&#20026;&#31361;&#35302;&#35302;&#21457;&#27493;&#38271;&#22788;&#29702;&#39592;&#39612;&#24207;&#21015;&#65292;&#24182;&#23558;&#29305;&#24449;&#35270;&#20026;&#31163;&#25955;&#30340;&#38543;&#26426;&#20449;&#21495;&#12290;&#35813;&#32593;&#32476;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;1D&#31361;&#35302;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;1D-SGN&#65289;&#21644;&#19968;&#20010;&#39057;&#29575;&#31361;&#35302;&#21367;&#31215;&#32593;&#32476;&#65288;FSN&#65289;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#36739;&#20302;&#33021;&#32791;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#39592;&#39612;&#21160;&#20316;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#21160;&#20316;&#35782;&#21035;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01701v1 Announce Type: new  Abstract: In skeletal-based action recognition, Graph Convolutional Networks (GCNs) based methods face limitations due to their complexity and high energy consumption. Spiking Neural Networks (SNNs) have gained attention in recent years for their low energy consumption, but existing methods combining GCNs and SNNs fail to fully utilize the temporal characteristics of skeletal sequences, leading to increased storage and computational costs. To address this issue, we propose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the temporal dimension of skeletal sequences as the spiking timestep and treats features as discrete stochastic signals. The core of the network consists of a 1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking Convolutional Network (FSN). The SGN performs graph convolution on single frames and incorporates spiking network characteristics to capture inter-frame temporal relationships, while th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#20687;&#32032;&#32423;&#26631;&#27880;&#19979;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#24179;&#34913;&#29109;&#65288;Balanced Entropy&#65292;BalEnt&#65289;&#30340;&#20687;&#32032;&#32423;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24230;&#24230;&#37327;&#65292;&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#20687;&#32032;&#36827;&#34892;&#26631;&#27880;&#65292;&#20174;&#32780;&#20197;&#36739;&#23567;&#30340;&#26631;&#27880;&#25104;&#26412;&#23454;&#29616;&#19982;&#23436;&#20840;&#30417;&#30563;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01694</link><description>&lt;p&gt;
Bayesian Active Learning for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#20687;&#32032;&#32423;&#26631;&#27880;&#19979;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#24179;&#34913;&#29109;&#65288;Balanced Entropy&#65292;BalEnt&#65289;&#30340;&#20687;&#32032;&#32423;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24230;&#24230;&#37327;&#65292;&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#20687;&#32032;&#36827;&#34892;&#26631;&#27880;&#65292;&#20174;&#32780;&#20197;&#36739;&#23567;&#30340;&#26631;&#27880;&#25104;&#26412;&#23454;&#29616;&#19982;&#23436;&#20840;&#30417;&#30563;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01694v1 Announce Type: new  Abstract: Fully supervised training of semantic segmentation models is costly and challenging because each pixel within an image needs to be labeled. Therefore, the sparse pixel-level annotation methods have been introduced to train models with a subset of pixels within each image. We introduce a Bayesian active learning framework based on sparse pixel-level annotation that utilizes a pixel-level Bayesian uncertainty measure based on Balanced Entropy (BalEnt) [84]. BalEnt captures the information between the models' predicted marginalized probability distribution and the pixel labels. BalEnt has linear scalability with a closed analytical form and can be calculated independently per pixel without relational computations with other pixels. We train our proposed active learning framework for Cityscapes, Camvid, ADE20K and VOC2012 benchmark datasets and show that it reaches supervised levels of mIoU using only a fraction of labeled pixels while outpe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;CNN&#27169;&#22411;&#65288;&#21253;&#25324;U-Net&#12289;LinkNet&#12289;PSPNet&#12289;FPN&#31561;&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#36866;&#29992;&#20110; landslide detection&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#20316;&#32773;&#24378;&#35843;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#29575;&#31561;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.01692</link><description>&lt;p&gt;
A Comparative Analysis of CNN-based Deep Learning Models for Landslide Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;CNN&#27169;&#22411;&#65288;&#21253;&#25324;U-Net&#12289;LinkNet&#12289;PSPNet&#12289;FPN&#31561;&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#36866;&#29992;&#20110; landslide detection&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#20316;&#32773;&#24378;&#35843;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#29575;&#31561;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01692v1 Announce Type: new  Abstract: Landslides inflict substantial societal and economic damage, underscoring their global significance as recurrent and destructive natural disasters. Recent landslides in northern parts of India and Nepal have caused significant disruption, damaging infrastructure and posing threats to local communities. Convolutional Neural Networks (CNNs), a type of deep learning technique, have shown remarkable success in image processing. Because of their sophisticated architectures, advanced CNN-based models perform better in landslide detection than conventional algorithms. The purpose of this work is to investigate CNNs' potential in more detail, with an emphasis on comparison of CNN based models for better landslide detection. We compared four traditional semantic segmentation models (U-Net, LinkNet, PSPNet, and FPN) and utilized the ResNet50 backbone encoder to implement them. Moreover, we have experimented with the hyperparameters such as learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#27450;&#35784;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#26469;&#25512;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01690</link><description>&lt;p&gt;
IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#27450;&#35784;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#26469;&#25512;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01690v1 Announce Type: new  Abstract: Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy.   In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection e
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#949;-&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#21487;&#25511;&#21435;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#30340;&#19981;&#21516;&#26399;&#26395;&#12290;</title><link>https://arxiv.org/abs/2408.01689</link><description>&lt;p&gt;
Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#949;-&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#21487;&#25511;&#21435;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#30340;&#19981;&#21516;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01689v1 Announce Type: cross  Abstract: While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\varepsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\varepsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearni
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamMo&#30340;Siamese&#22522;&#20110;&#36816;&#21160;&#30340;&#19977;&#32500;&#29289;&#20307;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;Siamese&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#20811;&#26381;&#20256;&#32479;&#21333;&#27969;&#36319;&#36394;&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#22312;Siamese&#29305;&#24449;&#20013;&#24341;&#20837;&#20102;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#23545;&#35937;&#30418;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#27169;&#22359;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#36319;&#36394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01688</link><description>&lt;p&gt;
SiamMo: Siamese Motion-Centric 3D Object Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamMo&#30340;Siamese&#22522;&#20110;&#36816;&#21160;&#30340;&#19977;&#32500;&#29289;&#20307;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;Siamese&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#20811;&#26381;&#20256;&#32479;&#21333;&#27969;&#36319;&#36394;&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#22312;Siamese&#29305;&#24449;&#20013;&#24341;&#20837;&#20102;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#23545;&#35937;&#30418;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#27169;&#22359;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#36319;&#36394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01688v1 Announce Type: new  Abstract: Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode obj
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21496;&#26426;&#34892;&#20026;&#20998;&#26512;&#20013;&#36827;&#34892;&#38271;&#25991;&#26412;&#25512;&#29702;&#65292;&#25913;&#36827;&#20102;&#23454;&#26102;&#30340;&#23433;&#20840;&#21644;&#24773;&#22659;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.01682</link><description>&lt;p&gt;
Multi-Frame Vision-Language Model for Long-form Reasoning in Driver Behavior Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21496;&#26426;&#34892;&#20026;&#20998;&#26512;&#20013;&#36827;&#34892;&#38271;&#25991;&#26412;&#25512;&#29702;&#65292;&#25913;&#36827;&#20102;&#23454;&#26102;&#30340;&#23433;&#20840;&#21644;&#24773;&#22659;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01682v1 Announce Type: cross  Abstract: Identifying risky driving behavior in real-world situations is essential for the safety of both drivers and pedestrians. However, integrating natural language models in this field remains relatively untapped. To address this, we created a novel multi-modal instruction tuning dataset and driver coaching inference system. Our primary use case is dashcam-based coaching for commercial drivers. The North American Dashcam Market is expected to register a CAGR of 15.4 percent from 2022 to 2027. Our dataset enables language models to learn visual instructions across various risky driving scenarios, emphasizing detailed reasoning crucial for effective driver coaching and managerial comprehension. Our model is trained on road-facing and driver-facing RGB camera footage, capturing the comprehensive scope of driving behavior in vehicles equipped with dashcams.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;iControl3D&#30340;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20801;&#35768;&#29992;&#25143;&#22312;&#29983;&#25104;&#21644;&#28210;&#26579;&#21487;&#33258;&#23450;&#20041;&#30340;3D&#22330;&#26223;&#26102;&#25317;&#26377;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;&#36825;&#19968;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#32467;&#21512;3D&#32593;&#26684;&#21644;2D&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#25511;&#21046;3D&#22330;&#26223;&#21019;&#24314;&#36807;&#31243;&#30340;&#30028;&#38754;&#12290;&#27492;&#22806;&#65292;&#31995;&#32479;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#30028;&#24863;&#30693;&#28145;&#24230;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#20102;3D&#29289;&#20307;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#36890;&#36807;&#36825;&#20123;&#25216;&#26415;&#65292;iControl3D&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#30340;&#30446;&#26631;&#65292;&#22635;&#34917;&#20102;&#29992;&#25143;&#21019;&#24847;&#19982;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#25511;&#21046;&#19981;&#36275;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2408.01678</link><description>&lt;p&gt;
iControl3D: An Interactive System for Controllable 3D Scene Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;iControl3D&#30340;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20801;&#35768;&#29992;&#25143;&#22312;&#29983;&#25104;&#21644;&#28210;&#26579;&#21487;&#33258;&#23450;&#20041;&#30340;3D&#22330;&#26223;&#26102;&#25317;&#26377;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;&#36825;&#19968;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#32467;&#21512;3D&#32593;&#26684;&#21644;2D&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#25511;&#21046;3D&#22330;&#26223;&#21019;&#24314;&#36807;&#31243;&#30340;&#30028;&#38754;&#12290;&#27492;&#22806;&#65292;&#31995;&#32479;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#30028;&#24863;&#30693;&#28145;&#24230;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#20102;3D&#29289;&#20307;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#36890;&#36807;&#36825;&#20123;&#25216;&#26415;&#65292;iControl3D&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#30340;&#30446;&#26631;&#65292;&#22635;&#34917;&#20102;&#29992;&#25143;&#21019;&#24847;&#19982;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#25511;&#21046;&#19981;&#36275;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01678v1 Announce Type: new  Abstract: 3D content creation has long been a complex and time-consuming process, often requiring specialized skills and resources. While recent advancements have allowed for text-guided 3D object and scene generation, they still fall short of providing sufficient control over the generation process, leading to a gap between the user's creative vision and the generated results. In this paper, we present iControl3D, a novel interactive system that empowers users to generate and render customizable 3D scenes with precise control. To this end, a 3D creator interface has been developed to provide users with fine-grained control over the creation process. Technically, we leverage 3D meshes as an intermediary proxy to iteratively merge individual 2D diffusion-generated images into a cohesive and unified 3D scene representation. To ensure seamless integration of 3D meshes, we propose to perform boundary-aware depth alignment before fusing the newly gener
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;HIVE&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#21270;&#30340;&#20307;&#31215;&#32534;&#30721;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#37325;&#24314;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#19977;&#32500;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2408.01677</link><description>&lt;p&gt;
HIVE: HIerarchical Volume Encoding for Neural Implicit Surface Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;HIVE&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#21270;&#30340;&#20307;&#31215;&#32534;&#30721;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#37325;&#24314;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#19977;&#32500;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01677v1 Announce Type: new  Abstract: Neural implicit surface reconstruction has become a new trend in reconstructing a detailed 3D shape from images. In previous methods, however, the 3D scene is only encoded by the MLPs which do not have an explicit 3D structure. To better represent 3D shapes, we introduce a volume encoding to explicitly encode the spatial information. We further design hierarchical volumes to encode the scene structures in multiple scales. The high-resolution volumes capture the high-frequency geometry details since spatially varying features could be learned from different 3D points, while the low-resolution volumes enforce the spatial consistency to keep the shape smooth since adjacent locations possess the same low-resolution feature. In addition, we adopt a sparse structure to reduce the memory consumption at high-resolution volumes, and two regularization terms to enhance results smoothness. This hierarchical volume encoding could be appended to any 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#35270;&#39057;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;SynopGround&#8221;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;&#27969;&#34892;&#30340;&#30005;&#35270;&#21095;&#35270;&#39057;&#21644;&#35814;&#32454;&#30340;&#21095;&#24773;&#27010;&#35201;&#65292;&#20197;&#20419;&#36827;&#23545;&#35270;&#39057;&#20869;&#23481;&#30340;&#28145;&#23618;&#27425;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.01669</link><description>&lt;p&gt;
SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01669
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#35270;&#39057;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;SynopGround&#8221;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;&#27969;&#34892;&#30340;&#30005;&#35270;&#21095;&#35270;&#39057;&#21644;&#35814;&#32454;&#30340;&#21095;&#24773;&#27010;&#35201;&#65292;&#20197;&#20419;&#36827;&#23545;&#35270;&#39057;&#20869;&#23481;&#30340;&#28145;&#23618;&#27425;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01669v1 Announce Type: new  Abstract: Video grounding is a fundamental problem in multimodal content understanding, aiming to localize specific natural language queries in an untrimmed video. However, current video grounding datasets merely focus on simple events and are either limited to shorter videos or brief sentences, which hinders the model from evolving toward stronger multimodal understanding capabilities. To address these limitations, we present a large-scale video grounding dataset named SynopGround, in which more than 2800 hours of videos are sourced from popular TV dramas and are paired with accurately localized human-written synopses. Each paragraph in the synopsis serves as a language query and is manually annotated with precise temporal boundaries in the long video. These paragraph queries are tightly correlated to each other and contain a wealth of abstract expressions summarizing video storylines and specific descriptions portraying event details, which enab
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MkfaNet&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#20013;&#30340;&#8220;deepfake&#8221;&#25216;&#26415;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#26469;&#24037;&#20316;&#65306;&#19968;&#26159;&#36890;&#36807;&#22810;&#26680;&#32858;&#21512;&#22120;&#36866;&#24212;&#24615;&#22320;&#36873;&#21462;&#19981;&#21516;&#21367;&#31215;&#25552;&#21462;&#30340;&#22120;&#23448;&#29305;&#24449;&#26469;&#24314;&#27169;&#30495;&#23454;&#19982;&#20266;&#36896;&#38754;&#37096;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#65292;&#20108;&#26159;&#36890;&#36807;&#22810;&#39057;&#29575;&#32858;&#21512;&#22120;&#22788;&#29702;&#39640;&#39057;&#25104;&#20998;&#65292;&#20174;&#32780;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#35782;&#21035;&#20986;&#35270;&#39057;&#20013;&#30340;&#20266;&#36896;&#30165;&#36857;&#65292;&#25552;&#39640;&#20102;deepfake&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01668</link><description>&lt;p&gt;
Multiple Contexts and Frequencies Aggregation Network forDeepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MkfaNet&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#20013;&#30340;&#8220;deepfake&#8221;&#25216;&#26415;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#26469;&#24037;&#20316;&#65306;&#19968;&#26159;&#36890;&#36807;&#22810;&#26680;&#32858;&#21512;&#22120;&#36866;&#24212;&#24615;&#22320;&#36873;&#21462;&#19981;&#21516;&#21367;&#31215;&#25552;&#21462;&#30340;&#22120;&#23448;&#29305;&#24449;&#26469;&#24314;&#27169;&#30495;&#23454;&#19982;&#20266;&#36896;&#38754;&#37096;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#65292;&#20108;&#26159;&#36890;&#36807;&#22810;&#39057;&#29575;&#32858;&#21512;&#22120;&#22788;&#29702;&#39640;&#39057;&#25104;&#20998;&#65292;&#20174;&#32780;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#35782;&#21035;&#20986;&#35270;&#39057;&#20013;&#30340;&#20266;&#36896;&#30165;&#36857;&#65292;&#25552;&#39640;&#20102;deepfake&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01668v1 Announce Type: new  Abstract: Deepfake detection faces increasing challenges since the fast growth of generative models in developing massive and diverse Deepfake technologies. Recent advances rely on introducing heuristic features from spatial or frequency domains rather than modeling general forgery features within backbones. To address this issue, we turn to the backbone design with two intuitive priors from spatial and frequency detectors, \textit{i.e.,} learning robust spatial attributes and frequency distributions that are discriminative for real and fake samples. To this end, we propose an efficient network for face forgery detection named MkfaNet, which consists of two core modules. For spatial contexts, we design a Multi-Kernel Aggregator that adaptively selects organ features extracted by multiple convolutions for modeling subtle facial differences between real and fake faces. For the frequency components, we propose a Multi-Frequency Aggregator to process 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#31354;&#38388;&#20013;&#35821;&#20041;&#23646;&#24615;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026;SAT3D&#65292;&#23427;&#33021;&#22815;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#23548;&#20986;&#24182;&#32534;&#36753;&#20855;&#20307;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#22914;&#30007;&#20154;&#30340;&#32993;&#39035;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#23646;&#24615;&#21644;&#39118;&#26684;&#20195;&#30721;&#36890;&#36947;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25506;&#32034;&#20102;3D-aware StyleGAN&#22522;&#30784;&#29983;&#25104;&#22120;&#30340;&#39118;&#26684;&#31354;&#38388;&#65292;&#24182;&#20026;&#27599;&#20010;&#23646;&#24615;&#24314;&#31435;&#20102;&#25551;&#36848;&#31526;&#32452;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#27979;&#37327;&#27169;&#22359;&#65292;&#20197;&#22312;&#22270;&#20687;&#20013;&#23450;&#37327;&#25551;&#36848;&#23646;&#24615;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2408.01664</link><description>&lt;p&gt;
SAT3D: Image-driven Semantic Attribute Transfer in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#31354;&#38388;&#20013;&#35821;&#20041;&#23646;&#24615;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026;SAT3D&#65292;&#23427;&#33021;&#22815;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#23548;&#20986;&#24182;&#32534;&#36753;&#20855;&#20307;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#22914;&#30007;&#20154;&#30340;&#32993;&#39035;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#23646;&#24615;&#21644;&#39118;&#26684;&#20195;&#30721;&#36890;&#36947;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25506;&#32034;&#20102;3D-aware StyleGAN&#22522;&#30784;&#29983;&#25104;&#22120;&#30340;&#39118;&#26684;&#31354;&#38388;&#65292;&#24182;&#20026;&#27599;&#20010;&#23646;&#24615;&#24314;&#31435;&#20102;&#25551;&#36848;&#31526;&#32452;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#27979;&#37327;&#27169;&#22359;&#65292;&#20197;&#22312;&#22270;&#20687;&#20013;&#23450;&#37327;&#25551;&#36848;&#23646;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01664v1 Announce Type: new  Abstract: GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor group
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep Patch Visual (DPV) SLAM&#30340;&#26032;&#30340;&#21333;&#30446;&#35270;&#35273;SLAM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;GPU&#26469;&#32500;&#25252;&#39640;&#24103;&#29575;&#24182;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#19982;&#20043;&#21069;&#26041;&#27861;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#25913;&#36827;&#65292;DPV-SLAM&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#25104;&#20026;&#35270;&#35273;SLAM&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.01654</link><description>&lt;p&gt;
Deep Patch Visual SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep Patch Visual (DPV) SLAM&#30340;&#26032;&#30340;&#21333;&#30446;&#35270;&#35273;SLAM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;GPU&#26469;&#32500;&#25252;&#39640;&#24103;&#29575;&#24182;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#19982;&#20043;&#21069;&#26041;&#27861;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#25913;&#36827;&#65292;DPV-SLAM&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#25104;&#20026;&#35270;&#35273;SLAM&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01654v1 Announce Type: new  Abstract: Recent work in visual SLAM has shown the effectiveness of using deep network backbones. Despite excellent accuracy, however, such approaches are often expensive to run or do not generalize well zero-shot. Their runtime can also fluctuate wildly while their frontend and backend fight for access to GPU resources. To address these problems, we introduce Deep Patch Visual (DPV) SLAM, a method for monocular visual SLAM on a single GPU. DPV-SLAM maintains a high minimum framerate and small memory overhead (5-7G) compared to existing deep SLAM systems. On real-world datasets, DPV-SLAM runs at 1x-4x real-time framerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir while running 2.5x faster using a fraction of the memory. DPV-SLAM is an extension to the DPVO visual odometry system; its code can be found in the same repository: https://github.com/princeton-vl/DPVO
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;MCPDepth&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#24133;&#22278;&#26609;&#24418;&#20840;&#26223;&#22270;&#20043;&#38388;&#30340;&#31435;&#20307;&#21305;&#37197;&#36827;&#34892;&#20840;&#26041;&#20301;&#28145;&#24230;&#20272;&#35745;&#65292;&#24182;&#19988;&#36890;&#36807;&#34701;&#21512;&#19981;&#21516;&#35270;&#35282;&#30340;&#28145;&#24230;&#22270;&#26469;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#29615;&#24418;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#32531;&#35299;&#22402;&#30452;&#26041;&#21521;&#19978;&#30340;&#22833;&#30495;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#32593;&#32476;&#32452;&#20214;&#31616;&#21270;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#65292;&#21516;&#26102;&#22312;&#19981;&#38656;&#35201;&#23450;&#21046;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;3D60&#21644;Deep360&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#30456;&#23545;&#20110;&#21069;&#20154;&#26041;&#27861;&#20998;&#21035;&#20943;&#23569;&#20102;18.8%&#21644;19.9%&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2408.01653</link><description>&lt;p&gt;
MCPDepth: Omnidirectional Depth Estimation via Stereo Matching from Multi-Cylindrical Panoramas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01653
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;MCPDepth&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#24133;&#22278;&#26609;&#24418;&#20840;&#26223;&#22270;&#20043;&#38388;&#30340;&#31435;&#20307;&#21305;&#37197;&#36827;&#34892;&#20840;&#26041;&#20301;&#28145;&#24230;&#20272;&#35745;&#65292;&#24182;&#19988;&#36890;&#36807;&#34701;&#21512;&#19981;&#21516;&#35270;&#35282;&#30340;&#28145;&#24230;&#22270;&#26469;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#29615;&#24418;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#32531;&#35299;&#22402;&#30452;&#26041;&#21521;&#19978;&#30340;&#22833;&#30495;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#32593;&#32476;&#32452;&#20214;&#31616;&#21270;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#65292;&#21516;&#26102;&#22312;&#19981;&#38656;&#35201;&#23450;&#21046;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;3D60&#21644;Deep360&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#30456;&#23545;&#20110;&#21069;&#20154;&#26041;&#27861;&#20998;&#21035;&#20943;&#23569;&#20102;18.8%&#21644;19.9%&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01653v1 Announce Type: new  Abstract: We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a two-stage framework for omnidirectional depth estimation via stereo matching between multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for initial stereo matching and then fuses the resulting depth maps across views. A circular attention module is employed to overcome the distortion along the vertical axis. MCPDepth exclusively utilizes standard network components, simplifying deployment to embedded devices and outperforming previous methods that require custom kernels. We theoretically and experimentally compare spherical and cylindrical projections for stereo matching, highlighting the advantages of the cylindrical projection. MCPDepth achieves state-of-the-art performance with an 18.8% reduction in mean absolute error (MAE) for depth on the outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor real-scene dataset 3D60.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20351;&#29992;Segment Anything Model 2 (SAM 2)&#36827;&#34892;&#38646;&#26679;&#26412;&#25163;&#26415;&#24037;&#20855;&#22312;&#21333;&#30446;&#35270;&#39057;&#20013;&#30340;&#20998;&#21106;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#19979;&#30340;&#25163;&#26415;&#35270;&#39057;&#20998;&#26512;&#25928;&#29575;&#65292;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#35299;&#26512;&#33021;&#21147;&#21644;&#20302;&#20869;&#23384;&#28040;&#32791;&#65292;&#20026;&#25163;&#26415;&#22120;&#26800;&#35782;&#21035;&#21644;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.01648</link><description>&lt;p&gt;
Zero-Shot Surgical Tool Segmentation in Monocular Video Using Segment Anything Model 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01648
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20351;&#29992;Segment Anything Model 2 (SAM 2)&#36827;&#34892;&#38646;&#26679;&#26412;&#25163;&#26415;&#24037;&#20855;&#22312;&#21333;&#30446;&#35270;&#39057;&#20013;&#30340;&#20998;&#21106;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#19979;&#30340;&#25163;&#26415;&#35270;&#39057;&#20998;&#26512;&#25928;&#29575;&#65292;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#35299;&#26512;&#33021;&#21147;&#21644;&#20302;&#20869;&#23384;&#28040;&#32791;&#65292;&#20026;&#25163;&#26415;&#22120;&#26800;&#35782;&#21035;&#21644;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01648v1 Announce Type: cross  Abstract: The Segment Anything Model 2 (SAM 2) is the latest generation foundation model for image and video segmentation. Trained on the expansive Segment Anything Video (SA-V) dataset, which comprises 35.5 million masks across 50.9K videos, SAM 2 advances its predecessor's capabilities by supporting zero-shot segmentation through various prompts (e.g., points, boxes, and masks). Its robust zero-shot performance and efficient memory usage make SAM 2 particularly appealing for surgical tool segmentation in videos, especially given the scarcity of labeled data and the diversity of surgical procedures. In this study, we evaluate the zero-shot video segmentation performance of the SAM 2 model across different types of surgeries, including endoscopy and microscopy. We also assess its performance on videos featuring single and multiple tools of varying lengths to demonstrate SAM 2's applicability and effectiveness in the surgical domain. We found tha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36733;GNSS&#21644;&#26222;&#36890;&#36710;&#36742;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#22270;&#20687;&#25968;&#25454;&#33258;&#21160;&#26500;&#24314;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36947;&#36335;&#20013;&#24515;&#32447;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26368;&#23567;&#21270;&#20102;&#23545;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#20351;&#36947;&#36335;&#22270;&#30340;&#33258;&#21160;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01640</link><description>&lt;p&gt;
Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36733;GNSS&#21644;&#26222;&#36890;&#36710;&#36742;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#22270;&#20687;&#25968;&#25454;&#33258;&#21160;&#26500;&#24314;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36947;&#36335;&#20013;&#24515;&#32447;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26368;&#23567;&#21270;&#20102;&#23545;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#20351;&#36947;&#36335;&#22270;&#30340;&#33258;&#21160;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01640v1 Announce Type: cross  Abstract: Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today's advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data's time 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JambaTalk&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#28151;&#21512;&#30340;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;3D&#35828;&#35805;&#20154;&#22836;&#20687;&#29983;&#25104;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35828;&#35805;&#20154;&#22836;&#20687;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#20102;&#21767;&#21160;&#21516;&#27493;&#12289;&#38754;&#37096;&#34920;&#24773;&#34920;&#36798;&#21644;&#22836;&#37096;&#23039;&#24577;&#30340;&#33258;&#28982;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2408.01627</link><description>&lt;p&gt;
JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01627
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JambaTalk&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#28151;&#21512;&#30340;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;3D&#35828;&#35805;&#20154;&#22836;&#20687;&#29983;&#25104;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35828;&#35805;&#20154;&#22836;&#20687;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#20102;&#21767;&#21160;&#21516;&#27493;&#12289;&#38754;&#37096;&#34920;&#24773;&#34920;&#36798;&#21644;&#22836;&#37096;&#23039;&#24577;&#30340;&#33258;&#28982;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01627v1 Announce Type: new  Abstract: In recent years, talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high video quality. However, no single model has yet achieved equivalence across all these metrics. This paper aims to animate a 3D face using Jamba, a hybrid Transformers-Mamba model. Mamba, a pioneering Structured State Space Model (SSM) architecture, was designed to address the constraints of the conventional Transformer architecture. Nevertheless, it has several drawbacks. Jamba merges the advantages of both Transformer and Mamba approaches, providing a holistic solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and speed through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#19982;&#20154;&#31867;&#20132;&#20114;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#23454;&#29616;&#20174;&#22270;&#20687;&#20013;&#31934;&#30830;&#21010;&#20998;&#20986;&#19981;&#21516;&#32452;&#32455;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.01620</link><description>&lt;p&gt;
MedUHIP: Towards Human-In-the-Loop Medical Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#19982;&#20154;&#31867;&#20132;&#20114;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#23454;&#29616;&#20174;&#22270;&#20687;&#20013;&#31934;&#30830;&#21010;&#20998;&#20986;&#19981;&#21516;&#32452;&#32455;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01620v1 Announce Type: cross  Abstract: Although segmenting natural images has shown impressive performance, these techniques cannot be directly applied to medical image segmentation. Medical image segmentation is particularly complicated by inherent uncertainties. For instance, the ambiguous boundaries of tissues can lead to diverse but plausible annotations from different clinicians. These uncertainties cause significant discrepancies in clinical interpretations and impact subsequent medical interventions. Therefore, achieving quantitative segmentations from uncertain medical images becomes crucial in clinical practice. To address this, we propose a novel approach that integrates an \textbf{uncertainty-aware model} with \textbf{human-in-the-loop interaction}. The uncertainty-aware model proposes several plausible segmentations to address the uncertainties inherent in medical images, while the human-in-the-loop interaction iteratively modifies the segmentation under clinici
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20840;&#38754;&#22238;&#39038;&#21644;&#25193;&#23637;&#20102;&#22312;&#23545;&#35937;&#32423;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#20013;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#30340;&#20219;&#21153;&#39046;&#22495;&#65292;&#24182;&#35782;&#21035;&#20102;&#20116;&#31181;&#31574;&#30053;&#26469;&#24212;&#23545;&#35813;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#20419;&#36827;&#28145;&#24230;&#23398;&#20064;&#22312;OBIA&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.01607</link><description>&lt;p&gt;
Deep Learning Meets OBIA: Tasks, Challenges, Strategies, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20840;&#38754;&#22238;&#39038;&#21644;&#25193;&#23637;&#20102;&#22312;&#23545;&#35937;&#32423;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#20013;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#30340;&#20219;&#21153;&#39046;&#22495;&#65292;&#24182;&#35782;&#21035;&#20102;&#20116;&#31181;&#31574;&#30053;&#26469;&#24212;&#23545;&#35813;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#20419;&#36827;&#28145;&#24230;&#23398;&#20064;&#22312;OBIA&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01607v1 Announce Type: new  Abstract: Deep learning has gained significant attention in remote sensing, especially in pixel- or patch-level applications. Despite initial attempts to integrate deep learning into object-based image analysis (OBIA), its full potential remains largely unexplored. In this article, as OBIA usage becomes more widespread, we conducted a comprehensive review and expansion of its task subdomains, with or without the integration of deep learning. Furthermore, we have identified and summarized five prevailing strategies to address the challenge of deep learning's limitations in directly processing unstructured object data within OBIA, and this review also recommends some important future research directions. Our goal with these endeavors is to inspire more exploration in this fascinating yet overlooked area and facilitate the integration of deep learning into OBIA processing workflows.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20799;&#31461;&#32819;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;VGG16&#21644;MobileNet&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#38271;&#26399;&#25968;&#25454;&#36319;&#36394;&#20013;&#26174;&#31034;&#20102;&#22312;&#20799;&#31461;&#32676;&#20307;&#20013;&#35782;&#21035;&#32819;&#32441;&#30340;&#21487;&#34892;&#24615;&#65292;&#22635;&#34917;&#20102;&#32819;&#32441;&#35782;&#21035;&#25216;&#26415;&#22312;&#20799;&#31461;&#24180;&#40836;&#27573;&#24212;&#29992;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2408.01588</link><description>&lt;p&gt;
Deep Learning Approach for Ear Recognition and Longitudinal Evaluation in Children
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20799;&#31461;&#32819;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;VGG16&#21644;MobileNet&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#38271;&#26399;&#25968;&#25454;&#36319;&#36394;&#20013;&#26174;&#31034;&#20102;&#22312;&#20799;&#31461;&#32676;&#20307;&#20013;&#35782;&#21035;&#32819;&#32441;&#30340;&#21487;&#34892;&#24615;&#65292;&#22635;&#34917;&#20102;&#32819;&#32441;&#35782;&#21035;&#25216;&#26415;&#22312;&#20799;&#31461;&#24180;&#40836;&#27573;&#24212;&#29992;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01588v1 Announce Type: new  Abstract: Ear recognition as a biometric modality is becoming increasingly popular, with promising broader application areas. While current applications involve adults, one of the challenges in ear recognition for children is the rapid structural changes in the ear as they age. This work introduces a foundational longitudinal dataset collected from children aged 4 to 14 years over a 2.5-year period and evaluates ear recognition performance in this demographic. We present a deep learning based approach for ear recognition, using an ensemble of VGG16 and MobileNet, focusing on both adult and child datasets, with an emphasis on longitudinal evaluation for children.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;THOR2&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20445;&#30041;3D&#24418;&#29366;&#30340;&#20999;&#29255;&#24335;&#25299;&#25169;&#34920;&#31034;&#24182;&#21033;&#29992;Mapper&#31639;&#27861;&#23545;&#33394;&#24425;&#20449;&#24687;&#36827;&#34892;&#20999;&#29255;&#24335;&#32534;&#30721;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#23545;&#26410;&#30693;&#21644;&#26434;&#20081;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01579</link><description>&lt;p&gt;
THOR2: Leveraging Topological Soft Clustering of Color Space for Human-Inspired Object Recognition in Unseen Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01579
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;THOR2&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20445;&#30041;3D&#24418;&#29366;&#30340;&#20999;&#29255;&#24335;&#25299;&#25169;&#34920;&#31034;&#24182;&#21033;&#29992;Mapper&#31639;&#27861;&#23545;&#33394;&#24425;&#20449;&#24687;&#36827;&#34892;&#20999;&#29255;&#24335;&#32534;&#30721;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#23545;&#26410;&#30693;&#21644;&#26434;&#20081;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01579v1 Announce Type: new  Abstract: Visual object recognition in unseen and cluttered indoor environments is a challenging problem for mobile robots. This study presents a 3D shape and color-based descriptor, TOPS2, for point clouds generated from RGB-D images and an accompanying recognition framework, THOR2. The TOPS2 descriptor embodies object unity, a human cognition mechanism, by retaining the slicing-based topological representation of 3D shape from the TOPS descriptor while capturing object color information through slicing-based color embeddings computed using a network of coarse color regions. These color regions, analogous to the MacAdam ellipses identified in human color perception, are obtained using the Mapper algorithm, a topological soft-clustering technique. THOR2, trained using synthetic data, demonstrates markedly improved recognition accuracy compared to THOR, its 3D shape-based predecessor, on two benchmark real-world datasets: the OCID dataset capturing
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#20351;&#29992;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#65288;DAE&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#22238;&#24402;&#20013;&#29983;&#25104;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#20551;&#35774;&#35299;&#37322;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#25805;&#20316;&#65292;&#35813;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36830;&#32493;&#22320;&#21487;&#35270;&#21270;&#22312;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22312;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#25968;&#25454;&#25110;&#21333;&#29420;&#30340;&#29305;&#24615;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;DAE&#30340;&#33021;&#21147;&#65292;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#23500;&#26377;&#35821;&#20041;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01571</link><description>&lt;p&gt;
Counterfactual Explanations for Medical Image Classification and Regression using Diffusion Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#20351;&#29992;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#65288;DAE&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#22238;&#24402;&#20013;&#29983;&#25104;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#20551;&#35774;&#35299;&#37322;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#25805;&#20316;&#65292;&#35813;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36830;&#32493;&#22320;&#21487;&#35270;&#21270;&#22312;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22312;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#25968;&#25454;&#25110;&#21333;&#29420;&#30340;&#29305;&#24615;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;DAE&#30340;&#33021;&#21147;&#65292;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#23500;&#26377;&#35821;&#20041;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01571v1 Announce Type: new  Abstract: Counterfactual explanations (CEs) aim to enhance the interpretability of machine learning models by illustrating how alterations in input features would affect the resulting predictions. Common CE approaches require an additional model and are typically constrained to binary counterfactuals. In contrast, we propose a novel method that operates directly on the latent space of a generative model, specifically a Diffusion Autoencoder (DAE). This approach offers inherent interpretability by enabling the generation of CEs and the continuous visualization of the model's internal representation across decision boundaries.   Our method leverages the DAE's ability to encode images into a semantically rich latent space in an unsupervised manner, eliminating the need for labeled data or separate feature extraction models. We show that these latent representations are helpful for medical condition classification and the ordinal regression of severit
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#39564;&#35777;&#20102;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#29992;&#20110;&#26816;&#32034;&#32452;&#32455;&#22270;&#20687;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#24555;&#36895;&#20934;&#30830;&#22320;&#27604;&#36739;&#30149;&#29702;&#27169;&#24335;&#21644;&#26597;&#25214;&#31867;&#20284;&#30149;&#20363;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01570</link><description>&lt;p&gt;
On Validation of Search &amp; Retrieval of Tissue Images in Digital Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#39564;&#35777;&#20102;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#29992;&#20110;&#26816;&#32034;&#32452;&#32455;&#22270;&#20687;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#24555;&#36895;&#20934;&#30830;&#22320;&#27604;&#36739;&#30149;&#29702;&#27169;&#24335;&#21644;&#26597;&#25214;&#31867;&#20284;&#30149;&#20363;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01570v1 Announce Type: cross  Abstract: Medical images play a crucial role in modern healthcare by providing vital information for diagnosis, treatment planning, and disease monitoring. Fields such as radiology and pathology rely heavily on accurate image interpretation, with radiologists examining X-rays, CT scans, and MRIs to diagnose conditions from fractures to cancer, while pathologists use microscopy and digital images to detect cellular abnormalities for diagnosing cancers and infections. The technological advancements have exponentially increased the volume and complexity of medical images, necessitating efficient tools for management and retrieval. Content-Based Image Retrieval (CBIR) systems address this need by searching and retrieving images based on visual content, enhancing diagnostic accuracy by allowing clinicians to find similar cases and compare pathological patterns. Comprehensive validation of image search engines in medical applications involves evaluati
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20840;&#35282;&#24230;&#22836;&#37096;&#23039;&#24577;&#20960;&#20309;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31934;&#30830;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#35282;&#30340;&#30830;&#23450;&#65292;&#20197;&#21450;&#23545;&#26059;&#36716;&#30697;&#38453;&#30340;&#20960;&#20309;&#22686;&#24378;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36793;&#30028;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26356;&#24191;&#27867;&#30340;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2408.01566</link><description>&lt;p&gt;
Full-range Head Pose Geometric Data Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20840;&#35282;&#24230;&#22836;&#37096;&#23039;&#24577;&#20960;&#20309;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31934;&#30830;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#35282;&#30340;&#30830;&#23450;&#65292;&#20197;&#21450;&#23545;&#26059;&#36716;&#30697;&#38453;&#30340;&#20960;&#20309;&#22686;&#24378;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36793;&#30028;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26356;&#24191;&#27867;&#30340;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01566v1 Announce Type: new  Abstract: Many head pose estimation (HPE) methods promise the ability to create full-range datasets, theoretically allowing the estimation of the rotation and positioning of the head from various angles. However, these methods are only accurate within a range of head angles; exceeding this specific range led to significant inaccuracies. This is dominantly explained by unclear specificity of the coordinate systems and Euler Angles used in the foundational rotation matrix calculations. Here, we addressed these limitations by presenting (1) methods that accurately infer the correct coordinate system and Euler angles in the correct axis-sequence, (2) novel formulae for 2D geometric augmentations of the rotation matrices under the (SPECIFIC) coordinate system, (3) derivations for the correct drawing routines for rotation matrices and poses, and (4) mathematical experimentation and verification that allow proper pitch-yaw coverage for full-range head po
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21033;&#29992;&#20102;&#30456;&#26426;&#26412;&#36523;&#30340;&#20869;&#37096;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#35774;&#22791;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#26426;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#24615;&#33021;&#24182;&#35299;&#20915;&#23610;&#24230;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01565</link><description>&lt;p&gt;
Self-Supervised Depth Estimation Based on Camera Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21033;&#29992;&#20102;&#30456;&#26426;&#26412;&#36523;&#30340;&#20869;&#37096;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#35774;&#22791;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#26426;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#24615;&#33021;&#24182;&#35299;&#20915;&#23610;&#24230;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01565v1 Announce Type: new  Abstract: Depth estimationn is a critical topic for robotics and vision-related tasks. In monocular depth estimation, in comparison with supervised learning that requires expensive ground truth labeling, self-supervised methods possess great potential due to no labeling cost. However, self-supervised learning still has a large gap with supervised learning in depth estimation performance. Meanwhile, scaling is also a major issue for monocular unsupervised depth estimation, which commonly still needs ground truth scale from GPS, LiDAR, or existing maps to correct. In deep learning era, while existing methods mainly rely on the exploration of image relationships to train the unsupervised neural networks, fundamental information provided by the camera itself has been generally ignored, which can provide extensive supervision information for free, without the need for any extra equipment to provide supervision signals. Utilizing the camera itself's int
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#20840;&#23616;&#32622;&#20449;&#24230;&#35780;&#20998;&#26469;&#21152;&#36895;&#22522;&#20110;&#22495;&#24847;&#35782;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22270;&#20687;&#21644;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#21516;&#31867;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#25552;&#39640;&#22270;&#20687;&#30340;&#20840;&#23616;&#32622;&#20449;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#20154;&#20026;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2408.01558</link><description>&lt;p&gt;
Accelerating Domain-Aware Electron Microscopy Analysis Using Deep Learning Models with Synthetic Data and Image-Wide Confidence Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#20840;&#23616;&#32622;&#20449;&#24230;&#35780;&#20998;&#26469;&#21152;&#36895;&#22522;&#20110;&#22495;&#24847;&#35782;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22270;&#20687;&#21644;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#21516;&#31867;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#25552;&#39640;&#22270;&#20687;&#30340;&#20840;&#23616;&#32622;&#20449;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#20154;&#20026;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01558v1 Announce Type: new  Abstract: The integration of machine learning (ML) models enhances the efficiency, affordability, and reliability of feature detection in microscopy, yet their development and applicability are hindered by the dependency on scarce and often flawed manually labeled datasets and a lack of domain awareness. We addressed these challenges by creating a physics-based synthetic image and data generator, resulting in a machine learning model that achieves comparable precision (0.86), recall (0.63), F1 scores (0.71), and engineering property predictions (R2=0.82) to a model trained on human-labeled data. We enhanced both models by using feature prediction confidence scores to derive an image-wide confidence metric, enabling simple thresholding to eliminate ambiguous and out-of-domain images resulting in performance boosts of 5-30% with a filtering-out rate of 25%. Our study demonstrates that synthetic data can eliminate human reliance in ML and provides a 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#24418;&#24577;&#23398;&#25216;&#26415;&#31934;&#30830;&#26500;&#24314;&#26893;&#20837;&#33181;&#20851;&#33410;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#21160;&#20998;&#21106;&#21644;&#24418;&#24577;&#29983;&#25104;&#31639;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#33181;&#20851;&#33410;&#26893;&#20837;&#29289;&#27169;&#22411;&#37325;&#24314;&#30340;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#23545;&#20248;&#21270;&#25163;&#26415;&#35268;&#21010;&#21644;&#25552;&#39640;&#26415;&#21518;&#24247;&#22797;&#25928;&#26524;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.01557</link><description>&lt;p&gt;
Enhanced Knee Kinematics: Leveraging Deep Learning and Morphing Algorithms for 3D Implant Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#24418;&#24577;&#23398;&#25216;&#26415;&#31934;&#30830;&#26500;&#24314;&#26893;&#20837;&#33181;&#20851;&#33410;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#21160;&#20998;&#21106;&#21644;&#24418;&#24577;&#29983;&#25104;&#31639;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#33181;&#20851;&#33410;&#26893;&#20837;&#29289;&#27169;&#22411;&#37325;&#24314;&#30340;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#23545;&#20248;&#21270;&#25163;&#26415;&#35268;&#21010;&#21644;&#25552;&#39640;&#26415;&#21518;&#24247;&#22797;&#25928;&#26524;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01557v1 Announce Type: cross  Abstract: Accurate reconstruction of implanted knee models is crucial in orthopedic surgery and biomedical engineering, enhancing preoperative planning, optimizing implant design, and improving surgical outcomes. Traditional methods rely on labor-intensive and error-prone manual segmentation. This study proposes a novel approach using machine learning (ML) algorithms and morphing techniques for precise 3D reconstruction of implanted knee models.   The methodology begins with acquiring preoperative imaging data, such as fluoroscopy or X-ray images of the patient's knee joint. A convolutional neural network (CNN) is then trained to automatically segment the femur contour of the implanted components, significantly reducing manual effort and ensuring high accuracy.   Following segmentation, a morphing algorithm generates a personalized 3D model of the implanted knee joint, using the segmented data and biomechanical principles. This algorithm conside
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#39318;&#27425;&#23558;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#23545;&#32963;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#35786;&#26029;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#25968;&#25454;&#31232;&#32570;&#21644;&#20559;&#35265;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20811;&#26381;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#26465;&#20214;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01554</link><description>&lt;p&gt;
Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#39318;&#27425;&#23558;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#23545;&#32963;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#35786;&#26029;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#25968;&#25454;&#31232;&#32570;&#21644;&#20559;&#35265;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20811;&#26381;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#26465;&#20214;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01554v1 Announce Type: new  Abstract: In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104; adversarial &#32593;&#32476;&#30340; unsupervised &#32534;&#36753;&#26041;&#27861; GUE&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20219;&#21153;&#30340;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#65292;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#27169;&#24335;&#26469;&#23454;&#29616;&#22270;&#20687;&#32534;&#36753;&#65292;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#22312; GAN &#30340; latent &#31354;&#38388;&#20013;&#25214;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#22788;&#29702;&#21151;&#33021;&#65292;&#20026; SAR &#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#30340; unsupervised &#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01553</link><description>&lt;p&gt;
Multi-task SAR Image Processing via GAN-based Unsupervised Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104; adversarial &#32593;&#32476;&#30340; unsupervised &#32534;&#36753;&#26041;&#27861; GUE&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20219;&#21153;&#30340;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#65292;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#27169;&#24335;&#26469;&#23454;&#29616;&#22270;&#20687;&#32534;&#36753;&#65292;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#22312; GAN &#30340; latent &#31354;&#38388;&#20013;&#25214;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#22788;&#29702;&#21151;&#33021;&#65292;&#20026; SAR &#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#30340; unsupervised &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01553v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) have shown tremendous potential in synthesizing a large number of realistic SAR images by learning patterns in the data distribution. Some GANs can achieve image editing by introducing latent codes, demonstrating significant promise in SAR image processing. Compared to traditional SAR image processing methods, editing based on GAN latent space control is entirely unsupervised, allowing image processing to be conducted without any labeled data. Additionally, the information extracted from the data is more interpretable. This paper proposes a novel SAR image processing framework called GAN-based Unsupervised Editing (GUE), aiming to address the following two issues: (1) disentangling semantic directions in the GAN latent space and finding meaningful directions; (2) establishing a comprehensive SAR image processing framework while achieving multiple image processing functions. In the implementation of 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28857;&#26435;&#35299;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#28857;&#20113;&#20998;&#21106;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22359;&#33021;&#22815;&#35299;&#20915;&#28857;&#20113;&#25237;&#24433;&#36807;&#31243;&#20013;&#20002;&#22833;&#28857;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#28857;&#20113;&#21407;&#26377;&#23494;&#24230;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#23545;&#32570;&#22833;&#30340;&#28857;&#36827;&#34892;&#27491;&#30830;&#30340;&#31867;&#21035;&#39044;&#27979;&#65292;&#20026;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29615;&#22659;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01548</link><description>&lt;p&gt;
Trainable Pointwise Decoder Module for Point Cloud Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01548
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28857;&#26435;&#35299;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#28857;&#20113;&#20998;&#21106;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22359;&#33021;&#22815;&#35299;&#20915;&#28857;&#20113;&#25237;&#24433;&#36807;&#31243;&#20013;&#20002;&#22833;&#28857;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#28857;&#20113;&#21407;&#26377;&#23494;&#24230;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#23545;&#32570;&#22833;&#30340;&#28857;&#36827;&#34892;&#27491;&#30830;&#30340;&#31867;&#21035;&#39044;&#27979;&#65292;&#20026;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29615;&#22659;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01548v1 Announce Type: new  Abstract: Point cloud segmentation (PCS) aims to make per-point predictions and enables robots and autonomous driving cars to understand the environment. The range image is a dense representation of a large-scale outdoor point cloud, and segmentation models built upon the image commonly execute efficiently. However, the projection of the point cloud onto the range image inevitably leads to dropping points because, at each image coordinate, only one point is kept despite multiple points being projected onto the same location. More importantly, it is challenging to assign correct predictions to the dropped points that belong to the classes different from the kept point class. Besides, existing post-processing methods, such as K-nearest neighbor (KNN) search and kernel point convolution (KPConv), cannot be trained with the models in an end-to-end manner or cannot process varying-density outdoor point clouds well, thereby enabling the models to achiev
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#38750;&#32447;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;Recurrence Plot&#25216;&#26415;&#22312;&#22810;&#36890;&#36947;ECG&#20449;&#21495;&#20013;&#25104;&#21151;&#20998;&#31867;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#22312;PhysioNet&#25968;&#25454;&#24211;&#30340;PTB&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#23545;&#24515;&#33039;&#30149;&#12289;&#26463;&#25903;&#38459;&#28382;&#12289;&#24515;&#32908;&#30149;&#21644;&#24515;&#24459;&#22833;&#24120;&#31561;&#22235;&#31181;&#30142;&#30149;&#30340;100%&#20934;&#30830;&#29575;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2408.01542</link><description>&lt;p&gt;
Non-linear Analysis Based ECG Classification of Cardiovascular Disorders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#38750;&#32447;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;Recurrence Plot&#25216;&#26415;&#22312;&#22810;&#36890;&#36947;ECG&#20449;&#21495;&#20013;&#25104;&#21151;&#20998;&#31867;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#22312;PhysioNet&#25968;&#25454;&#24211;&#30340;PTB&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#23545;&#24515;&#33039;&#30149;&#12289;&#26463;&#25903;&#38459;&#28382;&#12289;&#24515;&#32908;&#30149;&#21644;&#24515;&#24459;&#22833;&#24120;&#31561;&#22235;&#31181;&#30142;&#30149;&#30340;100%&#20934;&#30830;&#29575;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01542v1 Announce Type: new  Abstract: Multi-channel ECG-based cardiac disorders detection has an impact on cardiac care and treatment. Limitations of existing methods included variation in ECG waveforms due to the location of electrodes, high non-linearity in the signal, and amplitude measurement in millivolts. The present study reports a non-linear analysis-based methodology that utilizes Recurrence plot visualization. The patterned occurrence of well-defined structures, such as the QRS complex, can be exploited effectively using Recurrence plots. This Recurrence-based method is applied to the publicly available Physikalisch-Technische Bundesanstalt (PTB) dataset from PhysioNet database, where we studied four classes of different cardiac disorders (Myocardial infarction, Bundle branch blocks, Cardiomyopathy, and Dysrhythmia) and healthy controls, achieving an impressive classification accuracy of 100%. Additionally, t-SNE plot visualizations of the latent space embeddings d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#35780;&#20215;(IQA)&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;25&#31181;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#25239;&#32431;&#20928;&#12289;&#23545;&#25239;&#35757;&#32451;&#20197;&#21450;&#35748;&#35777;&#40065;&#26834;&#24615;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;14&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36866;&#24212;&#24615;&#21644;&#36866;&#24212;&#24615;&#25915;&#20987;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23545;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#30340;&#24212;&#29992;&#33539;&#22260;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#33021;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#21644;&#22270;&#20687;&#36136;&#37327;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#24182;&#19988;&#21521;&#25552;&#20132;&#26032;&#30340;&#38450;&#24481;&#31574;&#30053;&#24320;&#25918;&#12290;</title><link>https://arxiv.org/abs/2408.01541</link><description>&lt;p&gt;
Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#35780;&#20215;(IQA)&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;25&#31181;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#25239;&#32431;&#20928;&#12289;&#23545;&#25239;&#35757;&#32451;&#20197;&#21450;&#35748;&#35777;&#40065;&#26834;&#24615;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;14&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36866;&#24212;&#24615;&#21644;&#36866;&#24212;&#24615;&#25915;&#20987;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23545;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#30340;&#24212;&#29992;&#33539;&#22260;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#33021;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#21644;&#22270;&#20687;&#36136;&#37327;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#24182;&#19988;&#21521;&#25552;&#20132;&#26032;&#30340;&#38450;&#24481;&#31574;&#30053;&#24320;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01541v1 Announce Type: new  Abstract: In the field of Image Quality Assessment (IQA), the adversarial robustness of the metrics poses a critical concern. This paper presents a comprehensive benchmarking study of various defense mechanisms in response to the rise in adversarial attacks on IQA. We systematically evaluate 25 defense strategies, including adversarial purification, adversarial training, and certified robustness methods. We applied 14 adversarial attack algorithms of various types in both non-adaptive and adaptive settings and tested these defenses against them. We analyze the differences between defenses and their applicability to IQA tasks, considering that they should preserve IQA scores and image quality. The proposed benchmark aims to guide future developments and accepts submissions of new methods, with the latest results available online: https://videoprocessing.ai/benchmarks/iqa-defenses.html.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;SceneMotion&#27169;&#22411;&#65292;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#22810;&#20256;&#24863;&#22120;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#23558;&#23616;&#37096;&#20195;&#29702;&#29305;&#24449;&#26144;&#23556;&#21040;&#22330;&#26223;&#32423;&#21035;&#30340;&#39044;&#27979;&#20013;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#22810;&#20010;&#20132;&#36890;&#20195;&#29702;&#22330;&#26223;&#32423;&#36816;&#21160;&#27169;&#24335;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01537</link><description>&lt;p&gt;
SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;SceneMotion&#27169;&#22411;&#65292;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#22810;&#20256;&#24863;&#22120;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#23558;&#23616;&#37096;&#20195;&#29702;&#29305;&#24449;&#26144;&#23556;&#21040;&#22330;&#26223;&#32423;&#21035;&#30340;&#39044;&#27979;&#20013;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#22810;&#20010;&#20132;&#36890;&#20195;&#29702;&#22330;&#26223;&#32423;&#36816;&#21160;&#27169;&#24335;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01537v1 Announce Type: cross  Abstract: Self-driving vehicles rely on multimodal motion forecasts to effectively interact with their environment and plan safe maneuvers. We introduce SceneMotion, an attention-based model for forecasting scene-wide motion modes of multiple traffic agents. Our model transforms local agent-centric embeddings into scene-wide forecasts using a novel latent context module. This module learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling. The competitive performance in the Waymo Open Interaction Prediction Challenge demonstrates the effectiveness of our approach. Moreover, we cluster future waypoints in time and space to quantify the interaction between agents. We merge all modes and analyze each mode independently to determine which clusters are resolved through interaction or result in conflict. Our implementation is available at: https://github.com/kit-mrt/future-motion
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#23398;&#20064;&#27169;&#24577;&#20043;&#38388;&#30340;&#36129;&#29486;&#29305;&#24449;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#21495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24615;&#27169;&#24577;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01532</link><description>&lt;p&gt;
Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#23398;&#20064;&#27169;&#24577;&#20043;&#38388;&#30340;&#36129;&#29486;&#29305;&#24449;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#21495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24615;&#27169;&#24577;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25913;&#36827;&#30340;&#26629;&#26684;&#24335;&#27004;&#23618;&#24179;&#38754;&#22270;&#35821;&#20041;&#20998;&#21106;&#30340;&#22810;&#21333;&#20803;&#27004;&#23618;&#24179;&#38754;&#35782;&#21035;&#19982;&#37325;&#26500;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#25913;&#36827;&#36339;&#38142;&#25509;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#23618;&#38477;&#32500;U-Net&#65288;MDAU-Net&#65289;&#21644;&#22810;&#23618;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65288;MACU-Net&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#27004;&#23618;&#24179;&#38754;&#22270;&#30690;&#37327;&#21270;&#37325;&#26500;&#27969;&#31243;&#65292;&#25104;&#21151;&#23558;2D&#24314;&#31569;&#24179;&#38754;&#22270;&#36716;&#25442;&#20026;3D&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#22478;&#24066;&#31649;&#29702;&#12289;&#32039;&#24613;&#35268;&#21010;&#20013;&#30340;&#25968;&#23383;&#21452;&#32990;&#32974;&#30340;&#21019;&#24314;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#65292;&#23588;&#20854;&#26159;&#22312;&#24418;&#25104;&#32039;&#24613;&#36867;&#29983;&#36335;&#32447;&#12289;&#25913;&#21892;&#26041;&#21521;&#24863;&#20197;&#21450;&#21152;&#36895;&#25937;&#25588;&#24178;&#39044;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20854;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20026;&#31361;&#20986;&#65292;&#20026;3D&#20449;&#24687;&#20174;2D&#24179;&#38754;&#22270;&#30340;&#33258;&#21160;&#21270;&#21512;&#25104;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01526</link><description>&lt;p&gt;
Multi-Unit Floor Plan Recognition and Reconstruction Using Improved Semantic Segmentation of Raster-Wise Floor Plans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25913;&#36827;&#30340;&#26629;&#26684;&#24335;&#27004;&#23618;&#24179;&#38754;&#22270;&#35821;&#20041;&#20998;&#21106;&#30340;&#22810;&#21333;&#20803;&#27004;&#23618;&#24179;&#38754;&#35782;&#21035;&#19982;&#37325;&#26500;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#25913;&#36827;&#36339;&#38142;&#25509;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#23618;&#38477;&#32500;U-Net&#65288;MDAU-Net&#65289;&#21644;&#22810;&#23618;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65288;MACU-Net&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#27004;&#23618;&#24179;&#38754;&#22270;&#30690;&#37327;&#21270;&#37325;&#26500;&#27969;&#31243;&#65292;&#25104;&#21151;&#23558;2D&#24314;&#31569;&#24179;&#38754;&#22270;&#36716;&#25442;&#20026;3D&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#22478;&#24066;&#31649;&#29702;&#12289;&#32039;&#24613;&#35268;&#21010;&#20013;&#30340;&#25968;&#23383;&#21452;&#32990;&#32974;&#30340;&#21019;&#24314;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#65292;&#23588;&#20854;&#26159;&#22312;&#24418;&#25104;&#32039;&#24613;&#36867;&#29983;&#36335;&#32447;&#12289;&#25913;&#21892;&#26041;&#21521;&#24863;&#20197;&#21450;&#21152;&#36895;&#25937;&#25588;&#24178;&#39044;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20854;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20026;&#31361;&#20986;&#65292;&#20026;3D&#20449;&#24687;&#20174;2D&#24179;&#38754;&#22270;&#30340;&#33258;&#21160;&#21270;&#21512;&#25104;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01526v1 Announce Type: new  Abstract: Digital twins have a major potential to form a significant part of urban management in emergency planning, as they allow more efficient designing of the escape routes, better orientation in exceptional situations, and faster rescue intervention. Nevertheless, creating the twins still remains a largely manual effort, due to a lack of 3D-representations, which are available only in limited amounts for some new buildings. Thus, in this paper we aim to synthesize 3D information from commonly available 2D architectural floor plans. We propose two novel pixel-wise segmentation methods based on the MDA-Unet and MACU-Net architectures with improved skip connections, an attention mechanism, and a training objective together with a reconstruction part of the pipeline, which vectorizes the segmented plans to create a 3D model. The proposed methods are compared with two other state-of-the-art techniques and several benchmark datasets. On the commonl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;CNN&#27169;&#22411;&#65292;&#22312;&#25552;&#39640;&#20102;&#35780;&#20272;&#23398;&#29983;&#32472;&#30011;&#21019;&#24847;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#30456;&#23545;&#20110;&#20154;&#24037;&#35780;&#20998;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35780;&#20215;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2408.01481</link><description>&lt;p&gt;
Using a CNN Model to Assess Visual Artwork's Creativity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;CNN&#27169;&#22411;&#65292;&#22312;&#25552;&#39640;&#20102;&#35780;&#20272;&#23398;&#29983;&#32472;&#30011;&#21019;&#24847;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#30456;&#23545;&#20110;&#20154;&#24037;&#35780;&#20998;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35780;&#20215;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01481v1 Announce Type: new  Abstract: Assessing artistic creativity has long challenged researchers, with traditional methods proving time-consuming. Recent studies have applied machine learning to evaluate creativity in drawings, but not paintings. Our research addresses this gap by developing a CNN model to automatically assess the creativity of students' paintings. Using a dataset of 600 paintings by professionals and children, our model achieved 90% accuracy and faster evaluation times than human raters. This approach demonstrates the potential of machine learning in advancing artistic creativity assessment, offering a more efficient alternative to traditional methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#21033;&#29992;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65288;SD&#65289;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#22312;&#32447;&#36947;&#36335;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;SD&#22320;&#22270;&#30340;rasterized&#34920;&#31034;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#22312;&#32447;&#26144;&#23556;&#26550;&#26500;&#20013;&#65292;&#24182;&#25193;&#23637;OpenLane-V2&#25968;&#25454;&#38598;&#20197;&#32467;&#21512;OpenStreetMaps&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#22270;&#24418;SD&#22320;&#22270;&#34920;&#31034;&#65292;&#33021;&#22815;&#20026;&#33258;&#21160;&#36710;&#36742;&#22312;&#22478;&#24066;&#21644;&#39640;&#36895;&#20844;&#36335;&#29615;&#22659;&#19979;&#23548;&#33322;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;HD&#22320;&#22270;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;SD&#22320;&#22270;&#31574;&#30053;&#65292;&#25552;&#39640;&#22312;&#32447;&#26144;&#23556;&#31995;&#32479;&#23545;&#20110;&#21160;&#24577;&#29615;&#22659;&#20013;&#22823;&#37327;&#36974;&#25377;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01471</link><description>&lt;p&gt;
Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#21033;&#29992;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65288;SD&#65289;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#22312;&#32447;&#36947;&#36335;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;SD&#22320;&#22270;&#30340;rasterized&#34920;&#31034;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#22312;&#32447;&#26144;&#23556;&#26550;&#26500;&#20013;&#65292;&#24182;&#25193;&#23637;OpenLane-V2&#25968;&#25454;&#38598;&#20197;&#32467;&#21512;OpenStreetMaps&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#22270;&#24418;SD&#22320;&#22270;&#34920;&#31034;&#65292;&#33021;&#22815;&#20026;&#33258;&#21160;&#36710;&#36742;&#22312;&#22478;&#24066;&#21644;&#39640;&#36895;&#20844;&#36335;&#29615;&#22659;&#19979;&#23548;&#33322;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;HD&#22320;&#22270;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;SD&#22320;&#22270;&#31574;&#30053;&#65292;&#25552;&#39640;&#22312;&#32447;&#26144;&#23556;&#31995;&#32479;&#23545;&#20110;&#21160;&#24577;&#29615;&#22659;&#20013;&#22823;&#37327;&#36974;&#25377;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01471v1 Announce Type: cross  Abstract: Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definition (SD) maps-in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encode
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PreIndex&#30340;&#39044;&#27979;&#25351;&#26631;&#26469;&#20272;&#31639;&#27169;&#22411;&#22240;&#29615;&#22659;&#21464;&#21270;&#25110;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#25913;&#21464;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#29615;&#22659;&#25104;&#26412;&#65292;&#21253;&#25324;&#35745;&#31639;&#36164;&#28304;&#21644;&#30899;&#25490;&#25918;&#12290;&#36825;&#26377;&#21161;&#20110;&#20248;&#21270;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#65292;&#23454;&#29616;&#26356;&#29615;&#20445;&#21644;&#21487;&#25345;&#32493;&#30340;AI/&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.01446</link><description>&lt;p&gt;
Estimating Environmental Cost Throughout Model's Adaptive Life Cycle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PreIndex&#30340;&#39044;&#27979;&#25351;&#26631;&#26469;&#20272;&#31639;&#27169;&#22411;&#22240;&#29615;&#22659;&#21464;&#21270;&#25110;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#25913;&#21464;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#29615;&#22659;&#25104;&#26412;&#65292;&#21253;&#25324;&#35745;&#31639;&#36164;&#28304;&#21644;&#30899;&#25490;&#25918;&#12290;&#36825;&#26377;&#21161;&#20110;&#20248;&#21270;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#65292;&#23454;&#29616;&#26356;&#29615;&#20445;&#21644;&#21487;&#25345;&#32493;&#30340;AI/&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01446v1 Announce Type: cross  Abstract: With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Img2CAD&#30340;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;GPT-4V&#65289;&#21644;&#26465;&#20214;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#20174;&#22270;&#20687;&#20013;&#21453;&#21521;&#24037;&#31243;&#20986;&#31934;&#30830;&#30340;3D CAD&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20174;&#22270;&#20687;&#21040;CAD&#27169;&#22411;&#30340;&#25361;&#25112;&#24615;&#36716;&#25442;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01437</link><description>&lt;p&gt;
Img2CAD: Reverse Engineering 3D CAD Models from Images through VLM-Assisted Conditional Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Img2CAD&#30340;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;GPT-4V&#65289;&#21644;&#26465;&#20214;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#20174;&#22270;&#20687;&#20013;&#21453;&#21521;&#24037;&#31243;&#20986;&#31934;&#30830;&#30340;3D CAD&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20174;&#22270;&#20687;&#21040;CAD&#27169;&#22411;&#30340;&#25361;&#25112;&#24615;&#36716;&#25442;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01437v1 Announce Type: new  Abstract: Reverse engineering 3D computer-aided design (CAD) models from images is an important task for many downstream applications including interactive editing, manufacturing, architecture, robotics, etc. The difficulty of the task lies in vast representational disparities between the CAD output and the image input. CAD models are precise, programmatic constructs that involves sequential operations combining discrete command structure with continuous attributes -- making it challenging to learn and optimize in an end-to-end fashion. Concurrently, input images introduce inherent challenges such as photo-metric variability and sensor noise, complicating the reverse engineering process. In this work, we introduce a novel approach that conditionally factorizes the task into two sub-problems. First, we leverage large foundation models, particularly GPT-4V, to predict the global discrete base structure with semantic information. Second, we propose T
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#24182;&#37319;&#29992;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#20102;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#20013;&#30340;&#35270;&#28857;&#35268;&#21010;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23545;&#24314;&#31569;&#34920;&#38754;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2408.01435</link><description>&lt;p&gt;
A New Clustering-based View Planning Method for Building Inspection with Drone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#24182;&#37319;&#29992;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#20102;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#20013;&#30340;&#35270;&#28857;&#35268;&#21010;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23545;&#24314;&#31569;&#34920;&#38754;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01435v1 Announce Type: cross  Abstract: With the rapid development of drone technology, the application of drones equipped with visual sensors for building inspection and surveillance has attracted much attention. View planning aims to find a set of near-optimal viewpoints for vision-related tasks to achieve the vision coverage goal. This paper proposes a new clustering-based two-step computational method using spectral clustering, local potential field method, and hyper-heuristic algorithm to find near-optimal views to cover the target building surface. In the first step, the proposed method generates candidate viewpoints based on spectral clustering and corrects the positions of candidate viewpoints based on our newly proposed local potential field method. In the second step, the optimization problem is converted into a Set Covering Problem (SCP), and the optimal viewpoint subset is solved using our proposed hyper-heuristic algorithm. Experimental results show that the pro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#35780;&#20272;&#24182;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#35270;&#35273;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#20004;&#31181;&#20808;&#36827;&#30340;LLMs&#65292;&#21363;GPT-4V&#21644;LLaVA&#22312;&#20004;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#36710;&#36742;&#39640;&#32423;&#36741;&#21161;&#39550;&#39542;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01433</link><description>&lt;p&gt;
Evaluating and Enhancing Trustworthiness of LLMs in Perception Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01433
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#35780;&#20272;&#24182;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#35270;&#35273;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#20004;&#31181;&#20808;&#36827;&#30340;LLMs&#65292;&#21363;GPT-4V&#21644;LLaVA&#22312;&#20004;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#36710;&#36742;&#39640;&#32423;&#36741;&#21161;&#39550;&#39542;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01433v1 Announce Type: new  Abstract: Today's advanced driver assistance systems (ADAS), like adaptive cruise control or rear collision warning, are finding broader adoption across vehicle classes. Integrating such advanced, multimodal Large Language Models (LLMs) on board a vehicle, which are capable of processing text, images, audio, and other data types, may have the potential to greatly enhance passenger comfort. Yet, an LLM's hallucinations are still a major challenge to be addressed. In this paper, we systematically assessed potential hallucination detection strategies for such LLMs in the context of object detection in vision-based data on the example of pedestrian detection and localization. We evaluate three hallucination detection strategies applied to two state-of-the-art LLMs, the proprietary GPT-4V and the open LLaVA, on two datasets (Waymo/US and PREPER CITY/Sweden). Our results show that these LLMs can describe a traffic situation to an impressive level of det
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;VLG-CBM&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;-&#35821;&#35328;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;CBM&#65288;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65289;&#30340;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#27010;&#24565;&#35299;&#37322;&#19981;&#20934;&#30830;&#21644;&#27010;&#24565;&#20869;&#21547;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#21152;&#21487;&#20449;&#30340;&#22270;&#20687;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2408.01432</link><description>&lt;p&gt;
VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;VLG-CBM&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;-&#35821;&#35328;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;CBM&#65288;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65289;&#30340;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#27010;&#24565;&#35299;&#37322;&#19981;&#20934;&#30830;&#21644;&#27010;&#24565;&#20869;&#21547;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#21152;&#21487;&#20449;&#30340;&#22270;&#20687;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01432v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models (LLMs) and pre-trained Vision-Language Models (VLMs) to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boos
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUSTechGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#38632;&#22812;&#65289;&#19979;&#30340;&#39550;&#39542;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#30446;&#26631;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;YOLOv5&#30446;&#26631;&#35782;&#21035;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#26032;&#26679;&#26412;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#29983;&#25104;&#30340;&#26631;&#30340;&#22270;&#20687;&#65292;&#23601;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20854;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#30446;&#26631;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01430</link><description>&lt;p&gt;
SUSTechGAN: Image Generation for Object Recognition in Adverse Conditions of Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUSTechGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#38632;&#22812;&#65289;&#19979;&#30340;&#39550;&#39542;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#30446;&#26631;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;YOLOv5&#30446;&#26631;&#35782;&#21035;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#26032;&#26679;&#26412;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#29983;&#25104;&#30340;&#26631;&#30340;&#22270;&#20687;&#65292;&#23601;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20854;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#30446;&#26631;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01430v1 Announce Type: new  Abstract: Autonomous driving significantly benefits from data-driven deep neural networks. However, the data in autonomous driving typically fits the long-tailed distribution, in which the critical driving data in adverse conditions is hard to collect. Although generative adversarial networks (GANs) have been applied to augment data for autonomous driving, generating driving images in adverse conditions is still challenging. In this work, we propose a novel SUSTechGAN with dual attention modules and multi-scale generators to generate driving images for improving object recognition of autonomous driving in adverse conditions. We test the SUSTechGAN and the existing well-known GANs to generate driving images in adverse conditions of rain and night and apply the generated images to retrain object recognition networks. Specifically, we add generated images into the training datasets to retrain the well-known YOLOv5 and evaluate the improvement of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#20154;&#33080;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#26576;&#31181;&#38754;&#37096;&#29305;&#24449;&#65288;&#22914;&#22918;&#23481;&#20449;&#24687;&#65289;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#22320;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#21046;&#20316;&#20986;&#36924;&#30495;&#19988;&#39640;&#24230;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2408.01428</link><description>&lt;p&gt;
Transferable Adversarial Facial Images for Privacy Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#20154;&#33080;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#26576;&#31181;&#38754;&#37096;&#29305;&#24449;&#65288;&#22914;&#22918;&#23481;&#20449;&#24687;&#65289;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#22320;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#21046;&#20316;&#20986;&#36924;&#30495;&#19988;&#39640;&#24230;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01428v1 Announce Type: new  Abstract: The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent sear
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Siamese Transformer&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#32593;&#32476;&#21516;&#26102;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;K-shot&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01427</link><description>&lt;p&gt;
Siamese Transformer Networks for Few-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Siamese Transformer&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#32593;&#32476;&#21516;&#26102;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;K-shot&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01427v1 Announce Type: new  Abstract: Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global featur
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Hallu-PI&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#20026;&#22270;&#20687;&#25200;&#21160;&#19979;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#65292;&#22914;&#22270;&#20687;&#35009;&#21098;&#25110;&#27169;&#31946;&#65292;&#24182;&#19988;&#21253;&#25324;&#20102;7&#31181;&#19981;&#21516;&#30340;&#25805;&#32437;&#24773;&#26223;&#65292;&#20849;1260&#24352;&#24102;&#26377;&#31934;&#32454;&#26631;&#27880;&#30340;&#22270;&#20687;&#65292;&#20026;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#24819;&#35937;&#21147;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.01355</link><description>&lt;p&gt;
Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Hallu-PI&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#20026;&#22270;&#20687;&#25200;&#21160;&#19979;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#65292;&#22914;&#22270;&#20687;&#35009;&#21098;&#25110;&#27169;&#31946;&#65292;&#24182;&#19988;&#21253;&#25324;&#20102;7&#31181;&#19981;&#21516;&#30340;&#25805;&#32437;&#24773;&#26223;&#65292;&#20849;1260&#24352;&#24102;&#26377;&#31934;&#32454;&#26631;&#27880;&#30340;&#22270;&#20687;&#65292;&#20026;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#24819;&#35937;&#21147;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01355v2 Announce Type: replace  Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable performance on various visual-language understanding and generation tasks. However, MLLMs occasionally generate content inconsistent with the given images, which is known as "hallucination". Prior works primarily center on evaluating hallucination using standard, unperturbed benchmarks, which overlook the prevalent occurrence of perturbed inputs in real-world scenarios-such as image cropping or blurring-that are critical for a comprehensive assessment of MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI, the first benchmark designed to evaluate Hallucination in MLLMs within Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios, containing 1,260 perturbed images from 11 object types. Each image is accompanied by detailed annotations, which include fine-grained hallucination types, such as existence, attribute, and
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#27531;&#24046;&#25193;&#25955;&#27169;&#22411;&#65288;CIResDiff&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#24182;&#29983;&#25104;&#24739;&#32773;&#38543;&#35775;CT&#22270;&#20687;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#27531;&#24046;&#25193;&#25955;&#27169;&#22411;&#65288;CIResDiff&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#24182;&#29983;&#25104;&#24739;&#32773;&#38543;&#35775;CT&#22270;&#20687;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v2 Announce Type: replace-cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21333;&#19968;&#20998;&#24418;&#22270;&#24418;&#21644;&#36731;&#24494;&#21464;&#24418;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#23567;&#21270;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#19982;ImageNet-1k&#31561;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#38598;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#26497;&#20854;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#35757;&#32451;&#20173;&#28982;&#33021;&#22815;&#21462;&#24471;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#19978;&#33021;&#22815;&#19982;&#20351;&#29992;&#22823;&#35268;&#27169;&#30495;&#23454;&#22270;&#20687;&#38598;&#30340;&#39044;&#35757;&#32451;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#20960;&#20046;&#19981;&#21487;&#21306;&#20998;&#65292;&#20294;&#23545;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#20123;&#24046;&#24322;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25506;&#32034;&#20102;&#26500;&#24314;&#20551;&#24819;&#31867;&#21035;&#25152;&#38656;&#30340;&#19968;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#33719;&#21462;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#32456;&#65292;&#36825;&#39033;&#30740;&#31350;&#20026;&#22914;&#20309;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.00677</link><description>&lt;p&gt;
Scaling Backwards: Minimal Synthetic Pre-training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21333;&#19968;&#20998;&#24418;&#22270;&#24418;&#21644;&#36731;&#24494;&#21464;&#24418;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#23567;&#21270;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#19982;ImageNet-1k&#31561;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#38598;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#26497;&#20854;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#35757;&#32451;&#20173;&#28982;&#33021;&#22815;&#21462;&#24471;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#19978;&#33021;&#22815;&#19982;&#20351;&#29992;&#22823;&#35268;&#27169;&#30495;&#23454;&#22270;&#20687;&#38598;&#30340;&#39044;&#35757;&#32451;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#20960;&#20046;&#19981;&#21487;&#21306;&#20998;&#65292;&#20294;&#23545;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#20123;&#24046;&#24322;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25506;&#32034;&#20102;&#26500;&#24314;&#20551;&#24819;&#31867;&#21035;&#25152;&#38656;&#30340;&#19968;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#33719;&#21462;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#32456;&#65292;&#36825;&#39033;&#30740;&#31350;&#20026;&#22914;&#20309;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00677v2 Announce Type: replace  Abstract: Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we inve
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Stain Normalized Pathology Foundational Model&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#23454;&#39564;&#23460;&#21644;&#25195;&#25551;&#20202;&#24341;&#36215;&#30340;&#39068;&#33394;&#24046;&#24322;&#36827;&#34892;&#26579;&#26009;&#26631;&#20934;&#21270;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;&#20840;&#20999;&#29255;&#30149;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#25552;&#21462;&#20013;&#20986;&#29616;&#30340;&#24037;&#20316;&#38598;&#29305;&#23450;&#29305;&#24449;&#23849;&#28291;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#30142;&#30149;&#31867;&#22411;&#21644;&#31361;&#21457;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00380</link><description>&lt;p&gt;
Enhancing Whole Slide Pathology Foundation Models through Stain Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Stain Normalized Pathology Foundational Model&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#23454;&#39564;&#23460;&#21644;&#25195;&#25551;&#20202;&#24341;&#36215;&#30340;&#39068;&#33394;&#24046;&#24322;&#36827;&#34892;&#26579;&#26009;&#26631;&#20934;&#21270;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;&#20840;&#20999;&#29255;&#30149;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#25552;&#21462;&#20013;&#20986;&#29616;&#30340;&#24037;&#20316;&#38598;&#29305;&#23450;&#29305;&#24449;&#23849;&#28291;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#30142;&#30149;&#31867;&#22411;&#21644;&#31361;&#21457;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00380v2 Announce Type: replace-cross  Abstract: Recent advancements in digital pathology have led to the development of numerous foundational models that utilize self-supervised learning on patches extracted from gigapixel whole slide images (WSIs). While this approach leverages vast amounts of unlabeled data, we have discovered a significant issue: features extracted from these self-supervised models tend to cluster by individual WSIs, a phenomenon we term WSI-specific feature collapse. This problem can potentially limit the model's generalization ability and performance on various downstream tasks. To address this issue, we introduce Stain Normalized Pathology Foundational Model, a novel foundational model trained on patches that have undergone stain normalization. Stain normalization helps reduce color variability arising from different laboratories and scanners, enabling the model to learn more consistent features. Stain Normalized Pathology Foundational Model is trained
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;MSA$^2$Net&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#26377;&#25928;&#30340;&#25463;&#24452;&#36830;&#25509;&#35774;&#35745;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#34701;&#21512;&#21644;&#21152;&#26435;&#31895;&#32454;&#29305;&#24449;&#65292;&#20026;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#21644;&#24418;&#29366;&#30340;&#21307;&#23398;&#22270;&#20687;&#32467;&#26500;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#21644;&#39640;&#23618;&#27425;&#20449;&#24687;&#25972;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.21640</link><description>&lt;p&gt;
MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;MSA$^2$Net&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#26377;&#25928;&#30340;&#25463;&#24452;&#36830;&#25509;&#35774;&#35745;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#34701;&#21512;&#21644;&#21152;&#26435;&#31895;&#32454;&#29305;&#24449;&#65292;&#20026;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#21644;&#24418;&#29366;&#30340;&#21307;&#23398;&#22270;&#20687;&#32467;&#26500;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#21644;&#39640;&#23618;&#27425;&#20449;&#24687;&#25972;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21640v2 Announce Type: replace-cross  Abstract: Medical image segmentation involves identifying and separating object instances in a medical image to delineate various tissues and structures, a task complicated by the significant variations in size, shape, and density of these features. Convolutional neural networks (CNNs) have traditionally been used for this task but have limitations in capturing long-range dependencies. Transformers, equipped with self-attention mechanisms, aim to address this problem. However, in medical image segmentation it is beneficial to merge both local and global features to effectively integrate feature maps across various scales, capturing both detailed features and broader semantic elements for dealing with variations in structures. In this paper, we introduce MSA$^2$Net, a new deep segmentation framework featuring an expedient design of skip-connections. These connections facilitate feature fusion by dynamically weighting and combining coarse-
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ControlMM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#25972;&#20010;&#20154;&#20307;&#21160;&#20316;&#30340;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#12289;&#35821;&#38899;&#25110;&#38899;&#20048;&#31561;&#22810;&#31181;&#36755;&#20837;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ControlMM-Attn&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#21516;&#26102;&#22788;&#29702;&#20154;&#20307;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#25299;&#25169;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#26465;&#20214;&#20043;&#38388;&#26377;&#25928;&#23398;&#20064;&#21644;&#36716;&#31227;&#21160;&#20316;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#36755;&#20837;&#26465;&#20214;&#24046;&#24322;&#21270;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#20102;&#22312;&#22810;&#31181;&#22797;&#26434;&#26465;&#20214;&#19979;&#30340;&#20248;&#21270;&#22788;&#29702;&#65292;&#20026;&#35270;&#39057;&#29983;&#25104;&#21644;&#20154;&#21160;&#30011;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.21136</link><description>&lt;p&gt;
Adding Multimodal Controls to Whole-body Human Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ControlMM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#25972;&#20010;&#20154;&#20307;&#21160;&#20316;&#30340;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#12289;&#35821;&#38899;&#25110;&#38899;&#20048;&#31561;&#22810;&#31181;&#36755;&#20837;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ControlMM-Attn&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#21516;&#26102;&#22788;&#29702;&#20154;&#20307;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#25299;&#25169;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#26465;&#20214;&#20043;&#38388;&#26377;&#25928;&#23398;&#20064;&#21644;&#36716;&#31227;&#21160;&#20316;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#36755;&#20837;&#26465;&#20214;&#24046;&#24322;&#21270;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#20102;&#22312;&#22810;&#31181;&#22797;&#26434;&#26465;&#20214;&#19979;&#30340;&#20248;&#21270;&#22788;&#29702;&#65292;&#20026;&#35270;&#39057;&#29983;&#25104;&#21644;&#20154;&#21160;&#30011;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EAR&#30340;&#36793;&#32536;&#24863;&#30693;&#37325;&#24314;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#36793;&#32536;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#39057;&#35889;&#22686;&#24378;&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;&#36793;&#32536;&#20449;&#24687;&#30340;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#21152;&#24378;&#23545;&#33034;&#26609;&#19981;&#35268;&#21017;&#26894;&#20307;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#32508;&#21512;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#32593;&#32476;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#20174;X&#23556;&#32447;&#22270;&#20687;&#37325;&#24314;3D&#33034;&#26609;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2407.20937</link><description>&lt;p&gt;
EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from bi-planar X-ray images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EAR&#30340;&#36793;&#32536;&#24863;&#30693;&#37325;&#24314;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#36793;&#32536;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#39057;&#35889;&#22686;&#24378;&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;&#36793;&#32536;&#20449;&#24687;&#30340;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#21152;&#24378;&#23545;&#33034;&#26609;&#19981;&#35268;&#21017;&#26894;&#20307;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#32508;&#21512;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#32593;&#32476;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#20174;X&#23556;&#32447;&#22270;&#20687;&#37325;&#24314;3D&#33034;&#26609;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20937v2 Announce Type: replace-cross  Abstract: X-ray images ease the diagnosis and treatment process due to their rapid imaging speed and high resolution. However, due to the projection process of X-ray imaging, much spatial information has been lost. To accurately provide efficient spinal morphological and structural information, reconstructing the 3-D structures of the spine from the 2-D X-ray images is essential. It is challenging for current reconstruction methods to preserve the edge information and local shapes of the asymmetrical vertebrae structures. In this study, we propose a new Edge-Aware Reconstruction network (EAR) to focus on the performance improvement of the edge information and vertebrae shapes. In our network, by using the auto-encoder architecture as the backbone, the edge attention module and frequency enhancement module are proposed to strengthen the perception of the edge reconstruction. Meanwhile, we also combine four loss terms, including reconstruc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19977;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#25351;&#26631;&#65292;&#21253;&#25324;&#29420;&#31435;&#30340;&#32763;&#35793;&#21644;&#26059;&#36716;&#31934;&#24230;&#35780;&#20215;&#65292;&#20197;&#21450;&#32508;&#21512;&#30340;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36317;&#31163;&#35823;&#24046;&#30340;&#32047;&#31215;&#39057;&#25968;&#26041;&#27861;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20026;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2407.20391</link><description>&lt;p&gt;
Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19977;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#25351;&#26631;&#65292;&#21253;&#25324;&#29420;&#31435;&#30340;&#32763;&#35793;&#21644;&#26059;&#36716;&#31934;&#24230;&#35780;&#20215;&#65292;&#20197;&#21450;&#32508;&#21512;&#30340;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36317;&#31163;&#35823;&#24046;&#30340;&#32047;&#31215;&#39057;&#25968;&#26041;&#27861;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20026;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20391v2 Announce Type: replace-cross  Abstract: We propose three novel metrics for evaluating the accuracy of a set of estimated camera poses given the ground truth: Translation Alignment Score (TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS evaluates the translation accuracy independently of the rotations, and the RAS evaluates the rotation accuracy independently of the translations. The PAS is the average of the two scores, evaluating the combined accuracy of both translations and rotations. The TAS is computed in four steps: (1) Find the upper quartile of the closest-pair-distances, $d$. (2) Align the estimated trajectory to the ground truth using a robust registration method. (3) Collect all distance errors and obtain the cumulative frequencies for multiple thresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up these cumulative frequencies and normalize them such that the theoretical maximum is 1. The TAS has practical
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22478;&#24066;&#20132;&#36890;&#23433;&#20840;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.19719</link><description>&lt;p&gt;
Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22478;&#24066;&#20132;&#36890;&#23433;&#20840;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19719v2 Announce Type: replace  Abstract: Measuring urban safety perception is an important and complex task that traditionally relies heavily on human resources. This process often involves extensive field surveys, manual data collection, and subjective assessments, which can be time-consuming, costly, and sometimes inconsistent. Street View Images (SVIs), along with deep learning methods, provide a way to realize large-scale urban safety detection. However, achieving this goal often requires extensive human annotation to train safety ranking models, and the architectural differences between cities hinder the transferability of these models. Thus, a fully automated method for conducting safety evaluations is essential. Recent advances in multimodal large language models (MLLMs) have demonstrated powerful reasoning and analytical capabilities. Cutting-edge models, e.g., GPT-4 have shown surprising performance in many tasks. We employed these models for urban safety ranking o
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VersusDebias&#30340;&#20840;&#26032;&#36890;&#29992;&#38646;&#26679;&#26412;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#24615;&#23545;&#25239;&#26426;&#21046;&#65292;&#33021;&#22815;&#38024;&#23545;&#19981;&#21516;T2I&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#33258;&#25105;&#36866;&#24212;&#24615;&#20248;&#21270;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#27169;&#22411;&#24187;&#35937;&#24341;&#36215;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2407.19524</link><description>&lt;p&gt;
VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VersusDebias&#30340;&#20840;&#26032;&#36890;&#29992;&#38646;&#26679;&#26412;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#24615;&#23545;&#25239;&#26426;&#21046;&#65292;&#33021;&#22815;&#38024;&#23545;&#19981;&#21516;T2I&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#33258;&#25105;&#36866;&#24212;&#24615;&#20248;&#21270;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#27169;&#22411;&#24187;&#35937;&#24341;&#36215;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19524v2 Announce Type: replace  Abstract: With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I
&lt;/p&gt;</description></item><item><title>&#20165;&#21453;&#39304;&#12300;&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Vision Mamba&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#38750;&#22240;&#26524;&#29366;&#24577;&#31354;&#38388;&#23545;&#20598;&#24615;&#65288;State Space Duality&#65289;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22240;&#26524;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;VSSD&#22312;&#22788;&#29702;&#38750;&#22240;&#26524;&#35270;&#35273;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#38544;&#34255;&#29366;&#24577;&#19982;token&#20043;&#38388;&#30340;&#26435;&#37325;&#20851;&#31995;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;&#12301;</title><link>https://arxiv.org/abs/2407.18559</link><description>&lt;p&gt;
VSSD: Vision Mamba with Non-Causal State Space Duality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18559
&lt;/p&gt;
&lt;p&gt;
&#20165;&#21453;&#39304;&#12300;&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Vision Mamba&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#38750;&#22240;&#26524;&#29366;&#24577;&#31354;&#38388;&#23545;&#20598;&#24615;&#65288;State Space Duality&#65289;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22240;&#26524;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;VSSD&#22312;&#22788;&#29702;&#38750;&#22240;&#26524;&#35270;&#35273;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#38544;&#34255;&#29366;&#24577;&#19982;token&#20043;&#38388;&#30340;&#26435;&#37325;&#20851;&#31995;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;&#12301;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18559v2 Announce Type: replace  Abstract: Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Relational Priors Distillation (RPD)&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;2D&#21464;&#25442;&#22120;&#20013;&#25552;&#21462;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#26469;&#26174;&#33879;&#25552;&#39640;&#36328;&#22495;&#28857;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#36328;&#22495;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#21019;&#26032;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#28857;&#20113;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#33539;&#22260;&#20869;&#30340;&#24418;&#29366;&#21464;&#21270;&#65292;&#20197;&#21450;&#22312;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20013;&#30340;&#34920;&#29616;&#21463;&#38480;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;RPD&#19981;&#20165;&#33021;&#22815;&#25429;&#25417;&#21040;&#23616;&#37096;&#20960;&#20309;&#32454;&#33410;&#65292;&#36824;&#33021;&#29702;&#35299;&#23616;&#37096;&#20960;&#20309;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2407.18534</link><description>&lt;p&gt;
Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Relational Priors Distillation (RPD)&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;2D&#21464;&#25442;&#22120;&#20013;&#25552;&#21462;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#26469;&#26174;&#33879;&#25552;&#39640;&#36328;&#22495;&#28857;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#36328;&#22495;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#21019;&#26032;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#28857;&#20113;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#33539;&#22260;&#20869;&#30340;&#24418;&#29366;&#21464;&#21270;&#65292;&#20197;&#21450;&#22312;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20013;&#30340;&#34920;&#29616;&#21463;&#38480;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;RPD&#19981;&#20165;&#33021;&#22815;&#25429;&#25417;&#21040;&#23616;&#37096;&#20960;&#20309;&#32454;&#33410;&#65292;&#36824;&#33021;&#29702;&#35299;&#23616;&#37096;&#20960;&#20309;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18534v2 Announce Type: replace  Abstract: Semantic pattern of an object point cloud is determined by its topological configuration of local geometries. Learning discriminative representations can be challenging due to large shape variations of point sets in local regions and incomplete surface in a global perspective, which can be made even more severe in the context of unsupervised domain adaptation (UDA). In specific, traditional 3D networks mainly focus on local geometric details and ignore the topological structure between local geometries, which greatly limits their cross-domain generalization. Recently, the transformer-based models have achieved impressive performance gain in a range of image-based tasks, benefiting from its strong generalization capability and scalability stemming from capturing long range correlation across local patches. Inspired by such successes of visual transformers, we propose a novel Relational Priors Distillation (RPD) method to extract relat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35265;&#35299;&#21435;&#38500;&#26694;&#26550;&#26469;&#25913;&#36827;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19987;&#23478;&#21644;&#33258;&#25105;&#35265;&#35299;&#21435;&#38500;&#25216;&#26415;&#23558;&#30693;&#35782;&#36171;&#20104;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20854;&#22312;&#22810;&#20010;&#19981;&#21516;&#20020;&#24202;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.18449</link><description>&lt;p&gt;
Towards A Generalizable Pathology Foundation Model via Unified Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35265;&#35299;&#21435;&#38500;&#26694;&#26550;&#26469;&#25913;&#36827;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19987;&#23478;&#21644;&#33258;&#25105;&#35265;&#35299;&#21435;&#38500;&#25216;&#26415;&#23558;&#30693;&#35782;&#36171;&#20104;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20854;&#22312;&#22810;&#20010;&#19981;&#21516;&#20020;&#24202;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18449v2 Announce Type: replace-cross  Abstract: Foundation models pretrained on large-scale datasets are revolutionizing the field of computational pathology (CPath). The generalization ability of foundation models is crucial for the success in various downstream clinical tasks. However, current foundation models have only been evaluated on a limited type and number of tasks, leaving their generalization ability and overall performance unclear. To address this gap, we established a most comprehensive benchmark to evaluate the performance of off-the-shelf foundation models across six distinct clinical task types, encompassing a total of 39 specific tasks. Our findings reveal that existing foundation models excel at certain task types but struggle to effectively handle the full breadth of clinical tasks. To improve the generalization of pathology foundation models, we propose a unified knowledge distillation framework consisting of both expert and self knowledge distillation, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MARINE&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#38024;&#23545;&#24555;&#36895;&#21160;&#29289;&#21160;&#20316;&#35774;&#35745;&#30340;&#36816;&#21160;&#22522;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#21487;&#35757;&#32451;&#20998;&#31867;&#22836;&#30340;DINOv2&#29305;&#24449;&#25552;&#21462;&#65292;&#22312;&#36776;&#21035;&#21160;&#29289;&#35270;&#39057;&#20013;&#30340;&#25429;&#39135;&#32773;&#25915;&#20987;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#29616;&#20102;&#22312;&#29305;&#23450;&#29642;&#29786;&#30977;&#21644;&#26356;&#22823;&#30340;&#21160;&#29289;&#29579;&#22269;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.18289</link><description>&lt;p&gt;
MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MARINE&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#38024;&#23545;&#24555;&#36895;&#21160;&#29289;&#21160;&#20316;&#35774;&#35745;&#30340;&#36816;&#21160;&#22522;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#21487;&#35757;&#32451;&#20998;&#31867;&#22836;&#30340;DINOv2&#29305;&#24449;&#25552;&#21462;&#65292;&#22312;&#36776;&#21035;&#21160;&#29289;&#35270;&#39057;&#20013;&#30340;&#25429;&#39135;&#32773;&#25915;&#20987;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#29616;&#20102;&#22312;&#29305;&#23450;&#29642;&#29786;&#30977;&#21644;&#26356;&#22823;&#30340;&#21160;&#29289;&#29579;&#22269;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18289v2 Announce Type: replace  Abstract: Encounters between predator and prey play an essential role in ecosystems, but their rarity makes them difficult to detect in video recordings. Although advances in action recognition (AR) and temporal action detection (AD), especially transformer-based models and vision foundation models, have achieved high performance on human action datasets, animal videos remain relatively under-researched. This thesis addresses this gap by proposing the model MARINE, which utilizes motion-based frame selection designed for fast animal actions and DINOv2 feature extraction with a trainable classification head for action recognition. MARINE outperforms VideoMAE in identifying predator attacks in videos of fish, both on a small and specific coral reef dataset (81.53\% against 52.64\% accuracy), and on a subset of the more extensive Animal Kingdom dataset (94.86\% against 83.14\% accuracy). In a multi-label setting on a representative sample of Anim
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#23558;DINOv2&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;FairMOT&#22810;&#23545;&#35937;&#36319;&#36394;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;FairMOT&#33021;&#22815;&#21033;&#29992;DINOv2&#22312;&#22810;&#23545;&#35937;&#36319;&#36394;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#34429;&#28982;&#36825;&#31181;&#36716;&#31227;&#23548;&#33268;&#20102;&#26816;&#27979;&#36895;&#24230;&#30340;&#36731;&#24494;&#19979;&#38477;&#65292;&#20294;&#25972;&#20307;&#30340;&#36319;&#36394;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2407.18288</link><description>&lt;p&gt;
Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18288
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#23558;DINOv2&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;FairMOT&#22810;&#23545;&#35937;&#36319;&#36394;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;FairMOT&#33021;&#22815;&#21033;&#29992;DINOv2&#22312;&#22810;&#23545;&#35937;&#36319;&#36394;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#34429;&#28982;&#36825;&#31181;&#36716;&#31227;&#23548;&#33268;&#20102;&#26816;&#27979;&#36895;&#24230;&#30340;&#36731;&#24494;&#19979;&#38477;&#65292;&#20294;&#25972;&#20307;&#30340;&#36319;&#36394;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18288v2 Announce Type: replace  Abstract: Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32852;&#21512;&#27861;&#24459;&#26381;&#21153;&#26694;&#26550;&#65292;&#21517;&#20026;LawLuo&#65292;&#26088;&#22312;&#20026;&#38750;&#27861;&#24459;&#32972;&#26223;&#29992;&#25143;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#27861;&#24459;&#20107;&#21153;&#25152;&#30340;&#21672;&#35810;&#20307;&#39564;&#12290;LawLuo&#36890;&#36807;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#21046;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#26597;&#35810;&#21644;&#39057;&#32321;&#30340;&#23545;&#35805;&#36718;&#27425;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#35299;&#20915;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.16252</link><description>&lt;p&gt;
LawLuo: A Chinese Law Firm Co-run by LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32852;&#21512;&#27861;&#24459;&#26381;&#21153;&#26694;&#26550;&#65292;&#21517;&#20026;LawLuo&#65292;&#26088;&#22312;&#20026;&#38750;&#27861;&#24459;&#32972;&#26223;&#29992;&#25143;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#27861;&#24459;&#20107;&#21153;&#25152;&#30340;&#21672;&#35810;&#20307;&#39564;&#12290;LawLuo&#36890;&#36807;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#21046;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#26597;&#35810;&#21644;&#39057;&#32321;&#30340;&#23545;&#35805;&#36718;&#27425;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#35299;&#20915;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16252v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate substantial potential in delivering legal consultation services to users without a legal background, attributed to their superior text comprehension and generation capabilities. Nonetheless, existing Chinese legal LLMs limit interaction to a single model-user dialogue, unlike the collaborative consultations typical of law firms, where multiple staff members contribute to a single consultation. This limitation prevents an authentic consultation experience. Additionally, extant Chinese legal LLMs suffer from critical limitations: (1) insufficient control over the quality of instruction fine-tuning data; (2) increased model hallucination resulting from users' ambiguous queries; and (3) a reduction in the model's ability to follow instructions over multiple dialogue turns. In response to these challenges, we propose a novel legal dialogue framework that leverages the collaborative capabiliti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGMN&#30340;Spatiotemporal Graph Guided Multi-modal Network&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25351;&#23548;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21033;&#29992;&#38144;&#21806;&#20154;&#21592;&#21475;&#35821;&#20869;&#23481;&#24341;&#23548;&#27169;&#22411;&#20851;&#27880;&#24847;&#22270;&#20135;&#21697;&#65292;&#21516;&#26102;&#35299;&#20915;&#35270;&#39057;&#21644;&#26631;&#20934;&#20135;&#21697;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#25773;&#20135;&#21697;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2407.16248</link><description>&lt;p&gt;
Spatiotemporal Graph Guided Multi-modal Network for Livestreaming Product Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGMN&#30340;Spatiotemporal Graph Guided Multi-modal Network&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25351;&#23548;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21033;&#29992;&#38144;&#21806;&#20154;&#21592;&#21475;&#35821;&#20869;&#23481;&#24341;&#23548;&#27169;&#22411;&#20851;&#27880;&#24847;&#22270;&#20135;&#21697;&#65292;&#21516;&#26102;&#35299;&#20915;&#35270;&#39057;&#21644;&#26631;&#20934;&#20135;&#21697;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#25773;&#20135;&#21697;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16248v3 Announce Type: replace  Abstract: With the rapid expansion of e-commerce, more consumers have become accustomed to making purchases via livestreaming. Accurately identifying the products being sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a fundamental and daunting challenge. The LPR task encompasses three primary dilemmas in real-world scenarios: 1) the recognition of intended products from distractor products present in the background; 2) the video-image heterogeneity that the appearance of products showcased in live streams often deviates substantially from standardized product images in stores; 3) there are numerous confusing products with subtle visual nuances in the shop. To tackle these challenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN). First, we employ a text-guided attention mechanism that leverages the spoken content of salespeople to guide the model to focus toward intended products, emphasizing their s
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#26500;&#24314;&#20102;&#21253;&#21547;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#26368;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#30149;&#29702;&#35786;&#26029;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;H&amp;E&#26579;&#33394;&#20999;&#29255;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#22312;32&#31181;&#30284;&#30151;&#31867;&#22411;&#20013;&#30340;10,275&#21517;&#24739;&#32773;&#20013;&#30340;&#39640;&#25928;&#30149;&#29702;&#25253;&#21578;&#29983;&#25104;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#39640;&#20102;&#30149;&#29702;&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#32508;&#21512;&#21033;&#29992;&#20026;&#20010;&#24615;&#21270;&#21307;&#23398;&#21644;&#31934;&#20934;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2407.15362</link><description>&lt;p&gt;
A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#26500;&#24314;&#20102;&#21253;&#21547;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#26368;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#30149;&#29702;&#35786;&#26029;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;H&amp;E&#26579;&#33394;&#20999;&#29255;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#22312;32&#31181;&#30284;&#30151;&#31867;&#22411;&#20013;&#30340;10,275&#21517;&#24739;&#32773;&#20013;&#30340;&#39640;&#25928;&#30149;&#29702;&#25253;&#21578;&#29983;&#25104;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#39640;&#20102;&#30149;&#29702;&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#32508;&#21512;&#21033;&#29992;&#20026;&#20010;&#24615;&#21270;&#21307;&#23398;&#21644;&#31934;&#20934;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15362v2 Announce Type: replace  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\&amp;E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#28210;&#26579;&#26102;&#38388;&#19968;&#33268;&#30340;&#31070;&#32463;&#39640;&#21160;&#24577;&#33539;&#22260;&#35270;&#39057;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#25554;&#20540;&#30456;&#37051;&#30340;&#20302;&#21160;&#24577;&#33539;&#22260;&#24103;&#26469;&#37325;&#24314;&#32570;&#22833;&#26333;&#20809;&#20449;&#24687;&#30340;&#20302;&#21160;&#24577;&#33539;&#22260;&#24103;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#25139;&#19978;&#25552;&#20379;&#23436;&#25972;&#30340;&#26333;&#20809;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;HDR&#35270;&#39057;&#30340;&#28210;&#26579;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2407.13309</link><description>&lt;p&gt;
Exposure Completing for Temporally Consistent Neural High Dynamic Range Video Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.13309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#28210;&#26579;&#26102;&#38388;&#19968;&#33268;&#30340;&#31070;&#32463;&#39640;&#21160;&#24577;&#33539;&#22260;&#35270;&#39057;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#25554;&#20540;&#30456;&#37051;&#30340;&#20302;&#21160;&#24577;&#33539;&#22260;&#24103;&#26469;&#37325;&#24314;&#32570;&#22833;&#26333;&#20809;&#20449;&#24687;&#30340;&#20302;&#21160;&#24577;&#33539;&#22260;&#24103;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#25139;&#19978;&#25552;&#20379;&#23436;&#25972;&#30340;&#26333;&#20809;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;HDR&#35270;&#39057;&#30340;&#28210;&#26579;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.13309v2 Announce Type: replace  Abstract: High dynamic range (HDR) video rendering from low dynamic range (LDR) videos where frames are of alternate exposure encounters significant challenges, due to the exposure change and absence at each time stamp. The exposure change and absence make existing methods generate flickering HDR results. In this paper, we propose a novel paradigm to render HDR frames via completing the absent exposure information, hence the exposure information is complete and consistent. Our approach involves interpolating neighbor LDR frames in the time dimension to reconstruct LDR frames for the absent exposures. Combining the interpolated and given LDR frames, the complete set of exposure information is available at each time stamp. This benefits the fusing process for HDR results, reducing noise and ghosting artifacts therefore improving temporal consistency. Extensive experimental evaluations on standard benchmarks demonstrate that our method achieves s
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36866;&#24212;&#30340;&#28857;Former&#65288;PointFormer&#65289;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#25913;&#32534;&#33258;2D&#35270;&#35273;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20998;&#26512;3D&#28857;&#20113;&#25968;&#25454;&#65292;&#26080;&#38656;&#23558;&#28857;&#20113;&#26144;&#23556;&#21040;&#22270;&#20687;&#12290;&#36825;&#31181;&#30452;&#25509;&#36866;&#24212;2D&#30693;&#35782;&#22788;&#29702;3D&#25968;&#25454;&#30340;&#31574;&#30053;&#65292;&#22312;&#20445;&#25345;&#23569;&#37327;&#21442;&#25968;&#30340;&#21516;&#26102;&#23545;2D&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#32469;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#25152;&#38656;&#30340;&#22270;&#20687;&#26144;&#23556;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;3D&#28857;&#20113;&#20998;&#26512;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2407.13200</link><description>&lt;p&gt;
Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.13200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36866;&#24212;&#30340;&#28857;Former&#65288;PointFormer&#65289;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#25913;&#32534;&#33258;2D&#35270;&#35273;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20998;&#26512;3D&#28857;&#20113;&#25968;&#25454;&#65292;&#26080;&#38656;&#23558;&#28857;&#20113;&#26144;&#23556;&#21040;&#22270;&#20687;&#12290;&#36825;&#31181;&#30452;&#25509;&#36866;&#24212;2D&#30693;&#35782;&#22788;&#29702;3D&#25968;&#25454;&#30340;&#31574;&#30053;&#65292;&#22312;&#20445;&#25345;&#23569;&#37327;&#21442;&#25968;&#30340;&#21516;&#26102;&#23545;2D&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#32469;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#25152;&#38656;&#30340;&#22270;&#20687;&#26144;&#23556;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;3D&#28857;&#20113;&#20998;&#26512;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.13200v2 Announce Type: replace  Abstract: Pre-trained large-scale models have exhibited remarkable efficacy in computer vision, particularly for 2D image analysis. However, when it comes to 3D point clouds, the constrained accessibility of data, in contrast to the vast repositories of images, poses a challenge for the development of 3D pre-trained models. This paper therefore attempts to directly leverage pre-trained models with 2D prior knowledge to accomplish the tasks for 3D point cloud analysis. Accordingly, we propose the Adaptive PointFormer (APF), which fine-tunes pre-trained 2D models with only a modest number of parameters to directly process point clouds, obviating the need for mapping to images. Specifically, we convert raw point clouds into point embeddings for aligning dimensions with image tokens. Given the inherent disorder in point clouds, in contrast to the structured nature of images, we then sequence the point embeddings to optimize the utilization of 2D a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20840;&#23616;&#33258;&#32534;&#30721;&#22120;&#65288;Quantised Global Autoencoder&#65289;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#12290;&#23427;&#25682;&#24323;&#20102;&#20197;&#24448;&#22312;&#37327;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#23616;&#37096;&#26001;&#22359;&#28982;&#21518;&#27599;&#20010;&#26001;&#22359;&#36890;&#36807;&#19968;&#20010;&#32534;&#30721;&#31526;&#34920;&#31034;&#30340;&#20570;&#27861;&#65292;&#32780;&#26159;&#36890;&#36807;&#19968;&#31181;&#31867;&#20284;&#20110;&#39057;&#35889;&#20998;&#35299;&#30340;&#24605;&#36335;&#65292;&#23558;&#36755;&#20837;&#20449;&#21495;&#36716;&#21270;&#20026;&#19968;&#32452;&#33258;&#23450;&#20041;&#22522;&#30784;&#20989;&#25968;&#30340;&#36229;&#30690;&#37327;&#65292;&#36825;&#32452;&#22522;&#30784;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#23398;&#20064;&#20986;&#26469;&#65292;&#20197;&#26356;&#22909;&#22320;&#20195;&#34920;&#20840;&#23616;&#22270;&#20687;&#20869;&#23481;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#22270;&#20687;&#30340;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23616;&#37096;&#20449;&#24687;&#30340;&#32454;&#33410;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#34920;&#31034;&#19978;&#20570;&#20986;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2407.11913</link><description>&lt;p&gt;
Quantised Global Autoencoder: A Holistic Approach to Representing Visual Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20840;&#23616;&#33258;&#32534;&#30721;&#22120;&#65288;Quantised Global Autoencoder&#65289;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#12290;&#23427;&#25682;&#24323;&#20102;&#20197;&#24448;&#22312;&#37327;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#23616;&#37096;&#26001;&#22359;&#28982;&#21518;&#27599;&#20010;&#26001;&#22359;&#36890;&#36807;&#19968;&#20010;&#32534;&#30721;&#31526;&#34920;&#31034;&#30340;&#20570;&#27861;&#65292;&#32780;&#26159;&#36890;&#36807;&#19968;&#31181;&#31867;&#20284;&#20110;&#39057;&#35889;&#20998;&#35299;&#30340;&#24605;&#36335;&#65292;&#23558;&#36755;&#20837;&#20449;&#21495;&#36716;&#21270;&#20026;&#19968;&#32452;&#33258;&#23450;&#20041;&#22522;&#30784;&#20989;&#25968;&#30340;&#36229;&#30690;&#37327;&#65292;&#36825;&#32452;&#22522;&#30784;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#23398;&#20064;&#20986;&#26469;&#65292;&#20197;&#26356;&#22909;&#22320;&#20195;&#34920;&#20840;&#23616;&#22270;&#20687;&#20869;&#23481;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#22270;&#20687;&#30340;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23616;&#37096;&#20449;&#24687;&#30340;&#32454;&#33410;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#34920;&#31034;&#19978;&#20570;&#20986;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11913v2 Announce Type: replace  Abstract: In quantised autoencoders, images are usually split into local patches, each encoded by one token. This representation is redundant in the sense that the same number of tokens is spend per region, regardless of the visual information content in that region. Adaptive discretisation schemes like quadtrees are applied to allocate tokens for patches with varying sizes, but this just varies the region of influence for a token which nevertheless remains a local descriptor. Modern architectures add an attention mechanism to the autoencoder which infuses some degree of global information into the local tokens. Despite the global context, tokens are still associated with a local image region. In contrast, our method is inspired by spectral decompositions which transform an input signal into a superposition of global frequencies. Taking the data-driven perspective, we learn custom basis functions corresponding to the codebook entries in our VQ
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HPC&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20998;&#36776;&#29575;&#27531;&#24046;&#36752;&#23556;&#22330;&#21644;&#31471;&#21040;&#31471;&#28176;&#36827;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21333;&#27169;&#22411;&#22810;&#20998;&#36776;&#29575;&#35270;&#39057;&#21387;&#32553;&#65292;&#28385;&#36275;&#19981;&#21516;&#32593;&#32476;&#21644;&#35774;&#22791;&#33021;&#21147;&#19979;&#30340;&#28789;&#27963;&#21464;&#37327;&#27604;&#29305;&#29575;&#38656;&#27714;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;NeRF&#35270;&#39057;&#25216;&#26415;&#38754;&#20020;&#30340;&#20307;&#31215;&#22823;&#12289;&#21387;&#32553;&#19982;&#20256;&#36755;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.09026</link><description>&lt;p&gt;
HPC: Hierarchical Progressive Coding Framework for Volumetric Video
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.09026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HPC&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20998;&#36776;&#29575;&#27531;&#24046;&#36752;&#23556;&#22330;&#21644;&#31471;&#21040;&#31471;&#28176;&#36827;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21333;&#27169;&#22411;&#22810;&#20998;&#36776;&#29575;&#35270;&#39057;&#21387;&#32553;&#65292;&#28385;&#36275;&#19981;&#21516;&#32593;&#32476;&#21644;&#35774;&#22791;&#33021;&#21147;&#19979;&#30340;&#28789;&#27963;&#21464;&#37327;&#27604;&#29305;&#29575;&#38656;&#27714;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;NeRF&#35270;&#39057;&#25216;&#26415;&#38754;&#20020;&#30340;&#20307;&#31215;&#22823;&#12289;&#21387;&#32553;&#19982;&#20256;&#36755;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.09026v2 Announce Type: replace  Abstract: Volumetric video based on Neural Radiance Field (NeRF) holds vast potential for various 3D applications, but its substantial data volume poses significant challenges for compression and transmission. Current NeRF compression lacks the flexibility to adjust video quality and bitrate within a single model for various network and device capacities. To address these issues, we propose HPC, a novel hierarchical progressive volumetric video coding framework achieving variable bitrate using a single model. Specifically, HPC introduces a hierarchical representation with a multi-resolution residual radiance field to reduce temporal redundancy in long-duration sequences while simultaneously generating various levels of detail. Then, we propose an end-to-end progressive learning approach with a multi-rate-distortion loss function to jointly optimize both hierarchical representation and compression. Our HPC trained only once can realize multiple
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25581;&#31034;&#20102;&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#21457;&#23637;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#24378;&#35843;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#21644;&#25968;&#25454;&#24320;&#21457;&#20013;&#30340;&#30456;&#20114;&#20419;&#36827;&#20316;&#29992;&#65292;&#20026;&#25171;&#36896;&#26356;&#20026;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#25968;&#25454;&#24320;&#21457;&#27169;&#24335;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25745;&#12290;</title><link>https://arxiv.org/abs/2407.08583</link><description>&lt;p&gt;
The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.08583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25581;&#31034;&#20102;&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#21457;&#23637;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#24378;&#35843;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#21644;&#25968;&#25454;&#24320;&#21457;&#20013;&#30340;&#30456;&#20114;&#20419;&#36827;&#20316;&#29992;&#65292;&#20026;&#25171;&#36896;&#26356;&#20026;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#25968;&#25454;&#24320;&#21457;&#27169;&#24335;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08583v2 Announce Type: replace-cross  Abstract: The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs; on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approache
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SvANet&#30340;&#26032;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#25972;&#21512;&#23610;&#24230;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23567;&#22411;&#21307;&#23398;&#25104;&#20687;&#23545;&#35937;&#30340;&#36136;&#37327;&#20998;&#21106;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.07720</link><description>&lt;p&gt;
Exploiting Scale-Variant Attention for Segmenting Small Medical Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.07720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SvANet&#30340;&#26032;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#25972;&#21512;&#23610;&#24230;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23567;&#22411;&#21307;&#23398;&#25104;&#20687;&#23545;&#35937;&#30340;&#36136;&#37327;&#20998;&#21106;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.07720v4 Announce Type: replace-cross  Abstract: Early detection and accurate diagnosis can predict the risk of malignant disease transformation, thereby increasing the probability of effective treatment. Identifying mild syndrome with small pathological regions serves as an ominous warning and is fundamental in the early diagnosis of diseases. While deep learning algorithms, particularly convolutional neural networks (CNNs), have shown promise in segmenting medical objects, analyzing small areas in medical images remains challenging. This difficulty arises due to information losses and compression defects from convolution and pooling operations in CNNs, which become more pronounced as the network deepens, especially for small medical objects. To address these challenges, we propose a novel scale-variant attention-based network (SvANet) for accurately segmenting small-scale objects in medical images. The SvANet consists of scale-variant attention, cross-scale guidance, Monte 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36328;&#25991;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#25991;&#21270;&#24847;&#35782;&#21644;&#25991;&#21270;&#22810;&#26679;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#25991;&#21270;&#36164;&#20135;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#21019;&#24314;&#20102;CUBE&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36328;&#25991;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;CUBE&#28085;&#30422;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#21644;&#22320;&#21306;&#30340;&#25991;&#21270;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.06863</link><description>&lt;p&gt;
Beyond Aesthetics: Cultural Competence in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.06863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36328;&#25991;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#25991;&#21270;&#24847;&#35782;&#21644;&#25991;&#21270;&#22810;&#26679;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#25991;&#21270;&#36164;&#20135;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#21019;&#24314;&#20102;CUBE&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36328;&#25991;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;CUBE&#28085;&#30422;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#21644;&#22320;&#21306;&#30340;&#25991;&#21270;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.06863v4 Announce Type: replace  Abstract: Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concept
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#37325;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23545;&#20110;&#34701;&#21512;&#36828;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.06159</link><description>&lt;p&gt;
A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.06159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#37325;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23545;&#20110;&#34701;&#21512;&#36828;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.06159v2 Announce Type: replace  Abstract: Multi-modality image fusion aims at fusing specific-modality and shared-modality information from two source images. To tackle the problem of insufficient feature extraction and lack of semantic awareness for complex scenes, this paper focuses on how to model correlation-driven decomposing features and reason high-level graph representation by efficiently extracting complementary features and multi-guided feature aggregation. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. The transformer with Multi-Dconv Transposed Attention and Local-enhanced Feed Forward network is used to extract shallow features after the depthwise convolution. In the three parallel branches encoder, Cross Attention and Invertible Block (CAI) enables to extract local features and preserve high-frequency texture details. Base feature extraction module (BFE) with residual connections can capture
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ViG-Bias&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#34920;&#31034;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#21457;&#29616;&#21644;&#32531;&#35299;&#35270;&#35273;&#35782;&#21035;&#31995;&#32479;&#30340;&#28508;&#22312;&#20559;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#36328;&#27169;&#24577;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#20197;&#27492;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#33391;&#24615;&#33021;&#27169;&#24335;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ViG-Bias&#33021;&#22815;&#25581;&#31034;&#38544;&#34255;&#30340;&#12289;&#38750;&#30452;&#25509;&#25551;&#36848;&#24615;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#20026;&#35270;&#35273;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#20559;&#24046;&#35782;&#21035;&#21644;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2407.01996</link><description>&lt;p&gt;
ViG-Bias: Visually Grounded Bias Discovery and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.01996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ViG-Bias&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#34920;&#31034;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#21457;&#29616;&#21644;&#32531;&#35299;&#35270;&#35273;&#35782;&#21035;&#31995;&#32479;&#30340;&#28508;&#22312;&#20559;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#36328;&#27169;&#24577;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#20197;&#27492;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#33391;&#24615;&#33021;&#27169;&#24335;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ViG-Bias&#33021;&#22815;&#25581;&#31034;&#38544;&#34255;&#30340;&#12289;&#38750;&#30452;&#25509;&#25551;&#36848;&#24615;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#20026;&#35270;&#35273;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#20559;&#24046;&#35782;&#21035;&#21644;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.01996v3 Announce Type: replace  Abstract: The proliferation of machine learning models in critical decision making processes has underscored the need for bias discovery and mitigation strategies. Identifying the reasons behind a biased system is not straightforward, since in many occasions they are associated with hidden spurious correlations which are not easy to spot. Standard approaches rely on bias audits performed by analyzing model performance in pre-defined subgroups of data samples, usually characterized by common attributes like gender or ethnicity when it comes to people, or other specific attributes defining semantically coherent groups of images. However, it is not always possible to know a-priori the specific attributes defining the failure modes of visual recognition systems. Recent approaches propose to discover these groups by leveraging large vision language models, which enable the extraction of cross-modal embeddings and the generation of textual descripti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMHalSnowball&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#20043;&#21069;&#29983;&#25104;&#30340;&#24187;&#35273;&#26102;&#26159;&#21542;&#20250;&#21463;&#21040;&#35823;&#23548;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36935;&#21040;&#24187;&#35273;&#30456;&#20851;&#30340;&#26597;&#35810;&#26102;&#65292;&#24320;&#28304;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#20102;&#33267;&#23569;31%&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25509;&#21463;&#29983;&#25104;&#30340;&#24187;&#35273;&#24182;&#20316;&#20986;&#38169;&#35823;&#30340;&#38472;&#36848;&#12290;</title><link>https://arxiv.org/abs/2407.00569</link><description>&lt;p&gt;
Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMHalSnowball&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#20043;&#21069;&#29983;&#25104;&#30340;&#24187;&#35273;&#26102;&#26159;&#21542;&#20250;&#21463;&#21040;&#35823;&#23548;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36935;&#21040;&#24187;&#35273;&#30456;&#20851;&#30340;&#26597;&#35810;&#26102;&#65292;&#24320;&#28304;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#20102;&#33267;&#23569;31%&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25509;&#21463;&#29983;&#25104;&#30340;&#24187;&#35273;&#24182;&#20316;&#20986;&#38169;&#35823;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00569v4 Announce Type: replace  Abstract: Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;STGC&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#20013;&#30340;token&#32423;&#21035;&#26799;&#24230;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Mixture-of-Experts&#26041;&#27861;&#22312;&#22788;&#29702;Large Vision-Language Models&#26102;&#20135;&#29983;&#30340;token&#32423;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#19987;&#23478;&#22788;&#29702;tokens&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2406.19905</link><description>&lt;p&gt;
Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.19905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;STGC&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#20013;&#30340;token&#32423;&#21035;&#26799;&#24230;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Mixture-of-Experts&#26041;&#27861;&#22312;&#22788;&#29702;Large Vision-Language Models&#26102;&#20135;&#29983;&#30340;token&#32423;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#19987;&#23478;&#22788;&#29702;tokens&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InterCLIP-MEP&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23545;&#25991;&#26412;&#19982;&#22270;&#20687;&#20132;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312; overestimate&#24615;&#33021;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2406.16464</link><description>&lt;p&gt;
InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.16464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InterCLIP-MEP&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23545;&#25991;&#26412;&#19982;&#22270;&#20687;&#20132;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312; overestimate&#24615;&#33021;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16464v3 Announce Type: replace-cross  Abstract: The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Existing multi-modal sarcasm detection methods have been proven to overestimate performance, as they struggle to effectively capture the intricate sarcastic cues that arise from the interaction between an image and text. To address these issues, we propose InterCLIP-MEP, a novel framework for multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP (InterCLIP) as the backbone to extract text-image representations, enhancing them by embedding cross-modality information directly within each encoder, thereby improving the representations to capture text-image interactions better. Furthermore, an efficient training strategy is designed to adapt InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic, fixed-length dual-channel mem
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;YOAS framework&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#23558;&#31232;&#30095;&#36890;&#36947;&#30340;EEG&#20449;&#21495;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#23494;&#38598;&#36890;&#36947;EEG&#20449;&#21495;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#36328;&#36890;&#36947;EEG&#20449;&#21495;&#29983;&#25104;&#30340;&#31934;&#24230;&#65292;&#24182;&#22312;&#25104;&#26412;&#25928;&#30410;&#21644;&#20415;&#25658;&#24615;&#26041;&#38754;&#20026;&#23494;&#38598;&#36890;&#36947;EEG&#20449;&#21495;&#30340;&#39640;&#31934;&#24230;&#37319;&#38598;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2406.15269</link><description>&lt;p&gt;
You Only Acquire Sparse-channel (YOAS): A Unified Framework for Dense-channel EEG Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.15269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;YOAS framework&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#23558;&#31232;&#30095;&#36890;&#36947;&#30340;EEG&#20449;&#21495;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#23494;&#38598;&#36890;&#36947;EEG&#20449;&#21495;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#36328;&#36890;&#36947;EEG&#20449;&#21495;&#29983;&#25104;&#30340;&#31934;&#24230;&#65292;&#24182;&#22312;&#25104;&#26412;&#25928;&#30410;&#21644;&#20415;&#25658;&#24615;&#26041;&#38754;&#20026;&#23494;&#38598;&#36890;&#36947;EEG&#20449;&#21495;&#30340;&#39640;&#31934;&#24230;&#37319;&#38598;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.15269v2 Announce Type: replace  Abstract: High-precision acquisition of dense-channel electroencephalogram (EEG) signals is often impeded by the costliness and lack of portability of equipment. In contrast, generating dense-channel EEG signals effectively from sparse channels shows promise and economic viability. However, sparse-channel EEG poses challenges such as reduced spatial resolution, information loss, signal mixing, and heightened susceptibility to noise and interference. To address these challenges, we first theoretically formulate the dense-channel EEG generation problem as by optimizing a set of cross-channel EEG signal generation problems. Then, we propose the YOAS framework for generating dense-channel data from sparse-channel EEG signals. The YOAS totally consists of four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG Generation, and Synthetic EEG Generation. Data Preparation and Preprocessing carefully consider the distribution of EEG ele
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CM2-Net&#30340;&#36830;&#32493;&#36328;&#27169;&#24577;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#26032;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20174;&#20197;&#24448;&#27169;&#24577;&#33719;&#24471;&#30340;&#25351;&#31034;&#20449;&#24687;&#26469;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#26174;&#33879;&#22495;&#38388;&#38553;&#12290;</title><link>https://arxiv.org/abs/2406.11340</link><description>&lt;p&gt;
CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.11340
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CM2-Net&#30340;&#36830;&#32493;&#36328;&#27169;&#24577;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#26032;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20174;&#20197;&#24448;&#27169;&#24577;&#33719;&#24471;&#30340;&#25351;&#31034;&#20449;&#24687;&#26469;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#26174;&#33879;&#22495;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31867;&#20284;&#21644;&#19981;&#31867;&#20284;&#22270;&#20687;&#23545;&#30340;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#26631;&#27880;&#25104;&#26412;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2406.10107</link><description>&lt;p&gt;
Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.10107
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31867;&#20284;&#21644;&#19981;&#31867;&#20284;&#22270;&#20687;&#23545;&#30340;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#26631;&#27880;&#25104;&#26412;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.10107v2 Announce Type: replace  Abstract: Deep metric learning (DML) has shown to be effective for content-based image retrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a high number of annotated images to accurately learn model parameters of deep neural networks (DNNs). However, gathering such data is time-consuming and costly. To address this, we propose an annotation cost-efficient active learning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to create a small but informative training set made up of similar and dissimilar image pairs to be utilized for accurately learning a metric space. The informativeness of image pairs is evaluated by combining uncertainty and diversity criteria. To assess the uncertainty of image pairs, we introduce two algorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary classifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically estimates a threshold value that acts
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2406.04485</link><description>&lt;p&gt;
GenAI Arena: An Open Evaluation Platform for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04485v2 Announce Type: replace-cross  Abstract: Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source g
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIP&#30340;&#26032;&#22411;&#22270;&#20687;&#22806;&#23637;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#21644;&#32452;&#32455;&#22270;&#20687;&#30340;&#36974;&#32617;&#21644;&#38750;&#36974;&#32617;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20123;&#25991;&#26412;&#25552;&#31034;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#21046;&#22806;&#23637;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#20013;&#24515;-&#25972;&#20307;-&#21608;&#22260;&#65288;CTS&#65289;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22806;&#23637;&#32454;&#33410;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#22806;&#23637;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2406.01059</link><description>&lt;p&gt;
VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.01059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIP&#30340;&#26032;&#22411;&#22270;&#20687;&#22806;&#23637;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#21644;&#32452;&#32455;&#22270;&#20687;&#30340;&#36974;&#32617;&#21644;&#38750;&#36974;&#32617;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20123;&#25991;&#26412;&#25552;&#31034;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#21046;&#22806;&#23637;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#20013;&#24515;-&#25972;&#20307;-&#21608;&#22260;&#65288;CTS&#65289;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22806;&#23637;&#32454;&#33410;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#22806;&#23637;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.01059v2 Announce Type: replace  Abstract: In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirement of users. First of all, we take advantage of a Multimodal Large Language Model (MLLM) that automatically extracts and organizes the corresponding textual descriptions of the masked and unmasked part of a given image. Accordingly, the obtained text prompts are introduced to endow our model with the capacity to customize the outpainting results. In addition, a special Cross-Attention module, namely Center-Total-Surrounding (CTS), is elaborately designed to enhance further the the interact
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Twin Deformable Point Convolutions (TDConvs)&#8221;&#30340;&#26032;&#22411;&#21367;&#31215;&#25805;&#20316;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#32428;&#24230;&#21644;&#32463;&#24230;&#24179;&#38754;&#20197;&#21450;&#28023;&#25300;&#26041;&#21521;&#19978;&#23398;&#20064;&#21487;&#36866;&#24212;&#30340;&#37319;&#26679;&#28857;&#65292;&#26469;&#36798;&#21040;&#23545;&#36965;&#24863;&#28857;&#20113;&#29305;&#24449;&#30340;&#28789;&#27963;&#23398;&#20064;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#28857;&#20113;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2405.19735</link><description>&lt;p&gt;
Twin Deformable Point Convolutions for Point Cloud Semantic Segmentation in Remote Sensing Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.19735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Twin Deformable Point Convolutions (TDConvs)&#8221;&#30340;&#26032;&#22411;&#21367;&#31215;&#25805;&#20316;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#32428;&#24230;&#21644;&#32463;&#24230;&#24179;&#38754;&#20197;&#21450;&#28023;&#25300;&#26041;&#21521;&#19978;&#23398;&#20064;&#21487;&#36866;&#24212;&#30340;&#37319;&#26679;&#28857;&#65292;&#26469;&#36798;&#21040;&#23545;&#36965;&#24863;&#28857;&#20113;&#29305;&#24449;&#30340;&#28789;&#27963;&#23398;&#20064;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#28857;&#20113;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19735v2 Announce Type: replace  Abstract: Thanks to the application of deep learning technology in point cloud processing of the remote sensing field, point cloud segmentation has become a research hotspot in recent years, which can be applied to real-world 3D, smart cities, and other fields. Although existing solutions have made unprecedented progress, they ignore the inherent characteristics of point clouds in remote sensing fields that are strictly arranged according to latitude, longitude, and altitude, which brings great convenience to the segmentation of point clouds in remote sensing fields. To consider this property cleverly, we propose novel convolution operators, termed Twin Deformable point Convolutions (TDConvs), which aim to achieve adaptive feature learning by learning deformable sampling points in the latitude-longitude plane and altitude direction, respectively. First, to model the characteristics of the latitude-longitude plane, we propose a Cylinder-wise De
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;GELU&#28608;&#27963;&#20989;&#25968;&#20026;&#26680;&#24515;&#32452;&#20214;&#30340;Vision Transformer&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#30340;&#21019;&#26032;&#24212;&#29992;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20248;&#21270;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2405.15953</link><description>&lt;p&gt;
Activator: GLU Activation Function as the Core Component of a Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.15953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;GELU&#28608;&#27963;&#20989;&#25968;&#20026;&#26680;&#24515;&#32452;&#20214;&#30340;Vision Transformer&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#30340;&#21019;&#26032;&#24212;&#29992;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20248;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.15953v2 Announce Type: replace  Abstract: Transformer architecture currently represents the main driver behind many successes in a variety of tasks addressed by deep learning, especially the recent advances in natural language processing (NLP) culminating with large language models (LLM). In addition, transformer architecture has found a wide spread of interest from computer vision (CV) researchers and practitioners, allowing for many advancements in vision-related tasks and opening the door for multi-task and multi-modal deep learning architectures that share the same principle of operation. One drawback to these architectures is their reliance on the scaled dot product attention mechanism with the softmax activation function, which is computationally expensive and requires large compute capabilities both for training and inference. This paper investigates substituting the attention mechanism usually adopted for transformer architecture with an architecture incorporating ga
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SE3D&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;3D&#25104;&#20687;&#20013;&#35780;&#20272;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#20855;&#20307;&#38024;&#23545;3D CNN&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19987;&#20026;&#20854;&#35774;&#35745;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2405.14584</link><description>&lt;p&gt;
SE3D: A Framework For Saliency Method Evaluation In 3D Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SE3D&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;3D&#25104;&#20687;&#20013;&#35780;&#20272;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#20855;&#20307;&#38024;&#23545;3D CNN&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19987;&#20026;&#20854;&#35774;&#35745;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14584v2 Announce Type: replace  Abstract: For more than a decade, deep learning models have been dominating in various 2D imaging tasks. Their application is now extending to 3D imaging, with 3D Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and CT scans, with significant implications for fields such as autonomous driving and medical imaging. In these critical settings, explaining the model's decisions is fundamental. Despite recent advances in Explainable Artificial Intelligence, however, little effort has been devoted to explaining 3D CNNs, and many works explain these models via inadequate extensions of 2D saliency methods.   A fundamental limitation to the development of 3D saliency methods is the lack of a benchmark to quantitatively assess these on 3D data. To address this issue, we propose SE3D: a framework for Saliency method Evaluation in 3D imaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and evaluation metrics 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20943;&#23569;&#23384;&#20648;&#30340;&#30452;&#25509;&#24352;&#37327;&#29615;&#20998;&#35299;&#65288;RSDTR&#65289;&#30340; convolutional neural networks (CNNs) &#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26356;&#39640;&#30340;&#24490;&#29615;&#27169;&#24335;&#25490;&#21015;&#28789;&#27963;&#24615;&#21644;&#36739;&#22823;&#30340;&#21442;&#25968;&#21644;FLOPS&#21387;&#32553;&#29575;&#65292;&#22312;&#20445;&#25345;&#33391;&#22909;&#30340;&#21387;&#32553;&#32593;&#32476;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#27604;&#20854;&#20182;&#24403;&#21069;&#20808;&#36827;CNN&#21387;&#32553;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2405.10802</link><description>&lt;p&gt;
Reduced storage direct tensor ring decomposition for convolutional neural networks compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.10802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20943;&#23569;&#23384;&#20648;&#30340;&#30452;&#25509;&#24352;&#37327;&#29615;&#20998;&#35299;&#65288;RSDTR&#65289;&#30340; convolutional neural networks (CNNs) &#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26356;&#39640;&#30340;&#24490;&#29615;&#27169;&#24335;&#25490;&#21015;&#28789;&#27963;&#24615;&#21644;&#36739;&#22823;&#30340;&#21442;&#25968;&#21644;FLOPS&#21387;&#32553;&#29575;&#65292;&#22312;&#20445;&#25345;&#33391;&#22909;&#30340;&#21387;&#32553;&#32593;&#32476;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#27604;&#20854;&#20182;&#24403;&#21069;&#20808;&#36827;CNN&#21387;&#32553;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10802v2 Announce Type: replace  Abstract: Convolutional neural networks (CNNs) are among the most widely used machine learning models for computer vision tasks, such as image classification. To improve the efficiency of CNNs, many CNNs compressing approaches have been developed. Low-rank methods approximate the original convolutional kernel with a sequence of smaller convolutional kernels, which leads to reduced storage and time complexities. In this study, we propose a novel low-rank CNNs compression method that is based on reduced storage direct tensor ring decomposition (RSDTR). The proposed method offers a higher circular mode permutation flexibility, and it is characterized by large parameter and FLOPS compression rates, while preserving a good classification accuracy of the compressed network. The experiments, performed on the CIFAR-10 and ImageNet datasets, clearly demonstrate the efficiency of RSDTR in comparison to other state-of-the-art CNNs compression approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FRACTAL&#30340;&#22823;&#35268;&#27169;&#33322;&#27979;&#28608;&#20809;&#38647;&#36798;(ALS)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;3D&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35206;&#30422;&#20102;250&#24179;&#26041;&#20844;&#37324;&#65292;&#21253;&#25324;100,000&#20010;&#23494;&#38598;&#28857;&#20113;&#65292;&#20197;&#21450;&#26469;&#33258;&#20116;&#20010;&#27861;&#22269;&#22320;&#21306;&#30340;&#39640;&#36136;&#37327;&#26631;&#31614;&#65292;&#26377;&#21161;&#20110;&#25903;&#25345;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2405.04634</link><description>&lt;p&gt;
FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic Segmentation of Diverse Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.04634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FRACTAL&#30340;&#22823;&#35268;&#27169;&#33322;&#27979;&#28608;&#20809;&#38647;&#36798;(ALS)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;3D&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35206;&#30422;&#20102;250&#24179;&#26041;&#20844;&#37324;&#65292;&#21253;&#25324;100,000&#20010;&#23494;&#38598;&#28857;&#20113;&#65292;&#20197;&#21450;&#26469;&#33258;&#20116;&#20010;&#27861;&#22269;&#22320;&#21306;&#30340;&#39640;&#36136;&#37327;&#26631;&#31614;&#65292;&#26377;&#21161;&#20110;&#25903;&#25345;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.04634v3 Announce Type: replace  Abstract: Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a new tool to monitor territory and support public policies. Processing ALS data at scale requires efficient point classification methods that perform well over highly diverse territories. To evaluate them, researchers need large annotated Lidar datasets, however, current Lidar benchmark datasets have restricted scope and often cover a single urban area. To bridge this data gap, we present the FRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an ultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with high-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is built upon France's nationwide open Lidar data. It achieves spatial and semantic diversity via a sampling scheme that explicitly concentrates rare classes and challenging landscapes from five French regions. It should support the development of 3D deep 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27969;&#23884;&#20837;&#32593;&#32476;&#30340;JOSENet&#65292;&#23427;&#21033;&#29992;&#32852;&#21512;&#27969;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#20102;&#22312;&#30417;&#25511;&#35270;&#39057;&#20013;&#26816;&#27979;&#26292;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21152;&#24555;&#20102;&#26816;&#27979;&#36895;&#24230;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2405.02961</link><description>&lt;p&gt;
JOSENet: A Joint Stream Embedding Network for Violence Detection in Surveillance Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.02961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27969;&#23884;&#20837;&#32593;&#32476;&#30340;JOSENet&#65292;&#23427;&#21033;&#29992;&#32852;&#21512;&#27969;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#20102;&#22312;&#30417;&#25511;&#35270;&#39057;&#20013;&#26816;&#27979;&#26292;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21152;&#24555;&#20102;&#26816;&#27979;&#36895;&#24230;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.02961v2 Announce Type: replace  Abstract: The increasing proliferation of video surveillance cameras and the escalating demand for crime prevention have intensified interest in the task of violence detection within the research community. Compared to other action recognition tasks, violence detection in surveillance videos presents additional issues, such as the wide variety of real fight scenes. Unfortunately, existing datasets for violence detection are relatively small in comparison to those for other action recognition tasks. Moreover, surveillance footage often features different individuals in each video and varying backgrounds for each camera. In addition, fast detection of violent actions in real-life surveillance videos is crucial to prevent adverse outcomes, thus necessitating models that are optimized for reduced memory usage and computational costs. These challenges complicate the application of traditional action recognition methods. To tackle all these issues, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoReX&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#22522;&#20110;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20197;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#65288;&#22914;&#29983;&#29289;&#23398;&#65289;&#25581;&#31034;&#21644;&#35780;&#20272;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#38480;&#21046;&#39044;&#27979;&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36827;&#34892;&#20102;&#22810;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;CoReX&#22312;&#35299;&#37322;&#21644;&#35780;&#20272;CNN&#27169;&#22411;&#20915;&#31574;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2405.01661</link><description>&lt;p&gt;
When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01661
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoReX&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#22522;&#20110;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20197;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#65288;&#22914;&#29983;&#29289;&#23398;&#65289;&#25581;&#31034;&#21644;&#35780;&#20272;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#38480;&#21046;&#39044;&#27979;&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36827;&#34892;&#20102;&#22810;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;CoReX&#22312;&#35299;&#37322;&#21644;&#35780;&#20272;CNN&#27169;&#22411;&#20915;&#31574;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#20013;&#22411;&#37327;&#23376;&#27604;&#29305;&#26550;&#26500;&#30340;&#39640;&#25928;&#27169;&#25311;&#65292;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#33016;&#29255;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#38271;&#23614;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20102;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2405.00156</link><description>&lt;p&gt;
Expanding the Horizon: Enabling Hybrid Quantum Transfer Learning for Long-Tailed Chest X-Ray Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#20013;&#22411;&#37327;&#23376;&#27604;&#29305;&#26550;&#26500;&#30340;&#39640;&#25928;&#27169;&#25311;&#65292;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#33016;&#29255;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#38271;&#23614;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20102;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00156v2 Announce Type: replace  Abstract: Quantum machine learning (QML) has the potential for improving the multi-label classification of rare, albeit critical, diseases in large-scale chest x-ray (CXR) datasets due to theoretical quantum advantages over classical machine learning (CML) in sample efficiency and generalizability. While prior literature has explored QML with CXRs, it has focused on binary classification tasks with small datasets due to limited access to quantum hardware and computationally expensive simulations. To that end, we implemented a Jax-based framework that enables the simulation of medium-sized qubit architectures with significant improvements in wall-clock time over current software offerings. We evaluated the performance of our Jax-based framework in terms of efficiency and performance for hybrid quantum transfer learning for long-tailed classification across 8, 14, and 19 disease labels using large-scale CXR datasets. The Jax-based framework resu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;AI-&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#35780;&#20215;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2404.17762</link><description>&lt;p&gt;
Large Multi-modality Model Assisted AI-Generated Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.17762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;AI-&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#35780;&#20215;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17762v2 Announce Type: replace  Abstract: Traditional deep neural network (DNN)-based image quality assessment (IQA) models leverage convolutional neural networks (CNN) or Transformer to learn the quality-aware feature representation, achieving commendable performance on natural scene images. However, when applied to AI-Generated images (AGIs), these DNN-based IQA models exhibit subpar performance. This situation is largely due to the semantic inaccuracies inherent in certain AGIs caused by uncontrollable nature of the generation process. Thus, the capability to discern semantic content becomes crucial for assessing the quality of AGIs. Traditional DNN-based IQA models, constrained by limited parameter complexity and training data, struggle to capture complex fine-grained semantic features, making it challenging to grasp the existence and coherence of semantic content of the entire image. To address the shortfall in semantic content perception of current IQA models, we intro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlenderAlchemy&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#22914;GPT-4V&#65292;&#36890;&#36807;&#27719;&#32858;&#35270;&#35273;&#20449;&#24687;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#25628;&#32034;&#24182;&#29983;&#25104;&#20248;&#21270;&#35774;&#35745;&#30340;&#31934;&#30830;&#24207;&#21015;&#34892;&#21160;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;3D&#22270;&#24418;&#35774;&#35745;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.17672</link><description>&lt;p&gt;
BlenderAlchemy: Editing 3D Graphics with Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.17672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlenderAlchemy&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#22914;GPT-4V&#65292;&#36890;&#36807;&#27719;&#32858;&#35270;&#35273;&#20449;&#24687;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#25628;&#32034;&#24182;&#29983;&#25104;&#20248;&#21270;&#35774;&#35745;&#30340;&#31934;&#30830;&#24207;&#21015;&#34892;&#21160;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;3D&#22270;&#24418;&#35774;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17672v3 Announce Type: replace  Abstract: Graphics design is important for various applications, including movie production and game design. To create a high-quality scene, designers usually need to spend hours in software like Blender, in which they might need to interleave and repeat operations, such as connecting material nodes, hundreds of times. Moreover, slightly different design goals may require completely different sequences, making automation difficult. In this paper, we propose a system that leverages Vision-Language Models (VLMs), like GPT-4V, to intelligently search the design action space to arrive at an answer that can satisfy a user's intent. Specifically, we design a vision-based edit generator and state evaluator to work together to find the correct sequence of actions to achieve the goal. Inspired by the role of visual imagination in the human design process, we supplement the visual reasoning capabilities of VLMs with "imagined" reference images from imag
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Halo-NeRF&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#20960;&#20309;&#24341;&#23548;&#30340;&#35821;&#20041;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#26053;&#28216;&#26223;&#28857;&#30340;&#32593;&#32476;&#22270;&#20687;&#38598;&#30340;&#39640;&#32423;&#29702;&#35299;&#21644;&#25506;&#32034;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.16845</link><description>&lt;p&gt;
HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.16845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Halo-NeRF&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#20960;&#20309;&#24341;&#23548;&#30340;&#35821;&#20041;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#26053;&#28216;&#26223;&#28857;&#30340;&#32593;&#32476;&#22270;&#20687;&#38598;&#30340;&#39640;&#32423;&#29702;&#35299;&#21644;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.16845v2 Announce Type: replace  Abstract: Internet image collections containing photos captured by crowds of photographers show promise for enabling digital exploration of large-scale tourist landmarks. However, prior works focus primarily on geometric reconstruction and visualization, neglecting the key role of language in providing a semantic interface for navigation and fine-grained understanding. In constrained 3D domains, recent methods have leveraged vision-and-language models as a strong prior of 2D visual semantics. While these models display an excellent understanding of broad visual semantics, they struggle with unconstrained photo collections depicting such tourist landmarks, as they lack expert knowledge of the architectural domain. In this work, we present a localization system that connects neural representations of scenes depicting large-scale landmarks with text describing a semantic region within the scene, by harnessing the power of SOTA vision-and-language
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CT&#22270;&#20687;&#21644;&#38750;&#23545;&#27604;&#24615;&#35786;&#26029;&#25253;&#21578;&#65292;&#20197;&#39044;&#27979;&#33041;&#21330;&#20013;&#30340;&#27835;&#30103;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#35299;&#37322;&#25991;&#26412;&#20449;&#24687;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#22270;&#20687;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#12290;&#23613;&#31649;&#20165;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;Transformer&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#24403;&#32467;&#21512;&#24102;&#26377;&#20020;&#24202;meta&#29305;&#24449;&#35786;&#26029;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;&#33041;&#21330;&#20013;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.12634</link><description>&lt;p&gt;
Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CT&#22270;&#20687;&#21644;&#38750;&#23545;&#27604;&#24615;&#35786;&#26029;&#25253;&#21578;&#65292;&#20197;&#39044;&#27979;&#33041;&#21330;&#20013;&#30340;&#27835;&#30103;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#35299;&#37322;&#25991;&#26412;&#20449;&#24687;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#22270;&#20687;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#12290;&#23613;&#31649;&#20165;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;Transformer&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#24403;&#32467;&#21512;&#24102;&#26377;&#20020;&#24202;meta&#29305;&#24449;&#35786;&#26029;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;&#33041;&#21330;&#20013;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12634v2 Announce Type: replace  Abstract: This study proposes a multi-modal fusion framework Multitrans based on the Transformer architecture and self-attention mechanism. This architecture combines the study of non-contrast computed tomography (NCCT) images and discharge diagnosis reports of patients undergoing stroke treatment, using a variety of methods based on Transformer architecture approach to predicting functional outcomes of stroke treatment. The results show that the performance of single-modal text classification is significantly better than single-modal image classification, but the effect of multi-modal combination is better than any single modality. Although the Transformer model only performs worse on imaging data, when combined with clinical meta-diagnostic information, both can learn better complementary information and make good contributions to accurately predicting stroke treatment effects..
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026; Fact &#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#35270;&#35273;&#32534;&#31243;&#20195;&#30721;&#26469;&#21046;&#20316;&#31934;&#30830;&#12289;&#32039;&#20945;&#12289;&#21487;&#36716;&#31227;&#30340;&#22810;&#27169;&#24577;&#27880;&#37322;&#65292;&#26088;&#22312;&#25913;&#36827; MLLM &#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2404.11129</link><description>&lt;p&gt;
Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.11129
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026; Fact &#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#35270;&#35273;&#32534;&#31243;&#20195;&#30721;&#26469;&#21046;&#20316;&#31934;&#30830;&#12289;&#32039;&#20945;&#12289;&#21487;&#36716;&#31227;&#30340;&#22810;&#27169;&#24577;&#27880;&#37322;&#65292;&#26088;&#22312;&#25913;&#36827; MLLM &#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11129v2 Announce Type: replace  Abstract: The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks. Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision. Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Sky-GVIO&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#20840;&#21367;&#31215;&#32593;&#32476;(FCN)&#30340;&#23627;&#39030;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#65292;&#26377;&#25928;&#26816;&#27979;GNSS&#30340;&#38750;&#35270;&#36317;(NLOS)&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;S-NDM&#30340;&#26816;&#27979;&#21644;&#32531;&#35299;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;GNSS/INS/&#35270;&#35273;&#23548;&#33322;&#31995;&#32479;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.11070</link><description>&lt;p&gt;
Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based sky-segmentation in urban canyon
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.11070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Sky-GVIO&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#20840;&#21367;&#31215;&#32593;&#32476;(FCN)&#30340;&#23627;&#39030;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#65292;&#26377;&#25928;&#26816;&#27979;GNSS&#30340;&#38750;&#35270;&#36317;(NLOS)&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;S-NDM&#30340;&#26816;&#27979;&#21644;&#32531;&#35299;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;GNSS/INS/&#35270;&#35273;&#23548;&#33322;&#31995;&#32479;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11070v2 Announce Type: replace  Abstract: Accurate, continuous, and reliable positioning is a critical component of achieving autonomous driving. However, in complex urban canyon environments, the vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused by high buildings, trees, and elevated structures seriously affect positioning results. To address these challenges, a sky-view images segmentation algorithm based on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection. Building upon this, a novel NLOS detection and mitigation algorithm (named S-NDM) is extended to the tightly coupled Global Navigation Satellite Systems (GNSS), Inertial Measurement Units (IMU), and visual feature system which is called Sky-GVIO, with the aim of achieving continuous and accurate positioning in urban canyon environments. Furthermore, the system harmonizes Single Point Positioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its operational ver
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MK-SGN&#30340;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#34701;&#21512;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#29992;&#20110;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#12290;MK-SGN&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#36716;&#25442;&#20026;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#20102;&#35782;&#21035;&#31934;&#24230;&#12290;&#36825;&#39033;&#21019;&#26032;&#24037;&#20316;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25506;&#32034;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2404.10210</link><description>&lt;p&gt;
MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.10210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MK-SGN&#30340;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#34701;&#21512;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#29992;&#20110;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#12290;MK-SGN&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#36716;&#25442;&#20026;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#20102;&#35782;&#21035;&#31934;&#24230;&#12290;&#36825;&#39033;&#21019;&#26032;&#24037;&#20316;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25506;&#32034;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.10210v2 Announce Type: replace  Abstract: In recent years, skeleton-based action recognition, leveraging multimodal Graph Convolutional Networks (GCN), has achieved remarkable results. However, due to their deep structure and reliance on continuous floating-point operations, GCN-based methods are energy-intensive. We propose an innovative Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation (MK-SGN) to address this issue. By merging the energy efficiency of Spiking Neural Network (SNN) with the graph representation capability of GCN, the proposed MK-SGN reduces energy consumption while maintaining recognition accuracy. Firstly, we convert Graph Convolutional Networks (GCN) into Spiking Graph Convolutional Networks (SGN) establishing a new benchmark and paving the way for future research exploration. During this process, we introduce a spiking attention mechanism and design a Spiking-Spatio Graph Convolution module with a Spatial Global Spikin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#21307;&#23398;&#24433;&#20687;&#30340;&#22810;&#36890;&#36947;&#34701;&#21512;&#19982;&#21512;&#25104;&#37325;&#24314;&#65292;&#23588;&#20854;&#26159;&#22312;PET/CT&#25104;&#20687;&#20013;&#30340;&#20302;&#21058;&#37327;&#25104;&#20687;&#36136;&#37327;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.08748</link><description>&lt;p&gt;
Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Synergistic Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.08748
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#21307;&#23398;&#24433;&#20687;&#30340;&#22810;&#36890;&#36947;&#34701;&#21512;&#19982;&#21512;&#25104;&#37325;&#24314;&#65292;&#23588;&#20854;&#26159;&#22312;PET/CT&#25104;&#20687;&#20013;&#30340;&#20302;&#21058;&#37327;&#25104;&#20687;&#36136;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.08748v2 Announce Type: replace-cross  Abstract: This paper presents a novel approach for learned synergistic reconstruction of medical images using multi-branch generative models. Leveraging variational autoencoders (VAEs), our model learns from pairs of images simultaneously, enabling effective denoising and reconstruction. Synergistic image reconstruction is achieved by incorporating the trained models in a regularizer that evaluates the distance between the images and the model. We demonstrate the efficacy of our approach on both Modified National Institute of Standards and Technology (MNIST) and positron emission tomography (PET)/computed tomography (CT) datasets, showcasing improved image quality for low-dose imaging. Despite challenges such as patch decomposition and model limitations, our results underscore the potential of generative models for enhancing medical imaging reconstruction.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;OpenBias&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#20559;&#35265;&#36827;&#34892;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#35265;&#38598;&#21512;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20559;&#35265;&#65292;&#30446;&#26631;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#21450;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#21644;&#31243;&#24230;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#21487;&#39044;&#35265;&#20559;&#35265;&#38382;&#39064;&#19978;&#30340;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.07990</link><description>&lt;p&gt;
OpenBias: Open-set Bias Detection in Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07990
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;OpenBias&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#20559;&#35265;&#36827;&#34892;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#35265;&#38598;&#21512;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20559;&#35265;&#65292;&#30446;&#26631;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#21450;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#21644;&#31243;&#24230;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#21487;&#39044;&#35265;&#20559;&#35265;&#38382;&#39064;&#19978;&#30340;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07990v2 Announce Type: replace  Abstract: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previou
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;VRSO&#65292;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#29992;&#20110;&#38745;&#24577;&#23545;&#35937;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#22312;Waymo Open Dataset&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#36739;&#20302;&#30340;&#37325;&#26032;&#25237;&#24433;&#35823;&#24046;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;Waymo Open Datasetlabels&#20302;&#20102;&#32422;&#22235;&#20493;&#65288;&#21069;&#32773;&#20026;10.6&#20687;&#32032;&#65292;&#21518;&#32773;&#20165;&#20026;2.6&#20687;&#32032;&#65289;&#12290;VRSO&#26041;&#27861;&#22240;&#20854;&#25104;&#26412;&#20302;&#24265;&#12289;&#25928;&#29575;&#39640;&#12289;&#36136;&#37327;&#39640;&#32780;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#20165;&#36890;&#36807;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#23601;&#33021;&#24674;&#22797;&#38745;&#24577;&#23545;&#35937;&#22312;3D&#31354;&#38388;&#20013;&#30340;&#26631;&#27880;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#36895;&#24230;&#21644;&#26631;&#27880;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15026</link><description>&lt;p&gt;
VRSO: Visual-Centric Reconstruction for Static Object Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;VRSO&#65292;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#29992;&#20110;&#38745;&#24577;&#23545;&#35937;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#22312;Waymo Open Dataset&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#36739;&#20302;&#30340;&#37325;&#26032;&#25237;&#24433;&#35823;&#24046;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;Waymo Open Datasetlabels&#20302;&#20102;&#32422;&#22235;&#20493;&#65288;&#21069;&#32773;&#20026;10.6&#20687;&#32032;&#65292;&#21518;&#32773;&#20165;&#20026;2.6&#20687;&#32032;&#65289;&#12290;VRSO&#26041;&#27861;&#22240;&#20854;&#25104;&#26412;&#20302;&#24265;&#12289;&#25928;&#29575;&#39640;&#12289;&#36136;&#37327;&#39640;&#32780;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#20165;&#36890;&#36807;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#23601;&#33021;&#24674;&#22797;&#38745;&#24577;&#23545;&#35937;&#22312;3D&#31354;&#38388;&#20013;&#30340;&#26631;&#27880;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#36895;&#24230;&#21644;&#26631;&#27880;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15026v2 Announce Type: replace  Abstract: As a part of the perception results of intelligent driving systems, static object detection (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labelling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive and time-consuming in practice. This paper introduces VRSO, a visual-centric approach for static object annotation. Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static objects in 3D space with only camer
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSAP&#30340;&#24418;&#29366;&#25935;&#24863;&#24615;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#23427;&#33021;&#22815;&#23545;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#30772;&#22351;&#12290;</title><link>https://arxiv.org/abs/2403.11515</link><description>&lt;p&gt;
SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSAP&#30340;&#24418;&#29366;&#25935;&#24863;&#24615;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#23427;&#33021;&#22815;&#23545;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11515v2 Announce Type: replace-cross  Abstract: Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;HAIFIT&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#24182;&#25429;&#25417;&#24191;&#27867;&#30340;&#29305;&#24449;&#22270;&#20381;&#36182;&#20851;&#31995;&#65292;&#25104;&#21151;&#22320;&#23558;&#26102;&#23578;&#33609;&#22270;&#36716;&#21270;&#20026;&#39640;&#20445;&#30495;&#24230;&#21644;&#36924;&#30495;&#30340;&#26381;&#35013;&#22270;&#20687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#30041;&#35774;&#35745;&#24072;&#24847;&#22270;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#35774;&#35745;&#24072;&#25552;&#20379;&#20102;&#26356;&#21152;&#31934;&#30830;&#30340;&#25104;&#21697;&#39044;&#35272;&#12290;</title><link>https://arxiv.org/abs/2403.08651</link><description>&lt;p&gt;
HAIFIT: Fashion Image Translation for Human-to-AI Style Learning and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;HAIFIT&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#24182;&#25429;&#25417;&#24191;&#27867;&#30340;&#29305;&#24449;&#22270;&#20381;&#36182;&#20851;&#31995;&#65292;&#25104;&#21151;&#22320;&#23558;&#26102;&#23578;&#33609;&#22270;&#36716;&#21270;&#20026;&#39640;&#20445;&#30495;&#24230;&#21644;&#36924;&#30495;&#30340;&#26381;&#35013;&#22270;&#20687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#30041;&#35774;&#35745;&#24072;&#24847;&#22270;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#35774;&#35745;&#24072;&#25552;&#20379;&#20102;&#26356;&#21152;&#31934;&#30830;&#30340;&#25104;&#21697;&#39044;&#35272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08651v4 Announce Type: replace  Abstract: In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances. The advent of sketch-to-image cross-modal translation technology has notably aided designers. However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept. This limitation hampers the ability to offer designers a precise preview of the final output. To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives. Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motion Mamba&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#22522;&#20110;&#31616;&#27905;&#19988;&#39640;&#25928;&#30340;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26550;&#26500;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#38271;&#24207;&#21015;&#30340;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;SSM&#36827;&#34892;&#23618;&#32423;&#21270;&#22788;&#29702;&#65292;&#24182;&#32467;&#21512;&#20102;U-Net&#32467;&#26500;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#24207;&#21015;&#29983;&#25104;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#29983;&#25104;&#22797;&#26434;&#36816;&#21160;&#36807;&#31243;&#20013;&#20445;&#25345;&#36830;&#32493;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;Motion Mamba&#30340;&#25104;&#21151;&#20026;&#22823;&#22411;&#12289;&#30495;&#23454;&#24863;&#36816;&#21160;&#25968;&#25454;&#24211;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#22788;&#29702;&#25552;&#20379;&#20102;&#21487;&#33021;&#65292;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.07487</link><description>&lt;p&gt;
Motion Mamba: Efficient and Long Sequence Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motion Mamba&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#22522;&#20110;&#31616;&#27905;&#19988;&#39640;&#25928;&#30340;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26550;&#26500;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#38271;&#24207;&#21015;&#30340;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;SSM&#36827;&#34892;&#23618;&#32423;&#21270;&#22788;&#29702;&#65292;&#24182;&#32467;&#21512;&#20102;U-Net&#32467;&#26500;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#24207;&#21015;&#29983;&#25104;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#29983;&#25104;&#22797;&#26434;&#36816;&#21160;&#36807;&#31243;&#20013;&#20445;&#25345;&#36830;&#32493;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;Motion Mamba&#30340;&#25104;&#21151;&#20026;&#22823;&#22411;&#12289;&#30495;&#23454;&#24863;&#36816;&#21160;&#25968;&#25454;&#24211;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#22788;&#29702;&#25552;&#20379;&#20102;&#21487;&#33021;&#65292;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07487v4 Announce Type: replace  Abstract: Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between fr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;PrimeComposer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#36895;&#24230;&#26356;&#24555;&#12289;&#36136;&#37327;&#26356;&#20248;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#24335;&#25193;&#25955;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#23427;&#19987;&#27880;&#20110;&#23616;&#37096;&#32534;&#36753;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32972;&#26223;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#36807;&#28193;&#21306;&#22495;&#30340;&#19981;&#33391;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;PrimeComposer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#36895;&#24230;&#26356;&#24555;&#12289;&#36136;&#37327;&#26356;&#20248;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#24335;&#25193;&#25955;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#23427;&#19987;&#27880;&#20110;&#23616;&#37096;&#32534;&#36753;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32972;&#26223;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#36807;&#28193;&#21306;&#22495;&#30340;&#19981;&#33391;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v2 Announce Type: replace  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. Current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only impedes their swift implementation but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tra
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#21512;&#25104;&#22256;&#38590;&#36127;&#38754;&#25991;&#26412;&#20363;&#23376;&#21040;&#39044;&#35757;&#32451;&#20013;&#65292;&#22686;&#24378;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#27010;&#24565;&#29702;&#35299;&#26041;&#38754;&#30340;&#31934;&#32454;&#24230;&#12290;&#36825;&#20123;&#22256;&#38590;&#30340;&#36127;&#38754;&#26679;&#26412;&#36890;&#36807;&#38543;&#26426;&#25171;&#20081;&#19982;&#35270;&#35273;&#27010;&#24565;&#30456;&#20851;&#30340;&#35789;&#27719;&#26469;&#21019;&#24314;&#65292;&#36825;&#23545;&#20110;&#20419;&#36827;&#26356;&#31934;&#32454;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#27010;&#24565;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#23567;&#32452;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22256;&#38590;&#25968;&#25454;&#38598;&#8220;InpaintCOCO&#8221;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39068;&#33394;&#12289;&#29289;&#20307;&#21644;&#22823;&#23567;&#30340;&#31934;&#32454;&#27010;&#24565;&#23545;&#40784;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;COCO&#22270;&#20687;&#20013;&#29983;&#25104;&#24182;&#23884;&#20837;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#25913;&#21464;&#35270;&#35273;&#27010;&#24565;&#26469;&#21435;&#38500;&#22270;&#20687;&#23545;&#21450;&#20854;&#21407;&#26631;&#39064;&#20043;&#38388;&#30340;&#21305;&#37197;&#20851;&#31995;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#32771;&#39564;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#24341;&#20837;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#35270;&#35273;&#27010;&#24565;&#30340;&#29702;&#35299;&#21644;&#20934;&#30830;&#24230;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.02875</link><description>&lt;p&gt;
Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#21512;&#25104;&#22256;&#38590;&#36127;&#38754;&#25991;&#26412;&#20363;&#23376;&#21040;&#39044;&#35757;&#32451;&#20013;&#65292;&#22686;&#24378;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#27010;&#24565;&#29702;&#35299;&#26041;&#38754;&#30340;&#31934;&#32454;&#24230;&#12290;&#36825;&#20123;&#22256;&#38590;&#30340;&#36127;&#38754;&#26679;&#26412;&#36890;&#36807;&#38543;&#26426;&#25171;&#20081;&#19982;&#35270;&#35273;&#27010;&#24565;&#30456;&#20851;&#30340;&#35789;&#27719;&#26469;&#21019;&#24314;&#65292;&#36825;&#23545;&#20110;&#20419;&#36827;&#26356;&#31934;&#32454;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#27010;&#24565;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#23567;&#32452;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22256;&#38590;&#25968;&#25454;&#38598;&#8220;InpaintCOCO&#8221;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39068;&#33394;&#12289;&#29289;&#20307;&#21644;&#22823;&#23567;&#30340;&#31934;&#32454;&#27010;&#24565;&#23545;&#40784;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;COCO&#22270;&#20687;&#20013;&#29983;&#25104;&#24182;&#23884;&#20837;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#25913;&#21464;&#35270;&#35273;&#27010;&#24565;&#26469;&#21435;&#38500;&#22270;&#20687;&#23545;&#21450;&#20854;&#21407;&#26631;&#39064;&#20043;&#38388;&#30340;&#21305;&#37197;&#20851;&#31995;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#32771;&#39564;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#24341;&#20837;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#35270;&#35273;&#27010;&#24565;&#30340;&#29702;&#35299;&#21644;&#20934;&#30830;&#24230;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02875v2 Announce Type: replace  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show s
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#27169;&#22359;&#20316;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26367;&#20195;&#65292;&#35813;&#27169;&#22359;&#21033;&#29992;&#32593;&#32476;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#30340;&#23616;&#37096;&#24863;&#21463;&#37326;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#28151;&#21512;&#26426;&#21046;&#26469;&#25552;&#21319;Transformer&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#27169;&#22359;&#20316;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26367;&#20195;&#65292;&#35813;&#27169;&#22359;&#21033;&#29992;&#32593;&#32476;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#30340;&#23616;&#37096;&#24863;&#21463;&#37326;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#28151;&#21512;&#26426;&#21046;&#26469;&#25552;&#21319;Transformer&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36741;&#21161;&#23545;&#25239;&#38450;&#24481;&#32593;&#32476;AADN&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#20687;&#36827;&#20837;Tracker&#20043;&#21069;&#25191;&#34892;&#38450;&#24481;&#24615;&#21464;&#25442;&#65292;&#22686;&#24378;&#22270;&#20687;&#36319;&#36394;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;AADN&#36890;&#36807;&#29305;&#23450;&#30340;DuAL-Loss&#35757;&#32451;&#29983;&#25104;&#21516;&#26102;&#25915;&#20987;Tracker&#20998;&#31867;&#21644;&#22238;&#24402;&#20998;&#25903;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28040;&#38500;&#20102;&#22270;&#20687;&#19978;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#12290;&#32463;&#36807;&#22312;OTB100&#12289;LaSOT&#21644;VOT2018&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AADN&#33021;&#22815;&#20445;&#25345;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#24615;&#21516;&#26102;&#19981;&#24433;&#21709;Tracker&#26412;&#36523;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17976</link><description>&lt;p&gt;
Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17976
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36741;&#21161;&#23545;&#25239;&#38450;&#24481;&#32593;&#32476;AADN&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#20687;&#36827;&#20837;Tracker&#20043;&#21069;&#25191;&#34892;&#38450;&#24481;&#24615;&#21464;&#25442;&#65292;&#22686;&#24378;&#22270;&#20687;&#36319;&#36394;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;AADN&#36890;&#36807;&#29305;&#23450;&#30340;DuAL-Loss&#35757;&#32451;&#29983;&#25104;&#21516;&#26102;&#25915;&#20987;Tracker&#20998;&#31867;&#21644;&#22238;&#24402;&#20998;&#25903;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28040;&#38500;&#20102;&#22270;&#20687;&#19978;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#12290;&#32463;&#36807;&#22312;OTB100&#12289;LaSOT&#21644;VOT2018&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AADN&#33021;&#22815;&#20445;&#25345;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#24615;&#21516;&#26102;&#19981;&#24433;&#21709;Tracker&#26412;&#36523;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17976v3 Announce Type: replace  Abstract: Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. However, there is still a lack of research on designing adversarial defense methods for object tracking. To address these issues, we propose an effective auxiliary pre-processing defense network, AADN, which performs defensive transformations on the input images before feeding them into the tracker. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without parameter adjustments. We train AADN using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that AADN maintains excellent defense robustness against adversarial att
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#28857;&#37319;&#26679;&#26041;&#27861;&#8212;AVS-Net&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#24335;voxel&#22823;&#23567;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#31934;&#32454;&#20960;&#20309;&#20449;&#24687;&#30340;&#21516;&#21515;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;voxel&#20013;&#24515;&#37319;&#26679;&#65292;&#24182;&#37319;&#29992;&#36866;&#24212;&#24615;voxel&#35843;&#25972;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;voxel&#22823;&#23567;&#30830;&#23450;&#21644;&#20851;&#38190;&#20960;&#20309;&#29305;&#24449;&#20445;&#23384;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;AVS-Net&#26082;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#21448;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#36776;&#35782;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17521</link><description>&lt;p&gt;
AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#28857;&#37319;&#26679;&#26041;&#27861;&#8212;AVS-Net&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#24335;voxel&#22823;&#23567;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#31934;&#32454;&#20960;&#20309;&#20449;&#24687;&#30340;&#21516;&#21515;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;voxel&#20013;&#24515;&#37319;&#26679;&#65292;&#24182;&#37319;&#29992;&#36866;&#24212;&#24615;voxel&#35843;&#25972;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;voxel&#22823;&#23567;&#30830;&#23450;&#21644;&#20851;&#38190;&#20960;&#20309;&#29305;&#24449;&#20445;&#23384;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;AVS-Net&#26082;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#21448;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#36776;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17521v3 Announce Type: replace  Abstract: The recent advancements in point cloud learning have enabled intelligent vehicles and robots to comprehend 3D environments better. However, processing large-scale 3D scenes remains a challenging problem, such that efficient downsampling methods play a crucial role in point cloud learning. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. For such purpose, this paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel centroid sampling as a foundation but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures that the sampling results exhibit a favorable distribution for comprehending var
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR-99&#30340;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22788;&#29702;&#26410;&#30693;&#27604;&#20363;&#21644;&#26497;&#31471;&#27604;&#20363;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;3&#28857;&#25277;&#26679;&#21644;&#39034;&#24207;&#20248;&#20808;&#21450;&#22522;&#20110;&#19977;&#20803;&#32452;&#27604;&#20363;&#19968;&#33268;&#24615;&#39044;&#31579;&#36873;&#30340;&#26426;&#21046;&#65292;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#30340;&#22330;&#26223;&#20013;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#21363;&#20351;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#29575;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#22312;&#20445;&#25345;&#26497;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26174;&#31034;&#20986;&#20102;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#38382;&#39064;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16598</link><description>&lt;p&gt;
PCR-99: A Practical Method for Point Cloud Registration with 99 Percent Outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR-99&#30340;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22788;&#29702;&#26410;&#30693;&#27604;&#20363;&#21644;&#26497;&#31471;&#27604;&#20363;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;3&#28857;&#25277;&#26679;&#21644;&#39034;&#24207;&#20248;&#20808;&#21450;&#22522;&#20110;&#19977;&#20803;&#32452;&#27604;&#20363;&#19968;&#33268;&#24615;&#39044;&#31579;&#36873;&#30340;&#26426;&#21046;&#65292;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#30340;&#22330;&#26223;&#20013;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#21363;&#20351;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#29575;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#22312;&#20445;&#25345;&#26497;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26174;&#31034;&#20986;&#20102;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#38382;&#39064;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16598v5 Announce Type: replace-cross  Abstract: We propose a robust method for point cloud registration that can handle both unknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a deterministic 3-point sampling approach with two novel mechanisms that significantly boost the speed: (1) an improved ordering of the samples based on pairwise scale consistency, prioritizing the point correspondences that are more likely to be inliers, and (2) an efficient outlier rejection scheme based on triplet scale consistency, prescreening bad samples and reducing the number of hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio, the proposed method achieves comparable performance to the state of the art. At 99% outlier ratio, however, it outperforms the state of the art for both known-scale and unknown-scale problems. Especially for the latter, we observe a clear superiority in terms of robustness and speed.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21306;&#22495;&#21464;&#25442;&#22120;&#65288;Regformer&#65289;&#30340;&#22522;&#20110;Transformer&#30340;&#21333;&#22270;&#20687;&#38477;&#38632;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#38632;&#24433;&#21709;&#30340;&#21644;&#19981;&#21463;&#24433;&#21709;&#30340;&#37096;&#20998;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#37325;&#24314;&#12290;&#25991;&#31456;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#24341;&#20837;&#20102;&#21306;&#22495;&#21464;&#25442;&#22120;&#22359;&#65288;RTB&#65289;&#65292;&#35813;&#22359;&#32467;&#21512;&#20102;&#21306;&#22495;&#36974;&#25377;&#27880;&#24847;&#21147;&#65288;RMA&#65289;&#26426;&#21046;&#21644;&#19968;&#20010;&#28151;&#21512;&#38376;&#27491;&#21521;&#22359;&#65288;MGFB&#65289;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#21306;&#20998;&#38632;&#28857;&#19982;&#32972;&#26223;&#12289;&#26080;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#19981;&#21516;&#21306;&#22495;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21333;&#22270;&#20687;&#38477;&#38632;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16033</link><description>&lt;p&gt;
Exploiting Regional Information Transformer for Single Image Deraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21306;&#22495;&#21464;&#25442;&#22120;&#65288;Regformer&#65289;&#30340;&#22522;&#20110;Transformer&#30340;&#21333;&#22270;&#20687;&#38477;&#38632;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#38632;&#24433;&#21709;&#30340;&#21644;&#19981;&#21463;&#24433;&#21709;&#30340;&#37096;&#20998;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#37325;&#24314;&#12290;&#25991;&#31456;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#24341;&#20837;&#20102;&#21306;&#22495;&#21464;&#25442;&#22120;&#22359;&#65288;RTB&#65289;&#65292;&#35813;&#22359;&#32467;&#21512;&#20102;&#21306;&#22495;&#36974;&#25377;&#27880;&#24847;&#21147;&#65288;RMA&#65289;&#26426;&#21046;&#21644;&#19968;&#20010;&#28151;&#21512;&#38376;&#27491;&#21521;&#22359;&#65288;MGFB&#65289;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#21306;&#20998;&#38632;&#28857;&#19982;&#32972;&#26223;&#12289;&#26080;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#19981;&#21516;&#21306;&#22495;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21333;&#22270;&#20687;&#38477;&#38632;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16033v2 Announce Type: replace  Abstract: Transformer-based Single Image Deraining (SID) methods have achieved remarkable success, primarily attributed to their robust capability in capturing long-range interactions. However, we've noticed that current methods handle rain-affected and unaffected regions concurrently, overlooking the disparities between these areas, resulting in confusion between rain streaks and background parts, and inabilities to obtain effective interactions, ultimately resulting in suboptimal deraining outcomes. To address the above issue, we introduce the Region Transformer (Regformer), a novel SID method that underlines the importance of independently processing rain-affected and unaffected regions while considering their combined impact for high-quality image reconstruction. The crux of our method is the innovative Region Transformer Block (RTB), which integrates a Region Masked Attention (RMA) mechanism and a Mixed Gate Forward Block (MGFB). Our RTB 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#35774;&#32622;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#20167;&#24680;&#34920;&#24773;&#21253;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#35774;&#32622;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#20167;&#24680;&#34920;&#24773;&#21253;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v2 Announce Type: replace-cross  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our anal
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReViT&#30340;&#25913;&#36827;&#22411;Vision Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#27531;&#24046;&#36830;&#25509;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#26356;&#28145;&#23618;&#32423;&#30340;&#29305;&#24449;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11301</link><description>&lt;p&gt;
ReViT: Enhancing Vision Transformers Feature Diversity with Attention Residual Connections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReViT&#30340;&#25913;&#36827;&#22411;Vision Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#27531;&#24046;&#36830;&#25509;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#26356;&#28145;&#23618;&#32423;&#30340;&#29305;&#24449;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11301v2 Announce Type: replace  Abstract: Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AONeuS&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#22768;&#20809;&#20256;&#24863;&#22120;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#39640;&#20998;&#36776;&#29575;RGB&#22270;&#20687;&#19982;&#20302;&#20998;&#36776;&#29575;&#22768;&#23398;&#28145;&#24230;&#25104;&#20687;&#25968;&#25454;&#34701;&#21512;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31354;&#38388;&#33539;&#22260;&#20869;&#20063;&#33021;&#36827;&#34892;&#20934;&#30830;&#30340;3D&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.03309</link><description>&lt;p&gt;
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AONeuS&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#22768;&#20809;&#20256;&#24863;&#22120;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#39640;&#20998;&#36776;&#29575;RGB&#22270;&#20687;&#19982;&#20302;&#20998;&#36776;&#29575;&#22768;&#23398;&#28145;&#24230;&#25104;&#20687;&#25968;&#25454;&#34701;&#21512;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31354;&#38388;&#33539;&#22260;&#20869;&#20063;&#33021;&#36827;&#34892;&#20934;&#30830;&#30340;3D&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.03309v3 Announce Type: replace  Abstract: Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive s
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65288;flags&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#21464;&#37327;&#36873;&#25321;&#12289;&#35823;&#24046;&#26368;&#23567;&#21270;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#27530;&#24773;&#20917;&#22788;&#29702;&#65292;&#20026;PCA&#21450;&#20854;&#25193;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#22312;flags&#30340;&#23618;&#32423;&#32467;&#26500;&#20013;&#23398;&#21040;&#20102;&#26032;&#30340;&#32500;&#24230;&#38477;&#20302;&#31639;&#27861;&#12290;&#36825;&#20026;&#30740;&#31350;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#26356;&#22810;&#30340;&#31639;&#27861;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2401.04071</link><description>&lt;p&gt;
Fun with Flags: Robust Principal Directions via Flag Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.04071
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65288;flags&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#21464;&#37327;&#36873;&#25321;&#12289;&#35823;&#24046;&#26368;&#23567;&#21270;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#27530;&#24773;&#20917;&#22788;&#29702;&#65292;&#20026;PCA&#21450;&#20854;&#25193;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#22312;flags&#30340;&#23618;&#32423;&#32467;&#26500;&#20013;&#23398;&#21040;&#20102;&#26032;&#30340;&#32500;&#24230;&#38477;&#20302;&#31639;&#27861;&#12290;&#36825;&#20026;&#30740;&#31350;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#26356;&#22810;&#30340;&#31639;&#27861;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04071v4 Announce Type: replace  Abstract: Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20197;&#20248;&#21270;&#29992;&#20110;&#19977;&#32500;&#24418;&#29366;&#20998;&#26512;&#30340;&#32452;&#22810;&#35270;&#22270; transformer &#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16477</link><description>&lt;p&gt;
Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20197;&#20248;&#21270;&#29992;&#20110;&#19977;&#32500;&#24418;&#29366;&#20998;&#26512;&#30340;&#32452;&#22810;&#35270;&#22270; transformer &#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#19977;&#32500;&#21644;&#20108;&#32500;&#20449;&#24687;&#30340;&#36328;&#25193;&#25955;&#27169;&#22411;&#65288;CrossDiff&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25972;&#21512;3D&#21644;2D&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20849;&#20139;&#30340;Transformer&#32593;&#32476;&#65292;&#32479;&#19968;&#20102;&#36816;&#21160;&#22122;&#22768;&#29305;&#24449;&#31354;&#38388;&#12290;&#36890;&#36807;&#36328;&#25193;&#25955;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;2D&#25110;3D&#22122;&#22768;&#36870;&#21521;&#36716;&#25442;&#22238;&#28165;&#26224;&#30340;&#36816;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#26356;&#32454;&#33147;&#30340;&#20154;&#31867;&#21160;&#20316;&#32454;&#33410;&#12290;&#22240;&#27492;&#65292;CrossDiff&#33021;&#22815;&#22312;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#25972;&#21512;&#20004;&#31181;&#20449;&#24687;&#28304;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2312.10993</link><description>&lt;p&gt;
Realistic Human Motion Generation with Cross-Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10993
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#19977;&#32500;&#21644;&#20108;&#32500;&#20449;&#24687;&#30340;&#36328;&#25193;&#25955;&#27169;&#22411;&#65288;CrossDiff&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25972;&#21512;3D&#21644;2D&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20849;&#20139;&#30340;Transformer&#32593;&#32476;&#65292;&#32479;&#19968;&#20102;&#36816;&#21160;&#22122;&#22768;&#29305;&#24449;&#31354;&#38388;&#12290;&#36890;&#36807;&#36328;&#25193;&#25955;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;2D&#25110;3D&#22122;&#22768;&#36870;&#21521;&#36716;&#25442;&#22238;&#28165;&#26224;&#30340;&#36816;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#26356;&#32454;&#33147;&#30340;&#20154;&#31867;&#21160;&#20316;&#32454;&#33410;&#12290;&#22240;&#27492;&#65292;CrossDiff&#33021;&#22815;&#22312;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#25972;&#21512;&#20004;&#31181;&#20449;&#24687;&#28304;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10993v3 Announce Type: replace  Abstract: We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel approach for generating high-quality human motion based on textual descriptions. Our method integrates 3D and 2D information using a shared transformer network within the training of the diffusion model, unifying motion noise into a single feature space. This enables cross-decoding of features into both 3D and 2D motion representations, regardless of their original dimension. The primary advantage of CrossDiff is its cross-diffusion mechanism, which allows the model to reverse either 2D or 3D noise into clean motion during training. This capability leverages the complementary information in both motion representations, capturing intricate human movement details often missed by models relying solely on 3D information. Consequently, CrossDiff effectively combines the strengths of both representations to generate more realistic motion sequences. In our experiments,
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#32463;&#26631;&#27880;&#30340;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#39062;&#26080;&#30417;&#30563;&#22330;&#26223;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#24110;&#21161;&#33258;&#21160;&#26426;&#22120;&#20154;&#32500;&#25345;&#26410;&#26469;&#31354;&#38388;&#26646;&#24687;&#22320;&#65292;&#22914;&#22826;&#31354;&#31449; Gateway&#12290;&#36890;&#36807;&#20462;&#25913; Expectation-Maximization &#39640;&#26031;&#28151;&#21512;&#27169;&#22411; (GMM) &#32858;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992; Earth Mover's Distance &#24230;&#37327;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#27604;&#20004;&#20010;&#26102;&#31354;&#28857;&#30340;&#21464;&#21270;&#65292;&#20026;&#26410;&#26469;&#30340;&#31354;&#38388;&#33258;&#21160;&#21270;&#30417;&#25511;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.02396</link><description>&lt;p&gt;
Unsupervised Change Detection for Space Habitats Using 3D Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#32463;&#26631;&#27880;&#30340;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#39062;&#26080;&#30417;&#30563;&#22330;&#26223;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#24110;&#21161;&#33258;&#21160;&#26426;&#22120;&#20154;&#32500;&#25345;&#26410;&#26469;&#31354;&#38388;&#26646;&#24687;&#22320;&#65292;&#22914;&#22826;&#31354;&#31449; Gateway&#12290;&#36890;&#36807;&#20462;&#25913; Expectation-Maximization &#39640;&#26031;&#28151;&#21512;&#27169;&#22411; (GMM) &#32858;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992; Earth Mover's Distance &#24230;&#37327;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#27604;&#20004;&#20010;&#26102;&#31354;&#28857;&#30340;&#21464;&#21270;&#65292;&#20026;&#26410;&#26469;&#30340;&#31354;&#38388;&#33258;&#21160;&#21270;&#30417;&#25511;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02396v3 Announce Type: replace  Abstract: This work presents an algorithm for scene change detection from point clouds to enable autonomous robotic caretaking in future space habitats. Autonomous robotic systems will help maintain future deep-space habitats, such as the Gateway space station, which will be uncrewed for extended periods. Existing scene analysis software used on the International Space Station (ISS) relies on manually-labeled images for detecting changes. In contrast, the algorithm presented in this work uses raw, unlabeled point clouds as inputs. The algorithm first applies modified Expectation-Maximization Gaussian Mixture Model (GMM) clustering to two input point clouds. It then performs change detection by comparing the GMMs using the Earth Mover's Distance. The algorithm is validated quantitatively and qualitatively using a test dataset collected by an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth images taken directly by Astro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#23558;&#24320;&#25918;&#19990;&#30028;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#20998;&#31867;&#21644;&#26410;&#23450;&#20041;&#36755;&#20837;&#30340;&#25298;&#25910;&#38382;&#39064;&#34701;&#21512;&#20026;&#19968;&#20010;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;$ K $-&#31867;&#21035;&#38382;&#39064;&#20998;&#35299;&#20026;$ K $&#20010;&#19968;&#23545;&#19968;-&#25152;&#26377;&#65288;OVA&#65289;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#30740;&#31350;&#32773;&#20026;$ K $&#20010;&#24050;&#30693;&#31867;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#22312;&#24050;&#30693;&#31867;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24050;&#30693;&#31867;&#21035;&#19982;&#26410;&#30693;&#31867;&#21035;&#30340;&#39640;&#25928;&#20998;&#36776;&#29575;&#21644;&#20302;&#25298;&#25910;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.13355</link><description>&lt;p&gt;
Unified Classification and Rejection: A One-versus-All Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#23558;&#24320;&#25918;&#19990;&#30028;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#20998;&#31867;&#21644;&#26410;&#23450;&#20041;&#36755;&#20837;&#30340;&#25298;&#25910;&#38382;&#39064;&#34701;&#21512;&#20026;&#19968;&#20010;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;$ K $-&#31867;&#21035;&#38382;&#39064;&#20998;&#35299;&#20026;$ K $&#20010;&#19968;&#23545;&#19968;-&#25152;&#26377;&#65288;OVA&#65289;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#30740;&#31350;&#32773;&#20026;$ K $&#20010;&#24050;&#30693;&#31867;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#22312;&#24050;&#30693;&#31867;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24050;&#30693;&#31867;&#21035;&#19982;&#26410;&#30693;&#31867;&#21035;&#30340;&#39640;&#25928;&#20998;&#36776;&#29575;&#21644;&#20302;&#25298;&#25910;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13355v2 Announce Type: replace  Abstract: Classifying patterns of known classes and rejecting ambiguous and novel (also called as out-of-distribution (OOD)) inputs are involved in open world pattern recognition. Deep neural network models usually excel in closed-set classification while performs poorly in rejecting OOD inputs. To tackle this problem, numerous methods have been designed to perform open set recognition (OSR) or OOD rejection/detection tasks. Previous methods mostly take post-training score transformation or hybrid models to ensure low scores on OOD inputs while separating known classes. In this paper, we attempt to build a unified framework for building open set classifiers for both classification and OOD rejection. We formulate the open set recognition of $ K $-known-class as a $ (K+1) $-class classification problem with model trained on known-class samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all (OVA) binary classification task
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;De-fine&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;benders&#20998;&#35299;&#21644;&#33258;&#21160;&#21453;&#39304;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;&#31243;&#24207;&#30340;&#33258;&#21160;&#20998;&#35299;&#21644; refinement&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12890</link><description>&lt;p&gt;
De-fine: Decomposing and Refining Visual Programs with Auto-Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;De-fine&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;benders&#20998;&#35299;&#21644;&#33258;&#21160;&#21453;&#39304;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;&#31243;&#24207;&#30340;&#33258;&#21160;&#20998;&#35299;&#21644; refinement&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12890v3 Announce Type: replace  Abstract: Visual programming, a modular and generalizable paradigm, integrates different modules and Python operators to solve various vision-language tasks. Unlike end-to-end models that need task-specific data, it advances in performing visual processing and reasoning in an unsupervised manner. Current visual programming methods generate programs in a single pass for each task where the ability to evaluate and optimize based on feedback, unfortunately, is lacking, which consequentially limits their effectiveness for complex, multi-step problems. Drawing inspiration from benders decomposition, we introduce De-fine, a training-free framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback. This model-agnostic approach can improve logical reasoning performance by integrating the strengths of multiple models. Our experiments across various visual tasks show that De-fine creates more ro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Asynchronous Bioplausible Neuron&#65288;ABN&#65289;&#30340;&#26032;&#22411;&#21160;&#24577;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20197;&#24212;&#23545;&#36755;&#20837;&#20449;&#21495;&#30340;&#21464;&#21160;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#24577;&#21644;&#33410;&#33021;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.11853</link><description>&lt;p&gt;
Asynchronous Bioplausible Neuron for SNN for Event Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11853
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Asynchronous Bioplausible Neuron&#65288;ABN&#65289;&#30340;&#26032;&#22411;&#21160;&#24577;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20197;&#24212;&#23545;&#36755;&#20837;&#20449;&#21495;&#30340;&#21464;&#21160;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#24577;&#21644;&#33410;&#33021;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11853v2 Announce Type: replace-cross  Abstract: Spiking Neural Networks (SNNs) offer a biologically inspired approach to computer vision that can lead to more efficient processing of visual data with reduced energy consumption. However, maintaining homeostasis within these networks is challenging, as it requires continuous adjustment of neural responses to preserve equilibrium and optimal processing efficiency amidst diverse and often unpredictable input signals. In response to these challenges, we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing mechanism to auto-adjust the variations in the input signal. Comprehensive evaluation across various datasets demonstrates ABN's enhanced performance in image classification and segmentation, maintenance of neural equilibrium, and energy efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Emu Video&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#22270;&#20687;&#26465;&#20214;&#21270;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20004;&#27493;&#65306;&#39318;&#20808;&#26681;&#25454;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65292;&#28982;&#21518;&#26681;&#25454;&#25991;&#26412;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#35270;&#39057;&#12290;&#25991;&#31456;&#35782;&#21035;&#20986;&#20851;&#38190;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#23545;&#25193;&#25955;&#36807;&#31243;&#30340;&#22122;&#22768;&#35843;&#24230;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;&#20808;&#21069;&#24037;&#20316;&#37027;&#26679;&#20381;&#36182;&#28145;&#23618;&#27169;&#22411; cascades&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#19982;&#25152;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Emu Video&#29983;&#25104;&#30340;&#35270;&#39057;&#22312;&#36136;&#37327;&#19978;&#34987; strongly preferred&#65292;&#20998;&#21035;&#20026; 81% &#23545;&#35895;&#27468;&#30340;Imagen Video&#65292;90% &#23545;&#33521;&#20255;&#36798;&#30340;PYOCO&#65292;&#20197;&#21450; 96% &#23545; Meta&#30340;Make-A-Video&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21830;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;RunwayML&#30340;Gen2&#21644;Pika Labs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#25991;&#26412;&#25552;&#31034;&#21160;&#30011;&#21270;&#22270;&#20687;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#29983;&#25104;&#29289;&#22312;&#29992;&#25143;&#20043;&#38388;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20854;&#20026; 96% &#23545;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2311.10709</link><description>&lt;p&gt;
Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Emu Video&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#22270;&#20687;&#26465;&#20214;&#21270;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20004;&#27493;&#65306;&#39318;&#20808;&#26681;&#25454;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65292;&#28982;&#21518;&#26681;&#25454;&#25991;&#26412;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#35270;&#39057;&#12290;&#25991;&#31456;&#35782;&#21035;&#20986;&#20851;&#38190;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#23545;&#25193;&#25955;&#36807;&#31243;&#30340;&#22122;&#22768;&#35843;&#24230;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;&#20808;&#21069;&#24037;&#20316;&#37027;&#26679;&#20381;&#36182;&#28145;&#23618;&#27169;&#22411; cascades&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#19982;&#25152;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Emu Video&#29983;&#25104;&#30340;&#35270;&#39057;&#22312;&#36136;&#37327;&#19978;&#34987; strongly preferred&#65292;&#20998;&#21035;&#20026; 81% &#23545;&#35895;&#27468;&#30340;Imagen Video&#65292;90% &#23545;&#33521;&#20255;&#36798;&#30340;PYOCO&#65292;&#20197;&#21450; 96% &#23545; Meta&#30340;Make-A-Video&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21830;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;RunwayML&#30340;Gen2&#21644;Pika Labs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#25991;&#26412;&#25552;&#31034;&#21160;&#30011;&#21270;&#22270;&#20687;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#29983;&#25104;&#29289;&#22312;&#29992;&#25143;&#20043;&#38388;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20854;&#20026; 96% &#23545;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10709v2 Announce Type: replace  Abstract: We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21306;&#22495;&#29983;&#38271;&#32593;&#32476;&#65292;&#29992;&#20110;&#22823;&#27668;&#28237;&#27969;&#26465;&#20214;&#19979;&#23545;&#35937;&#30340;&#20998;&#21106;&#65292;&#36890;&#36807;&#26816;&#27979;&#24182;&#29983;&#38271;&#30340;&#26041;&#26696;&#65292;&#39318;&#20808;&#35782;&#21035;&#20986;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#23569;&#37327;&#31227;&#21160;&#23545;&#35937;&#20687;&#32032;&#65292;&#28982;&#21518;&#36880;&#27493;&#20174;&#36825;&#20123;&#31181;&#23376;&#20687;&#32032;&#29983;&#38271;&#21069;&#26223;&#25513;&#27169;&#20197;&#20998;&#21106;&#25152;&#26377;&#31227;&#21160;&#23545;&#35937;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35270;&#39057;&#24103;&#20043;&#38388;&#30340;&#19968;&#33268;&#20960;&#20309;&#20851;&#31995;&#26469; disentangle &#19981;&#21516;&#30340;&#36816;&#21160;&#31867;&#22411;&#65292;&#24182;&#20351;&#29992; Sampson &#36317;&#31163;&#26469;&#21021;&#22987;&#21270;&#31181;&#23376;&#20687;&#32032;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#20351;&#29992;&#31354;&#38388;&#20998;&#32452;&#25439;&#22833;&#21644;&#26102;&#24207;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#32454;&#21270;&#27599;&#24103;&#21069;&#26223;&#25513;&#27169;&#65292;&#20197;&#30830;&#20445;&#20854;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.03572</link><description>&lt;p&gt;
Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03572
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21306;&#22495;&#29983;&#38271;&#32593;&#32476;&#65292;&#29992;&#20110;&#22823;&#27668;&#28237;&#27969;&#26465;&#20214;&#19979;&#23545;&#35937;&#30340;&#20998;&#21106;&#65292;&#36890;&#36807;&#26816;&#27979;&#24182;&#29983;&#38271;&#30340;&#26041;&#26696;&#65292;&#39318;&#20808;&#35782;&#21035;&#20986;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#23569;&#37327;&#31227;&#21160;&#23545;&#35937;&#20687;&#32032;&#65292;&#28982;&#21518;&#36880;&#27493;&#20174;&#36825;&#20123;&#31181;&#23376;&#20687;&#32032;&#29983;&#38271;&#21069;&#26223;&#25513;&#27169;&#20197;&#20998;&#21106;&#25152;&#26377;&#31227;&#21160;&#23545;&#35937;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35270;&#39057;&#24103;&#20043;&#38388;&#30340;&#19968;&#33268;&#20960;&#20309;&#20851;&#31995;&#26469; disentangle &#19981;&#21516;&#30340;&#36816;&#21160;&#31867;&#22411;&#65292;&#24182;&#20351;&#29992; Sampson &#36317;&#31163;&#26469;&#21021;&#22987;&#21270;&#31181;&#23376;&#20687;&#32032;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#20351;&#29992;&#31354;&#38388;&#20998;&#32452;&#25439;&#22833;&#21644;&#26102;&#24207;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#32454;&#21270;&#27599;&#24103;&#21069;&#26223;&#25513;&#27169;&#65292;&#20197;&#30830;&#20445;&#20854;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03572v2 Announce Type: replace  Abstract: Moving object segmentation in the presence of atmospheric turbulence is highly challenging due to turbulence-induced irregular and time-varying distortions. In this paper, we present an unsupervised approach for segmenting moving objects in videos downgraded by atmospheric turbulence. Our key approach is a detect-then-grow scheme: we first identify a small set of moving object pixels with high confidence, then gradually grow a foreground mask from those seeds to segment all moving objects. This method leverages rigid geometric consistency among video frames to disentangle different types of motions, and then uses the Sampson distance to initialize the seedling pixels. After growing per-frame foreground masks, we use spatial grouping loss and temporal consistency loss to further refine the masks in order to ensure their spatio-temporal consistency. Our method is unsupervised and does not require training on labeled data. For validatio
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#20869;&#23481;&#22635;&#20805;&#26041;&#27861;"Infusion"&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20869;&#37096;&#25193;&#25955;&#25216;&#26415;&#26469;&#22788;&#29702;&#21160;&#24577;&#32441;&#29702;&#21644;&#22797;&#26434;&#36816;&#21160;&#65292;&#36890;&#36807;&#38480;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20165;&#20351;&#29992;&#36755;&#20837;&#35270;&#39057;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#39057;&#20869;&#23481;&#22635;&#20805;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2311.01090</link><description>&lt;p&gt;
Infusion: internal diffusion for inpainting of dynamic textures and complex motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.01090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#20869;&#23481;&#22635;&#20805;&#26041;&#27861;"Infusion"&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20869;&#37096;&#25193;&#25955;&#25216;&#26415;&#26469;&#22788;&#29702;&#21160;&#24577;&#32441;&#29702;&#21644;&#22797;&#26434;&#36816;&#21160;&#65292;&#36890;&#36807;&#38480;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20165;&#20351;&#29992;&#36755;&#20837;&#35270;&#39057;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#39057;&#20869;&#23481;&#22635;&#20805;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.01090v2 Announce Type: replace  Abstract: Video inpainting is the task of filling a region in a video in a visually convincing manner. It is very challenging due to the high dimensionality of the data and the temporal consistency required for obtaining convincing results. Recently, diffusion models have shown impressive results in modeling complex data distributions, including images and videos. Such models remain nonetheless very expensive to train and to perform inference with, which strongly reduce their applicability to videos, and yields unreasonable computational loads. We show that in the case of video inpainting, thanks to the highly auto-similar nature of videos, the training data of a diffusion model can be restricted to the input video and still produce very satisfying results. This leads us to adopt an internal learning approach, which also allows us to greatly reduce the neural network size by about three orders of magnitude less than current diffusion models us
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#25506;&#31350;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35757;&#32451;&#20013;&#24930;&#25910;&#25947;&#21644;&#37319;&#26679;&#20013;&#33394;&#24425;&#20559;&#24046;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#20174;&#32780;&#25512;&#36827;&#20102;&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2310.08442</link><description>&lt;p&gt;
Unmasking Bias in Diffusion Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08442
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#25506;&#31350;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35757;&#32451;&#20013;&#24930;&#25910;&#25947;&#21644;&#37319;&#26679;&#20013;&#33394;&#24425;&#20559;&#24046;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#20174;&#32780;&#25512;&#36827;&#20102;&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08442v3 Announce Type: replace  Abstract: Denoising diffusion models have emerged as a dominant approach for image generation, however they still suffer from slow convergence in training and color shift issues in sampling. In this paper, we identify that these obstacles can be largely attributed to bias and suboptimality inherent in the default training paradigm of diffusion models. Specifically, we offer theoretical insights that the prevailing constant loss weight strategy in $\epsilon$-prediction of diffusion models leads to biased estimation during the training phase, hindering accurate estimations of original images. To address the issue, we propose a simple but effective weighting strategy derived from the unlocked biased part. Furthermore, we conduct a comprehensive and systematic exploration, unraveling the inherent bias problem in terms of its existence, impact and underlying reasons. These analyses contribute to advancing the understanding of diffusion models. Empi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20102;&#35299;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#26597;&#35810;&#19979;&#30340;&#34892;&#20026;&#20559;&#24046;&#21644;&#28508;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2309.01446</link><description>&lt;p&gt;
Open Sesame! Universal Black Box Jailbreaking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20102;&#35299;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#26597;&#35810;&#19979;&#30340;&#34892;&#20026;&#20559;&#24046;&#21644;&#28508;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01446v4 Announce Type: replace-cross  Abstract: Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#22270;&#20687;&#34701;&#21512;&#21644;&#26333;&#20809;&#26657;&#27491;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#29575;&#22495;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#39057;&#29575;&#27880;&#24847;&#21147;&#21644;&#21160;&#24577;&#39057;&#29575;&#21069;&#21521;&#32593;&#32476;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#31354;&#38388;&#22495;&#30456;&#20851;&#35745;&#31639;&#24471;&#21040;&#20102;&#25913;&#21892;&#21644;&#26367;&#20195;&#65292;&#20026;&#22270;&#20687;&#22788;&#29702;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#22788;&#29702;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2309.01183</link><description>&lt;p&gt;
Holistic Dynamic Frequency Transformer for Image Fusion and Exposure Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#22270;&#20687;&#34701;&#21512;&#21644;&#26333;&#20809;&#26657;&#27491;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#29575;&#22495;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#39057;&#29575;&#27880;&#24847;&#21147;&#21644;&#21160;&#24577;&#39057;&#29575;&#21069;&#21521;&#32593;&#32476;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#31354;&#38388;&#22495;&#30456;&#20851;&#35745;&#31639;&#24471;&#21040;&#20102;&#25913;&#21892;&#21644;&#26367;&#20195;&#65292;&#20026;&#22270;&#20687;&#22788;&#29702;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01183v2 Announce Type: replace  Abstract: The correction of exposure-related issues is a pivotal component in enhancing the quality of images, offering substantial implications for various computer vision tasks. Historically, most methodologies have predominantly utilized spatial domain recovery, offering limited consideration to the potentialities of the frequency domain. Additionally, there has been a lack of a unified perspective towards low-light enhancement, exposure correction, and multi-exposure fusion, complicating and impeding the optimization of image processing. In response to these challenges, this paper proposes a novel methodology that leverages the frequency domain to improve and unify the handling of exposure correction tasks. Our method introduces Holistic Frequency Attention and Dynamic Frequency Feed-Forward Network, which replace conventional correlation computation in the spatial-domain. They form a foundational building block that facilitates a U-shaped
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#26041;&#27861;&#30340;&#38750;&#30452;&#32447;&#35270;&#32447;&#65288;NLOS&#65289;&#25104;&#20687;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#20174;&#38544;&#34255;&#31354;&#38388;&#20013;&#37319;&#26679;&#28857;&#24182;&#22312;&#21487;&#35265;&#34920;&#38754;&#29305;&#24449;&#26174;&#33879;&#20943;&#23569;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#37325;&#24314;&#26102;&#38388;&#12290;&#36890;&#36807;&#35774;&#35745;&#29305;&#27530;&#30340;&#31574;&#30053;&#26469;&#25130;&#26029;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#27169;&#25311;&#35270;&#32447;&#20381;&#36182;&#30340;&#21453;&#23556;&#29575;&#65292;&#24182;&#20351;&#29992;&#27861;&#32447;&#26469;&#37325;&#24314;&#34920;&#38754;&#20960;&#20309;&#21644;&#30828;&#24230;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22495;&#20943;&#23569;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#22312;&#37325;&#24314;&#36807;&#31243;&#20013;&#36890;&#36807;&#36880;&#28176;&#32454;&#21270;&#28040;&#38500;&#31354;&#27934;&#21306;&#22495;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2308.10269</link><description>&lt;p&gt;
Domain Reduction Strategy for Non Line of Sight Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#26041;&#27861;&#30340;&#38750;&#30452;&#32447;&#35270;&#32447;&#65288;NLOS&#65289;&#25104;&#20687;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#20174;&#38544;&#34255;&#31354;&#38388;&#20013;&#37319;&#26679;&#28857;&#24182;&#22312;&#21487;&#35265;&#34920;&#38754;&#29305;&#24449;&#26174;&#33879;&#20943;&#23569;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#37325;&#24314;&#26102;&#38388;&#12290;&#36890;&#36807;&#35774;&#35745;&#29305;&#27530;&#30340;&#31574;&#30053;&#26469;&#25130;&#26029;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#27169;&#25311;&#35270;&#32447;&#20381;&#36182;&#30340;&#21453;&#23556;&#29575;&#65292;&#24182;&#20351;&#29992;&#27861;&#32447;&#26469;&#37325;&#24314;&#34920;&#38754;&#20960;&#20309;&#21644;&#30828;&#24230;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22495;&#20943;&#23569;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#22312;&#37325;&#24314;&#36807;&#31243;&#20013;&#36890;&#36807;&#36880;&#28176;&#32454;&#21270;&#28040;&#38500;&#31354;&#27934;&#21306;&#22495;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10269v2 Announce Type: replace  Abstract: This paper presents a novel optimization-based method for non-line-of-sight (NLOS) imaging that aims to reconstruct hidden scenes under general setups with significantly reduced reconstruction time. In NLOS imaging, the visible surfaces of the target objects are notably sparse. To mitigate unnecessary computations arising from empty regions, we design our method to render the transients through partial propagations from a continuously sampled set of points from the hidden space. Our method is capable of accurately and efficiently modeling the view-dependent reflectance using surface normals, which enables us to obtain surface geometry as well as albedo. In this pipeline, we propose a novel domain reduction strategy to eliminate superfluous computations in empty regions. During the optimization process, our domain reduction procedure periodically prunes the empty regions from our sampling domain in a coarse-to-fine manner, leading to 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#25506;&#35752;&#20102;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#22312;&#35782;&#21035;&#19981;&#21516;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#22270;&#20687;&#26102;&#30340;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#23616;&#38480;&#24615;&#21450;&#25552;&#21319;&#20854;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#26816;&#27979;&#22120;&#30340;&#25913;&#36827;&#21644;&#26356;&#26032;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25968;&#25454;&#25903;&#25345;&#21644;&#29702;&#35770;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2308.04177</link><description>&lt;p&gt;
How Generalizable are Deepfake Image Detectors? An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.04177
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#25506;&#35752;&#20102;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#22312;&#35782;&#21035;&#19981;&#21516;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#22270;&#20687;&#26102;&#30340;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#23616;&#38480;&#24615;&#21450;&#25552;&#21319;&#20854;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#26816;&#27979;&#22120;&#30340;&#25913;&#36827;&#21644;&#26356;&#26032;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25968;&#25454;&#25903;&#25345;&#21644;&#29702;&#35770;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.04177v2 Announce Type: replace  Abstract: Deepfakes are becoming increasingly credible, posing a significant threat given their potential to facilitate fraud or bypass access control systems. This has motivated the development of deepfake detection methods, in which deep learning models are trained to distinguish between real and synthesized footage. Unfortunately, existing detectors struggle to generalize to deepfakes from datasets they were not trained on, but little work has been done to examine why or how this limitation can be addressed. Especially, those single-modality deepfake images reveal little available forgery evidence, posing greater challenges than detecting deepfake videos. In this work, we present the first empirical study on the generalizability of deepfake detectors, an essential goal for detectors to stay one step ahead of attackers. Our study utilizes six deepfake datasets, five deepfake image detection methods, and two model augmentation approaches, con
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FreeDrag&#30340;&#22522;&#20110;&#29305;&#24449;&#25302;&#25341;&#30340;&#21487;&#38752;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26356;&#26032;&#27169;&#26495;&#29305;&#24449;&#21644;&#24102;&#26377;&#21453;&#21521;&#36319;&#36394;&#30340;&#32447;&#25628;&#32034;&#31639;&#27861;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#28857;&#36319;&#36394;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#20869;&#23481;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#25805;&#20316;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag: Feature Dragging for Reliable Point-based Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FreeDrag&#30340;&#22522;&#20110;&#29305;&#24449;&#25302;&#25341;&#30340;&#21487;&#38752;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26356;&#26032;&#27169;&#26495;&#29305;&#24449;&#21644;&#24102;&#26377;&#21453;&#21521;&#36319;&#36394;&#30340;&#32447;&#25628;&#32034;&#31639;&#27861;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#28857;&#36319;&#36394;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#20869;&#23481;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#25805;&#20316;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.04684v4 Announce Type: replace  Abstract: To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely "miss tracking", where difficulties arise in accurately tracking the predetermined handle points, and "ambiguous tracking", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter allev
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPrompt&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24378;&#21046;&#35757;&#32451;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26679;&#26412;&#30340;&#20004;&#20010;&#38543;&#26426;&#21464;&#20307;&#19978;&#24378;&#21046;&#19968;&#33268;&#24615;&#20197;&#21450;&#32467;&#21512;&#20102;&#27169;&#22411;&#24494;&#35843;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#20004;&#31181;&#25216;&#26415;&#65292;CoPrompt&#25552;&#21319;&#20102;&#24615;&#33021;&#24182;&#25552;&#39640;&#20102;&#35843;&#21442;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2306.01195</link><description>&lt;p&gt;
Consistency-guided Prompt Learning for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPrompt&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24378;&#21046;&#35757;&#32451;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26679;&#26412;&#30340;&#20004;&#20010;&#38543;&#26426;&#21464;&#20307;&#19978;&#24378;&#21046;&#19968;&#33268;&#24615;&#20197;&#21450;&#32467;&#21512;&#20102;&#27169;&#22411;&#24494;&#35843;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#20004;&#31181;&#25216;&#26415;&#65292;CoPrompt&#25552;&#21319;&#20102;&#24615;&#33021;&#24182;&#25552;&#39640;&#20102;&#35843;&#21442;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01195v4 Announce Type: replace  Abstract: We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and outp
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Graph Transformer&#30340;&#26032;&#29305;&#24449;&#25552;&#21462;&#22359;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GTNet&#30340;&#32593;&#32476;&#65292;&#20197;&#22312;&#26412;&#22320;&#21644;&#20840;&#29699;&#23618;&#38754;&#23398;&#20064;&#28857;&#20113;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#38745;&#24577;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#32593;&#32476;&#22312;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#26102;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2305.15213</link><description>&lt;p&gt;
GTNet: Graph Transformer Network for 3D Point Cloud Classification and Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Graph Transformer&#30340;&#26032;&#29305;&#24449;&#25552;&#21462;&#22359;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GTNet&#30340;&#32593;&#32476;&#65292;&#20197;&#22312;&#26412;&#22320;&#21644;&#20840;&#29699;&#23618;&#38754;&#23398;&#20064;&#28857;&#20113;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#38745;&#24577;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#32593;&#32476;&#22312;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#26102;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15213v3 Announce Type: replace  Abstract: Recently, graph-based and Transformer-based deep learning networks have demonstrated excellent performances on various point cloud tasks. Most of the existing graph methods are based on static graph, which take a fixed input to establish graph relations. Moreover, many graph methods apply maximization and averaging to aggregate neighboring features, so that only a single neighboring point affects the feature of centroid or different neighboring points have the same influence on the centroid's feature, which ignoring the correlation and difference between points. Most Transformer-based methods extract point cloud features based on global attention and lack the feature learning on local neighbors. To solve the problems of these two types of models, we propose a new feature extraction block named Graph Transformer and construct a 3D point point cloud learning network called GTNet to learn features of point clouds on local and global pat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;'Avatar Fingerprinting'&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#21512;&#25104;&#23454;&#26102;&#23545;&#35805;&#22836;&#20687;&#35270;&#39057;&#30340;&#23433;&#20840;&#24615;&#21644;&#29256;&#26435;&#65292;&#36890;&#36807;&#23398;&#20064;&#29420;&#31435;&#20110;&#38754;&#37096;&#22806;&#35266;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#35270;&#39057;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#24230;&#12290;</title><link>https://arxiv.org/abs/2305.03713</link><description>&lt;p&gt;
Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.03713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;'Avatar Fingerprinting'&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#21512;&#25104;&#23454;&#26102;&#23545;&#35805;&#22836;&#20687;&#35270;&#39057;&#30340;&#23433;&#20840;&#24615;&#21644;&#29256;&#26435;&#65292;&#36890;&#36807;&#23398;&#20064;&#29420;&#31435;&#20110;&#38754;&#37096;&#22806;&#35266;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#35270;&#39057;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.03713v3 Announce Type: replace  Abstract: Modern avatar generators allow anyone to synthesize photorealistic real-time talking avatars, ushering in a new era of avatar-based human communication, such as with immersive AR/VR interactions or videoconferencing with limited bandwidths. Their safe adoption, however, requires a mechanism to verify if the rendered avatar is trustworthy: does it use the appearance of an individual without their consent? We term this task avatar fingerprinting. To tackle it, we first introduce a large-scale dataset of real and synthetic videos of people interacting on a video call, where the synthetic videos are generated using the facial appearance of one person and the expressions of another. We verify the identity driving the expressions in a synthetic video, by learning motion signatures that are independent of the facial appearance shown. Our solution, the first in this space, achieves an average AUC of 0.85. Critical to its practical use, it al
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#22312;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24635;&#32467;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#22270;&#20687;&#32534;&#36753;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#23637;&#26395;&#12290;</title><link>https://arxiv.org/abs/2304.09854</link><description>&lt;p&gt;
Transformer-Based Visual Segmentation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.09854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#22312;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24635;&#32467;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#22270;&#20687;&#32534;&#36753;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.09854v4 Announce Type: replace  Abstract: Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-archit
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#23618;&#20043;&#38388;&#21152;&#24378;&#25552;&#31034;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#22686;&#24378;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#24615;&#29305;&#24449;&#37325;&#26500;&#26041;&#27861;&#65292;&#26088;&#22312;&#38450;&#27490;&#35270;&#35273;&#25552;&#31034;&#29305;&#24449;&#19982;&#22266;&#23450;CLIP&#35270;&#35273;&#29305;&#24449;&#20998;&#24067;&#30340;&#20005;&#37325;&#20559;&#31163;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2304.08386</link><description>&lt;p&gt;
Progressive Visual Prompt Learning with Contrastive Feature Re-formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08386
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#23618;&#20043;&#38388;&#21152;&#24378;&#25552;&#31034;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#22686;&#24378;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#24615;&#29305;&#24449;&#37325;&#26500;&#26041;&#27861;&#65292;&#26088;&#22312;&#38450;&#27490;&#35270;&#35273;&#25552;&#31034;&#29305;&#24449;&#19982;&#22266;&#23450;CLIP&#35270;&#35273;&#29305;&#24449;&#20998;&#24067;&#30340;&#20005;&#37325;&#20559;&#31163;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08386v3 Announce Type: replace  Abstract: Prompt learning has been designed as an alternative to fine-tuning for adapting Vision-language (V-L) models to the downstream tasks. Previous works mainly focus on text prompt while visual prompt works are limited for V-L models. The existing visual prompt methods endure either mediocre performance or unstable training process, indicating the difficulty of visual prompt learning. In this paper, we propose a new Progressive Visual Prompt (ProVP) structure to strengthen the interactions among prompts of different layers. More importantly, our ProVP could effectively propagate the image embeddings to deep layers and behave partially similar to an instance adaptive prompt method. To alleviate generalization deterioration, we further propose a new contrastive feature re-formation, which prevents the serious deviation of the prompted visual feature from the fixed CLIP visual feature distribution. Combining both, our method (ProVP-Ref) is 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIP4MC&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#23545;&#24378;&#21270;&#23398;&#20064;&#21451;&#22909;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24320;&#25918;&#24335;&#20219;&#21153;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;VLM&#35757;&#32451;&#30446;&#26631;&#20013;&#34701;&#20837;&#20219;&#21153;&#23436;&#25104;&#24230;&#30340;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#20195;&#29702;&#21306;&#20998;&#19981;&#21516;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;RL&#30340;&#21451;&#22909;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24178;&#20928;&#30340;YouTube&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22522;&#20110;&#35821;&#35328;&#25351;&#20196;&#30340;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.10571</link><description>&lt;p&gt;
Reinforcement Learning Friendly Vision-Language Model for Minecraft
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.10571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIP4MC&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#23545;&#24378;&#21270;&#23398;&#20064;&#21451;&#22909;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24320;&#25918;&#24335;&#20219;&#21153;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;VLM&#35757;&#32451;&#30446;&#26631;&#20013;&#34701;&#20837;&#20219;&#21153;&#23436;&#25104;&#24230;&#30340;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#20195;&#29702;&#21306;&#20998;&#19981;&#21516;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;RL&#30340;&#21451;&#22909;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24178;&#20928;&#30340;YouTube&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22522;&#20110;&#35821;&#35328;&#25351;&#20196;&#30340;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.10571v2 Announce Type: replace-cross  Abstract: One of the essential missions in the AI research community is to build an autonomous embodied agent that can achieve high-level performance across a wide spectrum of tasks. However, acquiring or manually designing rewards for all open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-ended tasks. Simply utilizing the similarity between the video snippet and the language prompt is not RL-friendly since standard VLMs may only capture the similarity at a coarse level. To achieve RL-friendliness, we incorporate the task completion degree into the VLM training objective, as this information can assist agents in distinguishing the importance between different states. Moreover, we provide neat YouTube datasets based on the l
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25512;&#33616;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#30340;&#24605;&#32771;&#65292;&#24182;&#24378;&#35843;&#36866;&#24212;&#24615;&#21644;&#30693;&#35782;&#36716;&#31227;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#20316;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#20063;&#33021;&#36890;&#36807;&#31616;&#21333;&#30340;&#21407;&#22411;&#29305;&#24449;&#26041;&#27861;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#21462;&#24471;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25991;&#31456;&#20316;&#32773;&#35748;&#20026;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36825;&#31181;&#36890;&#29992;&#21270;&#21644;&#36866;&#37197;&#24615;&#65292;&#30740;&#31350;&#32773;&#21487;&#20197;&#36991;&#20813;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#25773;&#21644;&#20219;&#21153;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.07338</link><description>&lt;p&gt;
Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.07338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25512;&#33616;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#30340;&#24605;&#32771;&#65292;&#24182;&#24378;&#35843;&#36866;&#24212;&#24615;&#21644;&#30693;&#35782;&#36716;&#31227;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#20316;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#20063;&#33021;&#36890;&#36807;&#31616;&#21333;&#30340;&#21407;&#22411;&#29305;&#24449;&#26041;&#27861;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#21462;&#24471;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25991;&#31456;&#20316;&#32773;&#35748;&#20026;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36825;&#31181;&#36890;&#29992;&#21270;&#21644;&#36866;&#37197;&#24615;&#65292;&#30740;&#31350;&#32773;&#21487;&#20197;&#36991;&#20813;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#25773;&#21644;&#20219;&#21153;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APARATE&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#34917;&#19969;&#26469;&#25913;&#36827;CNN&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#20013;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#20854;&#22312;&#38754;&#20020;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#33021;&#22815;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.01351</link><description>&lt;p&gt;
APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APARATE&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#34917;&#19969;&#26469;&#25913;&#36827;CNN&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#20013;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#20854;&#22312;&#38754;&#20020;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#33021;&#22815;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01351v3 Announce Type: replace-cross  Abstract: In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;MUlti-modal Generator&#65288;MUG&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;Web&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#30340;&#35299;&#37322;&#65292;&#20026;&#35774;&#35745;&#26032;&#30340;&#35270;&#35273;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;MUlti-modal Generator&#65288;MUG&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;Web&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#30340;&#35299;&#37322;&#65292;&#20026;&#35774;&#35745;&#26032;&#30340;&#35270;&#35273;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#20351;&#29992;&#20840;&#23616;&#24418;&#29366;&#25551;&#36848;&#31526;&#23545;&#29289;&#20307;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;&#65292;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#20004;&#31867;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#29305;&#23450;&#31867;&#21035;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#34892;&#20026;&#35777;&#25454;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22312;&#35821;&#20041;&#31867;&#21035;&#32570;&#38519;&#20013;&#65292;&#24739;&#32773;&#22312;&#20570;&#20986;&#24191;&#27867;&#30340;&#20998;&#31867;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#35760;&#20303;&#35814;&#32454;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#23545;&#20110;&#27010;&#24565;&#30340;&#19968;&#33324;&#20449;&#24687;&#23384;&#20648;&#26159;&#26356;&#19981;&#26131;&#21463;&#21040;&#35821;&#20041;&#35760;&#24518;&#25439;&#23475;&#24433;&#21709;&#30340;&#12290;&#25991;&#31456;&#36824;&#23637;&#31034;&#20102;&#24739;&#32773;&#22312;&#23376;&#31867;&#21035;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#20007;&#22833;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#31456;&#21033;&#29992;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#19968;&#20010;&#39069;&#22806;&#38454;&#27573;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#30340;&#35780;&#20215;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#35273;&#39046;&#22495;&#26377;&#25928;&#22320;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/1901.11398</link><description>&lt;p&gt;
A study on general visual categorization of objects into animal and plant groups using global shape descriptors with a focus on category-specific deficits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1901.11398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#20351;&#29992;&#20840;&#23616;&#24418;&#29366;&#25551;&#36848;&#31526;&#23545;&#29289;&#20307;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;&#65292;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#20004;&#31867;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#29305;&#23450;&#31867;&#21035;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#34892;&#20026;&#35777;&#25454;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22312;&#35821;&#20041;&#31867;&#21035;&#32570;&#38519;&#20013;&#65292;&#24739;&#32773;&#22312;&#20570;&#20986;&#24191;&#27867;&#30340;&#20998;&#31867;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#35760;&#20303;&#35814;&#32454;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#23545;&#20110;&#27010;&#24565;&#30340;&#19968;&#33324;&#20449;&#24687;&#23384;&#20648;&#26159;&#26356;&#19981;&#26131;&#21463;&#21040;&#35821;&#20041;&#35760;&#24518;&#25439;&#23475;&#24433;&#21709;&#30340;&#12290;&#25991;&#31456;&#36824;&#23637;&#31034;&#20102;&#24739;&#32773;&#22312;&#23376;&#31867;&#21035;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#20007;&#22833;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#31456;&#21033;&#29992;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#19968;&#20010;&#39069;&#22806;&#38454;&#27573;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#30340;&#35780;&#20215;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#35273;&#39046;&#22495;&#26377;&#25928;&#22320;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1901.11398v3 Announce Type: replace  Abstract: How do humans distinguish between general categories of objects? In a number of semantic category deficits, patients are good at making broad categorization but are unable to remember fine and specific details. It has been well accepted that general information about concepts is more robust to damages related to semantic memory. Results from patients with semantic memory disorders demonstrate the loss of ability in subcategory recognition. In this paper, we review the behavioral evidence for category specific disorder and show that general categories of animal and plant are visually distinguishable without processing textural information. To this aim, we utilize shape descriptors with an additional phase of feature learning. The results are evaluated with both supervised and unsupervised learning mechanisms and confirm that the proposed method can effectively discriminates between animal and plant object categories in visual domain.
&lt;/p&gt;</description></item></channel></rss>