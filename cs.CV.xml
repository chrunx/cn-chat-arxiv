<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>"&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32570;&#20047;&#33258;&#36866;&#24212;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#27807;&#36890;&#25928;&#29575;&#25552;&#39640;&#30340;&#20851;&#38190;&#12290;"</title><link>https://arxiv.org/abs/2408.01417</link><description>&lt;p&gt;
"&#23569;&#35828;&#35805;&#65292;&#22810;&#20114;&#21160;&#65306;&#22312;&#22810;&#27169;&#24577;LLM&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#23545;&#35805;&#36866;&#24212;&#24615;"
&lt;/p&gt;
&lt;p&gt;
Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01417
&lt;/p&gt;
&lt;p&gt;
"&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32570;&#20047;&#33258;&#36866;&#24212;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#27807;&#36890;&#25928;&#29575;&#25552;&#39640;&#30340;&#20851;&#38190;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#33258;&#21457;&#22320;&#20351;&#29992;&#36234;&#26469;&#36234;&#39640;&#25928;&#30340;&#35328;&#35821;&#65292;&#36890;&#36807;&#35843;&#25972;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#12290;&#36825;&#31181;&#29616;&#35937;&#24050;&#32463;&#22312;&#21442;&#32771;&#28216;&#25103;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#35821;&#35328;&#30340;&#19968;&#20123;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#36229;&#20986;&#20102;&#20256;&#36798;&#24847;&#22270;&#30340;&#33539;&#22260;&#12290;&#33267;&#20170;&#23578;&#26410;&#25506;&#35752;&#30340;&#26159;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#26159;&#21542;&#20250;&#20687;&#20154;&#31867;&#19968;&#26679;&#22312;&#20114;&#21160;&#20013;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#37319;&#29992;&#21738;&#20123;&#26426;&#21046;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;ICCA&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#20114;&#21160;&#20013;&#30340;&#23545;&#35805;&#36866;&#24212;&#24615;&#20316;&#20026;&#19968;&#31181;&#20869;&#22312;&#34892;&#20026;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#29702;&#35299;&#20182;&#20204;&#30340;&#23545;&#35805;&#32773;&#30340;&#35328;&#35821;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#25928;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20687;&#20154;&#31867;&#37027;&#26679;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#33258;&#21457;&#22320;&#20351;&#33258;&#24049;&#30340;&#35821;&#35328;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#21518;&#32773;&#33021;&#21147;&#21482;&#33021;&#22312;&#26576;&#20123;&#27169;&#22411;(&#20363;&#22914;GPT-4)&#20013;&#36890;&#36807;&#34987;&#21160;&#30340;&#25552;&#31034;&#26041;&#24335;&#28608;&#21457;&#20986;&#26469;&#12290;&#36825;&#34920;&#26126;&#20102;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#29305;&#24615;&#36824;&#27809;&#26377;&#23436;&#20840;&#34987;&#24403;&#21069;&#30340;&#27169;&#22411;&#25152;&#25484;&#25569;&#12290;"
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01417v1 Announce Type: cross  Abstract: Humans spontaneously use increasingly efficient language as interactions progress, by adapting and forming ad-hoc conventions. This phenomenon has been studied extensively using reference games, showing properties of human language that go beyond relaying intents. It remains unexplored whether multimodal large language models (MLLMs) similarly increase communication efficiency during interactions, and what mechanisms they may adopt for this purpose. We introduce ICCA, an automated framework to evaluate such conversational adaptation as an in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and observe that while they may understand the increasingly efficient language of their interlocutor, they do not spontaneously make their own language more efficient over time. This latter ability can only be elicited in some models (e.g., GPT-4) with heavy-handed prompting. This shows that this property of linguistic interacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#25805;&#20316;&#30340;&#31354;&#38388;&#20809;&#35889;&#24418;&#24577;&#40151;&#40060;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24182;&#22686;&#24378;&#31354;&#38388;&#20809;&#35889; token&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01372</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#20809;&#35889;&#24418;&#24577;&#23398;&#30340;&#40151;&#40060;&#27169;&#22411;&#29992;&#20110; hyperspectral &#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#25805;&#20316;&#30340;&#31354;&#38388;&#20809;&#35889;&#24418;&#24577;&#40151;&#40060;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24182;&#22686;&#24378;&#31354;&#38388;&#20809;&#35889; token&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;Transformer &#22312;&#36229;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#65288;HSIC&#65289;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#24179;&#26041;&#22686;&#21152;&#12290;Mamba &#26550;&#26500;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;Transformer&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#8212;&#8212;&#31354;&#38388;&#20809;&#35889;&#24418;&#24577;&#40151;&#40060;&#65288;MorpMamba&#65289;&#27169;&#22411;&#12290;&#22312;MorpMamba&#27169;&#22411;&#20013;&#65292;&#19968;&#20010; token &#29983;&#25104;&#27169;&#22359;&#39318;&#20808;&#23558;&#36229;&#20809;&#35889;&#22270;&#20687;&#65288;HSI&#65289;&#30340;&#34917;&#19969;&#36716;&#25442;&#20026;&#31354;&#38388;&#20809;&#35889; token&#12290;&#36825;&#20123; token &#38543;&#21518;&#30001;&#19968;&#20010;&#24418;&#24577;&#23398;&#22359;&#22788;&#29702;&#65292;&#24418;&#24577;&#23398;&#22359;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#25805;&#20316;&#35745;&#31639;&#32467;&#26500;&#20449;&#24687;&#21644;&#24418;&#29366;&#20449;&#24687;&#12290;&#19968;&#20010;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#36827;&#19968;&#27493;&#22686;&#24378;&#25552;&#21462;&#30340;&#20449;&#24687;&#65292;&#35813;&#27169;&#22359;&#26681;&#25454;&#35745;&#31639;&#30340;&#31354;&#38388;&#21644;&#24179;&#34892;&#36890;&#36947;&#30340; token &#35843;&#25972;&#23427;&#20204;&#30340;&#31354;&#38388;&#21644;&#24179;&#34892;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01372v1 Announce Type: new  Abstract: In recent years, Transformers have garnered significant attention for Hyperspectral Image Classification (HSIC) due to their self-attention mechanism, which provides strong classification performance. However, these models face major challenges in computational efficiency, as their complexity increases quadratically with the sequence length. The Mamba architecture, leveraging a State Space Model, offers a more efficient alternative to Transformers. This paper introduces the Spatial-Spectral Morphological Mamba (MorpMamba) model. In the MorpMamba model, a token generation module first converts the Hyperspectral Image (HSI) patch into spatial-spectral tokens. These tokens are then processed by a morphology block, which computes structural and shape information using depthwise separable convolutional operations. The extracted information is enhanced in a feature enhancement module that adjusts the spatial and spectral tokens based on the ce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20107;&#20214;&#21644;&#24815;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31383;&#21475;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21160;&#24577;&#21644;&#29031;&#26126;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#31934;&#30830;&#20107;&#20214;&#30456;&#26426;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#65292;&#22686;&#24378;&#20102;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01370</link><description>&lt;p&gt;
EVIT: &#22522;&#20110;&#20107;&#20214;&#30340;&#20107;&#20214;-&#24815;&#24615;&#21322;&#23494;&#38598;&#22320;&#22270;&#36319;&#36394;&#26041;&#27861;&#20351;&#29992;&#31383;&#38480;&#38750;&#32447;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20107;&#20214;&#21644;&#24815;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31383;&#21475;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21160;&#24577;&#21644;&#29031;&#26126;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#31934;&#30830;&#20107;&#20214;&#30456;&#26426;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#65292;&#22686;&#24378;&#20102;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01370v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20107;&#20214;&#25668;&#20687;&#22836;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#35270;&#35273;&#22806;&#24863;&#23448;&#22120;&#65292;&#23427;&#23545;&#20142;&#24230;&#21464;&#21270;&#20570;&#20986;&#21453;&#24212;&#65292;&#32780;&#19981;&#26159;&#25972;&#21512;&#32477;&#23545;&#22270;&#20687;&#24378;&#24230;&#12290;&#30001;&#20110;&#36825;&#31181;&#35774;&#35745;&#65292;&#20256;&#24863;&#22120;&#22312;&#21160;&#24577;&#21644;&#29031;&#26126;&#26465;&#20214;&#24694;&#21155;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20107;&#20214;&#39537;&#21160;&#30340;&#21516;&#27493;&#36319;&#36394;&#21644;&#26144;&#23556;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24050;&#32463;&#25351;&#20986;&#65292;&#20256;&#24863;&#22120;&#22312;&#22522;&#20110;&#20808;&#39564;&#21019;&#24314;&#30340;&#20256;&#32479;&#20256;&#24863;&#22120;&#22320;&#22270;&#30340;&#36319;&#36394;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#27880;&#20876;&#33539;&#24335;&#65292;&#30456;&#26426;&#30340;&#22823;&#33539;&#22260;&#29031;&#26126;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#33258;&#25105;&#36816;&#21160;&#21487;&#20197;&#22312;&#20934;&#30830;&#21019;&#24314;&#30340;&#20808;&#39564;&#22320;&#22270;&#19978;&#24471;&#21040;&#36319;&#36394;&#12290;&#26412;&#25991;&#24310;&#32493;&#20102;&#26368;&#36817;&#20171;&#32461;&#30340;&#20107;&#20214;&#39537;&#21160;&#20960;&#20309;&#21322;&#23494;&#38598;&#36319;&#36394;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#28155;&#21152;&#24815;&#24615;&#20449;&#21495;&#20197;&#22686;&#24378;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#28155;&#21152;&#30340;&#20449;&#21495;&#25552;&#20379;&#20102;&#26377;&#20851;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#36825;&#26679;&#21487;&#20197;&#22686;&#24378;&#23545;&#20107;&#20214;&#30456;&#26426;&#30340;&#31934;&#30830;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#20102;&#20107;&#20214;&#21644;&#24815;&#24615;&#27979;&#37327;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01370v1 Announce Type: cross  Abstract: Event cameras are an interesting visual exteroceptive sensor that reacts to brightness changes rather than integrating absolute image intensities. Owing to this design, the sensor exhibits strong performance in situations of challenging dynamics and illumination conditions. While event-based simultaneous tracking and mapping remains a challenging problem, a number of recent works have pointed out the sensor's suitability for prior map-based tracking. By making use of cross-modal registration paradigms, the camera's ego-motion can be tracked across a large spectrum of illumination and dynamics conditions on top of accurate maps that have been created a priori by more traditional sensors. The present paper follows up on a recently introduced event-based geometric semi-dense tracking paradigm, and proposes the addition of inertial signals in order to robustify the estimation. More specifically, the added signals provide strong cues for po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#38454;&#27573;&#21160;&#24577;&#35843;&#25972;&#22810;&#24863;&#23448;&#20248;&#20808;&#32423;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#22320;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#38454;&#27573;&#20351;&#29992;&#19981;&#21516;&#30340;&#24863;&#23448;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2408.01366</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#33310;&#21488;&#24341;&#23548;&#21160;&#24577;&#22810;&#24863;&#23448;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#26426;&#22120;&#25163;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#38454;&#27573;&#21160;&#24577;&#35843;&#25972;&#22810;&#24863;&#23448;&#20248;&#20808;&#32423;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#22320;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#38454;&#27573;&#20351;&#29992;&#19981;&#21516;&#30340;&#24863;&#23448;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#28789;&#27963;&#22320;&#20132;&#26367;&#19981;&#21516;&#24863;&#35273;&#30340;&#24778;&#20154;&#22825;&#36171;&#12290;&#24819;&#35937;&#19968;&#20010;&#21416;&#24072;&#29087;&#32451;&#22320;&#26681;&#25454;&#37197;&#26009;&#30340;&#21464;&#21270;&#21644;&#25511;&#21046;&#28201;&#24230;&#65292;&#26681;&#25454;&#39068;&#33394;&#12289;&#22768;&#38899;&#21644;&#27668;&#21619;&#30340;&#39068;&#33394;&#65292;&#26080;&#32541;&#22320;&#23548;&#33322;&#36890;&#36807;&#22797;&#26434;&#28921;&#39274;&#36807;&#31243;&#30340;&#27599;&#19968;&#20010;&#38454;&#27573;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#22522;&#20110;&#23545;&#20219;&#21153;&#38454;&#27573;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#22240;&#20026;&#23454;&#29616;&#27599;&#20010;&#38454;&#27573;&#30340;&#23376;&#30446;&#26631;&#26159;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#30340;&#24863;&#23448;&#12290;&#20026;&#20102;&#36171;&#20104;&#26426;&#22120;&#20154;&#31867;&#20284;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#22312;&#27169;&#20223;&#23398;&#20064;&#36807;&#31243;&#20013;&#32467;&#21512;&#20219;&#21153;&#38454;&#27573;&#65292;&#20197;&#30456;&#24212;&#30340;&#25351;&#23548;&#21160;&#24577;&#22810;&#24863;&#23448;&#34701;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MS-Bot&#65292;&#19968;&#31181;&#22522;&#20110;&#33310;&#21488;&#30340;&#21160;&#24577;&#22810;&#24863;&#23448;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20174;&#31895;&#21040;&#31934;&#30340;&#38454;&#27573;&#29702;&#35299;&#65292;&#26681;&#25454;&#24403;&#21069;&#39044;&#27979;&#38454;&#27573;&#30340;&#31934;&#32454;&#29366;&#24577;&#21160;&#24577;&#35843;&#25972;&#27169;&#24577;&#30340;&#20248;&#20808;&#32423;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#37197;&#22791;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#24863;&#23448;&#20449;&#24687;&#34701;&#21512;&#36807;&#31243;&#20013;&#36866;&#24212;&#20102;&#19981;&#21516;&#30340;&#20219;&#21153;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01366v1 Announce Type: cross  Abstract: Humans possess a remarkable talent for flexibly alternating to different senses when interacting with the environment. Picture a chef skillfully gauging the timing of ingredient additions and controlling the heat according to the colors, sounds, and aromas, seamlessly navigating through every stage of the complex cooking process. This ability is founded upon a thorough comprehension of task stages, as achieving the sub-goal within each stage can necessitate the utilization of different senses. In order to endow robots with similar ability, we incorporate the task stages divided by sub-goals into the imitation learning process to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a stage-guided dynamic multi-sensory fusion method with coarse-to-fine stage understanding, which dynamically adjusts the priority of modalities based on the fine-grained state within the predicted current stage. We train a robot system equipped
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#30456;&#20851;&#24615;&#21028;&#26029;&#22312;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#35780;&#20215;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;GPT-4V&#22312;&#30456;&#20851;&#24615;&#21028;&#26029;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CLIP&#21644;&#20854;&#20182;&#27169;&#22411;&#65292;&#20294;CLIPScore&#25351;&#26631;&#22312;&#37327;&#21270;&#35780;&#20215;&#20013;&#26356;&#20026;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2408.01363</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#35780;&#20215;&#33258;&#21160;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01363
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#30456;&#20851;&#24615;&#21028;&#26029;&#22312;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#35780;&#20215;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;GPT-4V&#22312;&#30456;&#20851;&#24615;&#21028;&#26029;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CLIP&#21644;&#20854;&#20182;&#27169;&#22411;&#65292;&#20294;CLIPScore&#25351;&#26631;&#22312;&#37327;&#21270;&#35780;&#20215;&#20013;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01363v1 Announce Type: &#20132;&#21449; &#25688;&#35201;: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#30456;&#20851;&#24615;&#21028;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#22312;&#38024;&#23545;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#35774;&#23450;&#30340;&#22823;&#35268;&#27169;&#8220;non-determined&#8221;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#35780;&#20272;&#20102;VLMs&#65292;&#21253;&#25324;CLIP&#12289;LLaVA&#21644;GPT-4V&#30340;&#30456;&#20851;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65306;(1)&#24403;&#19982;&#20154;&#31867;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30456;&#27604;&#36739;&#26102;&#65292;&#26080;&#35770;&#26159;&#24320;&#28304;&#36824;&#26159;&#38381;&#28304;&#30340;&#35270;&#35273;&#25351;&#23548;&#24615;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;LLaVA&#21644;GPT-4V&#37117;&#33021;&#36798;&#21040;&#26174;&#33879;&#30340; Kendall&#8217;s &#964; &#8764; 0.4&#12290;&#36825;&#19968;&#20998;&#25968;&#36229;&#36807;&#20102;CLIPScore&#25351;&#26631;&#12290;(2)&#34429;&#28982;CLIPScore&#26356;&#21463;&#38738;&#30544;&#65292;&#20294;LLMs&#23545;&#22522;&#20110;CLIP&#30340;&#26816;&#32034;&#31995;&#32479;&#30340;&#20559;&#35265;&#36739;&#23567;&#12290;(3)&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-4V&#30340;&#20998;&#25968;&#20998;&#24067;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#20026;&#21563;&#21512;&#65292;&#23454;&#29616;&#20102;Cohen&#8217;s &#954;&#20540;&#22312;0.08&#24038;&#21491;&#65292;&#36825;&#36229;&#36807;&#20102;CLIPScore&#30340;&#35780;&#20272;&#65292;&#20854;&#20013;GPT-4V&#30340;&#35780;&#20272;&#20998;&#25968;&#20998;&#24067;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#20026;&#25509;&#36817;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01363v1 Announce Type: cross  Abstract: Vision--Language Models (VLMs) have demonstrated success across diverse applications, yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs, including CLIP, LLaVA, and GPT-4V, within a large-scale \textit{ad hoc} retrieval task tailored for multimedia content creation in a zero-shot fashion. Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V, encompassing open-source and closed-source visual-instruction-tuned Large Language Models (LLMs), achieve notable Kendall's $\tau \sim 0.4$ when compared to human relevance judgments, surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems. (3) GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's $\kappa$ value of around 0.08, which outperforms CLIPScore at approx
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#34913;&#27531;&#24046;&#33976;&#39311;&#26694;&#26550;&#65288;BRD-CIL&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#25193;&#23637;&#32593;&#32476;&#32467;&#26500;&#21644;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;3D&#28857;&#20113;&#20998;&#31867;&#24335;&#22686;&#37327;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01356</link><description>&lt;p&gt;
&#24179;&#34913;&#27531;&#24046;&#33976;&#39311;&#23398;&#20064;&#26041;&#27861;&#22312;3D&#28857;&#20113;&#20998;&#31867;&#24335;&#22686;&#37327;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#34913;&#27531;&#24046;&#33976;&#39311;&#26694;&#26550;&#65288;BRD-CIL&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#25193;&#23637;&#32593;&#32476;&#32467;&#26500;&#21644;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;3D&#28857;&#20113;&#20998;&#31867;&#24335;&#22686;&#37327;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01356v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#21457;&#24067;  &#25688;&#35201;&#65306;&#30001;&#20110;&#33021;&#22815;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#26032;&#31867;&#21035;&#20013;&#26377;&#25928;&#22788;&#29702;&#20449;&#24687;&#30340;&#28044;&#20837;&#65292;&#24182;&#19988;&#19981;&#20250;&#24536;&#35760;&#26087;&#30693;&#35782;&#36896;&#25104;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#27492;&#20998;&#31867;&#24335;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#23578;&#26410;&#32771;&#34385;&#30446;&#21069;&#30740;&#31350;&#20013;CIL&#30340;&#24615;&#33021;&#31361;&#30772;&#65292;&#26377;&#25928;&#22320;&#32454;&#21270;&#26469;&#33258;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#21644;&#24179;&#34913;&#26032;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;CIL&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24179;&#34913;&#27531;&#24046;&#33976;&#39311;&#26694;&#26550;&#65288;BRD-CIL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;CIL&#30340;&#24615;&#33021;&#21040;&#19968;&#20010;&#26032;&#30340;&#26356;&#39640;&#27700;&#24179;&#12290;&#29305;&#21035;&#26159;&#65292;BRD-CIL&#35774;&#35745;&#20102;&#19968;&#31181;&#27531;&#24046;&#33976;&#39311;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#25351;&#23548;&#30697;&#38453;&#26469;&#21160;&#24577;&#25193;&#23637;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#22522;&#30784;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#27531;&#24046;&#30340;&#25351;&#23548;&#30697;&#38453;&#65292;&#26377;&#25928;&#22320;&#32454;&#21270;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BRD-CIL&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#23548;&#30697;&#38453;&#26469;&#24110;&#21161;&#23398;&#20064;&#31639;&#27861;&#23545;&#31867;&#21035;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#65292;&#21516;&#26102;&#30830;&#20445;&#26032;&#26087;&#30693;&#35782;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;BRD-CIL&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20811;&#26381;CIL&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;3D&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01356v1 Announce Type: new  Abstract: Class-incremental learning (CIL) thrives due to its success in processing the influx of information by learning from continuously added new classes while preventing catastrophic forgetting about the old ones. It is essential for the performance breakthrough of CIL to effectively refine past knowledge from the base model and balance it with new learning. However, such an issue has not yet been considered in current research. In this work, we explore the potential of CIL from these perspectives and propose a novel balanced residual distillation framework (BRD-CIL) to push the performance bar of CIL to a new higher level. Specifically, BRD-CIL designs a residual distillation learning strategy, which can dynamically expand the network structure to capture the residuals between the base and target models, effectively refining the past knowledge. Furthermore, BRD-CIL designs a balanced pseudo-label learning strategy by generating a guidance ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;PC&#178;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20266;&#20998;&#31867;&#21644;&#20266;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#26469;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01349</link><description>&lt;p&gt;
PC&#178;&#65306;&#22522;&#20110;&#20266;&#20998;&#31867;&#30340;&#20266;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;PC&#178;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20266;&#20998;&#31867;&#21644;&#20266;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#26469;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#30340;&#26080;&#32541;&#38598;&#25104;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;(NCL)&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#22122;&#22768;&#36890;&#24120;&#28304;&#20110;&#25968;&#25454;&#23545;&#30340;&#38169;&#37197;&#65292;&#36825;&#26159;&#19982;&#20256;&#32479;&#26377;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#30456;&#27604;&#30340;&#19968;&#20010;&#26174;&#33879;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20266;&#20998;&#31867;&#30340;&#20266;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;(PC&#178;)&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;PC&#178;&#25552;&#20379;&#20102;&#19968;&#20010;&#19977;&#37325;&#31574;&#30053;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#8220;&#20266;&#20998;&#31867;&#8221;&#20219;&#21153;&#65292;&#23558;&#25551;&#36848;&#35299;&#37322;&#20026;&#20998;&#31867;&#26631;&#31614;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#26426;&#21046;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#22270;&#20687;-&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;PC&#178;&#30340;&#20266;&#20998;&#31867;&#33021;&#21147;&#65292;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20266;&#25551;&#36848;&#30340;&#25391;&#33633;&#26469;&#36827;&#19968;&#27493;&#22686;&#21152;&#25439;&#22833;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#21319;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21516;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01349v1 Announce Type: cross  Abstract: In the realm of cross-modal retrieval, seamlessly integrating diverse modalities within multimedia remains a formidable challenge, especially given the complexities introduced by noisy correspondence learning (NCL). Such noise often stems from mismatched data pairs, which is a significant obstacle distinct from traditional noisy labels. This paper introduces Pseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address this challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an auxiliary "pseudo-classification" task that interprets captions as categorical labels, steering the model to learn image-text semantic similarity through a non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques, capitalizing on PC$^2$'s pseudo-classification capability, we generate pseudo-captions to provide more informative and tangible supervision for each mismatched pair. Thirdly, the oscillation of pse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#23427;&#33021;&#22815;&#23558;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#26377;&#25928;&#34701;&#21512;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01343</link><description>&lt;p&gt;
StitchFusion: &#19968;&#31181;&#34701;&#21512;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#23427;&#33021;&#22815;&#23558;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#26377;&#25928;&#34701;&#21512;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01343v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#21253;&#21547;&#19987;&#38376;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#36825;&#20123;&#27169;&#22359;&#29305;&#22320;&#20026;&#29305;&#23450;&#30340;&#27169;&#24577;&#35774;&#35745;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#36755;&#20837;&#30340;&#28789;&#27963;&#24615;&#21644;&#22686;&#21152;&#20102;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#19988;&#26377;&#25928;&#30340;&#22823;&#27169;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#35813;&#26694;&#26550;&#30452;&#25509;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#29305;&#24449;&#34701;&#21512;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#34701;&#21512;&#65292;&#36890;&#36807;&#20849;&#20139;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20449;&#24687;&#27969;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#30340;&#21452;&#21521;&#20256;&#36755;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#30340;&#36755;&#20837;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#34701;&#21512;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#24577;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;&#36866;&#37197;&#22120;&#27169;&#22359;&#65288;MultiAdapter&#65289;&#65292;&#20197;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#21033;&#29992;MultiAdapter&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#26377;&#25928;&#34701;&#21512;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01343v1 Announce Type: new  Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01334</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#38271;&#26399;&#20219;&#21153;&#29702;&#35299;&#30340;&#39592;&#24178;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Backbone for Long-Horizon Robot Task Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#31471;&#21040;&#31471;&#26426;&#22120;&#20154;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#20219;&#21153;&#39046;&#22495;&#65292;&#24120;&#24120;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#32467;&#26524;&#21644;&#19981;&#33391;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#21363;TBBF (Therblig-based Backbone Framework)&#65292;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;&#21644;&#36716;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;&#20351;&#29992;therbligs&#65288;&#22522;&#26412;&#21160;&#20316;&#20803;&#32032;&#65289;&#20316;&#20026;&#25903;&#25745;&#65292;&#24182;&#19982;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#29702;&#35299;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#12290;&#22312;&#31163;&#32447;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Meta-RGate SynerFusion (MGSF)&#32593;&#32476;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#21508;&#31181;&#20219;&#21153;&#30340;therbligs&#12290;&#22312;&#32447;&#27979;&#35797;&#38454;&#27573;&#65292;&#22312;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#28436;&#31034;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;MGSF&#32593;&#32476;&#25552;&#21462;&#39640;&#38454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;ActionREG&#65288;&#21160;&#20316;&#27880;&#20876;&#65289;&#23558;&#20854;&#32534;&#30721;&#25104;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;Meta-Learner&#65292;&#23427;&#21487;&#20197;&#20174;&#21333;&#20010;&#20219;&#21153;&#30340;&#34920;&#29616;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#27867;&#21270;&#21040;&#19981;&#21516;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#20013;&#36827;&#34892;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#25552;&#21319;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 Announce Type: new  Abstract: End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Additionally
&lt;/p&gt;</description></item><item><title>TopoNAS&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#31574;&#30053;&#21644;&#31616;&#21270;&#36335;&#24452;&#32467;&#26500;&#65292;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01311</link><description>&lt;p&gt;
TopoNAS: &#36890;&#36807;&#25299;&#25169;&#31616;&#21270;&#25552;&#39640;&#26799;&#24230;NAS&#25628;&#32034;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
TopoNAS: Boosting Search Efficiency of Gradient-based NAS via Topological Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01311
&lt;/p&gt;
&lt;p&gt;
TopoNAS&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#31574;&#30053;&#21644;&#31616;&#21270;&#36335;&#24452;&#32467;&#26500;&#65292;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01311v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#38395;  &#25688;&#35201;: Neural Architecture Search (NAS)&#30340;&#25628;&#32034;&#25928;&#29575;&#26159;&#30446;&#21069;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#25628;&#32034;&#31574;&#30053;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26410;&#33021;&#20943;&#23569;&#35745;&#31639;&#20013;&#30340;&#20887;&#20313;&#65292;&#23588;&#20854;&#26159;&#22312;&#19968;&#27425;&#24615;NAS&#26550;&#26500;&#20013;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;NAS&#26041;&#27861;&#22312;&#20687;DARTS&#36825;&#26679;&#30340;&#24120;&#29992;&#25628;&#32034;&#31354;&#38388;&#20013;&#26174;&#31034;&#20986;&#26080;&#25928;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23548;&#33268;&#25628;&#32034;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;TopoNAS&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#26799;&#24230;&#19968;&#27425;&#24615;NAS&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#25628;&#32034;&#36335;&#24452;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#29992;&#27169;&#22411;&#23545;&#25628;&#32034;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25581;&#31034;&#21442;&#25968;&#21270;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25299;&#25169;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#36845;&#20195;&#22320;&#24212;&#29992;&#27169;&#22359;&#20849;&#20139;&#31574;&#30053;&#26469;&#31616;&#21270;&#25628;&#32034;&#36335;&#24452;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#22312;&#28155;&#21152;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01311v1 Announce Type: new  Abstract: Improving search efficiency serves as one of the crucial objectives of Neural Architecture Search (NAS). However, many current approaches ignore the universality of the search strategy and fail to reduce the computational redundancy during the search process, especially in one-shot NAS architectures. Besides, current NAS methods show invalid reparameterization in non-linear search space, leading to poor efficiency in common search spaces like DARTS. In this paper, we propose TopoNAS, a model-agnostic approach for gradient-based one-shot NAS that significantly reduces searching time and memory usage by topological simplification of searchable paths. Firstly, we model the non-linearity in search spaces to reveal the parameterization difficulties. To improve the search efficiency, we present a topological simplification method and iteratively apply module-sharing strategies to simplify the topological structure of searchable paths. In addit
&lt;/p&gt;</description></item><item><title>TexGen&#26159;&#19968;&#20010;&#29992;&#20110;3D&#32441;&#29702;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#32452;&#35013;&#32441;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22810;&#35270;&#22270;&#37319;&#26679;&#21644;&#37325;&#37319;&#26679;&#25216;&#26415;&#20943;&#23569;&#20102;&#35270;&#22270;&#24046;&#24322;&#65292;&#24182;&#22312;&#20445;&#25345;&#32441;&#29702;&#32454;&#33410;&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#22122;&#22768;&#20272;&#31639;&#21644;&#36755;&#20837;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2408.01291</link><description>&lt;p&gt;
TexGen&#65306;&#20855;&#26377;&#22810;&#35270;&#22270;&#37319;&#26679;&#21644;&#37325;&#37319;&#26679;&#30340;&#25991;&#26412;&#25351;&#23548;3D&#32441;&#29702;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01291
&lt;/p&gt;
&lt;p&gt;
TexGen&#26159;&#19968;&#20010;&#29992;&#20110;3D&#32441;&#29702;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#32452;&#35013;&#32441;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22810;&#35270;&#22270;&#37319;&#26679;&#21644;&#37325;&#37319;&#26679;&#25216;&#26415;&#20943;&#23569;&#20102;&#35270;&#22270;&#24046;&#24322;&#65292;&#24182;&#22312;&#20445;&#25345;&#32441;&#29702;&#32454;&#33410;&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#22122;&#22768;&#20272;&#31639;&#21644;&#36755;&#20837;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;3D&#32593;&#26684;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#29983;&#25104;&#19982;&#20219;&#24847;&#25991;&#26412;&#25551;&#36848;&#30456;&#23545;&#24212;&#30340;3D&#32441;&#29702;&#12290;&#24403;&#21069;&#20174;&#37319;&#26679;&#35270;&#22270;&#29983;&#25104;&#30340;&#32441;&#29702;&#24448;&#24448;&#23548;&#33268;&#31361;&#20986;&#30340;&#25509;&#32541;&#25110;&#36807;&#24230;&#30340;&#24179;&#28369;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TexGen&#30340;&#26032;&#30340;&#22810;&#35270;&#22270;&#37319;&#26679;&#21644;&#37325;&#37319;&#26679;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#32441;&#29702;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;RGB&#31354;&#38388;&#20445;&#25345;&#19968;&#24352;&#32441;&#29702;&#22270;&#65292;&#35813;&#22270;&#21442;&#25968;&#21270;&#20110;&#21435;&#22122;&#27493;&#39588;&#65292;&#24182;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#27599;&#27425;&#37319;&#26679;&#27493;&#39588;&#21518;&#26356;&#26032;&#65292;&#20197;&#36880;&#28176;&#20943;&#23569;&#35270;&#22270;&#24046;&#24322;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#22810;&#35270;&#22270;&#37319;&#26679;&#31574;&#30053;&#26469;&#22312;&#35270;&#22270;&#20013;&#20256;&#25773;&#22806;&#35266;&#20449;&#24687;&#12290;&#20026;&#20102;&#20445;&#30041;&#32441;&#29702;&#32454;&#33410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22122;&#22768;&#37325;&#37319;&#26679;&#25216;&#26415;&#65292;&#23427;&#24110;&#21161;&#20272;&#35745;&#22122;&#22768;&#65292;&#29983;&#25104;&#23545;&#21518;&#32493;&#21435;&#22122;&#27493;&#39588;&#26377;&#25351;&#23548;&#24615;&#30340;&#36755;&#20837;&#65292;&#32780;&#36825;&#20123;&#27493;&#39588;&#26159;&#30001;&#25991;&#26412;&#25552;&#31034;&#21644;&#24403;&#21069;&#32441;&#29702;&#29366;&#24577;&#23450;&#21046;&#30340;&#12290;&#38543;&#21518;&#65292;&#19968;&#31181;&#33258;&#32858;&#28966;&#37329;&#23383;&#22612;&#32467;&#26500;&#34987;&#38598;&#25104;&#21040;&#32441;&#29702;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#23610;&#24230;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35843;&#33410;&#32441;&#29702;&#30340;&#19981;&#21516;&#29305;&#24449;&#65292;&#32441;&#29702;&#30340;&#24179;&#28369;&#24615;&#21644;&#32454;&#33410;&#31243;&#24230;&#21487;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#35843;&#25972;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TexGen&#33021;&#22815;&#39640;&#25928;&#31283;&#23450;&#22320;&#22312;&#22810;&#23039;&#24577;&#21644;&#22810;&#20809;&#29031;&#26465;&#20214;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32441;&#29702;&#65292;&#32780;&#20854;&#22810;&#23610;&#24230;&#32454;&#33410;&#25429;&#33719;&#21644;&#33258;&#32858;&#28966;&#26426;&#21046;&#33021;&#22815;&#36991;&#20813;&#22312;&#32441;&#29702;&#23545;&#40784;&#36807;&#31243;&#20013;&#20986;&#29616;&#26174;&#33879;&#30340;&#25509;&#32541;&#21644;&#27169;&#31946;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01291v1 Announce Type: new  Abstract: Given a 3D mesh, we aim to synthesize 3D textures that correspond to arbitrary textual descriptions. Current methods for generating and assembling textures from sampled views often result in prominent seams or excessive smoothing. To tackle these issues, we present TexGen, a novel multi-view sampling and resampling framework for texture generation leveraging a pre-trained text-to-image diffusion model. For view consistent sampling, first of all we maintain a texture map in RGB space that is parameterized by the denoising step and updated after each sampling step of the diffusion model to progressively reduce the view discrepancy. An attention-guided multi-view sampling strategy is exploited to broadcast the appearance information across views. To preserve texture details, we develop a noise resampling technique that aids in the estimation of noise, generating inputs for subsequent denoising steps, as directed by the text prompt and curre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20016;&#23500;&#35270;&#35273;&#25991;&#26723;&#20869;&#23481;&#29702;&#35299;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2408.01287</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20016;&#23500;&#35270;&#35273;&#25991;&#26723;&#20869;&#23481;&#29702;&#35299;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Visually Rich Document Content Understanding: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20016;&#23500;&#35270;&#35273;&#25991;&#26723;&#20869;&#23481;&#29702;&#35299;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01287v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; Abstract: &#20016;&#23500;&#35270;&#35273;&#25991;&#26723;&#65288;VRD&#65289;&#22312;&#23398;&#26415;&#12289;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#33829;&#38144;&#31561;&#39046;&#22495;&#22240;&#20854;&#22810;&#27169;&#24335;&#20449;&#24687;&#20869;&#23481;&#32780;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20174;VRD&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#21644;&#20154;&#24037;&#21171;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26082;&#26114;&#36149;&#21448;&#20302;&#25928;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20026;&#36825;&#20010;&#36807;&#31243;&#24102;&#26469;&#20102;&#38761;&#21629;&#65292;&#23427;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#22810;&#27169;&#24335;&#20449;&#24687;&#21253;&#25324;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#21457;&#23637;&#20840;&#38754;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20174;VRD&#25552;&#21462;&#20449;&#24687;&#30340;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#37492;&#20110;&#23545;&#20016;&#23500;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VRDU&#65289;&#30340;&#38656;&#27714;&#22686;&#38271;&#21644;&#24555;&#36895;&#21457;&#23637;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;VRDU&#26694;&#26550;&#30340;&#20840;&#38754;&#22238;&#39038;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#24182;&#20419;&#36827;&#26032;&#39062;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01287v1 Announce Type: cross  Abstract: Visually Rich Documents (VRDs) are essential in academia, finance, medical fields, and marketing due to their multimodal information content. Traditional methods for extracting information from VRDs depend on expert knowledge and manual labor, making them costly and inefficient. The advent of deep learning has revolutionized this process, introducing models that leverage multimodal information vision, text, and layout along with pretraining tasks to develop comprehensive document representations. These models have achieved state-of-the-art performance across various downstream tasks, significantly enhancing the efficiency and accuracy of information extraction from VRDs. In response to the growing demands and rapid developments in Visually Rich Document Understanding (VRDU), this paper provides a comprehensive review of deep learning-based VRDU frameworks. We systematically survey and analyze existing methods and benchmark datasets, ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#24067;&#22806;&#26816;&#27979;&#25216;&#26415;&#26469;&#35299;&#20915;&#38899;&#39057;&#35270;&#35273;&#27867;&#21270;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#22495;&#21457;&#29983;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#23545;&#26410;&#30693;&#31867;&#21035;&#30340;&#20934;&#30830;&#20998;&#31867;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01284</link><description>&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#27867;&#21270;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#65306;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#24067;&#22806;&#26816;&#27979;&#25216;&#26415;&#26469;&#35299;&#20915;&#38899;&#39057;&#35270;&#35273;&#27867;&#21270;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#22495;&#21457;&#29983;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#23545;&#26410;&#30693;&#31867;&#21035;&#30340;&#20934;&#30830;&#20998;&#31867;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;&#35270;&#35273;&#27867;&#21270;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;GZSL&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;GZSL&#20219;&#21153;&#35201;&#27714;&#23545;&#26082;&#26377;&#35757;&#32451;&#25968;&#25454;&#21448;&#26377;&#27979;&#35797;&#25968;&#25454;&#30340;&#26679;&#26412;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#22312;&#22810;&#27169;&#24335;&#36755;&#20837;&#20013;&#65292;&#21516;&#26102;&#21253;&#21547;&#35270;&#35273;&#21644;&#22768;&#23398;&#29305;&#24449;&#20351;&#20854;&#25104;&#20026;&#20102;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#23884;&#20837;&#26041;&#27861;&#21644;&#29983;&#25104;&#26041;&#27861;&#19978;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#21183;&#21516;&#26102;&#32531;&#35299;&#23427;&#20204;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#27169;&#25311;&#26410;&#35265;&#36807;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#35757;&#32451;&#19968;&#20010;OOD&#26816;&#27979;&#22120;&#12290;&#36825;&#26159;&#36890;&#29992;&#26694;&#26550;&#30340;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#27867;&#21270;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#20986;&#29616;&#30340;&#22495;&#21457;&#29983;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#37319;&#32435;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22312;&#19968;&#36215;&#26469;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01284v1 Announce Type: cross  Abstract: Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. However, generative training is difficult and unstable, while embedding-based methods often encounter domain shift problem. Thus, we find it promising to integrate both methods into a unified framework to leverage their advantages while mitigating their respective disadvantages. Our study introduces a general framework employing out-of-distribution (OOD) detection, aiming to harness the strengths of both approaches. We first employ generative adversarial networks to synthesize unseen features, enabling the training of an OOD detector alongside cla
&lt;/p&gt;</description></item><item><title>Wave-Mamba&#21033;&#29992;&#23567;&#27874;&#21464;&#25442;&#21644;&#26080;&#25439;&#19979;&#37319;&#26679;&#30340;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36229;&#39640;&#28165;&#20302;&#20809;&#29031;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#22270;&#20687;&#30340;&#28165;&#26224;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#21363;&#20351;&#22312;&#22122;&#22768;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#20445;&#25345;&#33391;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.01276</link><description>&lt;p&gt;
Wave-Mamba&#65306;&#36866;&#29992;&#20110;&#36229;&#39640;&#28165;&#20302;&#20809;&#29031;&#22686;&#24378;&#30340;&#27874;&#23572;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition Low-Light Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01276
&lt;/p&gt;
&lt;p&gt;
Wave-Mamba&#21033;&#29992;&#23567;&#27874;&#21464;&#25442;&#21644;&#26080;&#25439;&#19979;&#37319;&#26679;&#30340;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36229;&#39640;&#28165;&#20302;&#20809;&#29031;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#22270;&#20687;&#30340;&#28165;&#26224;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#21363;&#20351;&#22312;&#22122;&#22768;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#20445;&#25345;&#33391;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#39640;&#28165;&#65288;UHD&#65289;&#25216;&#26415;&#22240;&#20854;&#21331;&#36234;&#30340;&#35270;&#35273;&#25928;&#26524;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#23427;&#20063;&#23545;&#20302;&#20809;&#29031;&#22270;&#20687;&#22686;&#24378;&#65288;LLIE&#65289;&#25216;&#26415;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;UHD&#22270;&#20687;&#22266;&#26377;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#20351;&#24471;&#29616;&#26377;&#30340;UHD LLIE&#26041;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#26102;&#37319;&#29992;&#20102;&#39640;&#20493;&#29575;&#19979;&#37319;&#26679;&#65292;&#36825;&#21448;&#23548;&#33268;&#20102;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;&#23567;&#27874;&#21464;&#25442;&#19981;&#20165;&#20801;&#35768;&#26080;&#25439;&#19979;&#37319;&#26679;&#65292;&#32780;&#19988;&#21487;&#20197;&#23558;&#22270;&#20687;&#20869;&#23481;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;&#23427;&#20351;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#19981;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#38271;&#24207;&#21015;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#20102;SSM&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wave-Mamba&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#20004;&#20010;&#20851;&#38190;&#27934;&#23519;&#65306;1&#65289;&#22270;&#20687;&#20013;&#30340;&#32477;&#22823;&#22810;&#25968;&#20869;&#23481;&#20449;&#24687;&#23384;&#22312;&#20110;&#20302;&#39057;&#25104;&#20998;&#20013;&#65292;&#39640;&#39057;&#25104;&#20998;&#20013;&#30340;&#36739;&#23569;&#65307;2&#65289;SSMs&#21487;&#20197;&#36991;&#20813;&#22312;&#39640;&#22122;&#22768;&#26465;&#20214;&#19979;&#24314;&#27169;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23567;&#27874;&#21464;&#25442;&#20250;&#20998;&#31163;&#22122;&#22768;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#23519;&#65292;Wave-Mamba&#33021;&#22815;&#23454;&#29616;&#23545;&#36229;&#39640;&#28165;&#20302;&#20809;&#29031;&#22270;&#20687;&#30340;&#39640;&#25928;&#21644;&#39640;&#36136;&#37327;&#22686;&#24378;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#20687;&#30340;&#32454;&#33410;&#21644;&#23545;&#27604;&#24230;&#65292;&#24182;&#22312;&#22122;&#22768;&#27700;&#24179;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20445;&#25345;&#28165;&#26224;&#24230;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;Wave-Mamba&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;UHD LLIE&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01276v1 Announce Type: new  Abstract: Ultra-high-definition (UHD) technology has attracted widespread attention due to its exceptional visual quality, but it also poses new challenges for low-light image enhancement (LLIE) techniques. UHD images inherently possess high computational complexity, leading existing UHD LLIE methods to employ high-magnification downsampling to reduce computational costs, which in turn results in information loss. The wavelet transform not only allows downsampling without loss of information, but also separates the image content from the noise. It enables state space models (SSMs) to avoid being affected by noise when modeling long sequences, thus making full use of the long-sequence modeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel approach based on two pivotal insights derived from the wavelet domain: 1) most of the content information of an image exists in the low-frequency component, less in the high-frequency componen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21319;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#25913;&#21892;&#20102;&#22522;&#20110;3D Gaussians&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2408.01269</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;3D&#20869;&#23481;&#21019;&#24314;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#35789;&#27719;&#20016;&#23500;&#24615;&#25552;&#39640;3D GS&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
A General Framework to Boost 3D GS Initialization for Text-to-3D Generation by Lexical Richness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21319;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#25913;&#21892;&#20102;&#22522;&#20110;3D Gaussians&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01269v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#21019;&#24314;&#26368;&#36817;&#21463;&#21040;&#24456;&#22823;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;3D Gaussians Splatting&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#22522;&#20110;GS&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;&#21021;&#22987;&#21270;&#21644;&#28210;&#26579;&#20248;&#21270;&#12290;&#20026;&#20102;&#36798;&#21040;&#21021;&#22987;&#21270;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#30452;&#25509;&#24212;&#29992;&#38543;&#26426;&#29699;&#20307;&#21021;&#22987;&#21270;&#25110;&#32773;3D&#25193;&#25955;&#27169;&#22411;&#65292;&#27604;&#22914;Point-E&#65292;&#26469;&#33719;&#21462;&#21021;&#22987;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31574;&#30053;&#23384;&#22312;&#30528;&#20004;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#65306;1&#65289;&#21363;&#20351;&#32463;&#36807;&#35757;&#32451;&#65292;&#26368;&#32456;&#24418;&#29366;&#20173;&#28982;&#19982;&#21021;&#22987;&#24418;&#29366;&#30456;&#20284;&#65307;2&#65289;&#23545;&#20110;&#31616;&#21333;&#25991;&#26412;&#65292;&#22914;&#8220;&#19968;&#21482;&#29399;&#8221;&#65292;&#24418;&#29366;&#21487;&#20197;&#34987;&#20135;&#29983;&#65292;&#20294;&#23545;&#20110;&#35789;&#27719;&#20016;&#23500;&#30340;&#25991;&#26412;&#65292;&#22914;&#8220;&#19968;&#21482;&#29399;&#22352;&#22312;&#39134;&#26426;&#30340;&#39030;&#37096;&#8221;&#65292;&#21017;&#19981;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#35789;&#27719;&#20016;&#23500;&#24615;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;3D GS&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#23558;3D Gaussians&#32858;&#38598;&#21040;&#31354;&#38388;&#22343;&#21248;&#30340;voxels&#20013;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#22797;&#26434;&#24418;&#29366;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#20026;&#26356;&#20016;&#23500;&#25551;&#36848;&#30340;&#35821;&#35328;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;3D&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01269v1 Announce Type: new  Abstract: Text-to-3D content creation has recently received much attention, especially with the prevalence of 3D Gaussians Splatting. In general, GS-based methods comprise two key stages: initialization and rendering optimization. To achieve initialization, existing works directly apply random sphere initialization or 3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such strategies suffer from two critical yet challenging problems: 1) the final shapes are still similar to the initial ones even after training; 2) shapes can be produced only from simple texts, e.g., "a dog", not for lexically richer texts, e.g., "a dog is sitting on the top of the airplane". To address these problems, this paper proposes a novel general framework to boost the 3D GS Initialization for text-to-3D generation upon the lexical richness. Our key idea is to aggregate 3D Gaussians into spatially uniform voxels to represent complex shapes while enab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;S2TD-Face&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#20174;&#21333;&#24133;&#33609;&#22270;&#20013;&#39640;&#20445;&#30495;&#22320;&#37325;&#24314;&#20855;&#26377;&#35814;&#32454;&#20960;&#20309;&#32467;&#26500;&#21644;&#21487;&#25511;&#32441;&#29702;&#30340;3D&#38754;&#23380;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2408.01218</link><description>&lt;p&gt;
S2TD-Face: &#20174;&#21333;&#24133;&#33609;&#22270;&#37325;&#24314;&#20855;&#26377;&#21487;&#25511;&#32441;&#29702;&#30340;&#35814;&#32454;3D&#38754;&#23380;
&lt;/p&gt;
&lt;p&gt;
S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from a Single Sketch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;S2TD-Face&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#20174;&#21333;&#24133;&#33609;&#22270;&#20013;&#39640;&#20445;&#30495;&#22320;&#37325;&#24314;&#20855;&#26377;&#35814;&#32454;&#20960;&#20309;&#32467;&#26500;&#21644;&#21487;&#25511;&#32441;&#29702;&#30340;3D&#38754;&#23380;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33609;&#22270;&#37325;&#24314;&#21487;&#25511;&#32441;&#29702;&#21644;&#35814;&#32454;3D&#38754;&#23380;&#30340;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;S2TD-Face&#12290;S2TD-Face&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20960;&#20309;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#36755;&#20837;&#33609;&#22270;&#20013;&#37325;&#24314;&#35814;&#32454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#20102;&#20445;&#25345;&#20960;&#20309;&#32467;&#26500;&#19982;&#33609;&#22270;&#31934;&#32454;&#31508;&#35302;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33609;&#22270;&#33267;&#20960;&#20309;&#25439;&#22833;&#65292;&#21363; \textbf{descriptive text}&#65292;&#29992;&#20197;&#25351;&#23548;&#32593;&#32476;&#22312;&#20960;&#20309;&#37325;&#24314;&#36807;&#31243;&#20013;&#30340;&#31508;&#35302;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#30041;&#33609;&#22270;&#20013;&#32441;&#29702;&#30340;&#20016;&#23500;&#24615;&#65292;S2TD-Face&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20174;&#33609;&#22270;&#32441;&#29702;&#29983;&#25104;3D&#31354;&#38388;&#20869;&#32441;&#29702;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#37325;&#24314;&#30340;3D&#38754;&#23380;&#26082;&#20855;&#26377;&#32454;&#33410;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20063;&#35206;&#30422;&#20102;&#32441;&#29702;&#32454;&#33410;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;S2TD-Face&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#22914;&#21160;&#30011;&#21046;&#20316;&#12289;3D&#34394;&#25311;&#24418;&#35937;&#21019;&#24314;&#12289;&#33402;&#26415;&#35774;&#35745;&#12289;&#22833;&#36394;&#20154;&#21592;&#25628;&#32034;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01218v1 Announce Type: new  Abstract: 3D textured face reconstruction from sketches applicable in many scenarios such as animation, 3D avatars, artistic design, missing people search, etc., is a highly promising but underdeveloped research topic. On the one hand, the stylistic diversity of sketches leads to existing sketch-to-3D-face methods only being able to handle pose-limited and realistically shaded sketches. On the other hand, texture plays a vital role in representing facial appearance, yet sketches lack this information, necessitating additional texture control in the reconstruction process. This paper proposes a novel method for reconstructing controllable textured and detailed 3D faces from sketches, named S2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework that directly reconstructs detailed geometry from the input sketch. To keep geometry consistent with the delicate strokes of the sketch, we propose a novel sketch-to-geometry loss that 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#38656;&#35201;&#20687;&#32032;&#32423;&#26631;&#27880;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#30340;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#36523;&#20221;&#29305;&#24449;&#20294;&#25913;&#21464;&#31867;&#21035;&#23646;&#24615;&#30340;&#26032;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20102;&#31867;&#21035;&#30456;&#20851;&#21644;&#31867;&#21035;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26469;&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#27969;&#24418;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#38656;&#35201;&#20998;&#21106;&#30340;&#24322;&#24120;&#26679;&#26412;&#29983;&#25104;&#19968;&#20010;&#27491;&#24120;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.01191</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#19968;&#20010;&#24369;&#30417;&#30563;&#21644;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Weakly Supervised and Globally Explainable Learning Framework for Brain Tumor Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#38656;&#35201;&#20687;&#32032;&#32423;&#26631;&#27880;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#30340;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#36523;&#20221;&#29305;&#24449;&#20294;&#25913;&#21464;&#31867;&#21035;&#23646;&#24615;&#30340;&#26032;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20102;&#31867;&#21035;&#30456;&#20851;&#21644;&#31867;&#21035;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26469;&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#27969;&#24418;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#38656;&#35201;&#20998;&#21106;&#30340;&#24322;&#24120;&#26679;&#26412;&#29983;&#25104;&#19968;&#20010;&#27491;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01191v1 &#26032;&#38395;&#31867;&#22411;: &#26032;&#26356;&#26032; &#25688;&#35201;: &#22522;&#20110;&#26426;&#22120;&#30340;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#20570;&#20986;&#26356;&#22909;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#22823;&#33041;&#32959;&#30244;&#30340;&#22797;&#26434;&#32467;&#26500;&#20197;&#21450;&#26114;&#36149;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#32473;&#33258;&#21160;&#32959;&#30244;&#20998;&#21106;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#22312;&#19981;&#38656;&#35201;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#24322;&#24120;&#30340;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#26679;&#26412;&#30340;&#31867;&#21035;&#30456;&#20851;&#29305;&#24449;&#21644;&#31867;&#21035;&#26080;&#20851;&#29305;&#24449;&#65292;&#36890;&#36807;&#23884;&#20837;&#19981;&#21516;&#30340;&#31867;&#21035;&#30456;&#20851;&#29305;&#24449;&#65292;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#20197;&#20445;&#30041;&#36523;&#20221;&#29305;&#24449;&#24182;&#26356;&#25913;&#31867;&#21035;&#23646;&#24615;&#12290;&#25105;&#20204;&#23545;&#25552;&#21462;&#20986;&#30340;&#31867;&#21035;&#30456;&#20851;&#29305;&#24449;&#36827;&#34892;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65292;&#33719;&#21462;&#19968;&#20010;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#27969;&#24418;&#65292;&#23545;&#20110;&#35201;&#20998;&#21106;&#30340;&#27599;&#20010;&#24322;&#24120;&#26679;&#26412;&#65292;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#35268;&#21017;&#36335;&#24452;&#25351;&#23548;&#30340;&#27491;&#24120;&#26679;&#26412;&#65292;&#35813;&#36335;&#24452;&#35774;&#35745;&#26088;&#22312;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#26679;&#26412;&#20013;&#34920;&#36798;&#19981;&#21516;&#31867;&#21035;&#30340;&#31867;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01191v1 Announce Type: new  Abstract: Machine-based brain tumor segmentation can help doctors make better diagnoses. However, the complex structure of brain tumors and expensive pixel-level annotations present challenges for automatic tumor segmentation. In this paper, we propose a counterfactual generation framework that not only achieves exceptional brain tumor segmentation performance without the need for pixel-level annotations, but also provides explainability. Our framework effectively separates class-related features from class-unrelated features of the samples, and generate new samples that preserve identity features while altering class attributes by embedding different class-related features. We perform topological data analysis on the extracted class-related features and obtain a globally explainable manifold, and for each abnormal sample to be segmented, a meaningful normal sample could be effectively generated with the guidance of the rule-based paths designed w
&lt;/p&gt;</description></item><item><title>VAR-CLIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#25216;&#26415;&#21644;CLIP&#33021;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#23545;&#20855;&#20307;&#22270;&#20687;&#20869;&#23481;&#36827;&#34892;&#26356;&#31934;&#30830;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2408.01181</link><description>&lt;p&gt;
VAR-CLIP: &#20351;&#29992;&#35270;&#35273;&#33258;&#22238;&#24402;&#24314;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01181
&lt;/p&gt;
&lt;p&gt;
VAR-CLIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#25216;&#26415;&#21644;CLIP&#33021;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#23545;&#20855;&#20307;&#22270;&#20687;&#20869;&#23481;&#36827;&#34892;&#26356;&#31934;&#30830;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01181v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#28040;&#24687;  &#25688;&#35201;&#65306;VAR&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#8220;&#19979;&#19968;&#32423;&#39044;&#27979;&#8221;&#32780;&#19981;&#26159;&#8220;&#19979;&#19968;&#20010; token &#39044;&#27979;&#8221;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#36716;&#21464;&#33021;&#22815;&#20351;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#21464;&#25442;&#22120;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#23454;&#29616;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;VAR&#27169;&#22411;&#21482;&#33021;&#22312;&#31867;&#26465;&#20214;&#21512;&#25104;&#19979;&#20351;&#29992;&#65292;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#25351;&#23548;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VAR-CLIP&#30340;&#26032;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23427;&#23558;&#33258;&#22238;&#24402;&#35270;&#35273;&#25216;&#26415;&#19982;CLIP&#30340; capabilities &#32467;&#21512;&#36215;&#26469;&#12290;VAR-CLIP&#26694;&#26550;&#23558;&#25551;&#36848;&#32534;&#30721;&#20026;&#25991;&#26412;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#38543;&#21518;&#20316;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#25991;&#26412;&#26465;&#20214;&#20351;&#29992;&#12290;&#20026;&#20102;&#22312;&#20687;ImageNet&#36825;&#26679;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#20102;BLIP2&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;CLIP&#20013;&#21333;&#35789;&#23450;&#20301;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#29992;&#20110;&#25551;&#36848;&#25351;&#23548;&#30340;&#30446;&#30340;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#30830;&#35748;&#20102;VAR-CLIP&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#29983;&#25104;&#25551;&#36848;&#25968;&#25454;&#38598;&#20013;&#30340;&#20855;&#20307;&#22270;&#20687;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01181v1 Announce Type: new  Abstract: VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP's pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#39035;&#36827;&#34892;&#20999;&#29255;&#26631;&#31614;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#36873;&#25321;&#38382;&#39064;&#12290;&#30740;&#31350;&#23545;&#27604;&#20102;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#38024;&#23545;TCGA-NSCLC&#21644;Camelyon16&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2408.01167</link><description>&lt;p&gt;
&#37325;&#24605;&#38754;&#21521;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Rethinking Pre-trained Feature Extractor Selection in Multiple Instance Learning for Whole Slide Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#39035;&#36827;&#34892;&#20999;&#29255;&#26631;&#31614;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#36873;&#25321;&#38382;&#39064;&#12290;&#30740;&#31350;&#23545;&#27604;&#20102;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#38024;&#23545;TCGA-NSCLC&#21644;Camelyon16&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01167v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#23545;&#20110;&#22312;&#27809;&#26377;&#38656;&#35201;&#20999;&#29255;&#26631;&#31614;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#23545;&#30334;&#19975;&#20687;&#32032;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687; (WSIs) &#36827;&#34892;&#20998;&#31867;&#65292;&#22810;&#23454;&#20363;&#23398;&#20064; (MIL) &#24050;&#25104;&#20026;&#19968;&#20010;&#21463;&#21040;&#38738;&#30544;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#30740;&#31350;&#28909;&#28526;&#38598;&#20013;&#22312;&#22522;&#20110;&#23884;&#20837;&#30340;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#21040;&#20351;&#29992;&#39044;&#20808;&#22312; ImageNet-1K &#19978;&#35757;&#32451;&#30340; ResNet50 &#30417;&#30563;&#27169;&#22411;&#25552;&#21462;&#20999;&#29255;&#22359;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#28982;&#21518;&#34987;&#39304;&#36865;&#21040;&#19968;&#20010;&#22810;&#23454;&#20363;&#32858;&#21512;&#22120;&#20197;&#23545;&#25972;&#20010;&#20999;&#29255;&#36827;&#34892;&#32423;&#21035;&#39044;&#27979;&#12290;&#23613;&#31649;&#26377;&#21069;&#23548;&#30740;&#31350;&#24314;&#35758;&#22686;&#24378;&#22312; ImageNet-1K &#19978;&#39044;&#35757;&#32451;&#30340;&#28909;&#38376; ResNet50 &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#25552;&#21462;&#22120;&#20197;&#26368;&#22823;&#21270; WSI &#24615;&#33021;&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#22522;&#20110;&#22235;&#39033;&#26368;&#20808;&#36827;&#30340; MIL &#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#20849; WSIs &#25968;&#25454;&#38598; (TCGA-NSCLC &#21644; Camelyon16) &#19978;&#30340;&#19977;&#32500;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#39592;&#24178;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010; SOTA MIL &#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#28982;&#21518;&#34987;&#39304;&#36865;&#21040;&#19968;&#20010;&#22810;&#23454;&#20363;&#32858;&#21512;&#22120;&#20197;&#23545;&#25972;&#20010;&#20999;&#29255;&#36827;&#34892;&#32423;&#21035;&#39044;&#27979;&#12290;&#23613;&#31649;&#26377;&#21069;&#20027;&#23548;&#30740;&#31350;&#24314;&#35758;&#22686;&#24378;&#22312; ImageNet-1K &#19978;&#39044;&#35757;&#32451;&#30340;&#28909;&#38376; ResNet50 &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#25552;&#21462;&#22120;&#20197;&#26368;&#22823;&#21270; WSI &#24615;&#33021;&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#22522;&#20110;&#22235;&#39033;&#26368;&#20808;&#36827;&#30340; MIL &#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#20849; WSIs &#25968;&#25454;&#38598; (TCGA-NSCLC &#21644; Camelyon16) &#19978;&#30340;&#19977;&#32500;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#39592;&#24178;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010; SOTA MIL &#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#28982;&#21518;&#34987;&#39304;&#36865;&#21040;&#19968;&#20010;&#22810;&#23454;&#20363;&#32858;&#21512;&#22120;&#20197;&#23545;&#25972;&#20010;&#20999;&#29255;&#36827;&#34892;&#32423;&#21035;&#39044;&#27979;&#12290;&#23613;&#31649;&#26377;&#21069;&#20027;&#23548;&#30740;&#31350;&#24314;&#35758;&#22686;&#24378;&#22312; ImageNet-1K &#19978;&#39044;&#35757;&#32451;&#30340;&#28909;&#38376; ResNet50 &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#25552;&#21462;&#22120;&#20197;&#26368;&#22823;&#21270; WSI &#24615;&#33021;&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#22522;&#20110;&#22235;&#39033;&#26368;&#20808;&#36827;&#30340; MIL &#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#20849; WSIs &#25968;&#25454;&#38598; (TCGA-NSCLC &#21644; Camelyon16) &#19978;&#30340;&#19977;&#32500;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#39592;&#24178;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010; SOTA MIL &#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01167v1 Announce Type: new  Abstract: Multiple instance learning (MIL) has become a preferred method for classifying gigapixel whole slide images (WSIs), without requiring patch label annotation. The focus of the current MIL research stream is on the embedding-based MIL approach, which involves extracting feature vectors from patches using a pre-trained feature extractor. These feature vectors are then fed into an MIL aggregator for slide-level prediction. Despite prior research suggestions on enhancing the most commonly used ResNet50 supervised model pre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting the optimal feature extractor to maximize WSI performance. This study aims at addressing this gap by examining MIL feature extractors across three dimensions: pre-training dataset, backbone model, and pre-training method. Extensive experiments were carried out on the two public WSI datasets (TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The
&lt;/p&gt;</description></item><item><title>PreMix&#36890;&#36807;&#39044;&#35757;&#32451;&#20869;&#25209;&#37327;&#20999;&#29255;&#28151;&#21512;&#26041;&#27861;&#25552;&#39640;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01162</link><description>&lt;p&gt;
PreMix:&#36890;&#36807;&#39044;&#35757;&#32451;&#20869;&#25209;&#37327;&#20999;&#29255;&#28151;&#21512;&#25552;&#39640;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PreMix: Boosting Multiple Instance Learning in Digital Histopathology through Pre-training with Intra-Batch Slide Mixing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01162
&lt;/p&gt;
&lt;p&gt;
PreMix&#36890;&#36807;&#39044;&#35757;&#32451;&#20869;&#25209;&#37327;&#20999;&#29255;&#28151;&#21512;&#26041;&#27861;&#25552;&#39640;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01162v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: &#23545;&#20197;&#39640;&#20998;&#36776;&#29575;&#25195;&#25551;&#20202;&#33719;&#24471;&#30340;&#32452;&#32455;&#23398;&#20999;&#29255;&#25968;&#23383;&#21270;&#34920;&#31034;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;(WSIs)&#36827;&#34892;&#20998;&#31867;&#65292;&#38754;&#20020;&#30528;&#19982;&#32454;&#33268;&#19988;&#32791;&#26102;&#30340;&#31934;&#32454;&#31890;&#24230;&#26631;&#27880;&#30456;&#20851;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24369;&#30417;&#30563;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;(MIL)&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;MIL&#26041;&#27861;&#22312;&#20174;&#38646;&#24320;&#22987;&#22312;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#20013;&#35757;&#32451;MIL&#29305;&#24449;&#32858;&#21512;&#22120;&#30340;&#33021;&#21147;&#19978;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#36825;&#24448;&#24448;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#26410;&#26631;&#27880;WSIs&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;MIL&#29305;&#24449;&#32858;&#21512;&#22120;&#65292;&#20174;&#32780;&#38459;&#30861;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;PreMix&#36890;&#36807;&#20869;&#25209;&#37327;&#20999;&#29255;&#28151;&#21512;&#26041;&#27861;&#25193;&#23637;&#20102;MIL&#26694;&#26550;&#30340;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;PreMix&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32467;&#21512;&#20102;Barlow Twins Slide Mixing&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#19981;&#21516;WSIs&#22823;&#23567;&#30340;&#33021;&#21147;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#20102;&#26410;&#26631;&#27880;WSIs&#30340;&#20215;&#20540;&#12290;&#32467;&#21512;&#39640;&#20998;&#36776;&#29575;&#37319;&#26679;&#21644;&#21160;&#24577;&#38543;&#26426;&#25237;&#24433;&#65292;PreMix&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#33021;&#22815;&#36739;&#22909;&#22320;&#21306;&#20998;&#30149;&#29702;&#21644;&#27491;&#24120;&#32454;&#32990;&#65292;&#21363;&#20351;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#20219;&#21153;&#20013;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;MIL&#26041;&#27861;&#30456;&#27604;&#65292;PreMix&#22312;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01162v1 Announce Type: new  Abstract: The classification of gigapixel-sized whole slide images (WSIs), digital representations of histological slides obtained via a high-resolution scanner, faces significant challenges associated with the meticulous and time-consuming nature of fine-grained labeling. While weakly-supervised multiple instance learning (MIL) has emerged as a promising approach, current MIL methods are constrained by their limited ability to leverage the wealth of information embedded within unlabeled WSIs. This limitation often necessitates training MIL feature aggregators from scratch after the feature extraction process, hindering efficiency and accuracy. PreMix extends the general MIL framework by pre-training the MIL aggregator with an intra-batch slide mixing approach. Specifically, PreMix incorporates Barlow Twins Slide Mixing during pre-training, enhancing its ability to handle diverse WSI sizes and maximizing the utility of unlabeled WSIs. Combined wit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#22312;&#20840;&#29699;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#38543;&#39057;&#29575;&#30340;&#25351;&#25968;&#19979;&#38477;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20302;&#39057;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#27491;&#38754;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#39640;&#39057;&#29575;&#20449;&#21495;&#30340;&#36129;&#29486;&#19982;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#36127;&#30456;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2408.01139</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#29702;&#35889;&#37325;&#35201;&#24615;&#20998;&#35299;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#30340;&#20840;&#23616;&#25200;&#21160;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01139
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#22312;&#20840;&#29699;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#38543;&#39057;&#29575;&#30340;&#25351;&#25968;&#19979;&#38477;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20302;&#39057;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#27491;&#38754;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#39640;&#39057;&#29575;&#20449;&#21495;&#30340;&#36129;&#29486;&#19982;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#36127;&#30456;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01139v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25200;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#20102;&#27169;&#22411;&#23545;&#21508;&#31181;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#27745;&#26579;&#21644; adversarial&#25915;&#20987;&#12290;&#29702;&#35299;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#26426;&#21046;&#23545;&#20110;&#20840;&#23616;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#26426;&#21046;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#30340;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#21551;&#21457;&#12290;&#39318;&#20808;&#65292;&#20197;&#21069;&#30340;&#20840;&#29699;&#35299;&#37322;&#24615;&#24037;&#20316;&#19982;&#40065;&#26834;&#24615;&#22522;&#20934;&#65288;&#20363;&#22914;&#24179;&#22343;&#27745;&#26579;&#38169;&#35823;mCE&#65289;&#21516;&#26102;&#36827;&#34892;&#65292;&#24182;&#19981;&#26159;&#20026;&#20102;&#30452;&#25509;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#20013;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#38543;&#39057;&#29575;&#25351;&#25968;&#19979;&#38477;&#12290;&#36825;&#31181;&#24130;&#24459;&#31867;&#20284;&#30340;&#19979;&#38477;&#34920;&#26126;&#65306;&#20302;&#39057;&#20449;&#21495;&#36890;&#24120;&#27604;&#39640;&#39057;&#20449;&#21495;&#26356;&#40065;&#26834;&#8212;&#8212;&#28982;&#32780;&#65292;&#39640;&#20998;&#31867;&#31934;&#24230;&#24182;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#27934;&#23519;&#21040;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;-mCE&#21644;&#39640;&#39057;&#20449;&#21495;&#30340;&#36129;&#29486;&#26377;&#36127;&#30456;&#20851;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#39640;&#39057;&#20449;&#21495;&#36739;&#23567;&#30340;&#22270;&#20687;&#21306;&#22495;&#20013;&#65292;&#21363;&#20351;&#23384;&#22312;&#39640;&#22122;&#22768;&#27700;&#24179;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20063;&#24456;&#39640;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#39640;&#39057;&#29575;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#36127;&#38754;&#20316;&#29992;&#65292;&#24182;&#20026;&#27169;&#22411;&#32467;&#26500;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#23545;&#20110;&#36731;&#24230;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#20063;&#20542;&#21521;&#20110;&#22312;&#20302;&#39057;&#20449;&#21495;&#26356;&#22823;&#30340;&#31354;&#38388;&#21306;&#22495;&#20013;&#20445;&#25345;&#26356;&#39640;&#30340;SNR&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#23545;&#22270;&#20687;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20840;&#29699;&#35299;&#37322;&#24615;&#29702;&#35299;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21518;&#32493;&#30340;&#27169;&#22411;&#35774;&#35745;&#12289;&#29702;&#35299;&#21644;&#20248;&#21270;&#24037;&#20316;&#12290;&#19979;&#26041;&#26159;&#35813;&#35770;&#25991;&#30340;&#33521;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#65292;&#35831;&#27880;&#24847;&#65292;&#23545;&#20110;&#20197;&#19979;&#30340;tldr&#21644;en_tldr&#37096;&#20998;&#65292;&#25105;&#20250;&#24635;&#32467;&#20986;&#19968;&#20010;&#20013;&#25991;&#21644;&#33521;&#25991;&#29256;&#30340;&#27010;&#35201;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01139v1 Announce Type: cross  Abstract: Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals -- yet high classification accuracy can not be ac
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PGNeXt&#30340;&#20840;&#26032;&#21333;&#38454;&#27573;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;Transformer&#21644;CNN&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#37319;&#26679;&#28145;&#24230;&#19982;&#24863;&#21463;&#37326;&#22823;&#23567;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01137</link><description>&lt;p&gt;
PGNeXt&#65306;&#22522;&#20110;&#37329;&#23383;&#22612;&#23233;&#25509;&#32593;&#32476;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01137
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PGNeXt&#30340;&#20840;&#26032;&#21333;&#38454;&#27573;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;Transformer&#21644;CNN&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#37319;&#26679;&#28145;&#24230;&#19982;&#24863;&#21463;&#37326;&#22823;&#23567;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#26356;&#39640;&#38590;&#24230;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#65288;UHRSD&#65289;&#65292;&#20174;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26694;&#26550;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#32771;&#37327;&#12290;&#20026;&#20102;&#24357;&#34917;&#36807;&#21435;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#65288;HRSOD&#65289;&#25968;&#25454;&#38598;&#30340;&#31354;&#32570;&#65292;&#25105;&#20204;&#32454;&#24515;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21363;UHRSD&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;5,920&#24352;&#22270;&#29255;&#65292;&#20998;&#36776;&#29575;&#36798;&#21040;&#20102;4K-8K&#32423;&#21035;&#12290;&#25152;&#26377;&#30340;&#22270;&#29255;&#22343;&#36827;&#34892;&#20102;&#31934;&#32454;&#30340;&#20687;&#32032;&#32423;&#21035;&#26631;&#27880;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;&#20808;&#21069;&#20302;&#20998;&#36776;&#29575;SOD&#25968;&#25454;&#38598;&#30340;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36807;&#21435;&#26041;&#27861;&#20013;&#37319;&#26679;&#28145;&#24230;&#19982;&#24863;&#21463;&#37326;&#22823;&#23567;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;HR-SOD&#20219;&#21153;&#30340;&#20840;&#26032;&#21333;&#38454;&#27573;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#37329;&#23383;&#22612;&#23233;&#25509;&#26426;&#21046;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#22522;&#20110;Transformer&#21644;CNN&#30340;&#39592;&#24178;&#32593;&#32476;&#29420;&#31435;&#25552;&#21462;&#19981;&#21516;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#29305;&#24449;&#20174;Transformer&#20998;&#25903;&#23233;&#25509;&#21040;CNN&#20998;&#25903;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36328;&#27169;&#22411;&#23233;&#25509;&#27169;&#22359;&#65288;CMGM&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#27169;&#22411;&#30340;&#24378;&#20851;&#32852;&#24895;&#24847;&#65292;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01137v1 Announce Type: new  Abstract: We present an advanced study on more challenging high-resolution salient object detection (HRSOD) from both dataset and network framework perspectives. To compensate for the lack of HRSOD dataset, we thoughtfully collect a large-scale high resolution salient object detection dataset, called UHRSD, containing 5,920 images from real-world complex scenarios at 4K-8K resolutions. All the images are finely annotated in pixel-level, far exceeding previous low-resolution SOD datasets. Aiming at overcoming the contradiction between the sampling depth and the receptive field size in the past methods, we propose a novel one-stage framework for HR-SOD task using pyramid grafting mechanism. In general, transformer-based and CNN-based backbones are adopted to extract features from different resolution images independently and then these features are grafted from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) i
&lt;/p&gt;</description></item><item><title>IG-SLAM&#26159;&#19968;&#31181;&#20165;&#20351;&#29992;RGB&#30340;&#23494;&#38598;SLAM&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20581;&#22766;&#30340;&#23494;&#38598;SLAM&#36319;&#36394;&#26041;&#27861;&#21644;&#39640;&#26031;&#26001;&#28857;&#25216;&#26415;&#65292;&#20197;&#26500;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#23039;&#24577;&#21644;&#23494;&#38598;&#28145;&#24230;&#65292;&#31995;&#32479;&#33021;&#22815;&#20248;&#21270;&#22320;&#22270;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#34928;&#20943;&#31574;&#30053;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#36816;&#34892;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01126</link><description>&lt;p&gt;
IG-SLAM: &#21363;&#26102;&#39640;&#26031;SLAM
&lt;/p&gt;
&lt;p&gt;
IG-SLAM: Instant Gaussian SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01126
&lt;/p&gt;
&lt;p&gt;
IG-SLAM&#26159;&#19968;&#31181;&#20165;&#20351;&#29992;RGB&#30340;&#23494;&#38598;SLAM&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20581;&#22766;&#30340;&#23494;&#38598;SLAM&#36319;&#36394;&#26041;&#27861;&#21644;&#39640;&#26031;&#26001;&#28857;&#25216;&#26415;&#65292;&#20197;&#26500;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#23039;&#24577;&#21644;&#23494;&#38598;&#28145;&#24230;&#65292;&#31995;&#32479;&#33021;&#22815;&#20248;&#21270;&#22320;&#22270;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#34928;&#20943;&#31574;&#30053;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#36816;&#34892;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01126v1 &#23459;&#35328;&#31867;&#22411;: &#20132;&#21449; Abstract: 3D&#39640;&#26031;&#26001;&#28857;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;SLAM&#31995;&#32479;&#20013;&#22330;&#26223;&#34920;&#31034;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#19982;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#29992;&#20110;&#25351;&#23548;&#26144;&#23556;&#36807;&#31243;&#30340;&#23494;&#38598;&#28145;&#24230;&#22270;&#65292;&#35201;&#20040;&#22312;&#32771;&#34385;&#29615;&#22659;&#22823;&#23567;&#26102;&#27809;&#26377;&#35814;&#23613;&#30340;&#35757;&#32451;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IG-SLAM&#65292;&#19968;&#20010;&#20165;&#20351;&#29992;RGB&#30340;&#23494;&#38598;SLAM&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#20102;&#20581;&#22766;&#30340;&#23494;&#38598;SLAM&#36319;&#36394;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#39640;&#26031;&#26001;&#28857;&#30456;&#32467;&#21512;&#12290;&#20351;&#29992;&#36319;&#36394;&#25552;&#20379;&#30340;&#31934;&#30830;&#23039;&#24577;&#21644;&#23494;&#38598;&#28145;&#24230;&#65292;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#34987;&#26500;&#24314;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#26469;&#20248;&#21270;&#22320;&#22270;&#65292;&#20197;&#25552;&#39640;3D&#37325;&#24314;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#22320;&#22270;&#20248;&#21270;&#20013;&#20351;&#29992;&#30340;&#34928;&#20943;&#31574;&#30053;&#25552;&#39640;&#20102;&#25910;&#25947;&#24615;&#65292;&#24182;&#20801;&#35768;&#31995;&#32479;&#22312;&#19968;&#36827;&#31243;&#20013;&#20197;10&#24103;&#27599;&#31186;&#30340;&#36895;&#24230;&#36816;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#19982;&#29616;&#26377;&#25216;&#26415;&#20808;&#36827;&#30340;&#32431;RGB SLAM&#31995;&#32479;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01126v1 Announce Type: cross  Abstract: 3D Gaussian Splatting has recently shown promising results as an alternative scene representation in SLAM systems to neural implicit representations. However, current methods either lack dense depth maps to supervise the mapping process or detailed training designs that consider the scale of the environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only SLAM system that employs robust Dense-SLAM methods for tracking and combines them with Gaussian Splatting. A 3D map of the environment is constructed using accurate pose and dense depth provided by tracking. Additionally, we utilize depth uncertainty in map optimization to improve 3D reconstruction. Our decay strategy in map optimization enhances convergence and allows the system to run at 10 fps in a single process. We demonstrate competitive performance with state-of-the-art RGB-only SLAM systems while achieving faster operation speeds. We present our experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20445;&#25345;&#39640;&#25928;&#30340;&#21516;&#26102;&#20063;&#33021;&#26377;&#25928;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35805;&#35821;&#22659;&#19979;&#30340;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01120</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#25928;&#30340;Transformer&#35299;&#30721;&#22120;&#22522;&#22810;&#20219;&#21153;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20445;&#25345;&#39640;&#25928;&#30340;&#21516;&#26102;&#20063;&#33021;&#26377;&#25928;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35805;&#35821;&#22659;&#19979;&#30340;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#20381;&#36182;&#20110;Transformer&#26469;&#34701;&#21512;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#12290;&#20294;&#36825;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#32570;&#38519;&#65306;Transformer&#32534;&#30721;&#22120;&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#23548;&#33268;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#26041;&#22686;&#38271;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#25110;&#32773;&#38271;&#25991;&#26412;&#19978;&#19979;&#25991;&#26102;&#12290;&#36825;&#31181;&#35745;&#31639;&#25104;&#26412;&#30340;&#20108;&#27425;&#22686;&#38271;&#38480;&#21046;&#20102;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#22312;&#26356;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#23545;&#35805;&#25512;&#29702;&#20999;&#29255;&#65292;&#23427;&#38656;&#35201;&#38271;&#31687;&#30340;&#35821;&#35328;&#34920;&#36798;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#23450;&#20301;&#65288;EEVG&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#26041;&#38754;&#37117;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#22312;&#35821;&#35328;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;Transformer&#35299;&#30721;&#22120;&#26469;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#20854;&#20013;&#35821;&#35328;&#29305;&#24449;&#34987;&#36755;&#20837;&#20316;&#20026;&#35760;&#24518;&#65292;&#32780;&#35270;&#35273;&#29305;&#24449;&#21017;&#26159;&#27839;&#30528;&#32534;&#30721;&#22120;&#36880;&#23618;&#20256;&#36882;&#12290;&#36825;&#31181;&#35774;&#35745;&#22823;&#22823;&#20943;&#23569;&#20102;&#32467;&#26500;&#21270;&#30340;&#35821;&#35328;&#29305;&#24449;&#19982;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#19968;&#31995;&#21015;&#30340;&#33258;&#25105;&#27880;&#24847;&#25805;&#20316;&#30340;&#35745;&#31639;&#20195;&#20215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#21487;&#37325;&#29992;&#32452;&#20214;&#65292;&#22914;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#25928;&#34701;&#21512;&#35821;&#35328;&#20449;&#24687;&#21644;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#26356;&#24555;&#22320;&#25552;&#20379;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;EEVG&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#35821;&#22659;&#19979;&#30340;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#20855;&#20307;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#26631;&#31614;&#19978;&#19979;&#25991;&#26102;&#65292;&#20063;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01120v1 Announce Type: new  Abstract: Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20943;&#36731;&#26356;&#22823;&#39046;&#22495;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;m-PPOT&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#32622;&#26435;&#37325;&#21644;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#36328;&#39046;&#22495;&#36866;&#37197;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01089</link><description>&lt;p&gt;
&#21322;&#20248;&#21270;&#30340;&#20856;&#22411;&#37096;&#20998;&#26368;&#20248;&#36816;&#36755;&#29702;&#35770;&#22312;&#36328;&#39046;&#22495;&#36866;&#37197;&#20013;&#30340;&#26222;&#36941;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Prototypical Partial Optimal Transport for Universal Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20943;&#36731;&#26356;&#22823;&#39046;&#22495;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;m-PPOT&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#32622;&#26435;&#37325;&#21644;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#36328;&#39046;&#22495;&#36866;&#37197;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#36866;&#37197;&#65288;UniDA&#65289;&#26088;&#22312;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#39046;&#22495;&#21521;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#39046;&#22495;&#20256;&#36755;&#30693;&#35782;&#65292;&#26080;&#38656;&#20004;&#32773;&#20855;&#26377;&#30456;&#21516;&#30340;&#26631;&#31614;&#38598;&#12290;&#39046;&#22495;&#21644;&#31867;&#21035;&#20559;&#31227;&#30340;&#23384;&#22312;&#20351;&#24471;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#35201;&#27714;&#25105;&#20204;&#23545;&#20004;&#32773;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#21306;&#20998;&#65292;&#36825;&#20123;&#26679;&#26412;&#20855;&#26377;&#22312;&#39046;&#22495;&#20013;&#30340;&#26631;&#31614;&#65292;&#20294;&#21516;&#26102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#21364;&#27809;&#26377;&#26631;&#31614;&#12290;&#22312;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#20998;&#24067;&#21305;&#37197;&#38382;&#39064;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#20998;&#24067;&#36827;&#34892;&#37096;&#20998;&#23545;&#40784;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mini-batch Prototypical Partial Optimal Transport&#65288;m-PPOT&#65289;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;UniDA&#36827;&#34892;&#37096;&#20998;&#20998;&#24067;&#23545;&#40784;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#38500;&#20102;&#26368;&#23567;&#21270;m-PPOT&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#36816;&#36755;&#35745;&#21010;&#20013;&#37325;&#32622;&#28304;&#21407;&#22411;&#21644;&#30446;&#26631;&#26679;&#26412;&#30340;&#26435;&#37325;&#65292;&#24182;&#35774;&#35745;&#20102;&#37325;&#32622;&#29109;&#25439;&#22833;&#21644;&#37325;&#32622;&#26435;&#37325;&#26469;&#25913;&#36827;&#23398;&#20064;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;uniform converage criterion&#65292;&#24182;&#23450;&#20041;&#20102;&#22522;&#20110;&#35813;&#20934;&#21017;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#21152;&#24378;&#20102;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#36328;&#39046;&#22495;&#36866;&#37197;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#36731;&#39046;&#22495;&#24046;&#24322;&#21644;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01089v1 Announce Type: new  Abstract: Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain without requiring the same label sets of both domains. The existence of domain and category shift makes the task challenging and requires us to distinguish "known" samples (i.e., samples whose labels exist in both domains) and "unknown" samples (i.e., samples whose labels exist in only one domain) in both domains before reducing the domain gap. In this paper, we consider the problem from the point of view of distribution matching which we only need to align two distributions partially. A novel approach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is proposed to conduct partial distribution alignment for UniDA. In training phase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT to reweight source prototypes and target samples, and design reweighted entropy loss and reweigh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PhysMamba&#30340;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;Mamba-2&#27169;&#22411;&#30340;&#29366;&#24577;&#21644;&#21452;&#27969;&#26550;&#26500;&#26469;&#22686;&#24378;rPPG&#20449;&#21495;&#22312;&#23545;&#25239;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#36328;&#27880;&#24847;&#21147;&#29366;&#24577;&#31354;&#38388;&#23545;&#20598;&#27169;&#22359;&#25552;&#21319;&#20102;&#29305;&#24449;&#20114;&#34917;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01077</link><description>&lt;p&gt;
PhysMamba: &#21033;&#29992;&#21452;&#27969;&#36328;&#27880;&#24847;&#21147;SSD&#36827;&#34892;&#36828;&#31243;&#29983;&#29702;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote Physiological Measurement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PhysMamba&#30340;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;Mamba-2&#27169;&#22411;&#30340;&#29366;&#24577;&#21644;&#21452;&#27969;&#26550;&#26500;&#26469;&#22686;&#24378;rPPG&#20449;&#21495;&#22312;&#23545;&#25239;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#36328;&#27880;&#24847;&#21147;&#29366;&#24577;&#31354;&#38388;&#23545;&#20598;&#27169;&#22359;&#25552;&#21319;&#20102;&#29305;&#24449;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01077v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#36828;&#31243;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;rPPG&#65289;&#26159;&#19968;&#31181;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#25552;&#21462;&#29983;&#29702;&#20449;&#21495;&#30340;&#38750;&#25509;&#35302;&#25216;&#26415;&#65292;&#29992;&#20110;&#24773;&#32490;&#30417;&#27979;&#12289;&#21307;&#30103;&#25588;&#21161;&#21644;&#21453;&#38754;&#37096;&#27450;&#39575;&#31561;&#24212;&#29992;&#12290;&#19982;&#21463;&#25511;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#30456;&#27604;&#65292;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#32463;&#24120;&#23384;&#22312;&#36816;&#21160;&#24178;&#25200;&#21644;&#22122;&#22768;&#65292;&#36825;&#24433;&#21709;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhysMamba&#65292;&#19968;&#20010;&#22522;&#20110;Mamba&#30340;&#26102;&#38388;&#39057;&#35889;&#20132;&#20114;&#21452;&#27969;&#27169;&#22411;&#12290;PhysMamba&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;Mamba-2&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#20102;&#21452;&#27969;&#26550;&#26500;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;rPPG&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#22122;&#22768;&#26465;&#20214;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#36328;&#27880;&#24847;&#21147;&#29366;&#24577;&#31354;&#38388;&#23545;&#20598;&#65288;CASSD&#65289;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#20004;&#20010;&#27969;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#21644;&#29305;&#24449;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;PURE&#12289;UBFC-rPPG&#21644;MMPD&#39564;&#35777;&#20102;PhysMamba&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PhysMamba&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01077v1 Announce Type: new  Abstract: Remote Photoplethysmography (rPPG) is a non-contact technique for extracting physiological signals from facial videos, used in applications like emotion monitoring, medical assistance, and anti-face spoofing. Unlike controlled laboratory settings, real-world environments often contain motion artifacts and noise, affecting the performance of existing methods. To address this, we propose PhysMamba, a dual-stream time-frequency interactive model based on Mamba. PhysMamba integrates the state-of-the-art Mamba-2 model and employs a dual-stream architecture to learn diverse rPPG features, enhancing robustness in noisy conditions. Additionally, we designed the Cross-Attention State Space Duality (CASSD) module to improve information exchange and feature complementarity between the two streams. We validated PhysMamba using PURE, UBFC-rPPG and MMPD. Experimental results show that PhysMamba achieves state-of-the-art performance across various scen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#35821;&#20041;&#30693;&#35782;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#21644;&#20219;&#21153;&#20869;&#37096;&#25972;&#21512;&#35821;&#20041;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26032;&#25968;&#25454;&#30340;&#21487;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#20445;&#25345;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01076</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#35821;&#20041;&#30693;&#35782;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#35821;&#20041;&#30693;&#35782;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#21644;&#20219;&#21153;&#20869;&#37096;&#25972;&#21512;&#35821;&#20041;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26032;&#25968;&#25454;&#30340;&#21487;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#20445;&#25345;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01076v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032;&#25253;&#21578;&#31867;&#22411; &#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#20013;&#23398;&#20064;&#26032;&#25968;&#25454;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;&#25345;&#32493;&#23398;&#20064;&#36825;&#19968;&#25361;&#25112;&#26088;&#22312;&#20801;&#35768;&#27169;&#22411;&#22312;&#26032;&#30693;&#35782;&#23398;&#20064;&#30340;&#21516;&#26102;&#20445;&#30041;&#20197;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35270;&#35273;&#29305;&#24449;&#65292;&#32463;&#24120;&#24573;&#35270;&#22270;&#20687;&#26631;&#31614;&#20449;&#24687;&#20013;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#12290;&#22270;&#20687;&#26631;&#31614;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#20379;&#20102;&#19982;&#20808;&#21069;&#33719;&#24471;&#30340;&#20998;&#31867;&#30693;&#35782;&#30456;&#20851;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#24212;&#35813;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20219;&#21153;&#20043;&#38388;&#21644;&#20219;&#21153;&#20869;&#37096;&#25972;&#21512;&#35821;&#20041;&#24341;&#23548;&#12290;&#25105;&#20204;&#20174;CLIP&#27169;&#22411;&#24320;&#22987;&#65292;&#20351;&#29992;&#8220;Semantically-guided Representation Learning&#65288;SG-RL&#65289;&#8221;&#27169;&#22359;&#36827;&#34892;&#36719;&#20998;&#37197;&#65292;&#22312;&#20219;&#21153;T&#30340;&#30417;&#30563;&#19979;&#24471;&#20998;&#26368;&#22909;&#30340;&#25991;&#26412;embedding&#65292;&#35813;&#23884;&#20837;&#19982;&#26032;&#30340;&#20219;&#21153;T'&#20013;&#26816;&#32034;&#21040;&#30340;&#31867;&#21035;&#30456;&#20851;&#30340;&#22270;&#29255;embedding&#30456;&#20284;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#25913;&#32593;&#32476;&#22312;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#36801;&#31227;&#21040;&#27969;&#34892;&#30340;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#26102;&#33021;&#22815;&#36827;&#34892;&#21487;&#36801;&#31227;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#35821;&#20041;&#25351;&#23548;&#30340;&#25345;&#32493;&#23398;&#20064;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20445;&#25345;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01076v1 Announce Type: new  Abstract: Deep neural networks (DNNs) excel on fixed datasets but struggle with incremental and shifting data in real-world scenarios. Continual learning addresses this challenge by allowing models to learn from new data while retaining previously learned knowledge. Existing methods mainly rely on visual features, often neglecting the rich semantic information encoded in text. The semantic knowledge available in the label information of the images, offers important semantic information that can be related with previously acquired knowledge of semantic classes. Consequently, effectively leveraging this information throughout continual learning is expected to be beneficial. To address this, we propose integrating semantic guidance within and across tasks by capturing semantic similarity using text embeddings. We start from a pre-trained CLIP model, employ the \emph{Semantically-guided Representation Learning (SG-RL)} module for a soft-assignment tow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#20998;&#21106;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#33145;&#33108;&#38236;&#25163;&#26415;&#20013;&#22120;&#26800;&#30340;&#23436;&#25972;&#36974;&#25377;&#37096;&#20998;&#65292;&#25552;&#39640;&#20102;&#25163;&#26415;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01067</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#20998;&#21106;&#27861;&#22312;&#33145;&#33108;&#38236;&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Amodal Segmentation for Laparoscopic Surgery Video Instruments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#20998;&#21106;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#33145;&#33108;&#38236;&#25163;&#26415;&#20013;&#22120;&#26800;&#30340;&#23436;&#25972;&#36974;&#25377;&#37096;&#20998;&#65292;&#25552;&#39640;&#20102;&#25163;&#26415;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01067v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; Abstract: &#20998;&#21106;&#25163;&#26415;&#22120;&#26800;&#23545;&#20110;&#25552;&#39640;&#25163;&#26415;&#36136;&#37327;&#24182;&#30830;&#20445;&#24739;&#32773;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#20687;&#20108;&#20540;&#12289;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#36825;&#26679;&#30340;&#20256;&#32479;&#26041;&#27861;&#37117;&#26377;&#20849;&#21516;&#30340;&#38382;&#39064;&#65306;&#23427;&#20204;&#19981;&#33021;&#22788;&#29702;&#37027;&#20123;&#34987;&#32452;&#32455;&#25110;&#20854;&#23427;&#22120;&#26800;&#36974;&#25377;&#30340;&#22120;&#26800;&#37096;&#20998;&#12290;&#31934;&#30830;&#39044;&#27979;&#36825;&#20123;&#36974;&#25377;&#37096;&#20998;&#30340;&#20840;&#35980;&#21487;&#20197;&#22312;&#22810;&#20010;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#33145;&#33108;&#38236;&#25163;&#26415;&#65292;&#21253;&#25324;&#22312;&#25163;&#26415;&#36807;&#31243;&#20013;&#25552;&#20379;&#20851;&#38190;&#30340;&#25351;&#23548;&#12289;&#21327;&#21161;&#20998;&#26512;&#28508;&#22312;&#30340;&#25163;&#26415;&#38169;&#35823;&#20197;&#21450;&#20026;&#25945;&#32946;&#30446;&#30340;&#25552;&#20379;&#26381;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36328;&#27169;&#24577;&#20998;&#21106;&#25216;&#26415;&#24341;&#20837;&#21040;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#25163;&#26415;&#22120;&#26800;&#35782;&#21035;&#39046;&#22495;&#12290;&#36825;&#39033;&#25216;&#26415;&#33021;&#22815;&#35782;&#21035;&#29289;&#20307;&#30340;&#21487;&#35265;&#21644;&#36974;&#25377;&#37096;&#20998;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#27169;&#24577;&#22120;&#20855;&#20998;&#21106;&#65288;AIS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#37325;&#26032;&#27880;&#37322;&#27599;&#20010;&#22120;&#26800;&#30340;&#20840;&#35980;&#65292;&#21033;&#29992;&#20102;2017&#24180;MICCAI EndoVis Robotic&#25163;&#26415;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01067v1 Announce Type: new  Abstract: Segmentation of surgical instruments is crucial for enhancing surgeon performance and ensuring patient safety. Conventional techniques such as binary, semantic, and instance segmentation share a common drawback: they do not accommodate the parts of instruments obscured by tissues or other instruments. Precisely predicting the full extent of these occluded instruments can significantly improve laparoscopic surgeries by providing critical guidance during operations and assisting in the analysis of potential surgical errors, as well as serving educational purposes. In this paper, we introduce Amodal Segmentation to the realm of surgical instruments in the medical field. This technique identifies both the visible and occluded parts of an object. To achieve this, we introduce a new Amoal Instruments Segmentation (AIS) dataset, which was developed by reannotating each instrument with its complete mask, utilizing the 2017 MICCAI EndoVis Robotic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20687;&#32032;&#32423;&#30417;&#30563;&#30340;&#35270;&#32447;&#23545;&#35937;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;VFM&#30340;&#38598;&#25104;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#32447;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01044</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20687;&#32032;&#32423;&#30417;&#30563;&#25552;&#21319;&#35270;&#32447;&#23545;&#35937;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20687;&#32032;&#32423;&#30417;&#30563;&#30340;&#35270;&#32447;&#23545;&#35937;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;VFM&#30340;&#38598;&#25104;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#32447;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#32447;&#23545;&#35937;&#20998;&#21106;(GOS)&#65292;&#23427;&#26088;&#22312;&#25512;&#26029;&#20154;&#31867;&#35270;&#32447;&#34892;&#20026;&#25429;&#33719;&#30340;&#23545;&#35937;&#30340;&#20687;&#32032;&#32423;&#25513;&#30721;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#32447;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#26469;&#33258;VFM&#30340;&#20687;&#32032;&#32423;&#30417;&#30563;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#30001;&#20110;&#23545;&#35937;&#38752;&#24471;&#22826;&#36817;&#32780;&#20135;&#29983;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35270;&#32447;&#23545;&#35937;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01044v1 Announce Type: new  Abstract: Gaze object prediction (GOP) aims to predict the category and location of the object that a human is looking at. Previous methods utilized box-level supervision to identify the object that a person is looking at, but struggled with semantic ambiguity, ie, a single box may contain several items since objects are close together. The Vision foundation model (VFM) has improved in object segmentation using box prompts, which can reduce confusion by more precisely locating objects, offering advantages for fine-grained prediction of gaze objects. This paper presents a more challenging gaze object segmentation (GOS) task, which involves inferring the pixel-level mask corresponding to the object captured by human gaze behavior. In particular, we propose that the pixel-level supervision provided by VFM can be integrated into gaze object prediction to mitigate semantic ambiguity. This leads to our gaze object detection and segmentation framework th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#39072;&#31800;&#25968;&#25454;&#22359;&#20043;&#38388;&#36827;&#34892;&#39640;&#26031;&#22122;&#22768;&#28151;&#21512;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01040</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#21644;&#22122;&#26434;&#20999;&#28151;&#20998;&#24335; vision transformers &#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Split Learning with Vision Transformers using Patch-Wise Random and Noisy CutMix
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#39072;&#31800;&#25968;&#25454;&#22359;&#20043;&#38388;&#36827;&#34892;&#39640;&#26031;&#22122;&#22768;&#28151;&#21512;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01040v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;vision transformer&#65288;ViT&#65289;&#30001;&#20110;&#20854;&#25552;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#36880;&#28176;&#36229;&#36807;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#28982;&#32780;&#65292;ViT&#30340;&#22823;&#27169;&#22411;&#23610;&#23544;&#21644;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#20998;&#25104;&#23398;&#20064;&#65288;SL&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#30340;&#36164;&#28304;&#26469;&#35757;&#32451;ViT&#65292;&#21516;&#26102;&#21033;&#29992;&#20998;&#24067;&#22312;&#35774;&#22791;&#19978;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;SL&#22312;&#35774;&#22791;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#36825;&#21487;&#33021;&#20250;&#26292;&#38706;&#20110;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#25915;&#20987;&#20013;&#12290;&#20026;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20943;&#23569;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21463;CutMix&#27491;&#21017;&#21270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;SL&#26694;&#26550;&#65292;&#23427;&#21521;&#39072;&#31800;&#25968;&#25454;&#20013;&#27880;&#20837;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#22312;&#23458;&#25143;&#31471;&#30340;&#38543;&#26426;&#36873;&#25321;&#30340;&#39072;&#31800;&#25968;&#25454;&#22359;&#20043;&#38388;&#36827;&#34892;&#28151;&#21512;&#65292;&#34987;&#31216;&#20026;DP-CutMixSL&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;DP-CutMixSL&#26159;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01040v1 Announce Type: cross  Abstract: In computer vision, the vision transformer (ViT) has increasingly superseded the convolutional neural network (CNN) for improved accuracy and robustness. However, ViT's large model sizes and high sample complexity make it difficult to train on resource-constrained edge devices. Split learning (SL) emerges as a viable solution, leveraging server-side resources to train ViTs while utilizing private data from distributed devices. However, SL requires additional information exchange for weight updates between the device and the server, which can be exposed to various attacks on private training data. To mitigate the risk of data breaches in classification tasks, inspired from the CutMix regularization, we propose a novel privacy-preserving SL framework that injects Gaussian noise into smashed data and mixes randomly chosen patches of smashed data across clients, coined DP-CutMixSL. Our analysis demonstrates that DP-CutMixSL is a differenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#23545;&#26410;&#30693;&#24418;&#29366;&#30340;&#31354;&#38388;&#30862;&#29255;&#36827;&#34892;&#36816;&#21160;&#20272;&#35745;&#65292;&#24182;&#21516;&#26102;&#36755;&#20986;&#29289;&#20307;&#30340;&#19981;&#26126;&#24418;&#29366;&#21644;&#30456;&#20851;&#25668;&#20687;&#26426;&#30340;&#30456;&#23545;&#23039;&#24577;&#36712;&#36857;&#65292;&#36825;&#20123;&#20449;&#24687;&#29992;&#20110;&#31934;&#30830;&#20272;&#35745;&#30446;&#26631;&#30340;&#36816;&#21160;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2408.01035</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#20840;&#21160;&#24577;&#20272;&#35745;&#21644;&#26410;&#30693;&#24418;&#29366;&#31354;&#38388;&#30862;&#29255;&#30340;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Structure from Motion-based Motion Estimation and 3D Reconstruction of Unknown Shaped Space Debris
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#23545;&#26410;&#30693;&#24418;&#29366;&#30340;&#31354;&#38388;&#30862;&#29255;&#36827;&#34892;&#36816;&#21160;&#20272;&#35745;&#65292;&#24182;&#21516;&#26102;&#36755;&#20986;&#29289;&#20307;&#30340;&#19981;&#26126;&#24418;&#29366;&#21644;&#30456;&#20851;&#25668;&#20687;&#26426;&#30340;&#30456;&#23545;&#23039;&#24577;&#36712;&#36857;&#65292;&#36825;&#20123;&#20449;&#24687;&#29992;&#20110;&#31934;&#30830;&#20272;&#35745;&#30446;&#26631;&#30340;&#36816;&#21160;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01035v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#38543;&#30528;&#36817;&#24180;&#26469; spacecraft &#21457;&#23556;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#31354;&#38388;&#30862;&#29255;&#38382;&#39064;&#26085;&#30410;&#25104;&#20026;&#20154;&#31867;&#24517;&#39035;&#38754;&#23545;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31354;&#38388;&#21033;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#22312;&#36712;&#30340;&#30862;&#29255;&#28165;&#38500;&#20219;&#21153;&#21487;&#38752;&#24615;&#26159;&#20854;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#36712;&#28165;&#38500;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#65292;&#30446;&#26631;&#30340;&#39640;&#31934;&#24230;&#21160;&#37327;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#30862;&#29255;&#22833;&#25481;&#20102;&#23039;&#24577;&#21644;&#36712;&#36947;&#25511;&#21046;&#33021;&#21147;&#65292;&#20197;&#21450;&#24418;&#29366;&#22240;&#30772;&#30862;&#19981;&#26126;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#26377;&#38480;&#36164;&#28304;&#26469;&#25191;&#34892;&#19981;&#26126;&#24418;&#29366;&#31354;&#38388;&#30862;&#29255;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#20165;&#38656;&#35201;&#20108;&#32500;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36755;&#20986;&#29289;&#20307;&#30340;&#19981;&#26126;&#24418;&#29366;&#21644;&#30446;&#26631;&#19982;&#25668;&#20687;&#26426;&#20043;&#38388;&#30340;&#30456;&#23545;&#23039;&#24577;&#36712;&#36857;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#29992;&#26469;&#20272;&#35745;&#30446;&#26631;&#30340;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01035v1 Announce Type: new  Abstract: With the boost in the number of spacecraft launches in the current decades, the space debris problem is daily becoming significantly crucial. For sustainable space utilization, the continuous removal of space debris is the most severe problem for humanity. To maximize the reliability of the debris capture mission in orbit, accurate motion estimation of the target is essential. Space debris has lost its attitude and orbit control capabilities, and its shape is unknown due to the break. This paper proposes the Structure from Motion-based algorithm to perform unknown shaped space debris motion estimation with limited resources, where only 2D images are required as input. The method then outputs the reconstructed shape of the unknown object and the relative pose trajectory between the target and the camera simultaneously, which are exploited to estimate the target's motion. The method is quantitatively validated with the realistic image data
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35201;&#28857;&#65306;&#26412;&#25991;&#32508;&#36848;&#20102;&#23558;&#29289;&#29702;&#20449;&#24687;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#29992;&#20110;MIA&#30340;&#29289;&#29702;&#30693;&#35782;&#12289;&#24314;&#27169;&#26041;&#24335;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.01026</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#29289;&#29702;&#20449;&#24687;&#32593;&#32476;&#65306;&#19968;&#20010;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
PINNs for Medical Image Analysis: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01026
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35201;&#28857;&#65306;&#26412;&#25991;&#32508;&#36848;&#20102;&#23558;&#29289;&#29702;&#20449;&#24687;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#29992;&#20110;MIA&#30340;&#29289;&#29702;&#30693;&#35782;&#12289;&#24314;&#27169;&#26041;&#24335;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#29289;&#29702;&#20449;&#24687;&#30340;&#25972;&#21512;&#27491;&#22312;&#25913;&#21464;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MIA&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#22522;&#26412;&#30693;&#35782;&#21644;&#25351;&#23548;&#29289;&#29702;&#23450;&#24459;&#65292;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;80&#22810;&#31687;&#19987;&#27880;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MIA&#65289;&#30340;&#29289;&#29702;&#20449;&#24687;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#35843;&#26597;&#29992;&#20110;MIA&#30340;&#29289;&#29702;&#30693;&#35782;&#20197;&#21450;&#29992;&#20110;&#24314;&#27169;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;&#28145;&#20837;&#30740;&#31350;&#19968;&#31995;&#21015;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#21253;&#25324;&#25104;&#20687;&#12289;&#29983;&#25104;&#12289;&#39044;&#27979;&#12289;&#36870;&#25104;&#20687;&#65288;&#36229;&#20998;&#36776;&#29575;&#21644;&#37325;&#24314;&#65289;&#12289;&#26631;&#23450;&#21644;&#22270;&#20687;&#20998;&#26512;&#65288;&#20998;&#21106;&#21644;&#20998;&#31867;&#65289;&#12290;&#23545;&#20110;&#27599;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#37117;&#35814;&#32454;&#26816;&#26597;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#29289;&#29702;&#20449;&#24687;&#26041;&#27861;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01026v1 Announce Type: cross  Abstract: The incorporation of physical information in machine learning frameworks is transforming medical image analysis (MIA). By integrating fundamental knowledge and governing physical laws, these models achieve enhanced robustness and interpretability. In this work, we explore the utility of physics-informed approaches for MIA (PIMIA) tasks such as registration, generation, classification, and reconstruction. We present a systematic literature review of over 80 papers on physics-informed methods dedicated to MIA. We propose a unified taxonomy to investigate what physics knowledge and processes are modelled, how they are represented, and the strategies to incorporate them into MIA models. We delve deep into a wide range of image analysis tasks, from imaging, generation, prediction, inverse imaging (super-resolution and reconstruction), registration, and image analysis (segmentation and classification). For each task, we thoroughly examine an
&lt;/p&gt;</description></item><item><title>EIUP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26080;&#38656;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#19981;&#23433;&#20840;&#25552;&#31034;&#26469;&#28040;&#38500;&#38750;&#21512;&#35268;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01014</link><description>&lt;p&gt;
EIUP&#65306;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#26465;&#20214;&#25830;&#38500;&#38750;&#21512;&#35268;&#27010;&#24565;&#35757;&#32451;&#20813;&#36153;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EIUP: A Training-Free Approach to Erase Non-Compliant Concepts Conditioned on Implicit Unsafe Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01014
&lt;/p&gt;
&lt;p&gt;
EIUP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26080;&#38656;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#19981;&#23433;&#20840;&#25552;&#31034;&#26469;&#28040;&#38500;&#38750;&#21512;&#35268;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01014v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#26174;&#31034;&#20102;&#23398;&#20064;&#24191;&#27867;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#20135;&#29983;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#65292;&#36825;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#33021;&#36935;&#21040;&#30340;&#38382;&#39064;&#21253;&#25324;&#19981;&#36866;&#21512;&#24037;&#20316;&#65288;NSFW&#65289;&#30340;&#20869;&#23481;&#21644;&#28508;&#22312;&#30340;&#26679;&#24335;&#29256;&#26435;&#36829;&#35268;&#12290;&#30001;&#20110;&#22270;&#20687;&#29983;&#25104;&#26159;&#26681;&#25454;&#25991;&#26412;&#26469;&#26465;&#20214;&#30340;&#65292;&#32431;&#27905;&#25552;&#31034;&#23545;&#20110;&#20869;&#23481;&#23433;&#20840;&#26469;&#35828;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#37319;&#21462;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#21162;&#21147;&#26088;&#22312;&#36890;&#36807;&#32431;&#27905;&#25552;&#31034;&#26469;&#25511;&#21046;&#29983;&#25104;&#23433;&#20840;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#36825;&#20123;&#21162;&#21147;&#65292;&#38750;&#26377;&#27602;&#25991;&#26412;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#38750;&#21512;&#35268;&#30340;&#22270;&#20687;&#65292;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#38544;&#24335;&#19981;&#23433;&#20840;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26469;&#20174;&#27169;&#22411;&#26435;&#37325;&#20013;&#25830;&#38500;&#19981;&#24076;&#26395;&#30340;&#27010;&#24565;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#35757;&#32451;&#21608;&#26399;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26080;&#38656;&#26041;&#27861;EIUP&#65292;&#36890;&#36807;&#38544;&#24335;&#19981;&#23433;&#20840;&#25552;&#31034;&#26469;&#28040;&#38500;&#38750;&#21512;&#35268;&#27010;&#24565;'''&#19982;&#21407;&#25991;&#20013;&#25552;&#21040;&#30340;&#24615;&#33021;&#38382;&#39064;&#36827;&#34892;&#20102;&#30456;&#24212;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#36755;&#20986;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01014v1 Announce Type: new  Abstract: Text-to-image diffusion models have shown the ability to learn a diverse range of concepts. However, it is worth noting that they may also generate undesirable outputs, consequently giving rise to significant security concerns. Specifically, issues such as Not Safe for Work (NSFW) content and potential violations of style copyright may be encountered. Since image generation is conditioned on text, prompt purification serves as a straightforward solution for content safety. Similar to the approach taken by LLM, some efforts have been made to control the generation of safe outputs by purifying prompts. However, it is also important to note that even with these efforts, non-toxic text still carries a risk of generating non-compliant images, which is referred to as implicit unsafe prompts. Furthermore, some existing works fine-tune the models to erase undesired concepts from model weights. This type of method necessitates multiple training i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#25552;&#21462;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#31354;&#24433;&#20687;&#20013;&#29289;&#20307;&#39640;&#24230;&#30340;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26041;&#27861;&#22312;&#26410;&#26469;&#28608;&#20809;&#38647;&#36798;&#21644;&#24433;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2408.00967</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#20174;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#31354;&#24433;&#20687;&#20013;&#25552;&#21462;&#29289;&#20307;&#39640;&#24230;
&lt;/p&gt;
&lt;p&gt;
Extracting Object Heights From LiDAR &amp; Aerial Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#25552;&#21462;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#31354;&#24433;&#20687;&#20013;&#29289;&#20307;&#39640;&#24230;&#30340;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26041;&#27861;&#22312;&#26410;&#26469;&#28608;&#20809;&#38647;&#36798;&#21644;&#24433;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00967v1 &#20844;&#21578;&#31867;&#22411;: &#26356;&#26032;  &#25688;&#35201;: &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25552;&#21462;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#31354;&#24433;&#20687;&#20013;&#29289;&#20307;&#39640;&#24230;&#30340;&#36807;&#31243;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#33719;&#21462;&#39640;&#24230;&#65292;&#20197;&#21450;&#28608;&#20809;&#38647;&#36798;&#21644;&#24433;&#20687;&#22788;&#29702;&#30340;&#26410;&#26469;&#12290;&#26368;&#20808;&#36827;&#30340;&#23545;&#35937;&#20998;&#21106;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#28145;&#24230;&#23398;&#20064;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#33719;&#21462;&#29289;&#20307;&#39640;&#24230;&#12290;&#24037;&#31243;&#24072;&#23558;&#36861;&#36394;&#19990;&#30028;&#25968;&#25454;&#22312;&#19981;&#21516;&#20195;&#38469;&#20043;&#38388;&#30340;&#36827;&#23637;&#65292;&#24182;&#37325;&#26032;&#22788;&#29702;&#23427;&#20204;&#12290;&#20182;&#20204;&#23558;&#20351;&#29992;&#20687;&#26412;&#25991;&#20013;&#35752;&#35770;&#30340;&#37027;&#20123;&#36739;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#20123;&#36739;&#26087;&#30340;&#12289;&#22312;&#36825;&#37324;&#20063;&#35752;&#35770;&#30340;&#26041;&#27861;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#27491;&#22312;&#36229;&#36234;&#20998;&#26512;&#65292;&#36827;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#35206;&#30422;&#20102;&#36807;&#31243;&#26041;&#27861;&#21644;&#19982;&#35821;&#35328;&#27169;&#22411;&#19968;&#36215;&#25191;&#34892;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#21253;&#25324;&#28857;&#20113;&#12289;&#24433;&#20687;&#21644;&#25991;&#26412;&#32534;&#30721;&#65292;&#20801;&#35768;&#20154;&#24037;&#26234;&#33021;&#20855;&#22791;&#31354;&#38388;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00967v1 Announce Type: new  Abstract: This work shows a procedural method for extracting object heights from LiDAR and aerial imagery. We discuss how to get heights and the future of LiDAR and imagery processing. SOTA object segmentation allows us to take get object heights with no deep learning background. Engineers will be keeping track of world data across generations and reprocessing them. They will be using older procedural methods like this paper and newer ones discussed here. SOTA methods are going beyond analysis and into generative AI. We cover both a procedural methodology and the newer ones performed with language models. These include point cloud, imagery and text encoding allowing for spatially aware AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PrivateGaze&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#35270;&#32447;&#20272;&#35745;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#29992;&#25143;&#22312;&#40657;&#30418;&#24335;&#31227;&#21160;&#30524;&#21160;&#36319;&#36394;&#26381;&#21153;&#20013;&#30340;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2408.00950</link><description>&lt;p&gt;
PrivateGaze&#65306;&#20445;&#25252;&#40657;&#30418;&#24335;&#31227;&#21160;&#30524;&#21160;&#36319;&#36394;&#26381;&#21153;&#20013;&#29992;&#25143;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PrivateGaze&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#35270;&#32447;&#20272;&#35745;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#29992;&#25143;&#22312;&#40657;&#30418;&#24335;&#31227;&#21160;&#30524;&#21160;&#36319;&#36394;&#26381;&#21153;&#20013;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00950v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#30524;&#30555;&#30340;&#35270;&#32447;&#21253;&#21547;&#26377;&#20851;&#20154;&#31867;&#27880;&#24847;&#21147;&#21644;&#35748;&#30693;&#36807;&#31243;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#36825;&#39033;&#33021;&#21147;&#20351;&#24471;&#22522;&#26044;&#35270;&#32447;&#36319;&#36394;&#30340;&#24213;&#23618;&#25216;&#26415;&#25104;&#20026;&#35768;&#22810;&#26222;&#21450;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#21160;&#32773;&#65292;&#24182;&#28608;&#21457;&#20102;&#35270;&#32447;&#20272;&#35745;&#26381;&#21153;&#30340;&#24320;&#21457;&#12290;&#30830;&#23454;&#65292;&#36890;&#36807;&#21033;&#29992;&#24179;&#26495;&#30005;&#33041;&#21644;&#26234;&#33021;&#25163;&#26426;&#19978;&#26080;&#22788;&#19981;&#22312;&#30340;&#25668;&#20687;&#22836;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#35775;&#38382;&#35768;&#22810;&#35270;&#32447;&#20272;&#35745;&#26381;&#21153;&#12290;&#22312;&#36825;&#20123;&#26381;&#21153;&#20013;&#65292;&#29992;&#25143;&#24517;&#39035;&#23558;&#20182;&#20204;&#30340;&#20840;&#33080;&#22270;&#20687;&#25552;&#20379;&#32473;&#35270;&#32447;&#20272;&#31639;&#22120;&#65292;&#21518;&#32773;&#36890;&#24120;&#26159;&#40657;&#21283;&#23376;&#12290;&#36825;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23588;&#20854;&#26159;&#24403;&#19968;&#20010;&#24694;&#24847;&#30340;&#26381;&#21153;&#25552;&#20379;&#21830;&#25910;&#38598;&#20102;&#22823;&#37327;&#38754;&#23380;&#22270;&#20687;&#26469;&#20998;&#31867;&#25935;&#24863;&#30340;&#29992;&#25143;&#23646;&#24615;&#26102;&#12290;&#22312;&#36825;&#27425;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrivateGaze&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22312;&#40657;&#30418;&#24335;&#35270;&#32447;&#36319;&#36394;&#26381;&#21153;&#20013;&#26377;&#25928;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#21516;&#26102;&#21448;&#19981; compromising&#35270;&#32447;&#20272;&#35745;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#35270;&#32447;&#26631;&#35760;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#26631;&#35760;&#26469;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#19982;&#20043;&#21069;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#20570;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00950v1 Announce Type: new  Abstract: Eye gaze contains rich information about human attention and cognitive processes. This capability makes the underlying technology, known as gaze tracking, a critical enabler for many ubiquitous applications and has triggered the development of easy-to-use gaze estimation services. Indeed, by utilizing the ubiquitous cameras on tablets and smartphones, users can readily access many gaze estimation services. In using these services, users must provide their full-face images to the gaze estimator, which is often a black box. This poses significant privacy threats to the users, especially when a malicious service provider gathers a large collection of face images to classify sensitive user attributes. In this work, we present PrivateGaze, the first approach that can effectively preserve users' privacy in black-box gaze tracking services without compromising gaze estimation performance. Specifically, we proposed a novel framework to train a p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20219;&#21153;&#20114;&#24800;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20165;&#22522;&#20110;&#24739;&#32773;&#30340;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#34880;&#26643;&#20999;&#38500;&#26415;&#21518;&#30340;&#33041;&#20986;&#34880;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.00940</link><description>&lt;p&gt;
&#21452;&#20219;&#21153;&#20114;&#24800;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39044;&#27979;&#34880;&#26643;&#20999;&#38500;&#26415;&#21518;&#33041;&#20986;&#34880;
&lt;/p&gt;
&lt;p&gt;
A dual-task mutual learning framework for predicting post-thrombectomy cerebral hemorrhage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20219;&#21153;&#20114;&#24800;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20165;&#22522;&#20110;&#24739;&#32773;&#30340;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#34880;&#26643;&#20999;&#38500;&#26415;&#21518;&#30340;&#33041;&#20986;&#34880;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26775;&#27515;&#24615;&#21330;&#20013;&#26377;&#21487;&#33021;&#23548;&#33268;&#33041;&#32452;&#32455;&#32570;&#27687;&#27515;&#20129;&#12290;&#34880;&#26643;&#20999;&#38500;&#26415;&#22240;&#20854;&#21363;&#26102;&#25928;&#29992;&#24050;&#25104;&#20026;&#27835;&#30103;&#26775;&#27515;&#24615;&#21330;&#20013;&#30340;&#24120;&#29992;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#34880;&#26643;&#20999;&#38500;&#26415;&#23384;&#22312;&#26415;&#21518;&#33041;&#20986;&#34880;&#30340;&#39118;&#38505;&#12290;&#22312;&#20020;&#24202;&#19978;&#65292;&#26415;&#21518;0&#33267;72&#23567;&#26102;&#20869;&#30340;&#22810;&#27425;CT&#25195;&#25551;&#29992;&#20110;&#30417;&#27979;&#33041;&#20986;&#34880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#24739;&#32773;&#30340;&#36752;&#23556;&#21058;&#37327;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#33041;&#20986;&#34880;&#26816;&#27979;&#24310;&#36831;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20351;&#29992;&#24739;&#32773;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#26415;&#21518;&#33041;&#20986;&#34880;&#30340;&#39044;&#27979;&#26694;&#26550;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#20219;&#21153;&#20114;&#24800;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#21021;&#22987;CT&#25195;&#25551;&#20026;&#36755;&#20837;&#65292;&#21516;&#26102;&#20272;&#35745;&#21518;&#32493;CT&#25195;&#25551;&#21644;&#39044;&#27979;&#26631;&#31614;&#65292;&#20197;&#39044;&#27979;&#26415;&#21518;&#33041;&#20986;&#34880;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#39044;&#27979;&#30149;&#21464;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#39044;&#27979;&#26415;&#21518;CT&#22270;&#20687;&#20013;&#30340;&#20986;&#34880;&#24773;&#20917;&#65292;&#36824;&#21487;&#20197;&#39044;&#27979;&#33041;&#25439;&#20260;&#31243;&#24230;&#21644;&#19981;&#31283;&#23450;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#26415;&#21518;&#30417;&#27979;&#30340;&#31934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00940v1 Announce Type: cross  Abstract: Ischemic stroke is a severe condition caused by the blockage of brain blood vessels, and can lead to the death of brain tissue due to oxygen deprivation. Thrombectomy has become a common treatment choice for ischemic stroke due to its immediate effectiveness. But, it carries the risk of postoperative cerebral hemorrhage. Clinically, multiple CT scans within 0-72 hours post-surgery are used to monitor for hemorrhage. However, this approach exposes radiation dose to patients, and may delay the detection of cerebral hemorrhage. To address this dilemma, we propose a novel prediction framework for measuring postoperative cerebral hemorrhage using only the patient's initial CT scan. Specifically, we introduce a dual-task mutual learning framework to takes the initial CT scan as input and simultaneously estimates both the follow-up CT scan and prognostic label to predict the occurrence of postoperative cerebral hemorrhage. Our proposed framew
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#33258;&#21160;&#26631;&#27880;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#20197;&#27492;&#20943;&#23569;&#23545;&#22823;&#35268;&#27169;&#20154;&#24037;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#22478;&#24066;&#29615;&#22659;&#19979;&#22855;&#29305;&#29305;&#24449;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.00932</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22478;&#24066;&#29615;&#22659;&#38646;&#26679;&#26412;&#26631;&#27880;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#33258;&#21160;&#26631;&#27880;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#20197;&#27492;&#20943;&#23569;&#23545;&#22823;&#35268;&#27169;&#20154;&#24037;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#22478;&#24066;&#29615;&#22659;&#19979;&#22855;&#29305;&#29305;&#24449;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00932v1  &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#24179;&#31561;&#30340;&#22478;&#24066;&#20132;&#36890;&#24212;&#29992;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#22478;&#24066;&#29615;&#22659;&#25968;&#23383;&#34920;&#31034;&#65306;&#19981;&#21482;&#26159;&#34903;&#36947;&#21644;&#20154;&#34892;&#36947;&#65292;&#36824;&#21253;&#25324;&#33258;&#34892;&#36710;&#36947;&#12289;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#20132;&#21449;&#21475;&#12289;&#26639;&#26438;&#22369;&#36947;&#12289;&#38556;&#30861;&#29289;&#12289;&#20132;&#36890;&#20449;&#21495;&#12289;&#36335;&#26631;&#12289;&#36947;&#36335;&#26631;&#35760;&#12289;&#22353;&#27964;&#31561;&#12290;&#22312;&#35268;&#27169;&#19978;&#30452;&#25509;&#26816;&#26597;&#21644;&#25163;&#21160;&#27880;&#37322;&#25104;&#26412;&#36807;&#39640;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#27880;&#37322;&#22810;&#26679;&#22478;&#24066;&#30340;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#20943;&#23569;&#23545;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#20174;&#20154;&#31867;&#35270;&#35282;&#25293;&#25668;&#30340;&#22270;&#20687;&#25551;&#36848;&#24120;&#35265;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#22478;&#24066;&#29615;&#22659;&#20013;&#22855;&#29305;&#29305;&#24449;&#30340;&#24378;&#28872;&#20449;&#21495;&#65292;&#32780;&#19988;&#23427;&#20204;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#21487;&#33021;&#26377;&#38480;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#23545;&#22478;&#24066;&#29615;&#22659;&#29305;&#24449;&#30340;&#25551;&#36848;&#36136;&#37327;&#21644;&#25903;&#25345;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#24037;&#20316;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00932v1 Announce Type: new  Abstract: Equitable urban transportation applications require high-fidelity digital representations of the built environment: not just streets and sidewalks, but bike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions, traffic signals, signage, street markings, potholes, and more. Direct inspections and manual annotations are prohibitively expensive at scale. Conventional machine learning methods require substantial annotated training data for adequate performance. In this paper, we consider vision language models as a mechanism for annotating diverse urban features from satellite images, reducing the dependence on human annotation to produce large training sets. While these models have achieved impressive results in describing common objects in images captured from a human perspective, their training sets are less likely to include strong signals for esoteric features in the built environment, and their performance in these s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;&#24739;&#32773;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#30142;&#30149;&#36827;&#31243;&#20013;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;X&#23556;&#32447;&#22270;&#20687;&#65292;&#33021;&#22815;&#24110;&#21161;&#21307;&#23398;&#30417;&#27979;&#21644;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2408.00891</link><description>&lt;p&gt;
&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#26102;&#38388;&#28436;&#21270;: &#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;X&#23556;&#32447;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#24418;&#24577;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Evolution of Knee Osteoarthritis: A Diffusion-based Morphing Model for X-ray Medical Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;&#24739;&#32773;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#30142;&#30149;&#36827;&#31243;&#20013;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;X&#23556;&#32447;&#22270;&#20687;&#65292;&#33021;&#22815;&#24110;&#21161;&#21307;&#23398;&#30417;&#27979;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00891v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;(KOA)&#26159;&#19968;&#31181;&#24120;&#35265;&#20110;&#32769;&#24180;&#20154;&#19988;&#20005;&#37325;&#24433;&#21709;&#20854;&#27963;&#21160;&#33021;&#21147;&#30340;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20026;&#20102;&#30740;&#31350;&#30142;&#30149;&#30340;&#21160;&#24577;&#21464;&#21270;&#24182;&#23545;&#20854;&#36827;&#34892;&#32479;&#35745;&#30417;&#27979;&#65292;&#32463;&#24120;&#20351;&#29992;&#21547;&#26377;&#26102;&#38388;&#25968;&#25454;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#29992;&#20110;&#21512;&#25104;&#33181;X&#23556;&#32447;&#22270;&#20687;&#30340;&#27169;&#22411;&#21364;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#23618;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#21512;&#25104;&#29305;&#23450;&#24739;&#32773;&#20581;&#24247;&#33181;&#20851;&#33410;&#21644;&#20005;&#37325;KOA&#38454;&#27573;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;X&#23556;&#32447;&#22270;&#20687;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#22522;&#20110;&#24739;&#32773;&#30340;&#20581;&#24247;&#33181;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19968;&#26465;&#36830;&#32493;&#19988;&#26377;&#25928;&#30340;KOA X&#23556;&#32447;&#22270;&#20687;&#24207;&#21015;&#65292;&#24207;&#21015;&#20013;&#30340;&#22270;&#20687;&#34920;&#29616;&#20986;&#20102;&#19981;&#21516;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#24418;&#24577;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#20462;&#25913; denoising diffusion probabilistic model &#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25972;&#21512;&#20102;&#25193;&#25955; &#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20174;&#32780;&#20026;&#36827;&#19968;&#27493;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#30142;&#30149;&#30340;&#21160;&#24577;&#30417;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00891v1 Announce Type: cross  Abstract: Knee Osteoarthritis (KOA) is a common musculoskeletal disorder that significantly affects the mobility of older adults. In the medical domain, images containing temporal data are frequently utilized to study temporal dynamics and statistically monitor disease progression. While deep learning-based generative models for natural images have been widely researched, there are comparatively few methods available for synthesizing temporal knee X-rays. In this work, we introduce a novel deep-learning model designed to synthesize intermediate X-ray images between a specific patient's healthy knee and severe KOA stages. During the testing phase, based on a healthy knee X-ray, the proposed model can produce a continuous and effective sequence of KOA X-ray images with varying degrees of severity. Specifically, we introduce a Diffusion-based Morphing Model by modifying the Denoising Diffusion Probabilistic Model. Our approach integrates diffusion 
&lt;/p&gt;</description></item><item><title>&#21307;&#23398; SAM 2 &#26159;&#21033;&#29992; SAM 2 &#26694;&#26550;&#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#21307;&#23398;&#22270;&#20687;&#35270;&#20026;&#35270;&#39057;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;3D&#21307;&#23398;&#22270;&#20687;&#65292;&#36824;&#33021;&#33258;&#21160;&#22312;&#25152;&#26377;&#21518;&#32493;&#22270;&#20687;&#20013;&#20998;&#21106;&#30456;&#21516;&#30340;&#23545;&#35937;&#65292;&#19981;&#32771;&#34385;&#22270;&#20687;&#20043;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#65292;&#24615;&#33021;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.00874</link><description>&lt;p&gt;
&#21307;&#23398; SAM 2&#65306;&#36890;&#36807; Segment Anything Model 2 &#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#65292;&#23558;&#20854;&#35270;&#20026;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Medical SAM 2: Segment medical images as video via Segment Anything Model 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00874
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398; SAM 2 &#26159;&#21033;&#29992; SAM 2 &#26694;&#26550;&#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#21307;&#23398;&#22270;&#20687;&#35270;&#20026;&#35270;&#39057;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;3D&#21307;&#23398;&#22270;&#20687;&#65292;&#36824;&#33021;&#33258;&#21160;&#22312;&#25152;&#26377;&#21518;&#32493;&#22270;&#20687;&#20013;&#20998;&#21106;&#30456;&#21516;&#30340;&#23545;&#35937;&#65292;&#19981;&#32771;&#34385;&#22270;&#20687;&#20043;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#65292;&#24615;&#33021;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00874v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#22686; &#25688;&#35201;&#65306;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21307;&#23398; SAM 2&#65288;MedSAM-2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#32423;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992; SAM 2&#26694;&#26550;&#26469;&#35299;&#20915;&#20108;&#32500;&#21644;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#23558;&#21307;&#23398;&#22270;&#20687;&#35270;&#20026;&#35270;&#39057;&#30340;&#21746;&#23398;&#65292;MedSAM-2&#19981;&#20165;&#36866;&#29992;&#20110;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#65292;&#32780;&#19988;&#35299;&#38145;&#20102;&#26032;&#30340;&#8220;&#19968;&#27425;&#24615;&#25552;&#31034;&#8221;&#20998;&#21106;&#33021;&#21147;&#12290;&#36825;&#20801;&#35768;&#29992;&#25143;&#21482;&#20026;&#19968;&#20010;&#25110;&#29305;&#23450;&#22270;&#20687;&#30340;&#30446;&#26631;&#23545;&#35937;&#25552;&#20379;&#19968;&#20010;&#25552;&#31034;&#65292;&#20043;&#21518;&#27169;&#22411;&#21487;&#20197;&#29420;&#31435;&#22320;&#22312;&#25152;&#26377;&#21518;&#32493;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#30456;&#21516;&#30340;&#23545;&#35937;&#65292;&#32780;&#19981;&#32771;&#34385;&#22270;&#20687;&#20043;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#21307;&#23398;&#25104;&#20687;&#27169;&#24335;&#20013;&#35780;&#20272;&#20102; MedSAM-2&#65292;&#21253;&#25324;&#33145;&#37096;&#22120;&#23448;&#12289;&#35270;&#32593;&#33180;&#12289;&#33041;&#32959;&#30244;&#12289;&#30002;&#29366;&#33146;&#32467;&#33410;&#21644;&#30382;&#32932;&#30149;&#21464;&#65292;&#23558;&#20854;&#19982;&#29616;&#26377;&#25216;&#26415;&#22312;&#20256;&#32479;&#21644;&#20114;&#21160;&#20998;&#21106;&#35774;&#32622;&#20013;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;MedSAM-2&#19981;&#20165;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#32780;&#19988;&#34920;&#29616;&#20986;&#22312;&#26080;&#38656;&#29992;&#25143;&#35302;&#21457;&#30340;&#24773;&#20917;&#19979;&#23545;&#21333;&#19968;&#30446;&#26631;&#23545;&#35937;&#36827;&#34892;&#22810;&#24103;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00874v1 Announce Type: new  Abstract: In this paper, we introduce Medical SAM 2 (MedSAM-2), an advanced segmentation model that utilizes the SAM 2 framework to address both 2D and 3D medical image segmentation tasks. By adopting the philosophy of taking medical images as videos, MedSAM-2 not only applies to 3D medical images but also unlocks new One-prompt Segmentation capability. That allows users to provide a prompt for just one or a specific image targeting an object, after which the model can autonomously segment the same type of object in all subsequent images, regardless of temporal relationships between the images. We evaluated MedSAM-2 across a variety of medical imaging modalities, including abdominal organs, optic discs, brain tumors, thyroid nodules, and skin lesions, comparing it against state-of-the-art models in both traditional and interactive segmentation settings. Our findings show that MedSAM-2 not only surpasses existing models in performance but also exhi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;K-means&#32858;&#31867;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#20013;&#25991;&#25688;&#35201;&#20013;&#27599;&#20010;&#35789;&#30340;&#39068;&#33394;&#65292;&#21363;&#20351;&#23384;&#22312;&#35821;&#20041;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#65292;&#20294;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#39068;&#33394;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2408.00774</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#32593;&#31449;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#65306;&#22522;&#20110;K-means&#32858;&#31867;&#30340;&#20013;&#25991;&#25688;&#35201;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Logic Approach For Visual Analysis Of Websites With K-means Clustering-based Color Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;K-means&#32858;&#31867;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#20013;&#25991;&#25688;&#35201;&#20013;&#27599;&#20010;&#35789;&#30340;&#39068;&#33394;&#65292;&#21363;&#20351;&#23384;&#22312;&#35821;&#20041;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#65292;&#20294;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#39068;&#33394;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00774v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#32593;&#31449;&#26500;&#25104;&#20102;&#20114;&#32852;&#32593;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#20449;&#24687;&#20256;&#25773;&#21644;&#35775;&#38382;&#25968;&#23383;&#36164;&#28304;&#30340;&#24179;&#21488;&#12290;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#21442;&#19982;&#24191;&#27867;&#30340;&#20869;&#23481;&#21644;&#26381;&#21153;&#65292;&#25552;&#39640;&#20114;&#32852;&#32593;&#23545;&#25152;&#26377;&#20154;&#30340;&#23454;&#29992;&#24615;&#12290;&#32593;&#31449;&#30340;&#35774;&#35745;&#32654;&#23398;&#22312;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#22312;&#20840;&#19990;&#30028;&#20114;&#32852;&#32593;&#29992;&#25143;&#25968;&#37327;&#26085;&#30410;&#22686;&#38271;&#30340;&#22823;&#32972;&#26223;&#19979;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#31449;&#35774;&#35745;&#32654;&#23398;&#30340;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#30340;&#37325;&#35201;&#24615;&#12290;&#23427;&#24378;&#35843;&#20102;&#29992;&#25143;&#22312;50&#27627;&#31186;&#20869;&#24418;&#25104;&#30340;&#8220;&#31532;&#19968;&#21360;&#35937;&#8221;&#23545;&#29992;&#25143;&#23545;&#32593;&#31449;&#21560;&#24341;&#21147;&#21644;&#26131;&#29992;&#24615;&#30340;&#30475;&#27861;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39068;&#33394;&#21644;&#35856;&#24230;&#21644;&#23383;&#20307;&#27969;&#34892;&#24230;&#27979;&#37327;&#32593;&#31449;&#32654;&#23398;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#39044;&#27979;&#32654;&#23398;&#20559;&#22909;&#12290;&#25105;&#20204;&#23545;&#21253;&#21547;&#36817;200&#20010;&#28909;&#38376;&#21644;&#32463;&#24120;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39068;&#33394;&#25552;&#21462;&#30340;K-means&#32858;&#31867;&#26041;&#27861;&#26469;&#26377;&#25928;&#39044;&#27979;&#20013;&#25991;&#25688;&#35201;&#20013;&#27599;&#20010;&#35789;&#30340;&#39068;&#33394;&#65292;&#23613;&#31649;&#23384;&#22312;&#35821;&#20041;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#65292;&#20294;&#36825;&#31181;&#39068;&#33394;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00774v1 Announce Type: cross  Abstract: Websites form the foundation of the Internet, serving as platforms for disseminating information and accessing digital resources. They allow users to engage with a wide range of content and services, enhancing the Internet's utility for all. The aesthetics of a website play a crucial role in its overall effectiveness and can significantly impact user experience, engagement, and satisfaction. This paper examines the importance of website design aesthetics in enhancing user experience, given the increasing number of internet users worldwide. It emphasizes the significant impact of first impressions, often formed within 50 milliseconds, on users' perceptions of a website's appeal and usability. We introduce a novel method for measuring website aesthetics based on color harmony and font popularity, using fuzzy logic to predict aesthetic preferences. We collected our own dataset, consisting of nearly 200 popular and frequently used website 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;U-Net&#20998;&#21106;&#21644;EfficientNet&#20998;&#31867;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.00772</link><description>&lt;p&gt;
&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22312;&#22686;&#24378;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Deep Learning Framework for Enhanced Melanoma Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;U-Net&#20998;&#21106;&#21644;EfficientNet&#20998;&#31867;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30284;&#30151;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36843;&#20999;&#38656;&#35201;&#25913;&#36827;&#26089;&#26399;&#26816;&#27979;&#21644;&#27835;&#30103;&#25216;&#26415;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#12289;&#39640;&#25928;&#30340;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;U-Net&#20998;&#21106;&#27169;&#22411;&#21644;EfficientNet&#30382;&#32932;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#21019;&#26032;&#30340;&#34701;&#21512;&#26041;&#27861;&#25552;&#39640;&#40657;&#33394;&#32032;&#30244;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;HAM10000&#25968;&#25454;&#38598;&#31934;&#24515;&#35757;&#32451;&#20102;U-Net&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#31934;&#30830;&#22320;&#20998;&#21106;&#20986;&#30284;&#24615;&#21306;&#22495;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;ISIC 2020&#25968;&#25454;&#38598;&#23545;EfficientNet&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20248;&#21270;&#20102;&#23427;&#23545;&#30382;&#32932;&#30284;&#30151;&#30340;&#20108;&#20803;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#22312;ISIC 2020&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;99.01%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21331;&#36234;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26377;&#20102;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00772v1 Announce Type: cross  Abstract: Cancer is a leading cause of death worldwide, necessitating advancements in early detection and treatment technologies. In this paper, we present a novel and highly efficient melanoma detection framework that synergistically combines the strengths of U-Net for segmentation and EfficientNet for the classification of skin images. The primary objective of our study is to enhance the accuracy and efficiency of melanoma detection through an innovative hybrid approach. We utilized the HAM10000 dataset to meticulously train the U-Net model, enabling it to precisely segment cancerous regions. Concurrently, we employed the ISIC 2020 dataset to train the EfficientNet model, optimizing it for the binary classification of skin cancer. Our hybrid model demonstrates a significant improvement in performance, achieving a remarkable accuracy of 99.01% on the ISIC 2020 dataset. This exceptional result underscores the superiority of our approach compared
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20809;&#27969;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#20132;&#36890;&#20107;&#20214;&#30340;&#23454;&#26102;&#12289;&#39640;&#25928;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#20026;&#39550;&#39542;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#65292;&#35782;&#21035;&#21069;&#26041;&#36947;&#36335;&#28508;&#22312;&#30340;&#23041;&#32961;&#25110;&#31361;&#21457;&#20107;&#20214;&#65292;&#25552;&#39640;&#39550;&#39542;&#24773;&#20917;&#24863;&#30693;&#65292;&#24182;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00768</link><description>&lt;p&gt;
&#20351;&#29992;&#31354;&#38388; filling curves &#23545;&#27604;&#20809;&#27969;&#21644;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20809;&#27969;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#20132;&#36890;&#20107;&#20214;&#30340;&#23454;&#26102;&#12289;&#39640;&#25928;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#20026;&#39550;&#39542;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#65292;&#35782;&#21035;&#21069;&#26041;&#36947;&#36335;&#28508;&#22312;&#30340;&#23041;&#32961;&#25110;&#31361;&#21457;&#20107;&#20214;&#65292;&#25552;&#39640;&#39550;&#39542;&#24773;&#20917;&#24863;&#30693;&#65292;&#24182;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38754;&#20020;&#30528;&#22312;&#35270;&#39057;&#12289;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#31561;&#20132;&#36890;&#25968;&#25454;&#20013;&#35782;&#21035;&#20107;&#20214;&#21644;&#25910;&#38598;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#35780;&#20215;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#32467;&#26500;&#30340;&#12289;&#22810;&#27169;&#24577;&#30340;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#65292;&#19988;&#32570;&#20047;&#20803;&#25968;&#25454;&#25110;&#27880;&#37322;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20809;&#27969;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#39057;&#25968;&#25454;&#65288;&#26469;&#33258;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#65289;&#20013;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36710;&#36742;&#21608;&#22260;&#20809;&#27969;&#22330;&#30340;&#24178;&#25200;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#20107;&#20214;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#39550;&#39542;&#21592;&#30340;&#35270;&#32447;&#65292;&#20197;&#39044;&#27979;&#28508;&#22312;&#20107;&#20214;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#36755;&#36865;&#21040;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#19978;&#65292;&#20197;&#38477;&#20302;&#32500;&#24230;&#24182;&#23454;&#29616;&#35745;&#31639;&#19978;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#26631;&#20934;&#30340;&#23454;&#39564;&#65292;&#38024;&#23545;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#26816;&#27979;&#20107;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#39564;&#35777;&#20102;&#26412;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#35745;&#31639;&#25928;&#29575;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#31639;&#27861;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24635;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#31354;&#38388;&#35299;&#26500;&#30340;&#26102;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#27010;&#24565;&#25152;&#23637;&#31034;&#30340;&#31639;&#27861;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#20809;&#27969;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36798;&#21040;&#33391;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#26102;&#65292;&#26356;&#24555;&#22320;&#23436;&#25104;&#35745;&#31639;&#23494;&#38598;&#22411;&#25805;&#20316;&#12290;Our approach is designed to serve as an early warning or an auxiliary system that can provide real-time feedback to drivers or autonomous vehicles by identifying potential hazards or sudden events in the road ahead, improving situational awareness and potentially enhancing safety. In summary, this paper presents a novel framework for computationally efficient traffic event detection, which relies on optical flow, deep learning, and space-filling curves, offering a promising solution for the autonomous driving industry to achieve real-time event detection with minimal computational resources.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00768v1 Announce Type: new  Abstract: Gathering data and identifying events in various traffic situations remains an essential challenge for the systematic evaluation of a perception system's performance. Analyzing large-scale, typically unstructured, multi-modal, time series data obtained from video, radar, and LiDAR is computationally demanding, particularly when meta-information or annotations are missing. We compare Optical Flow (OF) and Deep Learning (DL) to feed computationally efficient event detection via space-filling curves on video data from a forward-facing, in-vehicle camera. Our first approach leverages unexpected disturbances in the OF field from vehicle surroundings; the second approach is a DL model trained on human visual attention to predict a driver's gaze to spot potential event locations. We feed these results to a space-filling curve to reduce dimensionality and achieve computationally efficient event retrieval. We systematically evaluate our concept b
&lt;/p&gt;</description></item><item><title>V2INet &#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#35282;&#24230;&#20449;&#24687;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#21333;&#19968;&#35270;&#35282;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#26657;&#27491;&#21518;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.00374</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#34701;&#21512;&#30340; conformal &#36712;&#36857;&#39044;&#27979;&#22312;&#21512;&#20316;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00374
&lt;/p&gt;
&lt;p&gt;
V2INet &#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#35282;&#24230;&#20449;&#24687;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#21333;&#19968;&#35270;&#35282;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#26657;&#27491;&#21518;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00374v2 Announce Type: replace-cross &#25688;&#35201;: &#30446;&#21069;&#20851;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#38543;&#30528;&#36830;&#25509;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22914;&#36710;&#23545;&#36710;&#65288;V2V&#65289;&#21644;&#36710;&#23545;&#22522;&#30784;&#35774;&#26045;&#65288;V2I&#65289;&#36890;&#20449;&#65292;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#25910;&#38598;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21464;&#24471;&#21487;&#29992;&#12290;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#38598;&#25104;&#26377;&#28508;&#21147;&#20811;&#26381;&#20165;&#20174;&#21333;&#19968;&#35270;&#35282;&#25910;&#38598;&#25968;&#25454;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#65292;&#22914;&#36974;&#25377;&#21644;&#26377;&#38480;&#35270;&#37326;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; V2INet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#21333;&#19968;&#35270;&#22270;&#27169;&#22411;&#26469;&#24314;&#27169;&#22810;&#35270;&#22270;&#25968;&#25454;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#24471;&#21040;&#20102;&#26657;&#27491;&#65292;&#20197;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00374v2 Announce Type: replace-cross  Abstract: Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;XLIP&#26694;&#26550;&#65292;&#36890;&#36807;&#26410;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#30149;&#29702;&#23398;&#20064;&#21644;&#29305;&#24449;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#27880;&#24847;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#21644;&#30446;&#26631;&#39537;&#21160;&#30340;&#36974;&#32617;&#35821;&#35328;&#24314;&#27169;&#27169;&#22359;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30149;&#29702;&#29305;&#24449;&#30340;&#20934;&#30830;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2407.19546</link><description>&lt;p&gt;
XLIP: &#36328;&#27169;&#24577;&#27880;&#24847;&#23631;&#34109;&#24314;&#27169;&#26041;&#27861;&#29992;&#20110;&#21307;&#30103;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;XLIP&#26694;&#26550;&#65292;&#36890;&#36807;&#26410;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#30149;&#29702;&#23398;&#20064;&#21644;&#29305;&#24449;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#27880;&#24847;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#21644;&#30446;&#26631;&#39537;&#21160;&#30340;&#36974;&#32617;&#35821;&#35328;&#24314;&#27169;&#27169;&#22359;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30149;&#29702;&#29305;&#24449;&#30340;&#20934;&#30830;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19546v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#21307;&#23398;&#39046;&#22495;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#19978;&#65292;&#23454;&#29616;&#36328;&#20219;&#21153;&#30340;&#26377;&#25928;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#26102;&#65292;&#22522;&#20110;&#36974;&#32617;&#24314;&#27169;&#31574;&#30053;&#30340;&#24403;&#21069;VLP&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#31232;&#32570;&#65292;&#24403;&#21069;&#27169;&#22411;&#38590;&#20197;&#20934;&#30830;&#37325;&#24314;&#30149;&#29702;&#24615;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#37319;&#29992;&#22270;&#20687;&#25991;&#26412;&#23545;&#25110;&#20165;&#22270;&#20687;&#25968;&#25454;&#65292;&#26410;&#33021;&#21033;&#29992;&#20004;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;XLIP&#65288;&#21307;&#30103;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#36974;&#32617;&#24314;&#27169;&#26041;&#27861;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26410;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#30149;&#29702;&#23398;&#20064;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27880;&#24847;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65288;AttMIM&#65289;&#21644;&#26041;&#27861;&#39537;&#21160;&#30340;&#36974;&#32617;&#35821;&#35328;&#24314;&#27169;&#27169;&#22359;&#65288;EntMLM&#65289;&#65292;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#22810;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#37325;&#24314;&#30149;&#29702;&#24615;&#35270;&#35273;&#21644;&#25991;&#26412; tokens&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19546v2 Announce Type: replace  Abstract: Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modelling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes a XLIP (Masked modelling for medical Language-Image Pre-training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modelling (AttMIM) and entity-driven masked language modelling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33258;&#35757;&#32451;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#22495;&#36866;&#24212;&#24615;&#32954;&#37096;&#32467;&#33410;&#26816;&#27979;&#65292;&#36890;&#36807;&#25913;&#36827;&#32467;&#33410;&#34920;&#31034;&#21644;&#25429;&#33719;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.19397</link><description>&lt;p&gt;
&#36328;&#22495;&#36866;&#24212;&#24615;&#32954;&#37096;&#32467;&#33410;&#26816;&#27979;&#22312;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Lung Nodule Detection in X-ray Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33258;&#35757;&#32451;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#22495;&#36866;&#24212;&#24615;&#32954;&#37096;&#32467;&#33410;&#26816;&#27979;&#65292;&#36890;&#36807;&#25913;&#36827;&#32467;&#33410;&#34920;&#31034;&#21644;&#25429;&#33719;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19397v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19397v2 Announce Type: replace  Abstract: Medical images from different healthcare centers exhibit varied data distributions, posing significant challenges for adapting lung nodule detection due to the domain shift between training and application phases. Traditional unsupervised domain adaptive detection methods often struggle with this shift, leading to suboptimal outcomes. To overcome these challenges, we introduce a novel domain adaptive approach for lung nodule detection that leverages mean teacher self-training and contrastive learning. First, we propose a hierarchical contrastive learning strategy to refine nodule representations and enhance the distinction between nodules and background. Second, we introduce a nodule-level domain-invariant feature learning (NDL) module to capture domain-invariant features through adversarial learning across different domains. Additionally, we propose a new annotated dataset of X-ray images to aid in advancing lung nodule detection re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#28145;&#24230;&#21367;&#31215;&#27169;&#22359;&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;ViT&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#21516;&#26102;&#25429;&#25417;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.19394</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#28145;&#24230;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#28145;&#24230;&#21367;&#31215;&#27169;&#22359;&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;ViT&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#21516;&#26102;&#25429;&#25417;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19394v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#36890;&#36807;&#23558;&#22270;&#20687;&#21010;&#20998;&#20026;&#22270;&#22359;&#24182;&#20351;&#29992;Transformer&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#33719;&#20840;&#23616;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;ViT&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#19968;&#24320;&#22987;&#23601;&#25429;&#25417;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#24573;&#30053;&#20102;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#30456;&#37051;&#20687;&#32032;&#20043;&#38388;&#22266;&#26377;&#30340;&#20851;&#31995;&#12290;&#21464;&#25442;&#22120;&#20027;&#35201;&#27880;&#37325;&#20840;&#23616;&#20449;&#24687;&#32780;&#24573;&#35270;&#20102;&#22270;&#20687;&#25110;&#35270;&#39057;&#25968;&#25454;&#30340;&#31934;&#32454;&#23616;&#37096;&#32454;&#33410;&#12290;&#22240;&#27492;&#65292;&#22312;&#20165;&#29992;&#23569;&#37327;&#25968;&#25454;&#23545;&#22270;&#20687;&#25110;&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;ViT&#32570;&#20047;&#24402;&#32435;&#20559;&#35265;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20381;&#38752;&#23616;&#37096;&#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#20854;&#22312;&#35757;&#32451;&#26102;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#27604;ViT&#26356;&#24555;&#65292;&#24182;&#19988;&#38656;&#35201;&#30340;&#25968;&#25454;&#26356;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#28145;&#24230;&#21367;&#31215;&#27169;&#22359;&#65292;&#20316;&#20026;ViT&#27169;&#22411;&#20013;&#30340;&#25463;&#24452;&#65292;&#32469;&#36807;&#25972;&#20010;Transformer&#22359;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19394v3 Announce Type: replace  Abstract: The Vision Transformer (ViT) leverages the Transformer's encoder to capture global information by dividing images into patches and achieves superior performance across various computer vision tasks. However, the self-attention mechanism of ViT captures the global context from the outset, overlooking the inherent relationships between neighboring pixels in images or videos. Transformers mainly focus on global information while ignoring the fine-grained local details. Consequently, ViT lacks inductive bias during image or video dataset training. In contrast, convolutional neural networks (CNNs), with their reliance on local filters, possess an inherent inductive bias, making them more efficient and quicker to converge than ViT with less data. In this paper, we present a lightweight Depth-Wise Convolution module as a shortcut in ViT models, bypassing entire Transformer blocks to ensure the models capture both local and global informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#8221;&#65288;CCVA-FL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#35299;&#20915;&#21307;&#30103;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861; CCVA-FL&#65306;&#24212;&#29992;&#20110;&#22522;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#8221;&#65288;CCVA-FL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#35299;&#20915;&#21307;&#30103;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v3 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#30340;&#20248;&#21270;&#65292;&#23398;&#20064;&#20102;&#30340;&#22270;&#20687;&#21387;&#32553;&#65288;LIC&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;PSNR&#24615;&#33021;&#65292;&#24182;&#22312;&#19982;VVC&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;14.39%&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2407.11590</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23398;&#20064;&#20102;&#22270;&#20687;&#21387;&#32553;&#65306;&#25152;&#26377;&#30340;&#20320;&#25152;&#38656;&#35201;&#30340;&#37117;&#26159;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Rethinking Learned Image Compression: Context is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#30340;&#20248;&#21270;&#65292;&#23398;&#20064;&#20102;&#30340;&#22270;&#20687;&#21387;&#32553;&#65288;LIC&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;PSNR&#24615;&#33021;&#65292;&#24182;&#22312;&#19982;VVC&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;14.39%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11590v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11590v3 Announce Type: replace-cross  Abstract: Since LIC has made rapid progress recently compared to traditional methods, this paper attempts to discuss the question about 'Where is the boundary of Learned Image Compression(LIC)?'. Thus this paper splits the above problem into two sub-problems:1)Where is the boundary of rate-distortion performance of PSNR? 2)How to further improve the compression gain and achieve the boundary? Therefore this paper analyzes the effectiveness of scaling parameters for encoder, decoder and context model, which are the three components of LIC. Then we conclude that scaling for LIC is to scale for context model and decoder within LIC. Extensive experiments demonstrate that overfitting can actually serve as an effective context. By optimizing the context, this paper further improves PSNR and achieves state-of-the-art performance, showing a performance gain of 14.39% with BD-RATE over VVC.
&lt;/p&gt;</description></item><item><title>OpenVid-1M &#26159;&#19968;&#20010;&#31934;&#30830;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25317;&#26377;&#22823;&#37327;&#30340;&#25991;&#26412;&#35270;&#39057;&#23545;&#65292;&#26088;&#22312;&#25903;&#25345;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2407.02371</link><description>&lt;p&gt;
OpenVid-1M &#22823;&#22411;&#39640;&#36136;&#37327;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.02371
&lt;/p&gt;
&lt;p&gt;
OpenVid-1M &#26159;&#19968;&#20010;&#31934;&#30830;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25317;&#26377;&#22823;&#37327;&#30340;&#25991;&#26412;&#35270;&#39057;&#23545;&#65292;&#26088;&#22312;&#25903;&#25345;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02371v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#32763;&#35793;&#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#26368;&#36817;&#22240;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;Sora&#30340;&#20852;&#36215;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;T2V&#29983;&#25104;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;1&#65289;&#32570;&#20047;&#31934;&#30830;&#24320;&#28304;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#20808;&#21069;&#27969;&#34892;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#22914;WebVid-10M&#21644;Panda-70M&#65292;&#35201;&#20040;&#36136;&#37327;&#20302;&#65292;&#35201;&#20040;&#22826;&#22823;&#65292;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#30740;&#31350;&#26426;&#26500;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;T2V&#29983;&#25104;&#26469;&#35828;&#65292;&#25910;&#38598;&#31934;&#30830;&#30340;&#39640;&#36136;&#37327;&#25991;&#26412;&#35270;&#39057;&#23545;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;2&#65289;&#24573;&#30053;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;T2V&#26041;&#27861;&#19987;&#27880;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#20132;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#29983;&#25104;&#35270;&#39057;&#65292;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#24443;&#24213;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#25552;&#21462;&#20986;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenVid-1M&#65292;&#19968;&#20010;&#31934;&#30830;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#23427;&#25317;&#26377;&#20016;&#23500;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#35828;&#27861;&#12290;&#36825;&#20010;&#24320;&#25918;&#24335;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25991;&#26412;&#35270;&#39057;&#23545;&#65292;&#20026;T2V&#30340;&#30740;&#21457;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02371v2 Announce Type: replace  Abstract: Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31354;&#38388;-&#35889;&#21435;&#22122;&#32593;&#32476;&#65292;&#21033;&#29992;CNN&#21644;Transformer&#30340;&#29305;&#24449;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#36335;&#24452;&#32593;&#32476;&#65292;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#31354;&#38388;&#32454;&#33410;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2406.08782</link><description>&lt;p&gt;
&#28151;&#21512;&#31354;&#38388;-&#35889;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36229;&#20809;&#35889;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.08782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31354;&#38388;-&#35889;&#21435;&#22122;&#32593;&#32476;&#65292;&#21033;&#29992;CNN&#21644;Transformer&#30340;&#29305;&#24449;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#36335;&#24452;&#32593;&#32476;&#65292;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#31354;&#38388;&#32454;&#33410;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.08782v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;  &#31616;&#20171;: &#36229;&#20809;&#35889;&#22270;&#20687;(HSI)&#21435;&#22122;&#26159;HSI&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#38750;&#23616;&#37096;&#24314;&#27169;&#19978;&#65292;&#24573;&#35270;&#20102;&#22270;&#20687;&#21435;&#22122;&#20013;&#23616;&#37096;&#30340; importance&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#35889;&#23398;&#20064;&#26426;&#21046;&#65292;&#36825;&#24341;&#20837;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;   &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31354;&#38388;-&#35889;&#21435;&#22122;&#32593;&#32476;(HSSD)&#65292;&#20854;&#20013;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#21452;&#36335;&#24452;&#32593;&#32476;&#65292;&#28789;&#24863;&#26469;&#33258;CNN&#21644;Transformer&#30340;&#29305;&#24449;&#65292;&#24341;&#23548;&#30528;&#25429;&#25417;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#31354;&#38388;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25233;&#21046;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#20294;&#26377;&#25928;&#30340;&#35299;&#32806;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23558;&#31354;&#38388;&#21644;&#35889;&#36890;&#36947;&#30340;&#23398;&#20064;&#20998;&#24320;&#65292;&#20854;&#20013;&#21442;&#25968;&#24456;&#23569;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#34987;&#29992;&#26469;&#23398;&#20064;&#35889;&#20043;&#38388;&#30340;&#20840;&#29699;&#20851;&#32852;&#12290;&#21512;&#25104;&#21644;&#29616;&#23454;&#27966;&#29983;&#24352;&#37327;&#21435;&#22122;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#21644;&#26356;&#20302;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;(SSIM)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.08782v2 Announce Type: replace-cross  Abstract: Hyperspectral image (HSI) denoising is an essential procedure for HSI applications. Unfortunately, the existing Transformer-based methods mainly focus on non-local modeling, neglecting the importance of locality in image denoising. Moreover, deep learning methods employ complex spectral learning mechanisms, thus introducing large computation costs.   To address these problems, we propose a hybrid spatial-spectral denoising network (HSSD), in which we design a novel hybrid dual-path network inspired by CNN and Transformer characteristics, leading to capturing both local and non-local spatial details while suppressing noise efficiently. Furthermore, to reduce computational complexity, we adopt a simple but effective decoupling strategy that disentangles the learning of space and spectral channels, where multilayer perception with few parameters is utilized to learn the global correlations among spectra. The synthetic and real exp
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#36825;&#26159;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#34394;&#25311;&#21161;&#25163;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#36825;&#20123;&#35760;&#24405;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;</title><link>https://arxiv.org/abs/2406.07867</link><description>&lt;p&gt;
&#29616;&#23454;&#23545;&#35805;&#65306;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.07867
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#36825;&#26159;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#34394;&#25311;&#21161;&#25163;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#36825;&#20123;&#35760;&#24405;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22788;&#29702;&#29992;&#25143;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#20316;&#20026;&#22238;&#24212;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#30340;&#34394;&#25311;&#21161;&#25163;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#65288;&#21363;&#22768;&#38899;&#21644;&#35270;&#35273;&#65289;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;MultiDialog&#21253;&#21547;&#20102;&#23545;&#35805;&#20249;&#20276;&#26681;&#25454;&#32473;&#23450;&#33050;&#26412;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#24182;&#36827;&#34892;&#24773;&#24863;&#26631;&#27880;&#30340;&#38899;&#39057;-&#35270;&#39057;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#26399;&#24453;&#36825;&#20123;&#35760;&#24405;&#23558;&#20026;&#22810;&#27169;&#24577;&#21512;&#25104;&#30740;&#31350;&#24320;&#36767;&#26032;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#32467;&#21512;&#20102;&#32463;&#36807;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#38899;&#39057;-&#35270;&#35273;&#21475;&#35821;&#23545;&#35805;&#39046;&#22495;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;-&#25991;&#26412;&#32852;&#21512;&#39044;&#35757;&#32451;&#32435;&#20837;&#27169;&#22411;&#20043;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.07867v2 Announce Type: replace  Abstract: In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20351;&#29305;&#23450;&#22320;&#21306;&#30340;&#22270;&#20687;&#34920;&#29616;&#19982;&#29616;&#23454;&#19990;&#30028;&#30456;&#31526;&#12290;</title><link>https://arxiv.org/abs/2406.04551</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#25913;&#36827;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20351;&#29305;&#23450;&#22320;&#21306;&#30340;&#22270;&#20687;&#34920;&#29616;&#19982;&#29616;&#23454;&#19990;&#30028;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04551v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04551v2 Announce Type: replace  Abstract: With the growing popularity of text-to-image generative models, there has been increasing focus on understanding their risks and biases. Recent work has found that state-of-the-art models struggle to depict everyday objects with the true diversity of the real world and have notable gaps between geographic regions. In this work, we aim to increase the diversity of generated images of common objects such that per-region variations are representative of the real world. We introduce an inference time intervention, contextualized Vendi Score Guidance (c-VSG), that guides the backwards steps of latent diffusion models to increase the diversity of a sample as compared to a "memory bank" of previously generated images while constraining the amount of variation within that of an exemplar set of real-world contextualizing images. We evaluate c-VSG with two geographically representative datasets and find that it substantially increases the dive
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; L-PR &#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#23545;&#22810;&#35270;&#22270;&#28857;&#20113;&#36827;&#34892;&#27880;&#20876;&#65292;&#24182;&#35299;&#20915;&#20102;&#20302;&#37325;&#21472;&#24773;&#20917;&#19979;&#30340;&#27880;&#20876;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2406.03298</link><description>&lt;p&gt;
L-PR: &#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#36827;&#34892;&#28857;&#20113;&#27880;&#20876;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.03298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; L-PR &#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#23545;&#22810;&#35270;&#22270;&#28857;&#20113;&#36827;&#34892;&#27880;&#20876;&#65292;&#24182;&#35299;&#20915;&#20102;&#20302;&#37325;&#21472;&#24773;&#20917;&#19979;&#30340;&#27880;&#20876;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
L-PR: &#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#36827;&#34892;&#20302;&#37325;&#21472;&#22810;&#35270;&#22270;&#28857;&#20113;&#27880;&#20876;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.03298v2 Announce Type: replace-cross  Abstract: Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework dubbed L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#20849;&#20139;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#22914;&#20309;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;RGB&#21644;IR&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23545;&#35937;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#30417;&#25511;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.18849</link><description>&lt;p&gt;
MiPa&#65306;&#28151;&#21512;&#34917;&#19969;&#32418;&#22806;-&#21487;&#35265;&#27169;&#24335; agnostic &#23545;&#35937;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#20849;&#20139;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#22914;&#20309;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;RGB&#21644;IR&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23545;&#35937;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#30417;&#25511;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18849v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#35793;&#25991;&#25688;&#35201;&#65306;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20351;&#29992;&#22810;&#31181;&#27169;&#24335;&#65292;&#22914;&#21487;&#35265;&#65288;RGB&#65289;&#21644;&#32418;&#22806;&#65288;IR&#65289;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#23545;&#35937;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#24615;&#33021;&#12290;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#65292;&#20854;&#20013;&#22810;&#20010;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;RGB&#21644;IR&#27169;&#24335;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#27169;&#24335;&#25110;&#21478;&#19968;&#20010;&#27169;&#24335;&#34987;&#21333;&#20010;&#20849;&#20139;&#35270;&#35273;&#32534;&#30721;&#22120;&#35266;&#23519;&#21040;&#12290;&#36825;&#31181;&#29616;&#23454;&#30340;&#24773;&#20917;&#35201;&#27714;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#19988;&#26356;&#36866;&#21512;&#20381;&#36182;&#20110;RGB&#21644;IR&#25968;&#25454;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#30417;&#25511;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20010;&#27169;&#24335;&#19978;&#35757;&#32451;&#21333;&#19968;&#32534;&#30721;&#22120;&#26102;&#65292;&#19968;&#31181;&#27169;&#24335;&#21487;&#33021;&#20250;&#20027;&#23548;&#21478;&#19968;&#31181;&#27169;&#24335;&#65292;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#35782;&#21035;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#21033;&#29992;RGB&#21644;IR&#27169;&#24335;&#65292;&#20197;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;transformer&#30340;OD&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18849v2 Announce Type: replace  Abstract: In real-world scenarios, using multiple modalities like visible (RGB) and infrared (IR) can greatly improve the performance of a predictive task such as object detection (OD). Multimodal learning is a common way to leverage these modalities, where multiple modality-specific encoders and a fusion module are used to improve performance. In this paper, we tackle a different way to employ RGB and IR modalities, where only one modality or the other is observed by a single shared vision encoder. This realistic setting requires a lower memory footprint and is more suitable for applications such as autonomous driving and surveillance, which commonly rely on RGB and IR data. However, when learning a single encoder on multiple modalities, one modality can dominate the other, producing uneven recognition results. This work investigates how to efficiently leverage RGB and IR modalities to train a common transformer-based OD vision encoder, while
&lt;/p&gt;</description></item><item><title>SPIdepth&#36890;&#36807;&#24378;&#21270;&#23039;&#21183;&#32593;&#32476;&#65292;&#22312;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#20013;&#23454;&#29616;&#20102;&#23545;&#22330;&#26223;&#32454;&#33410;&#30340;&#39640;&#32423;&#25429;&#25417;&#21644;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.12501</link><description>&lt;p&gt;
SPIdepth: &#24378;&#21270;&#23039;&#21183;&#20449;&#24687;&#29992;&#20110;&#33258;&#30417;&#30563;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SPIdepth: Strengthened Pose Information for Self-supervised Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12501
&lt;/p&gt;
&lt;p&gt;
SPIdepth&#36890;&#36807;&#24378;&#21270;&#23039;&#21183;&#32593;&#32476;&#65292;&#22312;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#20013;&#23454;&#29616;&#20102;&#23545;&#22330;&#26223;&#32454;&#33410;&#30340;&#39640;&#32423;&#25429;&#25417;&#21644;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12501v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;  &#32763;&#35793;&#25688;&#35201;: &#33258;&#30417;&#30563;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30001;&#20110;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#22312;&#21033;&#29992;&#35832;&#22914;&#33258;&#26597;&#35810;&#23618;(SQL)&#36825;&#26679;&#30340;&#25216;&#26415;&#20174;&#36816;&#21160;&#20013;&#25512;&#26029;&#28145;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#21152;&#24378;&#23039;&#21183;&#20449;&#24687;&#28508;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPIdepth&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#27880;&#37325;&#21152;&#24378;&#23039;&#21183;&#32593;&#32476;&#20197;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#12290;SPIdepth&#24314;&#31435;&#22312;SQL&#22522;&#30784;&#20043;&#19978;&#65292;&#24378;&#35843;&#20102;&#23039;&#21183;&#20449;&#24687;&#22312;&#25429;&#25417;&#31934;&#32454;&#22330;&#26223;&#32467;&#26500;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21152;&#24378;&#23039;&#21183;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;SPIdepth&#22312;&#22330;&#26223;&#29702;&#35299;&#21644;&#28145;&#24230;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#22312;KITTI&#12289;Cityscapes&#21644;Make3D&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;SPIdepth&#30340;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;SPIdepth&#22312;&#30446;&#26631;&#35782;&#21035;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#28145;&#24230;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25104;&#32489;&#65292;&#23427;&#37319;&#29992;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12501v2 Announce Type: replace  Abstract: Self-supervised monocular depth estimation has garnered considerable attention for its applications in autonomous driving and robotics. While recent methods have made strides in leveraging techniques like the Self Query Layer (SQL) to infer depth from motion, they often overlook the potential of strengthening pose information. In this paper, we introduce SPIdepth, a novel approach that prioritizes enhancing the pose network for improved depth estimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the importance of pose information in capturing fine-grained scene structures. By enhancing the pose network's capabilities, SPIdepth achieves remarkable advancements in scene understanding and depth estimation. Experimental results on benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's state-of-the-art performance, surpassing previous methods by significant margins. Specifically, SPIdepth tops the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#22312;&#30495;&#23454;&#20809;&#29031;&#19979;&#24182;&#26410;&#26174;&#33879;&#25552;&#21319;&#65292;&#34920;&#26126;&#22312;&#33258;&#28982;&#20809;&#29031;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.07514</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;&#65306;&#20809;&#29031;&#36755;&#20837;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Generalization Gap in Data Augmentation: Insights from Illumination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#22312;&#30495;&#23454;&#20809;&#29031;&#19979;&#24182;&#26410;&#26174;&#33879;&#25552;&#21319;&#65292;&#34920;&#26126;&#22312;&#33258;&#28982;&#20809;&#29031;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07514v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#30340;&#27169;&#25311;&#29305;&#24449;&#19982;&#33258;&#28982;&#35270;&#35273;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#23578;&#26410;&#34987;&#23436;&#20840;&#25581;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;&#35270;&#35273;&#34920;&#31034;&#21464;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#26469;&#23450;&#20041;&#20219;&#21153;&#20013;&#21487;&#33021;&#30340;&#35270;&#35273;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35270;&#35273;&#34920;&#31034;&#21464;&#37327;&#8220;&#20809;&#29031;&#8221;&#65292;&#36890;&#36807;&#27169;&#25311;&#20854;&#20998;&#24067;&#30340;&#36864;&#21270;&#65292;&#32771;&#23519;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#22312;&#25968;&#25454;&#22686;&#24378;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#22312;&#30495;&#23454;&#19990;&#30028;&#20809;&#29031;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20043;&#21518;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#20102;&#65292;&#20294;&#27867;&#21270;&#24615;&#33021;&#22312;&#20809;&#29031;&#26465;&#20214;&#19979;&#24182;&#27809;&#26377;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;&#36825;&#34920;&#26126;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#33021;&#22815;&#25552;&#20379;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#33258;&#28982;&#20809;&#29031;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26356;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#27867;&#21270;&#24046;&#36317;&#30340;&#23384;&#22312;&#21450;&#20854;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07514v2 Announce Type: replace  Abstract: In the field of computer vision, data augmentation is widely used to enrich the feature complexity of training datasets with deep learning techniques. However, regarding the generalization capabilities of models, the difference in artificial features generated by data augmentation and natural visual features has not been fully revealed. This study introduces the concept of "visual representation variables" to define the possible visual variations in a task as a joint distribution of these variables. We focus on the visual representation variable "illumination", by simulating its distribution degradation and examining how data augmentation techniques enhance model performance on a classification task. Our goal is to investigate the differences in generalization between models trained with augmented data and those trained under real-world illumination conditions. Results indicate that after applying various data augmentation methods, m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;3D&#20960;&#20309;&#20449;&#24687;&#19982;&#22810;&#35270;&#22270;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;2D&#23454;&#20363;&#20998;&#21106;&#25552;&#21319;&#21040;3D&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08372</link><description>&lt;p&gt;
SAM-&#25351;&#23548;&#30340;&#22270;&#20999;&#21106;&#26041;&#27861;&#29992;&#20110;3D&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SAM-guided Graph Cut for 3D Instance Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;3D&#20960;&#20309;&#20449;&#24687;&#19982;&#22810;&#35270;&#22270;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;2D&#23454;&#20363;&#20998;&#21106;&#25552;&#21319;&#21040;3D&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;3D&#23454;&#20363;&#20998;&#21106;&#25361;&#25112;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#20449;&#24687;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21040;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22240;&#20026;&#26631;&#27880;&#30340;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#22810;&#26679;&#24615;&#20302;&#32780;&#26080;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#27867;&#21270;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35797;&#22270;&#22312;&#22522;&#20110;&#24213;&#37096;&#30340;&#26694;&#26550;&#20013;&#23558;2D&#23454;&#20363;&#20998;&#21106;&#25552;&#21319;&#21040;3D&#12290;&#22312;&#19981;&#21516;&#35270;&#35282;&#20013;2D&#23454;&#20363;&#20998;&#21106;&#30340;&#19981;&#19968;&#33268;&#24615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;3D&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;3D-to-2D&#26597;&#35810;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;2D&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;3D&#23454;&#20363;&#20998;&#21106;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#22330;&#26223;&#39044;&#20808;&#20998;&#21106;&#25104;3D&#20013;&#30340;&#22810;&#20010;&#36229;&#28857;&#65292;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#22270;&#20999;&#21106;&#38382;&#39064;&#12290;&#36229;&#32423;&#28857;&#22270;&#26159;&#22522;&#20110;2D&#20998;&#21106;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#33410;&#28857;&#30340;&#20998;&#21106;&#36136;&#37327;&#36890;&#36807;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#65306;&#21333;&#20010;&#35270;&#22270;&#30340;&#23616;&#37096;&#20248;&#21183;&#21644;&#22270;&#20687;&#38388;&#30340;&#20449;&#24687;&#27969;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;3D&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08372v3 Announce Type: replace  Abstract: This paper addresses the challenge of 3D instance segmentation by simultaneously leveraging 3D geometric and multi-view image information. Many previous works have applied deep learning techniques to 3D point clouds for instance segmentation. However, these methods often failed to generalize to various types of scenes due to the scarcity and low-diversity of labeled 3D point cloud data. Some recent works have attempted to lift 2D instance segmentations to 3D within a bottom-up framework. The inconsistency in 2D instance segmentations among views can substantially degrade the performance of 3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to effectively exploit 2D segmentation models for 3D instance segmentation. Specifically, we pre-segment the scene into several superpoints in 3D, formulating the task into a graph cut problem. The superpoint graph is constructed based on 2D segmentation models, where node
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25581;&#31034;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#23545;&#19979;&#28216;&#32593;&#32476;&#32858;&#21512;&#27169;&#22411;&#37197;&#32622;&#30340;&#25935;&#24863;&#24615;&#65292;&#25351;&#20986;&#20256;&#32479;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#27169;&#22411;&#35780;&#20272;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#20854;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36873;&#25321;&#22522;&#30784;&#27169;&#22411;&#26102;&#38656;&#23545;&#32858;&#21512;&#27169;&#22411;&#30340;&#24433;&#21709;&#20104;&#20197;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2311.17804</link><description>&lt;p&gt;
&#19979;&#28216;&#32593;&#32476;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Downstream Networks in Digital Pathology Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25581;&#31034;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#23545;&#19979;&#28216;&#32593;&#32476;&#32858;&#21512;&#27169;&#22411;&#37197;&#32622;&#30340;&#25935;&#24863;&#24615;&#65292;&#25351;&#20986;&#20256;&#32479;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#27169;&#22411;&#35780;&#20272;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#20854;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36873;&#25321;&#22522;&#30784;&#27169;&#22411;&#26102;&#38656;&#23545;&#32858;&#21512;&#27169;&#22411;&#30340;&#24433;&#21709;&#20104;&#20197;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17804v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#25688;&#35201;&#65306;&#25968;&#23383;&#30149;&#29702;&#23398;&#36890;&#36807;&#20998;&#26512; gigapixel &#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#26174;&#33879;&#25552;&#39640;&#20102;&#30142;&#30149;&#26816;&#27979;&#21644;&#30149;&#29702;&#21307;&#29983;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;WSI&#39318;&#20808;&#34987;&#20998;&#25104;&#33509;&#24178;&#23567;&#22359;&#65292;&#24212;&#29992;&#19968;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#27169;&#22411;&#26469;&#33719;&#24471;&#29305;&#24449;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#21521;&#37327;&#20256;&#36882;&#32473;&#19968;&#20010;&#32858;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30456;&#24212;&#30340; WSI &#26631;&#31614;&#12290;&#38543;&#30528;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26032;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#34987;&#31216;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#23545;&#32858;&#21512;&#27169;&#22411;&#37197;&#32622;&#30340;&#25935;&#24863;&#24615;&#65292;&#34920;&#26126;&#22522;&#20110;&#36873;&#25321;&#30340;&#37197;&#32622;&#21487;&#33021;&#20250;&#20559;&#39047;&#24615;&#33021;&#30340;&#21487;&#27604;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#36825;&#31181;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#32467;&#26463;&#23545;&#29305;&#24449;&#25552;&#21462;&#22120;&#27169;&#22411;&#36873;&#25321;&#30340;&#26080;&#35270;&#65292;&#24182;&#21152;&#28145;&#23545;&#32858;&#21512;&#27169;&#22411;&#37197;&#32622;&#23545;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#30340;&#24863;&#21463;&#65292;&#26368;&#32456;&#36798;&#25104;&#23545;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17804v3 Announce Type: replace  Abstract: Digital pathology has significantly advanced disease detection and pathologist efficiency through the analysis of gigapixel whole-slide images (WSI). In this process, WSIs are first divided into patches, for which a feature extractor model is applied to obtain feature vectors, which are subsequently processed by an aggregation model to predict the respective WSI label. With the rapid evolution of representation learning, numerous new feature extractor models, often termed foundational models, have emerged. Traditional evaluation methods rely on a static downstream aggregation model setup, encompassing a fixed architecture and hyperparameters, a practice we identify as potentially biasing the results. Our study uncovers a sensitivity of feature extractor models towards aggregation model configurations, indicating that performance comparability can be skewed based on the chosen configurations. By accounting for this sensitivity, we fin
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32039;&#20945;&#30340;SNN&#27169;&#22359;&#12289;&#22810;&#20010;&#20195;&#34920;&#30456;&#21516;&#22320;&#28857;&#30340;SNN&#32452;&#21512;&#65292;&#20197;&#21450;&#23545;&#36830;&#32493;&#22270;&#20687;&#24207;&#21015;&#21305;&#37197;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25216;&#26415;&#20026;&#26426;&#22120;&#20154;&#35270;&#35273;&#31995;&#32479;&#30340;&#31934;&#30830;&#21644;&#20302;&#21151;&#29575;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2311.13186</link><description>&lt;p&gt;
&#12298;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
Applications of Spiking Neural Networks in Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13186
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32039;&#20945;&#30340;SNN&#27169;&#22359;&#12289;&#22810;&#20010;&#20195;&#34920;&#30456;&#21516;&#22320;&#28857;&#30340;SNN&#32452;&#21512;&#65292;&#20197;&#21450;&#23545;&#36830;&#32493;&#22270;&#20687;&#24207;&#21015;&#21305;&#37197;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25216;&#26415;&#20026;&#26426;&#22120;&#20154;&#35270;&#35273;&#31995;&#32479;&#30340;&#31934;&#30830;&#21644;&#20302;&#21151;&#29575;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13186v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22240;&#20854;&#22312;&#22823;&#22823;&#26410;&#23454;&#29616;&#30340;&#28508;&#21147;&#33021;&#28304;&#25928;&#29575;&#21644;&#20302;&#24310;&#36831;&#65292;&#29305;&#21035;&#26159;&#22312;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#26102;&#65292;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#24378;&#35843;&#20102;SNNs&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#65288;VPR&#65289;&#26041;&#38754;&#30340;&#19977;&#39033;&#20808;&#36827;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#12298;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12299;&#65292;&#20854;&#20013;&#27599;&#20010;SNN&#20195;&#34920;&#19968;&#32452;&#19981;&#37325;&#21472;&#30340;&#22320;&#29702;&#19981;&#21516;&#22320;&#28857;&#65292;&#20351;&#32593;&#32476;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#12298;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#12299;&#65292;&#20854;&#20013;&#22810;&#20010;&#32593;&#32476;&#20195;&#34920;&#21516;&#19968;&#22320;&#28857;&#65292;&#19982;&#21333;&#19968;&#32593;&#32476;&#27169;&#22411;&#30340;&#31934;&#24230;&#30456;&#27604;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;SNN&#27169;&#22359;&#38750;&#24120;&#32039;&#20945;&#65292;&#20165;&#21253;&#21547;1500&#20010;&#31070;&#32463;&#20803;&#21644;474k&#20010;&#31361;&#35302;&#65292;&#20351;&#20854;&#22240;&#20026;&#23427;&#20204;&#30340;&#20307;&#31215;&#23567;&#32780;&#38750;&#24120;&#36866;&#21512;&#32452;&#32676;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SNN&#22522;&#30784;VPR&#20013;&#24207;&#21015;&#21305;&#37197;&#30340;&#20316;&#29992;&#65292;&#19968;&#31181;&#25216;&#26415;&#65292;&#20351;&#29992;&#36830;&#32493;&#22270;&#20687;&#26469;&#32454;&#21270;&#20301;&#32622;&#35782;&#21035;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#37325;&#22797;&#20351;&#29992;&#21516;&#19968;&#20010;SNN&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26435;&#37325;&#26356;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#20301;&#32622;&#35782;&#21035;&#31934;&#24230;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#27169;&#22359;&#21270;SNN&#30340;&#38598;&#21512;&#21644;&#24207;&#21015;&#21305;&#37197;&#65292;&#23427;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#35270;&#35273;&#31995;&#32479;&#20013;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#20302;&#21151;&#29575;&#30340;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13186v2 Announce Type: replace-cross  Abstract: In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for their largely-unrealized potential energy efficiency and low latency particularly when implemented on neuromorphic hardware. Our paper highlights three advancements for SNNs in Visual Place Recognition (VPR). Firstly, we propose Modular SNNs, where each SNN represents a set of non-overlapping geographically distinct places, enabling scalable networks for large environments. Secondly, we present Ensembles of Modular SNNs, where multiple networks represent the same place, significantly enhancing accuracy compared to single-network models. Each of our Modular SNN modules is compact, comprising only 1500 neurons and 474k synapses, making them ideally suited for ensembling due to their small size. Lastly, we investigate the role of sequence matching in SNN-based VPR, a technique where consecutive images are used to refine place recognition. We analyze the re
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Unreal Engine&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.04421</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#19968;&#20010;&#38024;&#23545;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39550;&#39542;&#24773;&#26223;
&lt;/p&gt;
&lt;p&gt;
SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.04421
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Unreal Engine&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.04421v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#27773;&#36710;&#39046;&#22495;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25163;&#21183;&#21160;&#20316;&#25968;&#25454;&#24211;&#21487;&#33021;&#20250;&#24456;&#33392;&#38590;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#30001;&#34394;&#25311;3D&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25163;&#21183;&#25968;&#25454;&#30340;&#26500;&#24819;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#25552;&#20379;&#33258;&#23450;&#20041;&#36873;&#39033;&#24182;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#30340;&#21361;&#38505;&#12290;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#22810;&#31181;&#21464;&#20307;&#65292;&#21253;&#25324;&#25163;&#21183;&#36895;&#24230;&#12289;&#34920;&#29616;&#21147;&#21644;&#25163;&#24418;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19981;&#21516;&#20301;&#32622;&#30340;&#25668;&#20687;&#22836;&#21644;&#31867;&#22411;&#30340;&#25668;&#20687;&#22836;&#65292;&#22914;RGB&#12289;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#33719;&#21462;&#36825;&#20123;&#25668;&#20687;&#22836;&#30340;&#39069;&#22806;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#8220;SynthoGestures&#65288;https://github.com/amrgomaaelhady/SynthoGestures&#65289;&#8221;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#31934;&#24230;&#65292;&#24182;&#21487;&#20197;&#20195;&#26367;&#25110;&#34917;&#20805;&#30495;&#23454;&#25163;&#21183;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#65292;&#25105;&#20204;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.04421v2 Announce Type: replace  Abstract: Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures (https://github.com/amrgomaaelhady/SynthoGestures), improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#22270;&#20687;&#27604;&#36739;&#22120;&#23545;&#36755;&#20837;&#22270;&#20687;&#19982;&#26368;&#21487;&#33021;&#30340;K&#31867;&#26368;&#36817;&#30340;&#37051;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#26681;&#25454;&#27604;&#36739;&#32467;&#26524;&#23545;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#36827;&#34892;&#21152;&#26435;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#20102;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.13651</link><description>&lt;p&gt;
PCNN: &#27010;&#29575;&#20998;&#31867;&#26368;&#36817;&#37051;&#35299;&#37322;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#22270;&#20687;&#27604;&#36739;&#22120;&#23545;&#36755;&#20837;&#22270;&#20687;&#19982;&#26368;&#21487;&#33021;&#30340;K&#31867;&#26368;&#36817;&#30340;&#37051;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#26681;&#25454;&#27604;&#36739;&#32467;&#26524;&#23545;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#36827;&#34892;&#21152;&#26435;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#20102;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.13651v4 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25277;&#35937;&#65306; &#20256;&#32479;&#19978;&#65292;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#29992;&#20110;&#35745;&#31639;&#26368;&#32456;&#20915;&#23450;&#65292;&#20363;&#22914;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#25110;k-NN&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#21450;&#20026;&#29992;&#25143;&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#37051;&#30340;&#19968;&#31181;&#26032;&#39062;&#29992;&#36884;&#65306;&#25913;&#36827;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;C&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22270;&#20687;&#27604;&#36739;&#22120;S&#65292;&#20854;&#65288;1&#65289;&#23558;&#36755;&#20837;&#22270;&#20687;&#19982;&#26368;&#21487;&#33021;&#30340;K&#31867;&#26368;&#36817;&#30340;&#37051;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65307;&#24182;&#65288;2&#65289;&#20351;&#29992;S&#30340;&#36755;&#20986;&#20998;&#25968;&#23545;C&#30340;&#32622;&#20449;&#20998;&#25968;&#36827;&#34892;&#21152;&#26435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CUB-200&#12289;Cars-196&#21644;Dogs-120&#19978;&#19968;&#33268;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#20154;&#31867;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21521;&#25105;&#20204;&#26174;&#31034;&#27010;&#29575;&#20998;&#31867;&#26368;&#36817;&#37051;&#65288;PCNN&#65289;&#38477;&#20302;&#20102;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20182;&#20204;&#23545;&#24037;&#20316;&#30340;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#32780;&#36825;&#39033;&#24037;&#20316;&#27492;&#21069;&#30340;&#24037;&#20316;&#21482;&#23637;&#31034;&#20102;&#26368;&#21487;&#33021;&#30340;1&#31867;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.13651v4 Announce Type: replace  Abstract: Nearest neighbors (NN) are traditionally used to compute final decisions, e.g., in Support Vector Machines or k-NN classifiers, and to provide users with explanations for the model's decision. In this paper, we show a novel utility of nearest neighbors: To improve predictions of a frozen, pretrained classifier C. We leverage an image comparator S that (1) compares the input image with NN images from the top-K most probable classes; and (2) uses S' output scores to weight the confidence scores of C. Our method consistently improves fine-grained image classification accuracy on CUB-200, Cars-196, and Dogs-120. Also, a human study finds that showing lay users our probable-class nearest neighbors (PCNN) reduces over-reliance on AI, thus improving their decision accuracy over prior work which only shows only the top-1 class examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#33033;&#20914;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#22522;&#20110;&#20107;&#20214;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#21033;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#25552;&#39640;&#31232;&#30095;&#20107;&#20214;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#21160;&#24577;&#20107;&#20214;&#27969;&#20013;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2304.11857</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33033;&#20914;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;&#65306;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Accurate and Efficient Event-based Semantic Segmentation Using Adaptive Spiking Encoder-Decoder Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.11857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#33033;&#20914;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#22522;&#20110;&#20107;&#20214;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#21033;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#25552;&#39640;&#31232;&#30095;&#20107;&#20214;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#21160;&#24577;&#20107;&#20214;&#27969;&#20013;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2304.11857v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22240;&#20854;&#20302;&#21151;&#32791;&#30340;&#12289;&#20107;&#20214;&#39537;&#21160;&#30340;&#35745;&#31639;&#21644;&#22266;&#26377;&#30340;&#26102;&#38388;&#21160;&#24577;&#29305;&#24615;&#32780;&#36880;&#28176;&#34987;&#35748;&#20026;&#26159;&#21160;&#24577;&#12289;&#24322;&#27493;&#20449;&#21495;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#20174;&#22522;&#20110;&#20107;&#20214;&#30340;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#20449;&#21495;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;SNNs&#22312;&#35757;&#32451;&#21644;&#26550;&#26500;&#35774;&#35745;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#23548;&#33268;&#22312;&#22522;&#20110;&#20107;&#20214;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30456;&#27604;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#22823;&#35268;&#27169;&#22522;&#20110;&#20107;&#20214;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33033;&#20914;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;&#65288;SpikingEDN&#65289;&#12290;&#20026;&#20102;&#20174;&#21160;&#24577;&#20107;&#20214;&#27969;&#20013;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#26469;&#25552;&#39640;&#32593;&#32476;&#31934;&#24230;&#12289;&#31232;&#30095;&#24230;&#21644;&#22312;&#27969;&#24335;&#25512;&#29702;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;&#36335;&#24452;&#33033;&#20914;&#31354;&#38388;&#33258;&#36866;&#24212;&#35843;&#21046;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#19987;&#38376;&#38024;&#23545;&#22686;&#24378;&#31232;&#30095;&#20107;&#20214;&#21644;&#38750;&#27169;&#24335;&#36755;&#20837;&#30340;&#34920;&#31034;&#33021;&#21147;&#32780;&#35774;&#35745;&#65292;&#20174;&#32780;&#22312;&#23569;&#37327;&#20107;&#20214;&#19979;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.11857v3 Announce Type: replace  Abstract: Spiking neural networks (SNNs), known for their low-power, event-driven computation and intrinsic temporal dynamics, are emerging as promising solutions for processing dynamic, asynchronous signals from event-based sensors. Despite their potential, SNNs face challenges in training and architectural design, resulting in limited performance in challenging event-based dense prediction tasks compared to artificial neural networks (ANNs). In this work, we develop an efficient spiking encoder-decoder network (SpikingEDN) for large-scale event-based semantic segmentation tasks. To enhance the learning efficiency from dynamic event streams, we harness the adaptive threshold which improves network accuracy, sparsity and robustness in streaming inference. Moreover, we develop a dual-path Spiking Spatially-Adaptive Modulation module, which is specifically tailored to enhance the representation of sparse events and multi-modal inputs, thereby co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#19981;&#20165;&#20026;&#23494;&#38598;&#39044;&#27979;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#19988;&#26377;&#21147;&#30340;&#22522;&#32447;&#65292;&#32780;&#19988;&#36824;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ViT&#36827;&#34892;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#21508;&#31181;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#36827;&#19968;&#27493;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;ViT&#19981;&#20165;&#22312;&#24120;&#35268;&#20998;&#21106;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#36824;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#20110;&#31934;&#32454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#26032;&#21307;&#23398;&#35786;&#26029;&#33050;&#26412;&#12290;</title><link>https://arxiv.org/abs/2207.09339</link><description>&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65306;&#20174;&#35821;&#20041;&#20998;&#21106;&#21040;&#23494;&#38598;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers: From Semantic Segmentation to Dense Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.09339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#19981;&#20165;&#20026;&#23494;&#38598;&#39044;&#27979;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#19988;&#26377;&#21147;&#30340;&#22522;&#32447;&#65292;&#32780;&#19988;&#36824;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ViT&#36827;&#34892;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#21508;&#31181;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#36827;&#19968;&#27493;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;ViT&#19981;&#20165;&#22312;&#24120;&#35268;&#20998;&#21106;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#36824;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#20110;&#31934;&#32454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#26032;&#21307;&#23398;&#35786;&#26029;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09339v4 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#30340;&#20986;&#29616;&#25913;&#21464;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;ViTs&#22312;&#31532;&#23618;&#23545;&#25152;&#26377;&#22270;&#20687;&#22359;&#30340;&#20840;&#35270;&#22330;&#33539;&#22260;&#20869;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#65292;&#32780;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23618;&#38388;&#21644;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#30340;&#36880;&#28176;&#25193;&#22823;&#35270;&#22330;&#30456;&#27604;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;ViTs&#22312;&#20840;&#29699;&#19978;&#19979;&#25991;&#20013;&#23545;&#23494;&#38598;&#35270;&#35273;&#39044;&#27979;&#65288;&#22914;&#35821;&#20041;&#20998;&#21106;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#23618;&#23398;&#20064;&#20840;&#35270;&#22330;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;ViTs&#21487;&#33021;&#25429;&#33719;&#26356;&#24378;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#24207;&#21015;&#22359;&#65292;&#19968;&#31181;&#27809;&#26377;&#23616;&#37096;&#21367;&#31215;&#21644;&#20998;&#36776;&#29575;&#38477;&#32423;&#30340;&#21407;&#22987;ViT&#65292;&#21487;&#20197;&#20026;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#26356;&#24378;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#23454;&#39030;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#20998;&#21106;&#32593;&#32476;&#30456;&#27604;&#65292;ViT&#21363;&#20351;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#20063;&#34920;&#29616;&#20986;&#19982;&#31163;&#25955;&#32593;&#32476;&#21305;&#247;&#30340;&#33021;&#21147;&#12290; This study's findings not only provide a robust and compelling baseline for the dense prediction domain, but also demonstrate the feasibility of using ViTs for the task in a completely unsupervised manner. Further experiments on various dense prediction tasks also show that ViTs are highly competitive and generalize well beyond the conventional segmentation tasks to other real-world applications, such as new medical diagnosis scripts based on fine-grained biomedical image analysis.&#36890;&#36807;&#36827;&#19968;&#27493;&#22312;&#21508;&#31181;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;ViT&#22312;&#24120;&#35268;&#20998;&#21106;&#20219;&#21153;&#20043;&#22806;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#22522;&#20110;&#31934;&#32454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#26032;&#21307;&#23398;&#35786;&#26029;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09339v4 Announce Type: replace  Abstract: The emergence of vision transformers (ViTs) in image classification has shifted the methodologies for visual representation learning. In particular, ViTs learn visual representation at full receptive field per layer across all the image patches, in comparison to the increasing receptive fields of CNNs across layers and other alternatives (e.g., large kernels and atrous convolution). In this work, for the first time we explore the global context learning potentials of ViTs for dense visual prediction (e.g., semantic segmentation). Our motivation is that through learning global context at full receptive field layer by layer, ViTs may capture stronger long-range dependency information, critical for dense prediction tasks. We first demonstrate that encoding an image as a sequence of patches, a vanilla ViT without local convolution and resolution reduction can yield stronger visual representation for semantic segmentation. For example, ou
&lt;/p&gt;</description></item></channel></rss>