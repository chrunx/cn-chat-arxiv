<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Latent-INR&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21306;&#20998;&#24615;&#35821;&#20041;&#30340;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.02672</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;INR&#65289;: &#20855;&#26377;&#21306;&#20998;&#24615;&#35821;&#20041;&#30340;&#28789;&#27963;&#35270;&#39057;&#38544;&#24335;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Latent-INR&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21306;&#20998;&#24615;&#35821;&#20041;&#30340;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02672v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: &#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;INRs&#65289;&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#21508;&#31181;&#25968;&#25454;&#24418;&#24335;&#65288;&#21253;&#25324;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22330;&#26223;&#65289;&#30340;&#24378;&#22823;&#34920;&#31034;&#12290;&#23545;&#20110;&#35270;&#39057;&#26469;&#35828;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#21387;&#32553;&#20219;&#21153;&#30340;INRs&#65292;&#32780;&#19988;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#32534;&#30721;&#26102;&#38388;&#12289;&#23384;&#20648;&#21644;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#30340;&#34920;&#31034;&#32570;&#20047;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#27492;&#26080;&#27861;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#27492;&#31867;&#23646;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#26816;&#32034;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;INRs&#22312;&#23545;&#26631;&#20256;&#32479;&#32534;&#35299;&#30721;&#22120;&#26102;&#32570;&#20047;&#31454;&#20105;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#38500;&#20102;&#21387;&#32553;&#22806;&#27809;&#26377;&#20219;&#20309;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#32806;&#20102;&#35270;&#39057;INR&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#38754;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#27599;&#24103;&#30340;&#38544;&#24615;&#35789;&#27719;&#20197;&#21450;&#19968;&#32452;&#19987;&#38376;&#30340;&#35270;&#39057;&#19987;&#26377;&#36229;&#32593;&#32476;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36825;&#26679;&#65292;&#32473;&#23450;&#19968;&#20010;&#38544;&#24615;&#35789;&#27719;&#65292;&#36825;&#20123;&#36229;&#32593;&#32476;&#23601;&#33021;&#39044;&#27979;&#35270;&#39057;&#30340;&#26102;&#24207;&#29305;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#29983;&#25104;&#30340;&#34920;&#31034;&#19981;&#20165;&#21387;&#32553;&#33021;&#21147;&#24378;&#65292;&#32780;&#19988;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02672v1 Announce Type: new  Abstract: Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes. With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality. However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval. This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression. To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR. We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Lumina-mGPT&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;-&#32534;&#30721;&#22120;&#21464;&#20307;&#22312;&#24222;&#22823;&#30340;&#25991;&#26412;-&#22270;&#20687;&#24207;&#21015;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#29983;&#25104;&#28789;&#27963;&#21644;&#29616;&#23454;&#20027;&#20041;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28176;&#36827;&#24335;&#24378;&#21270;&#27491;&#21017;&#21270;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#24418;&#19979;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02657</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#29031;&#20142;&#28789;&#27963;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29616;&#23454;&#20027;&#20041;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Lumina-mGPT&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;-&#32534;&#30721;&#22120;&#21464;&#20307;&#22312;&#24222;&#22823;&#30340;&#25991;&#26412;-&#22270;&#20687;&#24207;&#21015;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#29983;&#25104;&#28789;&#27963;&#21644;&#29616;&#23454;&#20027;&#20041;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28176;&#36827;&#24335;&#24378;&#21270;&#27491;&#21017;&#21270;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#24418;&#19979;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Lumina-mGPT&#30340;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#27169;&#22411;&#31995;&#21015;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#28789;&#27963;&#30340;&#29616;&#23454;&#20027;&#20041;&#22270;&#20687;&#26041;&#38754;&#23588;&#20026;&#20986;&#33394;&#12290;&#19982;&#29616;&#26377;&#30340;&#33258;&#22238;&#24402;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#19981;&#21516;&#65292;Lumina-mGPT&#37319;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#27169;&#24577;&#30340;&#26631;&#35760;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#65288;mGPT&#65289;&#35299;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#23427;&#21033;&#29992;&#38543;&#26426;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30446;&#26631;&#65292;&#32780;&#19981;&#26159;&#22266;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24191;&#27867;&#21644;&#19968;&#33324;&#30340;&#36328;&#27169;&#24577;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28789;&#27963;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25552;&#20379;&#20102;&#20809;&#26126;&#12290;&#22522;&#20110;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#28176;&#36827;&#24335;&#24378;&#21270;&#27491;&#21017;&#21270;&#65288;Flexible Progressive Regularized Boosting&#65292;FPRB&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#38598;&#20013;&#23545;&#21333;&#20010;&#22270;&#20687;&#30340;&#39640;&#20998;&#36776;&#29575;&#30452;&#25509;&#36827;&#34892;&#26080;&#30417;&#30563;&#22270;&#20687;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20165;&#20351;&#29992;&#30417;&#30563;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#24773;&#26223;&#19979;&#20135;&#29983;&#30340;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#19982;&#30417;&#30563;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02657v1 Announce Type: new  Abstract: We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#26102;&#38388;&#39640;&#20445;&#30495;&#25193;&#25955;&#27169;&#22411;LaMamba-Diff&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Mamba&#22359;&#26469;&#21516;&#26102;&#22788;&#29702;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#36755;&#20837;&#30340;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02615</link><description>&lt;p&gt;
LaMamba-Diff: &#22522;&#20110;&#23616;&#37096;&#27880;&#24847;&#21147;&#19982;Mamba&#30340;&#32447;&#24615;&#26102;&#38388;&#39640;&#20445;&#30495;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#26102;&#38388;&#39640;&#20445;&#30495;&#25193;&#25955;&#27169;&#22411;LaMamba-Diff&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Mamba&#22359;&#26469;&#21516;&#26102;&#22788;&#29702;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#36755;&#20837;&#30340;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#30340;&#25193;&#25955;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#36755;&#20837;&#21333;&#35789;&#30340;&#20840;&#36830;&#25509;&#20132;&#20114;&#65292;&#20174;&#32780;&#21516;&#26102;&#22788;&#29702;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20108;&#27425;&#26041;&#30340;&#22797;&#26434;&#24615;&#23545;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#36755;&#20837;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#21517;&#20026;Mamba&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36890;&#36807;&#23558;&#28388;&#27874;&#21518;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#21387;&#32553;&#21040;&#19968;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#26469;&#25552;&#20379;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#12290;&#23613;&#31649;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#36825;&#31181;&#21387;&#32553;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20002;&#22833;&#23616;&#37096;&#35789;&#27719;&#20043;&#38388;&#30340;&#37325;&#35201;&#32454;&#33410;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#29983;&#25104;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LaMamba&#22359;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;Mamba&#30340;&#20248;&#28857;&#65292;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#32454;&#33410;&#12290;&#21033;&#29992;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#26550;&#26500;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#27880;&#24847;&#21147;&#21644;Mamba&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#36755;&#20837;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#25991;&#25551;&#36848;&#26469;&#25429;&#25417;&#21644;&#26816;&#27979;&#35773;&#21050;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#25991;&#19981;&#21305;&#37197;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02595</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#25991;&#25551;&#36848;&#25552;&#21462;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#20013;&#22686;&#24378;&#30340;&#22810;&#23618;&#32423;&#36328;&#27169;&#24577;&#35821;&#20041;&#19981;&#21305;&#37197;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#35270;&#35273;&#35821;&#20041;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#25991;&#25551;&#36848;&#26469;&#25429;&#25417;&#21644;&#26816;&#27979;&#35773;&#21050;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#25991;&#19981;&#21305;&#37197;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02595v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#35773;&#21050;&#26159;&#19968;&#31181;&#35773;&#21050;&#65292;&#20197;&#20854;&#22266;&#26377;&#30340;&#23383;&#38754;&#35299;&#35835;&#19982;&#24847;&#22270;&#20869;&#28085;&#20043;&#38388;&#30340;&#24046;&#24322;&#20026;&#29305;&#24449;&#12290;&#23613;&#31649;&#25991;&#26412;&#35773;&#21050;&#26816;&#27979;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#35752;&#65292;&#20294;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#20165;&#20973;&#25991;&#26412;&#36755;&#20837;&#21487;&#33021;&#19981;&#36275;&#20197;&#35299;&#35835;&#35773;&#21050;&#12290;&#20026;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#26377;&#25928;&#22320;&#35782;&#21035;&#35773;&#21050;&#65292;&#24517;&#39035;&#21253;&#21547;&#39069;&#22806;&#30340;&#24773;&#22659;&#32447;&#32034;&#65292;&#22914;&#22270;&#29255;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#36755;&#20837;&#19977;&#20803;&#32452;&#12290;&#36825;&#20123;&#19977;&#20803;&#32452;&#20013;&#30340;&#20004;&#20010;&#32452;&#25104;&#20803;&#32032;&#26159;&#36755;&#20837;&#25991;&#26412;&#21450;&#20854;&#20851;&#32852;&#30340;&#22270;&#29255;&#65292;&#22914;&#25968;&#25454;&#38598;&#20013;&#25152;&#25552;&#20379;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34917;&#20805;&#27169;&#24577;&#65292;&#21363;&#25551;&#36848;&#24615;&#30340;&#22270;&#29255;&#25551;&#36848;&#12290;&#24341;&#20837;&#36825;&#31181;&#35270;&#35273;&#35821;&#20041;&#34920;&#31034;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#25429;&#33719;&#25991;&#26412;&#19982;&#35270;&#35273;&#20869;&#23481;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#36825;&#23545;&#20110;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#26469;&#35828;&#26159;&#26681;&#26412;&#30340;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#26356;&#21152;&#25935;&#24863;&#22320;&#25429;&#25417;&#22270;&#25991;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#24341;&#20154;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#25991;&#26412;&#27169;&#22359;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;Facebook&#21644;Instagram&#19978;&#20998;&#20139;&#30340;&#22270;&#29255;&#21644;&#22270;&#25991;&#24086;&#23376;&#20013;&#21152;&#20837;&#25551;&#36848;&#24615;&#22270;&#29255;&#25551;&#36848;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22810;&#23618;&#32423;&#35821;&#20041;&#34920;&#31034;&#26469;&#25429;&#25417;&#35773;&#21050;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20854;&#20013;&#30340;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02595v1 Announce Type: new  Abstract: Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primar
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.02561</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
HQOD: Harmonious Quantization for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02561
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02561v1 Announce Type: new  Abstract: Task inharmony problem commonly occurs in modern object detectors, leading to inconsistent qualities between classification and regression tasks. The predicted boxes with high classification scores but poor localization positions or low classification scores but accurate localization positions will worsen the performance of detectors after Non-Maximum Suppression. Furthermore, when object detectors collaborate with Quantization-Aware Training (QAT), we observe that the task inharmony problem will be further exacerbated, which is considered one of the main causes of the performance degradation of quantized detectors. To tackle this issue, we propose the Harmonious Quantization for Object Detection (HQOD) framework, which consists of two components. Firstly, we propose a task-correlated loss to encourage detectors to focus on improving samples with lower task harmony quality during QAT. Secondly, a harmonious Intersection over Union (IoU) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#32423;&#19981;&#23436;&#20840;&#28023;&#39532;&#21453;&#36716;&#65288;IHI&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#22235;&#20010;&#35299;&#21078;&#23398;&#26631;&#20934;&#24182;&#23558;&#23427;&#20204;&#30456;&#21152;&#33719;&#24471;&#32508;&#21512;&#35780;&#20998;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20811;&#26381;&#27169;&#22411;&#23545;&#19981;&#21516;&#38431;&#21015;&#20013;&#29305;&#24449;&#31354;&#38388;&#30340;&#24046;&#24322;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#23558;&#19981;&#21516;&#38431;&#21015;&#30340;&#25968;&#25454;&#32479;&#19968;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#38431;&#21015;&#19978;&#36827;&#34892;&#39564;&#35777;&#21644;&#27979;&#35797;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#39640;&#35780;&#20998;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20063;&#21152;&#24555;&#20102;&#35780;&#20272;&#30340;&#36895;&#24230;&#21644;&#21487;&#34892;&#24615;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.02496</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#19981;&#23436;&#20840;&#28023;&#39532;&#21453;&#36716;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#32423;&#19981;&#23436;&#20840;&#28023;&#39532;&#21453;&#36716;&#65288;IHI&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#22235;&#20010;&#35299;&#21078;&#23398;&#26631;&#20934;&#24182;&#23558;&#23427;&#20204;&#30456;&#21152;&#33719;&#24471;&#32508;&#21512;&#35780;&#20998;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20811;&#26381;&#27169;&#22411;&#23545;&#19981;&#21516;&#38431;&#21015;&#20013;&#29305;&#24449;&#31354;&#38388;&#30340;&#24046;&#24322;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#23558;&#19981;&#21516;&#38431;&#21015;&#30340;&#25968;&#25454;&#32479;&#19968;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#38431;&#21015;&#19978;&#36827;&#34892;&#39564;&#35777;&#21644;&#27979;&#35797;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#39640;&#35780;&#20998;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20063;&#21152;&#24555;&#20102;&#35780;&#20272;&#30340;&#36895;&#24230;&#21644;&#21487;&#34892;&#24615;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02496v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#22312;&#22823;&#32422;20%&#30340;&#26222;&#36890;&#20154;&#32676;&#20013;&#65292;&#19981;&#23436;&#20840;&#30340;&#28023;&#39532;&#21453;&#36716;&#65288;IHI&#65289;&#65292;&#26377;&#26102;&#34987;&#31216;&#20026;&#28023;&#39532;&#26059;&#36716;&#24322;&#24120;&#65292;&#26159;&#22312;T1&#21152;&#26435;&#30913;&#20849;&#25391;&#22270;&#20687;&#30340;&#20896;&#29366;&#20999;&#29255;&#19978;&#33021;&#22815;&#35270;&#35273;&#35780;&#20272;&#30340;&#19968;&#31181;&#35299;&#21078;&#27169;&#24335;&#12290;IHI&#19982;&#20960;&#31181;&#33041;&#30149;&#65288;&#30315;&#30187;&#65292;&#31934;&#31070;&#20998;&#35010;&#30151;&#65289;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#26159;&#22522;&#20110;&#36739;&#23567;&#26679;&#26412;&#36827;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#23548;&#33268;IHI&#20135;&#29983;&#30340;&#22240;&#32032;&#65288;&#36951;&#20256;&#25110;&#29615;&#22659;&#22240;&#32032;&#65289;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;IHI&#20197;&#21450;&#23427;&#20204;&#19982;&#31070;&#32463;&#21644;&#31934;&#31070;&#30142;&#30149;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35780;&#20272;&#32791;&#26102;&#19988;&#32321;&#29712;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#27425;&#33258;&#21160;&#35780;&#32423;IHI&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#22235;&#20010;&#35299;&#21078;&#23398;&#26631;&#20934;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30456;&#21152;&#20197;&#33719;&#24471;&#19968;&#20010;&#32508;&#21512;&#35780;&#20998;&#12290;&#23613;&#31649;&#30740;&#31350;&#30340;&#38431;&#21015;&#24456;&#22823;&#65292;&#20294;&#19982;&#20043;&#21069;&#30340;&#21333;&#38431;&#21015;&#30740;&#31350;&#30456;&#27604;&#65292;&#20351;&#29992;&#36328;&#22810;&#20010;&#38431;&#21015;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#30340;&#26041;&#27861;&#20934;&#30830;&#24230;&#21487;&#33021;&#36739;&#20302;&#12290;&#25361;&#25112;&#22312;&#20110;&#20811;&#26381;&#27169;&#22411;&#23545;&#19981;&#21516;&#38431;&#21015;&#20013;&#29305;&#24449;&#31354;&#38388;&#30340;&#24046;&#24322;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19981;&#21516;&#38431;&#21015;&#30340;&#25968;&#25454;&#34701;&#21512;&#22312;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#38431;&#21015;&#19978;&#36827;&#34892;&#39564;&#35777;&#21644;&#27979;&#35797;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#38431;&#21015;&#30340;&#25968;&#25454;&#32479;&#19968;&#21518;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#26377;&#24076;&#26395;&#30340;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#38431;&#21015;&#20013;&#20445;&#25345;&#36739;&#39640;&#30340;&#35780;&#20998;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#33258;&#21160;&#35780;&#20998;&#31995;&#32479;&#20195;&#26367;&#25163;&#21160;&#35780;&#20998;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35780;&#20272;&#30340;&#36895;&#24230;&#21644;&#21487;&#34892;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02496v1 Announce Type: cross  Abstract: Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal malrotation, is an atypical anatomical pattern of the hippocampus found in about 20% of the general population. IHI can be visually assessed on coronal slices of T1 weighted MR images, using a composite score that combines four anatomical criteria. IHI has been associated with several brain disorders (epilepsy, schizophrenia). However, these studies were based on small samples. Furthermore, the factors (genetic or environmental) that contribute to the genesis of IHI are largely unknown. Large-scale studies are thus needed to further understand IHI and their potential relationships to neurological and psychiatric disorders. However, visual evaluation is long and tedious, justifying the need for an automatic method. In this paper, we propose, for the first time, to automatically rate IHI. We proceed by predicting four anatomical criteria, which are then summed up to for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HyperSpaceX &#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#24452;&#21521;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#30340;&#35282;&#21521;&#21644;&#24452;&#21521;&#29305;&#24449;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340; DistArc &#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#31867;&#38388;&#21306;&#20998;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02494</link><description>&lt;p&gt;
HyperSpaceX: &#36229;&#29699;&#38754;&#32500;&#24230;&#24452;&#21521;&#21644;&#35282;&#21521;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HyperSpaceX &#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#24452;&#21521;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#30340;&#35282;&#21521;&#21644;&#24452;&#21521;&#29305;&#24449;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340; DistArc &#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#31867;&#38388;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02494v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; Abstract: &#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#20110;&#35832;&#22914;softmax&#20132;&#21449;&#29109;&#21644;ArcFace&#25439;&#22833;&#36825;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#20998;&#31867;&#21644;&#38754;&#37096;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22312;&#36229;&#29699;&#24418;&#31354;&#38388;&#20013;&#25506;&#32034;&#35282;&#21521;&#29305;&#24449;&#65292;&#24448;&#24448;&#22240;&#20026;&#22810;&#20010;&#31867;&#21035;&#30340;&#23494;&#38598;&#35282;&#21521;&#25968;&#25454;&#32780;&#23548;&#33268;&#31867;&#20869;&#29305;&#24449;&#30456;&#20114;&#32416;&#32544;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#25506;&#32034;&#39046;&#22495;&#65292;&#31216;&#20026;HyperSpaceX&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;DistArc&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#20102;&#31867;&#21306;&#20998;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;DistArc&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#20102;&#19977;&#20010;&#29305;&#24449;&#24067;&#23616;&#32452;&#20214;&#65306;&#20004;&#20010;&#35282;&#21521;&#21644;&#19968;&#20010;&#24452;&#21521;&#65292;&#24378;&#21046;&#23454;&#26045;&#31867;&#20869;&#32465;&#23450;&#21644;&#31867;&#38388;&#20998;&#31163;&#22312;&#22810;&#24452;&#21521;&#23433;&#25490;&#20013;&#65292;&#25913;&#36827;&#20102;&#29305;&#24449;&#30340;&#21306;&#20998;&#24230;&#12290;&#23545;HyperSpaceX&#26694;&#26550;&#30340;&#26032;&#22411;&#34920;&#31034;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#25552;&#20986;&#30340;&#39044;&#27979;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#20860;&#39038;&#20102;&#35282;&#21521;&#21644;&#24452;&#21521;&#20803;&#32032;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#20851;&#20110;&#29305;&#24449;&#20998;&#24067;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#22312;&#38754;&#37096;&#35782;&#21035;&#21644;&#21160;&#29289;&#32441;&#29702;&#35782;&#21035;&#20219;&#21153;&#20013;&#36827;&#34892;&#30340;&#31995;&#32479;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#26368;&#20339;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;HyperSpaceX&#26550;&#26500;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23637;&#29616;&#26356;&#24378;&#30340;&#31867;&#38388;&#21306;&#20998;&#33021;&#21147;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#22242;&#38431;&#35745;&#21010;&#25193;&#23637; HyperSpaceX &#26694;&#26550;&#20197;&#25903;&#25345;&#26356;&#22810;&#31867;&#22411;&#30340;&#20998;&#31867;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02494v1 Announce Type: new  Abstract: Traditional deep learning models rely on methods such as softmax cross-entropy and ArcFace loss for tasks like classification and face recognition. These methods mainly explore angular features in a hyperspherical space, often resulting in entangled inter-class features due to dense angular data across many classes. In this paper, a new field of feature exploration is proposed known as HyperSpaceX which enhances class discrimination by exploring both angular and radial dimensions in multi-hyperspherical spaces, facilitated by a novel DistArc loss. The proposed DistArc loss encompasses three feature arrangement components: two angular and one radial, enforcing intra-class binding and inter-class separation in multi-radial arrangement, improving feature discriminability. Evaluation of HyperSpaceX framework for the novel representation utilizes a proposed predictive measure that accounts for both angular and radial elements, providing a mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#21333;&#24352;2D X&#23556;&#32447;&#22270;&#20687;&#20013;&#20934;&#30830;&#20998;&#21106;&#23380;&#38553;&#65292;&#20197;&#25552;&#39640;&#37329;&#23646;&#31881;&#26411;&#28155;&#21152;&#21058;&#21046;&#36896;&#30340;&#23380;&#38553;&#24230;&#20998;&#26512;&#25928;&#29575;&#12290;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02427</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#37329;&#23646;&#31881;&#26411;&#28155;&#21152;&#21058;&#21046;&#36896;2D X&#23556;&#32447;&#22270;&#20687;&#23380;&#38553;&#32570;&#38519;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Attenuation-adjusted deep learning of pore defects in 2D radiographs of additive manufacturing powders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#21333;&#24352;2D X&#23556;&#32447;&#22270;&#20687;&#20013;&#20934;&#30830;&#20998;&#21106;&#23380;&#38553;&#65292;&#20197;&#25552;&#39640;&#37329;&#23646;&#31881;&#26411;&#28155;&#21152;&#21058;&#21046;&#36896;&#30340;&#23380;&#38553;&#24230;&#20998;&#26512;&#25928;&#29575;&#12290;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;&#39640;&#23545;&#27604;&#24230;2D X&#23556;&#32447;&#22270;&#20687;&#20013;&#35782;&#21035;&#23380;&#38553;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#37329;&#23646;&#31881;&#26411;&#28155;&#21152;&#21058;&#21046;&#36896;&#20013;&#30340;&#23380;&#38553;&#29575;&#20998;&#26512;&#25928;&#29575;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32467;&#21512;X&#23556;&#32447;&#36879;&#36807;&#31890;&#23376;&#27169;&#22411;&#21644;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;UNet&#21464;&#20307;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#21333;&#24352;2D X&#23556;&#32447;&#22270;&#20687;&#20013;&#20934;&#30830;&#20998;&#21106;&#23380;&#38553;&#12290;&#19982;&#22522;&#32447;UNet&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#36798;&#21040;11.4%&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20197;&#19979;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#30340;&#20215;&#20540;&#65306;1) &#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;2) &#21046;&#20316;&#31934;&#30830;&#30340;&#24494;&#31890;&#35009;&#21098;&#22270;&#20687;&#65292;3) &#20351;&#29992;&#22522;&#20110;&#36317;&#31163;&#26144;&#23556;&#30340;&#26041;&#27861;&#21019;&#24314;&#29702;&#24819;&#30340;&#24494;&#31890;&#27169;&#22411;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;Lambert&#20013;&#24515;&#25237;&#24433;&#12290;&#36825;&#20123;&#25216;&#26415;&#21644;&#26041;&#27861;&#23545;&#20110;&#23454;&#29616;&#22312;&#32447;&#21046;&#36896;&#36807;&#31243;&#20013;&#23454;&#26102;&#23380;&#38553;&#24230;&#20998;&#26512;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02427v1 Announce Type: new  Abstract: The presence of gas pores in metal feedstock powder for additive manufacturing greatly affects the final AM product. Since current porosity analysis often involves lengthy X-ray computed tomography (XCT) scans with a full rotation around the sample, motivation exists to explore methods that allow for high throughput -- possibly enabling in-line porosity analysis during manufacturing. Through labelling pore pixels on single 2D radiographs of powders, this work seeks to simulate such future efficient setups. High segmentation accuracy is achieved by combining a model of X-ray attenuation through particles with a variant of the widely applied UNet architecture; notably, F1-score increases by $11.4\%$ compared to the baseline UNet. The proposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2) making tight particle cutouts, and 3) subtracting an ideal particle without pores generated from a distance map inspired by Lamber
&lt;/p&gt;</description></item><item><title>FPT+&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2408.02426</link><description>&lt;p&gt;
FPT+: &#19968;&#31181;&#38024;&#23545;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#39640;&#25928;&#21442;&#25968;&#21644;&#20869;&#23384;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02426
&lt;/p&gt;
&lt;p&gt;
FPT+&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02426v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#31687; &#25688;&#35201;&#65306;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24314;&#31435;&#20102;fine-tuning&#20316;&#20026;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#36827;&#34892;fine-tuning&#20195;&#20215;&#39640;&#26114;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#25928;&#21442;&#25968;&#36716;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#20316;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19979;&#28216;&#20219;&#21153;&#19978;&#12290;&#23613;&#31649;&#39640;&#25928;&#21442;&#25968;&#36716;&#31227;&#23398;&#20064;&#30340;&#20248;&#21183;&#26174;&#32780;&#26131;&#35265;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#21644;&#36755;&#20837;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#65292;PETL&#38754;&#20020;&#30340;&#25361;&#25112;&#20063;&#38543;&#20043;&#22686;&#21152;&#65292;&#22240;&#20026;&#35757;&#32451;&#20869;&#23384;&#28040;&#32791;&#24182;&#27809;&#26377;&#20687;&#21442;&#25968;&#20351;&#29992;&#37027;&#26679;&#24471;&#21040;&#26377;&#25928;&#38477;&#20302;&#12290;&#22312;&#27492;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Fine-grained Prompt Tuning plus&#65288;FPT+&#65289;&#65292;&#19968;&#31181;&#38024;&#23545;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#39640;&#25928;&#21442;&#25968;&#21644;&#20869;&#23384;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#30340;PETL&#26041;&#27861;&#30456;&#27604;&#65292;FPT+&#22312;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;FPT+&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#30340;&#20391;&#32593;&#32476;&#24182;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#30340;&#31934;&#32454;&#31895;&#31890;&#24230;&#25552;&#31034;&#35843;&#20248;&#25216;&#26415;&#65292;&#35775;&#38382;&#39044;&#35757;&#32451;&#30693;&#35782;&#26469;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02426v1 Announce Type: new  Abstract: The success of large-scale pre-trained models has established fine-tuning as a standard method for achieving significant improvements in downstream tasks. However, fine-tuning the entire parameter set of a pre-trained model is costly. Parameter-efficient transfer learning (PETL) has recently emerged as a cost-effective alternative for adapting pre-trained models to downstream tasks. Despite its advantages, the increasing model size and input resolution present challenges for PETL, as the training memory consumption is not reduced as effectively as the parameter usage. In this paper, we introduce Fine-grained Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical image classification, which significantly reduces memory consumption compared to other PETL methods. FPT+ performs transfer learning by training a lightweight side network and accessing pre-trained knowledge from a large pre-trained model (LPM) through fine
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986; MCGF &#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21160;&#24577;&#36866;&#24212;&#26410;&#30693;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#25552;&#21319;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02408</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02408
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986; MCGF &#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21160;&#24577;&#36866;&#24212;&#26410;&#30693;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#25552;&#21319;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02408v1 &#20844;&#21578;&#31867;&#22411;: &#26032; Abstract: &#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#22312;&#26080;GNSS&#20449;&#21495;&#30340;&#29615;&#22659;&#20013;&#26088;&#22312;&#36890;&#36807;&#21305;&#37197;&#26080;&#20154;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#19982;&#22823;&#37327;&#24102;&#26377;&#22320;&#29702;&#26631;&#31614;&#30340;&#21355;&#26143;&#22270;&#20687;&#26469;&#30830;&#23450;&#26410;&#30693;&#20301;&#32622;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#22825;&#27668;&#26465;&#20214;&#19979;&#23398;&#20064;&#21306;&#20998;&#24615;&#22270;&#20687;&#34920;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#24120;&#35265;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#30340;&#39057;&#32321;&#20986;&#29616;&#38459;&#30861;&#20102;&#36827;&#27493;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MCGF&#30340;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#23427;&#26088;&#22312;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21160;&#24577;&#36866;&#24212;&#26410;&#30693;&#22825;&#27668;&#26465;&#20214;&#12290;MCGF&#20026;&#22270;&#20687;&#24674;&#22797;&#21644;&#22320;&#29702;&#23450;&#20301;&#24314;&#31435;&#20102;&#32852;&#21512;&#20248;&#21270;&#12290;&#20026;&#20102;&#22270;&#20687;&#24674;&#22797;&#65292;MCGF&#24341;&#20837;&#20102;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;&#21435;&#22122;&#27169;&#22359;&#26469;&#24110;&#21161;&#20027;&#24178;&#28040;&#38500;&#19982;&#29305;&#23450;&#22825;&#27668;&#26377;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#22320;&#29702;&#23450;&#20301;&#65292;MCGF&#20351;&#29992;EVA-02&#20316;&#20026;&#20027;&#24178;&#26469;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#23450;&#20301;&#21644;&#22825;&#27668;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02408v1 Announce Type: new  Abstract: Cross-view geo-localization in GNSS-denied environments aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery. Recent research shows that learning discriminative image representations under specific weather conditions can significantly enhance performance. However, the frequent occurrence of unseen extreme weather conditions hinders progress. This paper introduces MCGF, a Multi-weather Cross-view Geo-localization Framework designed to dynamically adapt to unseen weather conditions. MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. For image restoration, MCGF incorporates a shared encoder and a lightweight restoration module to help the backbone eliminate weather-specific information. For geo-localization, MCGF uses EVA-02 as a backbone for feature extraction, with cross-entropy loss for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24352;&#37327;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#22823;&#24133;&#25552;&#21319;&#20102;&#22312;&#19977;&#32500;&#22270;&#20687;&#20013;&#26816;&#27979;&#26059;&#36716;&#29289;&#20307;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02398</link><description>&lt;p&gt;
&#24352;&#37327;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#65306;&#24555;&#36895;&#26816;&#27979;&#26059;&#36716;&#29289;&#20307;&#22312;&#19977;&#32500;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensorial template matching for fast cross-correlation with rotations and its application for tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24352;&#37327;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#22823;&#24133;&#25552;&#21319;&#20102;&#22312;&#19977;&#32500;&#22270;&#20687;&#20013;&#26816;&#27979;&#26059;&#36716;&#29289;&#20307;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#24352;&#37327;&#27169;&#26495;&#21305;&#37197;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23558;&#29289;&#20307;&#30340;&#26059;&#36716;&#36890;&#36807;&#24352;&#37327;&#22330;&#26469;&#34920;&#31034;&#12290;&#19982;&#20256;&#32479;&#30340;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#30456;&#27604;&#65292;&#24352;&#37327;&#27169;&#26495;&#21305;&#37197;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#26059;&#36716;&#31934;&#24230;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#22823;&#37327;&#19977;&#32500;&#22270;&#20687;&#65288;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#65289;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#20174;&#26029;&#23618;&#25195;&#25551;&#20013;&#33719;&#21462;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24352;&#37327;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#26174;&#33879;&#24555;&#20110;&#20256;&#32479;&#27169;&#26495;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02398v1 Announce Type: new  Abstract: Object detection is a main task in computer vision. Template matching is the reference method for detecting objects with arbitrary templates. However, template matching computational complexity depends on the rotation accuracy, being a limiting factor for large 3D images (tomograms). Here, we implement a new algorithm called tensorial template matching, based on a mathematical framework that represents all rotations of a template with a tensor field. Contrary to standard template matching, the computational complexity of the presented algorithm is independent of the rotation accuracy. Using both, synthetic and real data from tomography, we demonstrate that tensorial template matching is much faster than template matching and has the potential to improve its accuracy
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;CMR-Agent&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;2D-3D&#28151;&#21512;&#29366;&#24577;&#34920;&#31034;&#26469;&#20805;&#20998;&#21033;&#29992;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#25552;&#39640;&#27880;&#20876;&#31934;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02394</link><description>&lt;p&gt;
CMR-Agent: &#19968;&#31181;&#29992;&#20110;&#36845;&#20195;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#36328;&#23186;&#20307;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02394
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;CMR-Agent&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;2D-3D&#28151;&#21512;&#29366;&#24577;&#34920;&#31034;&#26469;&#20805;&#20998;&#21033;&#29992;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#25552;&#39640;&#27880;&#20876;&#31934;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02394v1 &#20844;&#21578;&#31867;&#22411;: cross  &#25688;&#35201;: &#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26088;&#22312;&#30830;&#23450;RGB&#22270;&#20687;&#30456;&#23545;&#20110;&#28857;&#20113;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#12290;&#23427;&#22312;&#39044;&#20808;&#26500;&#24314;&#30340;LiDAR&#22320;&#22270;&#20013;&#30340;&#30456;&#26426;&#23450;&#20301;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#23384;&#22312;&#23186;&#20307;&#31867;&#22411;&#24046;&#36317;&#65292;&#20294;&#22823;&#22810;&#25968;&#22522;&#20110;&#23398;&#20064;&#30340; methods &#26041;&#27861;&#22312;&#27809;&#26377;&#20219;&#20309;&#21453;&#39304;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24314;&#31435;&#20108;&#32500;&#21040;&#19977;&#32500;&#28857;&#23545;&#65292;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#36845;&#20195;&#30340;&#20248;&#21270;&#65292;&#32467;&#26524;&#23548;&#33268;&#20102;&#20302;&#31934;&#24230;&#21644;&#24179;&#24248;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27880;&#20876;&#36807;&#31243;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#36845;&#20195;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20801;&#35768;&#26681;&#25454;&#27599;&#20010;&#20013;&#38388;&#29366;&#24577;&#36827;&#34892;&#28176;&#36827;&#30340;&#30456;&#26426;&#23039;&#21183;&#35843;&#25972;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#21457;&#23637;&#19968;&#31181;&#36328;&#23186;&#20307;&#27880;&#20876;&#20195;&#29702;&#65288;CMR-Agent&#65289;&#65292;&#24182;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#26469;&#20026;&#23427;&#30340;&#27880;&#20876;&#31574;&#30053;&#21021;&#22987;&#21270;&#31283;&#23450;&#24615;&#65292;&#24182;&#20026;&#35757;&#32451;&#25552;&#20379;&#24555;&#36895;&#21551;&#21160;&#12290; &#26681;&#25454;&#20132;&#21449;&#23186;&#20307;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;2D-3D&#28151;&#21512;&#29366;&#24577;&#34920;&#31034;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02394v1 Announce Type: cross  Abstract: Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMR-Agent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36328;&#20266;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#26631;&#27880;&#30340;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#31934;&#30830;&#30340;&#22303;&#22320;&#20351;&#29992;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#20998;&#21106;&#27169;&#22411;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#32321;&#24537;&#22478;&#24066;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2408.02382</link><description>&lt;p&gt;
&#36328;&#20266;&#30417;&#30563;&#26694;&#26550;&#29992;&#20110;&#31232;&#30095;&#26631;&#27880;&#22320;&#29702;&#31354;&#38388;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36328;&#20266;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#26631;&#27880;&#30340;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#31934;&#30830;&#30340;&#22303;&#22320;&#20351;&#29992;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#20998;&#21106;&#27169;&#22411;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#32321;&#24537;&#22478;&#24066;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02382v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22303;&#22320;&#20351;&#29992;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#26144;&#23556;&#23545;&#20110;&#22478;&#24066;&#21644;&#36164;&#28304;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#21457;&#23637;&#26234;&#33021;&#21644;&#21487;&#25345;&#32493;&#22478;&#24066;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#21106;&#27169;&#22411;&#65292;&#29992;&#20110;&#20351;&#29992;&#26469;&#33258;&#21360;&#24230;&#19981;&#21516;&#22320;&#21306;&#30340;&#22810;&#26679;&#24615;&#25968;&#25454;&#20998;&#24067;&#30340;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;LULC&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#22312;different areas&#19981;&#21516;&#30340;&#22320;&#21306;&#20869;&#23545;&#24314;&#31569;&#29289;&#12289;&#36947;&#36335;&#12289;&#26641;&#26408;&#21644;&#27700;&#20307;&#36827;&#34892;robust generalization&#31283;&#20581;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#20462;&#25913;&#30340;&#36328;&#20266;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#26631;&#27880;&#25968;&#25454;&#19978;&#35757;&#32451;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;&#8220;&#36328;&#20266;&#30417;&#30563;&#8221;&#25216;&#26415;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#24102;&#26377;sparse&#21644;inaccurate&#26631;&#31614;&#30340;noisy&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#19978;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;approach&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;truncated edge area&#20462;&#21098;&#36793;&#32536;&#21306;&#22495;&#20869;&#30340;&#31934;&#20934;&#24230;&#21644;utility&#23454;&#29992;&#24615;&#12290;&#21452;&#21521;labeling methods&#21453;&#21521;&#26631;&#27880;&#26041;&#27861;&#22312;&#36825;&#20123;&#36793;&#32536;&#21306;&#22495;&#20869;&#20998;&#21106;&#34920;&#29616;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;significant bias&#26497;&#22823;&#30340;&#20559;&#24046;&#12290;&#36890;&#36807;&#36328;&#20266;&#30417;&#30563;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#39564;&#30340;&#65292;intrinsic faithfulness intrinsic &#30456;&#31526;&#24615;&#32422;&#26463;&#65292;&#20197;enhance &#22686;&#24378;&#27169;&#22411;&#30340;CAUSALITY&#22240;&#26524;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25682;&#24323;&#20102;&#27867;&#21270;&#21040;&#25972;&#20010;feature space&#29305;&#24449;&#31354;&#38388;&#30340;&#27010;&#24565;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110; fine-grained &#32454;&#33268;&#30340;segmentation&#32454;&#33410;&#20998;&#21106;&#65292;&#36825;&#23545;urban planning&#22478;&#24066;&#35268;&#21010;&#38750;&#24120;&#26377;&#30410;&#12290;Our proposed framework significantly outperforms conventional semisupervised models on various benchmarks. These experiments demonstrate the robustness and effectiveness of our approach, paving the way for cost-effective and efficient mapping strategies in resource-scarce urban environments.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02382v1 Announce Type: new  Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource planning and is one of the key elements in developing smart and sustainable cities. This study introduces a semi-supervised segmentation model for LULC prediction using high-resolution satellite images with a huge diversity in data distributions in different areas from the country of India. Our approach ensures a robust generalization across different types of buildings, roads, trees, and water bodies within these distinct areas. We propose a modified Cross Pseudo Supervision framework to train image segmentation models on sparsely labelled data. The proposed framework addresses the limitations of the popular "Cross Pseudo Supervision" technique for semi-supervised learning. Specifically, it tackles the challenges of training segmentation models on noisy satellite image data with sparse and inaccurate labels. This comprehensive approach enhances the accuracy and utili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NPU-ASLP&#22242;&#38431;&#22312;CNVSRC 2024&#31454;&#36187;&#20013;&#23545;VSR&#31995;&#32479;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#29616;&#25928;&#26524;&#65292;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2408.02369</link><description>&lt;p&gt;
&#12298;CNVSRC 2024 &#35270;&#35273;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#30340; NPU-ASLP &#25551;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NPU-ASLP&#22242;&#38431;&#22312;CNVSRC 2024&#31454;&#36187;&#20013;&#23545;VSR&#31995;&#32479;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#29616;&#25928;&#26524;&#65292;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102; NPU-ASLP&#65288;&#22242;&#38431;237&#65289;&#22312;&#26412;&#23626;&#20013;&#36830;&#32493;&#35270;&#35273;&#35828;&#35805;&#20154;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;CNVSRC 2024&#65289;&#20013;&#25552;&#20986;&#30340;&#35270;&#35273;&#35828;&#35805;&#20154;&#35782;&#21035;&#65288;VSR&#65289;&#31995;&#32479;&#65292;&#24182;&#21442;&#19982;&#20102;&#25152;&#26377;&#22235;&#20010;&#36187;&#36947;&#65292;&#21253;&#25324;&#21333;&#38899;&#35828;&#35805;&#20154;VSR&#20219;&#21153;&#21644;&#22810;&#38899;&#35828;&#35805;&#20154;VSR&#20219;&#21153;&#30340;&#22266;&#23450;&#21644;&#24320;&#25918;&#36187;&#36947;&#12290;&#22312;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#32447;&#25552;&#20379;&#30340;&#21767;&#21160;&#25552;&#21462;&#22120;&#26469;&#29983;&#25104;&#22810;&#23610;&#24230;&#35270;&#39057;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26399;&#38388;&#24212;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#25200;&#21160;&#12289;&#38543;&#26426;&#26059;&#36716;&#12289;&#27700;&#24179;&#32763;&#36716;&#21644;&#39068;&#33394;&#21464;&#25442;&#12290;VSR&#27169;&#22411;&#37319;&#29992;&#20102;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;CTC/&#27880;&#24847;&#21147;&#25439;&#22833;&#65292;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;ResNet3D&#35270;&#35273;&#21069;&#31471;&#12289;E-Branchformer&#32534;&#30721;&#22120;&#21644;&#21452;&#21521;Transformer&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#21333;&#38899;&#35828;&#35805;&#20154;&#20219;&#21153;&#21644;&#22810;&#38899;&#35828;&#35805;&#20154;&#20219;&#21153;&#20998;&#21035;&#36798;&#21040;&#20102;30.47%&#21644;34.30%&#30340;&#38169;&#35823;&#29575;&#65292;&#22312;&#21333;&#38899;&#35828;&#35805;&#20154;&#20219;&#21153;&#30340;&#24320;&#25918;&#36187;&#36947;&#20013;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#36890;&#36807;Deep Image Prior&#21644;&#38543;&#26426;&#36845;&#20195;&#25216;&#26415;&#25913;&#36827;&#20102;3D MRF&#22270;&#20687;&#30340;&#20934;&#30830;&#37325;&#24314;&#65292;&#26080;&#38656;&#22823;&#37327;ground-truth&#25968;&#25454;&#21363;&#21487;&#36827;&#34892;&#39640;&#25928;&#30340;&#19977;&#32500;&#37325;&#24314;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;3D&#25104;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.02367</link><description>&lt;p&gt;
StoDIP: &#36890;&#36807;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#21644;&#38543;&#26426;&#36845;&#20195;&#39640;&#25928;&#30340;3D MRF&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
StoDIP: Efficient 3D MRF image reconstruction with deep image priors and stochastic iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#36890;&#36807;Deep Image Prior&#21644;&#38543;&#26426;&#36845;&#20195;&#25216;&#26415;&#25913;&#36827;&#20102;3D MRF&#22270;&#20687;&#30340;&#20934;&#30830;&#37325;&#24314;&#65292;&#26080;&#38656;&#22823;&#37327;ground-truth&#25968;&#25454;&#21363;&#21487;&#36827;&#34892;&#39640;&#25928;&#30340;&#19977;&#32500;&#37325;&#24314;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;3D&#25104;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02367v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#30913;&#20849;&#25391;&#25351;&#32441;&#35782;&#21035;&#65288;MRF&#65289;&#26159;&#19968;&#31181;&#37327;&#21270;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#21442;&#25968;&#32452;&#32455;&#26144;&#23556;&#65292;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#12290;&#23450;&#37327;&#26144;&#23556;&#37325;&#24314;&#35201;&#27714;&#21435;&#38500;&#21387;&#32553;&#37319;&#26679;MRF&#37319;&#38598;&#20013;&#30340;&#20266;&#24433; artifacts&#12290;&#22312;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#20013;&#65292;&#35768;&#22810;&#26041;&#27861;&#20165;&#20851;&#27880;&#20108;&#32500;&#65288;2D&#65289;&#22270;&#20687;&#37325;&#24314;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20020;&#24202;&#19978;&#30340;&#37325;&#35201;&#24615;&#26356;&#39640;&#65292;&#20294;&#24573;&#35270;&#20102;&#21521;&#19977;&#32500;&#65288;3D&#65289;&#25195;&#25551;&#30340;&#25193;&#23637;&#12290;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#27809;&#26377;&#36866;&#24403;&#30340;&#32531;&#35299;&#25514;&#26045;&#30340;&#24773;&#20917;&#19979;&#65292;&#21521;3D&#25104;&#20687;&#36807;&#28193;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#35201;&#27714;&#22823;&#37327;&#30340;ground-truth&#65288;&#26080;&#20266;&#24433;&#65289;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;StoDIP&#65292;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#26080;ground-truth&#30340;Deep Image Prior&#65288;DIP&#65289;&#37325;&#24314;&#25104;&#29992;&#20110;3D MRF&#25104;&#20687;&#12290;StoDIP&#37319;&#29992;&#20869;&#23384;&#39640;&#25928;&#30340;&#38543;&#26426;&#26356;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26080;&#20266;&#24433;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#37325;&#24314;3D MRF&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#38543;&#26426;&#36845;&#20195;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;3D&#31354;&#38388;&#20013;&#25429;&#33719;&#26356;&#22810;&#32454;&#33410;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#20266;&#24433;&#12290;With the advancement of deep learning (DL), particularly in computer vision applications, the DIP approach gained attention for its ability to accurately reconstruct images by leveraging the inherent structures across a set of unlabeled images. However, despite its effectiveness in 2D image reconstruction, applying the DIP method to the 3D MRF imaging domain remained challenging due to the significant increase in data size and the computational complexity brought about by the transition to 3D.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02367v1 Announce Type: cross  Abstract: Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to quantitative MRI for multiparametric tissue mapping. The reconstruction of quantitative maps requires tailored algorithms for removing aliasing artefacts from the compressed sampled MRF acquisitions. Within approaches found in the literature, many focus solely on two-dimensional (2D) image reconstruction, neglecting the extension to volumetric (3D) scans despite their higher relevance and clinical value. A reason for this is that transitioning to 3D imaging without appropriate mitigations presents significant challenges, including increased computational cost and storage requirements, and the need for large amount of ground-truth (artefact-free) data for training. To address these issues, we introduce StoDIP, a new algorithm that extends the ground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging. StoDIP employs memory-efficient stochastic updates
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19982;&#38271;&#35270;&#39057;&#23545;&#40784;&#30340;&#20016;&#23500;&#29615;&#22659;&#25551;&#36848;&#20449;&#24687;&#65292;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#20869;&#23481;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#35821;&#35328;&#23545;&#20934;&#20219;&#21153;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02336</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;&#25104;&#20013;&#25991;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38271;&#35270;&#39057;&#35821;&#35328;&#23545;&#20934;&#20013;&#27880;&#20837;&#29615;&#22659;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Infusing Environmental Captions for Long-Form Video Language Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02336
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19982;&#38271;&#35270;&#39057;&#23545;&#40784;&#30340;&#20016;&#23500;&#29615;&#22659;&#25551;&#36848;&#20449;&#24687;&#65292;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#20869;&#23481;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#35821;&#35328;&#23545;&#20934;&#20219;&#21153;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#32763;&#35793;&#25104;&#20013;&#25991;&#65306;&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#26399;&#35270;&#39057;-&#35821;&#35328;&#23545;&#20934;&#65288;VLG&#65289;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#38271;&#35270;&#39057;&#21644;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#21040;&#22238;&#31572;&#26597;&#35810;&#30340;&#26102;&#38388;&#28857;&#12290;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;VLG&#20219;&#21153;&#65292;&#21363;&#20351;&#35270;&#39057;&#24456;&#38271;&#65292;&#20182;&#20204;&#20063;&#21487;&#20197;&#36890;&#36807;&#20016;&#23500;&#30340;&#32463;&#39564;&#30693;&#35782;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#26102;&#21051;&#12290;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#29616;&#26377;&#30340;VLG&#26041;&#27861;&#32463;&#24120;&#22312;&#23398;&#20064;&#21040;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#34920;&#38754;&#27169;&#24335;&#19978;&#22833;&#36133;&#65292;&#21363;&#20351;&#27491;&#30830;&#30340;&#26102;&#38388;&#28857;&#26159;&#22312;&#26080;&#20851;&#30340;&#24103;&#20013;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EI-VLG&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#25552;&#20379;&#30340;&#20016;&#23500;&#25991;&#26412;&#20449;&#24687;&#20316;&#20026;&#20154;&#31867;&#32463;&#39564;&#30340;&#26367;&#20195;&#21697;&#65292;&#24110;&#21161;&#25105;&#20204;&#26377;&#25928;&#25490;&#38500;&#26080;&#20851;&#24103;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;EgoNLQ&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoNFAP&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#38598;&#25104;&#20266;&#36896;&#24847;&#35782;&#32479;&#19968;&#39044;&#27979;&#21644;&#22122;&#22768;&#28151;&#21512;&#27169;&#22359;&#22686;&#24378;&#20102;&#22810;&#33080;&#25805;&#32437;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02306</link><description>&lt;p&gt;
&#36890;&#36807;&#22122;&#22768;&#28151;&#21512;&#22686;&#24378;&#30340;&#20266;&#36896;&#24847;&#35782;&#39044;&#27979;&#22120;&#29992;&#20110;&#22810;&#33080;&#25805;&#32437;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face Manipulation Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoNFAP&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#38598;&#25104;&#20266;&#36896;&#24847;&#35782;&#32479;&#19968;&#39044;&#27979;&#21644;&#22122;&#22768;&#28151;&#21512;&#27169;&#22359;&#22686;&#24378;&#20102;&#22810;&#33080;&#25805;&#32437;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#33080;&#25805;&#32437;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#22810;&#33080;&#22330;&#26223;&#20013;&#30340;&#20266;&#36896;&#22270;&#20687;&#36880;&#28176;&#25104;&#20026;&#19968;&#20010;&#26356;&#22797;&#26434;&#12289;&#26356;&#20855;&#29616;&#23454;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38024;&#23545;&#36825;&#31867;&#22810;&#33080;&#25805;&#32437;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#26041;&#27861;&#20173;&#28982;&#21457;&#23637;&#19981;&#36275;&#12290;&#20256;&#32479;&#30340;&#25163;&#21160;&#23450;&#20301;&#26041;&#27861;&#35201;&#20040;&#38388;&#25509;&#20174;&#23450;&#20301;&#25513;&#33180;&#20013;&#25512;&#23548;&#20986;&#26816;&#27979;&#32467;&#26524;&#65292;&#20174;&#32780;&#23548;&#33268;&#26816;&#27979;&#24615;&#33021;&#26377;&#38480;&#65292;&#35201;&#20040;&#37319;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#20998;&#25903;&#32467;&#26500;&#21516;&#26102;&#33719;&#24471;&#26816;&#27979;&#21644;&#23450;&#20301;&#32467;&#26524;&#65292;&#30001;&#20110;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#27809;&#26377;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#36825;&#19981;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#23450;&#20301;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;MoNFAP&#65292;&#19987;&#38376;&#29992;&#20110;&#22810;&#33080;&#25805;&#32437;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;MoNFAP&#20027;&#35201;&#24341;&#20837;&#20102;&#20004;&#39033;&#26032;&#30340;&#27169;&#22359;&#65306;&#20266;&#36896;&#24847;&#35782;&#32479;&#19968;&#39044;&#27979;&#65288;FUP&#65289;&#27169;&#22359;&#21644;&#22122;&#22768;&#28151;&#21512;&#27169;&#22359;&#65288;MNM&#65289;&#12290;FUP&#27169;&#22359;&#23558;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#25972;&#21512;&#22312;&#19968;&#36215;&#20351;&#29992;&#12290;MNM&#27169;&#22359;&#36890;&#36807;&#25552;&#21462;&#22122;&#22768;&#29305;&#24449;&#26469;&#22686;&#24378;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02306v1 Announce Type: new  Abstract: With the advancement of face manipulation technology, forgery images in multi-face scenarios are gradually becoming a more complex and realistic challenge. Despite this, detection and localization methods for such multi-face manipulations remain underdeveloped. Traditional manipulation localization methods either indirectly derive detection results from localization masks, resulting in limited detection performance, or employ a naive two-branch structure to simultaneously obtain detection and localization results, which cannot effectively benefit the localization capability due to limited interaction between two tasks. This paper proposes a new framework, namely MoNFAP, specifically tailored for multi-face manipulation detection and localization. The MoNFAP primarily introduces two novel modules: the Forgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module (MNM). The FUP integrates detection and localization tasks us
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Network Fission Ensembles&#30340;&#20302;&#25104;&#26412;&#38598;&#25104;&#23398;&#20064;&#19982;&#25512;&#35770;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#32593;&#32476;&#20013;&#29983;&#25104;&#22810;&#20010;&#36755;&#20986;&#26469;&#23454;&#29616;&#38598;&#25104;&#65292;&#28040;&#38500;&#20102;&#22810;&#37325;&#27169;&#22411;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.02301</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20302;&#25104;&#26412;&#33258;&#25105;&#23396;&#31435;&#38598;&#30340;&#32593;&#32476;&#20998;&#35010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Network Fission Ensembles for Low-Cost Self-Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02301
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Network Fission Ensembles&#30340;&#20302;&#25104;&#26412;&#38598;&#25104;&#23398;&#20064;&#19982;&#25512;&#35770;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#32593;&#32476;&#20013;&#29983;&#25104;&#22810;&#20010;&#36755;&#20986;&#26469;&#23454;&#29616;&#38598;&#25104;&#65292;&#28040;&#38500;&#20102;&#22810;&#37325;&#27169;&#22411;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2408.02301v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38656;&#35201;&#22312;&#38598;&#25104;&#25512;&#29702;&#20013;&#20351;&#29992;&#22810;&#20010;&#35757;&#32451;&#27169;&#22411;&#65292;&#24403;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#26102;&#65292;&#36825;&#26368;&#32456;&#23558;&#25104;&#20026;&#19968;&#39033;&#37325;&#22823;&#36127;&#25285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#25104;&#26412;&#38598;&#25104;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Network Fission Ensembles&#65288;NFE&#65289;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#32593;&#32476;&#33258;&#36523;&#36716;&#21270;&#20026;&#22810;&#20986;&#21475;&#32467;&#26500;&#12290;&#20174;&#19968;&#20010;&#32473;&#23450;&#30340;&#21021;&#22987;&#32593;&#32476;&#24320;&#22987;&#65292;&#25105;&#20204;&#39318;&#20808;&#21098;&#26525;&#19968;&#20123;&#26435;&#37325;&#20197;&#20943;&#23569;&#35757;&#32451;&#36127;&#25285;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#21097;&#20313;&#30340;&#26435;&#37325;&#20998;&#32452;&#20026;&#33509;&#24178;&#32452;&#65292;&#24182;&#20026;&#27599;&#32452;&#21019;&#24314;&#22810;&#20010;&#36741;&#21161;&#36335;&#24452;&#20197;&#26500;&#36896;&#22810;&#20986;&#21475;&#12290;&#25105;&#20204;&#31216;&#36825;&#19968;&#36807;&#31243;&#20026;Network Fission&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#32593;&#32476;&#20013;&#33719;&#24471;&#22810;&#20010;&#36755;&#20986;&#65292;&#36825;&#20351;&#24471;&#38598;&#25104;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#30001;&#20110;&#36825;&#20010;&#36807;&#31243;&#20165;&#23558;&#29616;&#26377;&#32593;&#32476;&#30340;&#32467;&#26500;&#25913;&#21464;&#20026;&#22810;&#20986;&#21475;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#39069;&#22806;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#39640;&#25928;&#22320;&#21019;&#24314;&#22810;&#20010;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#27010;&#29575;&#30340;&#19978;&#37319;&#26679;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#27809;&#26377;&#26174;&#33879;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02301v1 Announce Type: new  Abstract: Recent ensemble learning methods for image classification have been shown to improve classification accuracy with low extra cost. However, they still require multiple trained models for ensemble inference, which eventually becomes a significant burden when the model size increases. In this paper, we propose a low-cost ensemble learning and inference, called Network Fission Ensembles (NFE), by converting a conventional network itself into a multi-exit structure. Starting from a given initial network, we first prune some of the weights to reduce the training burden. We then group the remaining weights into several sets and create multiple auxiliary paths using each set to construct multi-exits. We call this process Network Fission. Through this, multiple outputs can be obtained from a single network, which enables ensemble learning. Since this process simply changes the existing network structure to multi-exits without using additional net
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#36890;&#36807;&#26657;&#20934;&#24863;&#30693;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#25913;&#36827;&#20102;&#36523;&#20307;&#20154;&#24037;&#26234;&#33021;&#30340;&#25628;&#32034;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.02297</link><description>&lt;p&gt;
&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#65306;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;&#22686;&#24378;&#36523;&#20307;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02297
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#36890;&#36807;&#26657;&#20934;&#24863;&#30693;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#25913;&#36827;&#20102;&#36523;&#20307;&#20154;&#24037;&#26234;&#33021;&#30340;&#25628;&#32034;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02297v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;&#30340;&#25688;&#35201;&#65306;&#36523;&#20307;&#20154;&#24037;&#26234;&#33021;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#29289;&#20307;&#25628;&#32034;&#31561;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#26377;&#25928;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24403;&#21069;&#25628;&#32034;&#26041;&#27861;&#20013;&#30340;&#20960;&#20010;&#24046;&#36317;&#65306;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#36807;&#26102;&#30340;&#24863;&#30693;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#32858;&#21512;&#65292;&#24182;&#19988;&#20174;&#22320;&#38754;&#30495;&#23454;&#30452;&#25509;&#36716;&#31227;&#21040;&#27979;&#35797;&#26102;&#30340;&#22122;&#22768;&#24863;&#30693;&#65292;&#32780;&#19981;&#32771;&#34385;&#21040;&#24863;&#30693;&#29366;&#24577;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#36890;&#36807;&#26657;&#20934;&#30340;&#24863;&#30693;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#32858;&#21512;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#20197;&#22788;&#29702;&#24207;&#21015;&#20219;&#21153;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#38598;&#25104;&#21040;&#19968;&#31995;&#21015;&#29616;&#26377;&#30340;&#25628;&#32034;&#26041;&#27861;&#20013;&#65292;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#27169;&#22411;&#21644;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#30830;&#35748;&#20102;&#26657;&#20934;&#24863;&#30693;&#27010;&#29575;&#22312;&#32858;&#21512;&#26041;&#27861;&#21644;&#19981;&#21516;&#24863;&#30693;&#27169;&#22411;&#21644;&#31574;&#30053;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02297v1 Announce Type: new  Abstract: Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through calibrated perception probabilities and uncertainty across aggregation and found decisions, thereby adapting the models for sequential tasks. The resulting methods can be directly integrated with pretrained models across a wide family of existing search approaches at no additional training cost. We perform extensive evaluations of aggregation methods across both different semantic perception models and policies, confirming the importance of calib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#23039;&#24577;&#20272;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#21407;&#22987;&#20851;&#33410;&#20449;&#24687;&#21644;&#36816;&#21160;&#29305;&#24449;&#21160;&#24577;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02285</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#23039;&#24577;&#20272;&#35745;&#30340;&#32852;&#21512;&#36816;&#21160;&#20114;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint-Motion Mutual Learning for Pose Estimation in Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#23039;&#24577;&#20272;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#21407;&#22987;&#20851;&#33410;&#20449;&#24687;&#21644;&#36816;&#21160;&#29305;&#24449;&#21160;&#24577;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#20154;&#33080;&#23039;&#24577;&#20272;&#35745;&#19968;&#30452;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#26082;&#24341;&#20154;&#20837;&#32988;&#21448;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#35270;&#39057;&#22330;&#26223;&#20013;&#30340;&#35270;&#39057;&#27169;&#31946;&#21644;&#33258;&#25105;&#36974;&#25377;&#31561;&#21407;&#22240;&#65292;&#35813;&#20219;&#21153;&#20173;&#28982;&#38750;&#24120;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#25972;&#21512;&#30001;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#30340;&#22810;&#24103;&#35270;&#35273;&#29305;&#24449;&#26469;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#21021;&#22987;&#28909;&#22270;&#20013;&#30340;&#26377;&#29992;&#20851;&#33410;&#20449;&#24687;&#65292;&#28909;&#22270;&#26159;&#39592;&#24178;&#29983;&#25104;&#30340;&#21103;&#20135;&#21697;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35797;&#22270;&#32454;&#21270;&#21021;&#22987;&#28909;&#22270;&#30340;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#20219;&#20309;&#31354;&#38388;&#21644;&#26102;&#38388;&#36816;&#21160;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#30340;&#24615;&#33021;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#21033;&#29992;&#23616;&#37096;&#20851;&#33410;&#65288;&#28909;&#22270;&#65289;&#20449;&#24687;&#21644;&#20840;&#23616;&#36816;&#21160;&#65288;&#29305;&#24449;&#65289;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23039;&#24577;&#20272;&#35745;&#30340;&#32852;&#21512;&#36816;&#21160;&#20114;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#38598;&#20013;&#22312;&#21516;&#26102;&#21033;&#29992;&#21407;&#22987;&#20851;&#33410;&#20449;&#24687;&#21644;&#36816;&#21160;&#29305;&#24449;&#21160;&#24577;&#19978;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#39057;&#20013;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02285v1 Announce Type: new  Abstract: Human pose estimation in videos has long been a compelling yet challenging task within the realm of computer vision. Nevertheless, this task remains difficult because of the complex video scenes, such as video defocus and self-occlusion. Recent methods strive to integrate multi-frame visual features generated by a backbone network for pose estimation. However, they often ignore the useful joint information encoded in the initial heatmap, which is a by-product of the backbone generation. Comparatively, methods that attempt to refine the initial heatmap fail to consider any spatio-temporal motion features. As a result, the performance of existing methods for pose estimation falls short due to the lack of ability to leverage both local joint (heatmap) information and global motion (feature) dynamics.   To address this problem, we propose a novel joint-motion mutual learning framework for pose estimation, which effectively concentrates on bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#32423;&#32852;&#31934;&#21270;&#35270;&#39057;&#21435;&#22122;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#21319;&#37319;&#26679;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;CRVD&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#24133;&#24230;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#21019;&#24314;&#19981;&#30830;&#23450;&#24230;&#22270;&#65292;&#35813;&#26041;&#27861;&#24179;&#22343;&#20943;&#23569;&#20102;25%&#30340;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.02284</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#21319;&#37319;&#26679;&#35270;&#39057;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cascading Refinement Video Denoising with Uncertainty Adaptivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#32423;&#32852;&#31934;&#21270;&#35270;&#39057;&#21435;&#22122;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#21319;&#37319;&#26679;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;CRVD&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#24133;&#24230;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#21019;&#24314;&#19981;&#30830;&#23450;&#24230;&#22270;&#65292;&#35813;&#26041;&#27861;&#24179;&#22343;&#20943;&#23569;&#20102;25%&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;arXiv:2408.02284v1&#20013;&#23459;&#24067;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25688;&#35201;&#12290;&#20934;&#30830;&#30340;&#20301;&#32622;&#21305;&#37197;&#26159;&#35270;&#39057;&#21435;&#22122;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#20272;&#35745;&#20301;&#32622;&#21305;&#37197;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32423;&#32852;&#31934;&#21270;&#35270;&#39057;&#21435;&#22122;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#31934;&#21270;&#20301;&#32622;&#21305;&#37197;&#30340;&#21516;&#26102;&#24674;&#22797;&#22270;&#20687;&#12290;&#26356;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#24674;&#22797;&#26356;&#22810;&#30340;&#32454;&#33410;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#26356;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#23558;&#24102;&#26469;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21305;&#37197;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#36229;&#36234;&#20102;&#29366;&#24577;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#21518;&#21019;&#24314;&#20102;&#19968;&#20010;&#19981;&#30830;&#23450;&#24230;&#22270;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#37027;&#20123;&#23481;&#26131;&#24674;&#22797;&#30340;&#35270;&#39057;&#65292;&#26080;&#38656;&#36827;&#34892;&#22810;&#20313;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24179;&#22343;&#20943;&#23569;&#20102;25%&#30340;&#24635;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02284v1 Announce Type: new  Abstract: Accurate alignment is crucial for video denoising. However, estimating alignment in noisy environments is challenging. This paper introduces a cascading refinement video denoising method that can refine alignment and restore images simultaneously. Better alignment enables restoration of more detailed information in each frame. Furthermore, better image quality leads to better alignment. This method has achieved SOTA performance by a large margin on the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an uncertainty map was created after each iteration. Because of this, redundant computation on the easily restored videos was avoided. By applying this method, the entire computation was reduced by 25% on average.
&lt;/p&gt;</description></item><item><title>&#26412;&#31995;&#32479;&#21033;&#29992;&#20960;&#20309;&#20195;&#25968;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;3D&#22330;&#26223;&#20013;&#23545;&#35937;&#31934;&#30830;&#37325;&#26032;&#23450;&#20301;&#65292;&#26080;&#38656;&#19987;&#19994;&#35757;&#32451;&#25968;&#25454;&#65292;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21363;&#21487;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2408.02275</link><description>&lt;p&gt;
&#20960;&#20309;&#20195;&#25968;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#65306;3D&#20132;&#20114;&#22411;&#21487;&#25511;&#22330;&#26223;&#20013;&#20998;&#31163;&#32593;&#26684;&#30340;&#25351;&#20196;&#39537;&#21160;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31995;&#32479;&#21033;&#29992;&#20960;&#20309;&#20195;&#25968;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;3D&#22330;&#26223;&#20013;&#23545;&#35937;&#31934;&#30830;&#37325;&#26032;&#23450;&#20301;&#65292;&#26080;&#38656;&#19987;&#19994;&#35757;&#32451;&#25968;&#25454;&#65292;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21363;&#21487;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#24247;&#24335;&#20960;&#20309;&#20195;&#25968;(CGA)&#30340;&#38598;&#25104;&#65292;&#23427;&#26088;&#22312;&#38761;&#26032;&#21487;&#25511;3D&#22330;&#26223;&#32534;&#36753;&#65292;&#29305;&#21035;&#26159;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#25163;&#21160;&#27969;&#31243;&#21644;&#19987;&#19994;&#25216;&#33021;&#12290;&#36825;&#20123;&#20256;&#32479;&#30340;&#25216;&#33402;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#32570;&#20047;&#23545;&#31934;&#30830;&#32534;&#36753;&#30340;&#27491;&#24335;&#35821;&#35328;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#8220;shenlong&#8221;&#20351;&#29992;CGA&#20316;&#20026;&#24378;&#22823;&#30340;&#27491;&#24335;&#35821;&#35328;&#65292;&#31934;&#30830;&#22320;&#24314;&#27169;&#20102;&#36827;&#34892;&#31934;&#30830;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#25152;&#38656;&#30340;&#31354;&#38388;&#21464;&#25442;&#12290;&#20511;&#21161;&#39044;&#35757;&#32451;LLMs&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#65292;&#8220;shenlong&#8221;&#21487;&#20197;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#32763;&#35793;&#25104;CGA&#25805;&#20316;&#65292;&#24182;&#23558;&#36825;&#20123;&#25805;&#20316;&#24212;&#29992;&#20110;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#21464;&#25442;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#25968;&#25454;&#39044;&#35757;&#32451;&#12290;&#22312;&#29616;&#23454;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#23454;&#26045;&#21518;&#65292;&#8220;shenlong&#8221;&#30830;&#20445;&#20102;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02275v1 Announce Type: new  Abstract: This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, shenlong ensures compatibility with existing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;COM&#21416;&#25151;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#30001;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#26410;&#32463;&#32534;&#36753;&#30340;&#19978;&#26041;&#35270;&#35282;&#28921;&#39274;&#35270;&#39057;&#32452;&#25104;&#65292;&#26088;&#22312;&#24110;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#21306;&#20013;&#23545;&#31243;&#24207;&#35270;&#39057;&#36827;&#34892;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2408.02272</link><description>&lt;p&gt;
COM&#21416;&#25151;&#65306;&#20316;&#20026;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#30340;&#26410;&#32463;&#32534;&#36753;&#30340;&#19978;&#26041;&#35270;&#35282;&#35270;&#39057;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
COM Kitchens: An Unedited Overhead-view Video Dataset as a Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;COM&#21416;&#25151;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#30001;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#26410;&#32463;&#32534;&#36753;&#30340;&#19978;&#26041;&#35270;&#35282;&#28921;&#39274;&#35270;&#39057;&#32452;&#25104;&#65292;&#26088;&#22312;&#24110;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#21306;&#20013;&#23545;&#31243;&#24207;&#35270;&#39057;&#36827;&#34892;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02272v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#21306;&#20013;&#65292;&#31243;&#24207;&#35270;&#39057;&#29702;&#35299;&#27491;&#24471;&#21040;&#20851;&#27880;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#39057;&#20998;&#26512;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#32593;&#19978;&#30340;&#35270;&#39057;&#20316;&#20026;&#35757;&#32451;&#36164;&#28304;&#65292;&#36825;&#22312;&#20174;&#21407;&#22987;&#35270;&#39057;&#35266;&#23519;&#20013;&#26597;&#35810;&#25351;&#23548;&#20869;&#23481;&#26041;&#38754;&#36896;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;COM&#21416;&#25151;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#26410;&#32463;&#32534;&#36753;&#30340;&#19978;&#26041;&#35270;&#35282;&#35270;&#39057;&#32452;&#25104;&#65292;&#21442;&#19982;&#32773;&#26681;&#25454;&#32473;&#20986;&#30340;&#39135;&#35889;&#36827;&#34892;&#39135;&#21697;&#20934;&#22791;&#12290;&#30001;&#20110;&#39640;&#39069;&#30340;&#25668;&#20687;&#26426;&#35774;&#32622;&#25104;&#26412;&#65292;&#22266;&#23450;&#35270;&#35282;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#36890;&#24120;&#32570;&#20047;&#29615;&#22659;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#20195;&#30340;&#23485;&#35270;&#35282;&#26234;&#33021;&#25163;&#26426;&#38236;&#22836;&#35206;&#30422;&#20174;&#27700;&#27133;&#21040;&#28809;&#28790;&#30340;&#19978;&#26041;&#35270;&#35282;&#28921;&#39274;&#21488;&#38754;&#65292;&#26080;&#38656;&#20154;&#24037;&#24110;&#21161;&#21363;&#21487;&#25429;&#25417;&#27963;&#21160;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#32622;&#65292;&#25105;&#20204;&#21521;&#21442;&#19982;&#32773;&#20998;&#21457;&#20102;&#26234;&#33021;&#25163;&#26426;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#22411;&#35270;&#39057;&#21040;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#8220;&#21363;&#26102;&#37325;&#26032;&#20307;&#29616;&#8221;&#21644;&#8220;&#20102;&#35299;&#25991;&#26412;&#30340;&#35270;&#35273;&#21453;&#23556;&#8221;&#20026;&#19968;&#20307;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02272v1 Announce Type: new  Abstract: Procedural video understanding is gaining attention in the vision and language community. Deep learning-based video analysis requires extensive data. Consequently, existing works often use web videos as training resources, making it challenging to query instructional contents from raw video observations. To address this issue, we propose a new dataset, COM Kitchens. The dataset consists of unedited overhead-view videos captured by smartphones, in which participants performed food preparation based on given recipes. Fixed-viewpoint video datasets often lack environmental diversity due to high camera setup costs. We used modern wide-angle smartphone lenses to cover cooking counters from sink to cooktop in an overhead view, capturing activity without in-person assistance. With this setup, we collected a diverse dataset by distributing smartphones to participants. With this dataset, we propose the novel video-to-text retrieval task Online Re
&lt;/p&gt;</description></item><item><title>VoxelTrack&#26159;&#19968;&#20010;&#26032;&#22411;3D&#23545;&#35937;&#36319;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#20307;&#20803;&#21270;&#21644;&#31232;&#30095;&#21367;&#31215;&#22359;&#26377;&#25928;&#24314;&#27169;&#20102;&#31934;&#30830;&#30340;3D&#31354;&#38388;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#36319;&#36394;&#23545;&#35937;&#20301;&#32622;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02263</link><description>&lt;p&gt;
VoxelTrack&#65306;&#25506;&#32034;&#29992;&#20110;3D&#28857;&#20113;&#23545;&#35937;&#36319;&#36394;&#30340;&#20307;&#20803;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02263
&lt;/p&gt;
&lt;p&gt;
VoxelTrack&#26159;&#19968;&#20010;&#26032;&#22411;3D&#23545;&#35937;&#36319;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#20307;&#20803;&#21270;&#21644;&#31232;&#30095;&#21367;&#31215;&#22359;&#26377;&#25928;&#24314;&#27169;&#20102;&#31934;&#30830;&#30340;3D&#31354;&#38388;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#36319;&#36394;&#23545;&#35937;&#20301;&#32622;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02263v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#24403;&#21069;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;3D&#21333;&#23545;&#35937;&#36319;&#36394;&#65288;SOT&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#32593;&#32476;&#12290;&#23613;&#31649;&#36825;&#20123;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30830;&#23454;&#23384;&#22312;&#19968;&#20123;&#26681;&#26412;&#38382;&#39064;&#65306;1&#65289;&#23427;&#21253;&#21547;&#27744;&#21270;&#25805;&#20316;&#20197;&#24212;&#23545;&#22266;&#26377;&#30340;&#26080;&#24207;&#28857;&#20113;&#65292;&#36825;&#38459;&#30861;&#20102;&#25429;&#33719;&#23545;&#20110;&#36319;&#36394;&#65292;&#21363;&#39044;&#27979;&#20219;&#21153;&#26377;&#29992;&#30340;3D&#31354;&#38388;&#20449;&#24687;&#12290;2&#65289;&#37319;&#29992;&#30340;&#38598;&#21512;&#25277;&#35937;&#25805;&#20316;&#24456;&#38590;&#22788;&#29702;&#23494;&#24230;&#19981;&#19968;&#33268;&#30340;&#28857;&#20113;&#65292;&#36825;&#20063;&#38459;&#27490;&#20102;&#23545;3D&#31354;&#38388;&#20449;&#24687;&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36319;&#36394;&#26694;&#26550;&#65292;&#31216;&#20026;VoxelTrack&#12290;&#36890;&#36807;&#23558;&#22266;&#26377;&#26080;&#24207;&#30340;&#28857;&#20113;&#20307;&#20803;&#21270;&#24182;&#20351;&#29992;&#31232;&#30095;&#21367;&#31215;&#22359;&#25552;&#21462;&#20854;&#29305;&#24449;&#65292;VoxelTrack&#26377;&#25928;&#22320;&#23545;3D&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#20174;&#32780;&#24341;&#23548;&#20102;&#23545;&#36319;&#36394;&#23545;&#35937;&#31934;&#30830;&#20301;&#32622;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;VoxelTrack&#32467;&#21512;&#20102;&#19968;&#20010;&#21452;&#27969;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#20855;&#26377;&#36328;&#23618;&#29305;&#24449;&#34701;&#21512;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#39640;&#32500;&#38271;&#30340;&#26368;&#30701;&#36335;&#24452;&#20248;&#21270;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;VoxelTrack&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;3D SOT&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#28857;&#20113;&#36319;&#36394;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02263v1 Announce Type: new  Abstract: Current LiDAR point cloud-based 3D single object tracking (SOT) methods typically rely on point-based representation network. Despite demonstrated success, such networks suffer from some fundamental problems: 1) It contains pooling operation to cope with inherently disordered point clouds, hindering the capture of 3D spatial information that is useful for tracking, a regression task. 2) The adopted set abstraction operation hardly handles density-inconsistent point clouds, also preventing 3D spatial information from being modeled. To solve these problems, we introduce a novel tracking framework, termed VoxelTrack. By voxelizing inherently disordered point clouds into 3D voxels and extracting their features via sparse convolution blocks, VoxelTrack effectively models precise and robust 3D spatial information, thereby guiding accurate position prediction for tracked objects. Moreover, VoxelTrack incorporates a dual-stream encoder with cros
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#21435;&#22122;&#25216;&#26415;&#30340;&#26032;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#25913;&#36827;&#22270;&#20687;&#29702;&#35299;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;RGB-D&#25968;&#25454;&#38598;&#19978;&#12290;</title><link>https://arxiv.org/abs/2408.02245</link><description>&lt;p&gt;
&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#21435;&#22122;&#25216;&#26415;&#30340;&#26032;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#25913;&#36827;&#22270;&#20687;&#29702;&#35299;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;RGB-D&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02245v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35838;&#31243;&#23398;&#20064;(CL)&#33539;&#24335;&#19979;&#29992;&#20110;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;RGB-D&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#20197;&#21450;&#21435;&#22122;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#65288;&#20363;&#22914;&#65292;MultiMAE&#65289;&#65292;&#35201;&#20040;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#65288;&#20363;&#22914;&#65292;Pri3D&#65289;&#65292;&#25110;&#32773;&#22312;&#21333;&#19968;&#30340;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;CMAE&#21644;CAV-MAE&#12290;&#28982;&#32780;&#65292;&#21333;&#19968;&#30340;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#37117;&#19981;&#33021;&#36866;&#29992;&#20110;RGB-D&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#26041;&#27861;&#30340;&#34920;&#29616;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CL&#30340;&#26032;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#31532;&#19968;&#38454;&#27573;&#33719;&#24471;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#20351;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#21644;&#21435;&#22122;/&#22122;&#22768;&#39044;&#27979;&#32487;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#31532;&#19977;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#36873;&#25321;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;RGB-D&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#39564;&#35777;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; ProCreate &#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#24046;&#24322;&#21270;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#21019;&#36896;&#21147;&#65292;&#24182;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#22797;&#12290;&#23427;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25512;&#21160;&#29983;&#25104;&#30340;&#22270;&#20687;&#36828;&#31163;&#21442;&#32771;&#22270;&#20687;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#31574;&#30053;&#22312;&#22788;&#29702;&#20843;&#20010;&#19981;&#21516;&#31867;&#21035;&#23569;&#26679;&#26412;&#21019;&#24847;&#29983;&#25104;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#25991;&#26412;&#25552;&#31034;&#35780;&#20272;&#20013;&#20063;&#26174;&#31034;&#20986;&#20102;&#38450;&#27490;&#20877;&#29616;&#35757;&#32451;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02226</link><description>&lt;p&gt;
&#21019;&#29983;&#32780;&#38750;&#22797;&#21046;&#65281;&#29992;&#20110;&#21019;&#24847;&#29983;&#25104;&#30340;&#25512;&#21160;&#33021;&#25193;&#25955;&#27861;
&lt;/p&gt;
&lt;p&gt;
ProCreate, Don\'t Reproduce! Propulsive Energy Diffusion for Creative Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; ProCreate &#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#24046;&#24322;&#21270;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#21019;&#36896;&#21147;&#65292;&#24182;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#22797;&#12290;&#23427;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25512;&#21160;&#29983;&#25104;&#30340;&#22270;&#20687;&#36828;&#31163;&#21442;&#32771;&#22270;&#20687;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#31574;&#30053;&#22312;&#22788;&#29702;&#20843;&#20010;&#19981;&#21516;&#31867;&#21035;&#23569;&#26679;&#26412;&#21019;&#24847;&#29983;&#25104;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#25991;&#26412;&#25552;&#31034;&#35780;&#20272;&#20013;&#20063;&#26174;&#31034;&#20986;&#20102;&#38450;&#27490;&#20877;&#29616;&#35757;&#32451;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ProCreate &#30340;&#31616;&#21333;&#19988;&#26131;&#20110;&#23454;&#26045;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#24046;&#24322;&#21270;&#22522;&#30784;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20854;&#21019;&#36896;&#21147;&#65292;&#24182;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#29616;&#12290;&#35813;&#31574;&#30053;&#22312;&#23545;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#25805;&#20316;&#26102;&#65292;&#20027;&#21160;&#25512;&#21160;&#29983;&#25104;&#30340;&#22270;&#20687;&#23884;&#20837;&#36828;&#31163;&#21442;&#32771;&#23884;&#20837;&#65292;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#8220;&#23569;&#26679;&#26412;&#21019;&#24847;&#29983;&#25104;8&#65288;Few-Shot Creative Generation 8&#65289;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20843;&#20010;&#19981;&#21516;&#31867;&#21035;&#65288;&#28085;&#30422;&#19981;&#21516;&#27010;&#24565;&#12289;&#39118;&#26684;&#21644;&#35774;&#32622;&#65289;&#19978;&#30340;&#23569;&#26679;&#26412;&#21019;&#24847;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;ProCreate&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#26368;&#39640;&#30340;&#25104;&#23601;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; ProCreate &#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#25991;&#26412;&#25552;&#31034;&#35780;&#20272;&#20013;&#38450;&#27490;&#20877;&#29616;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#37117;&#21487;&#22312; https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public &#19978;&#20813;&#36153;&#33719;&#21462;&#12290;&#26356;&#22810;&#35814;&#24773;&#21487;&#35775;&#38382; https://procreate-diffusion.github.io&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02226v1 Announce Type: new  Abstract: In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts. Code and FSCG-8 are available at https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The project page is available at https://procreate-diffusion.github.io.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#22522;&#20934;&#65292;&#20197;&#31934;&#32454;&#31890;&#24230;&#23398;&#20064;&#20026;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;AI&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#29616;&#26377;&#30693;&#35782;&#30028;&#38480;&#65292;&#25429;&#25417;&#24182;&#20256;&#36882;&#26356;&#35814;&#32454;&#30340;&#30142;&#30149;&#29366;&#24577;&#20449;&#24687;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#30340;AI&#35786;&#26029;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.02214</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#26159;&#38451;&#24615;&#19982;&#38452;&#24615;&#65306;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#20256;&#36798;&#31934;&#32454;&#31890;&#24230;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
More Than Positive and Negative: Communicating Fine Granularity in Medical Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02214
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#22522;&#20934;&#65292;&#20197;&#31934;&#32454;&#31890;&#24230;&#23398;&#20064;&#20026;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;AI&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#29616;&#26377;&#30693;&#35782;&#30028;&#38480;&#65292;&#25429;&#25417;&#24182;&#20256;&#36882;&#26356;&#35814;&#32454;&#30340;&#30142;&#30149;&#29366;&#24577;&#20449;&#24687;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#30340;AI&#35786;&#26029;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02214v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#22312;&#33258;&#21160;&#33016;&#37096;X&#20809;&#29255;&#65288;CXR&#65289;&#20998;&#26512;&#30340;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#24314;&#35774;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;AI&#27169;&#22411;&#37117;&#26159;&#22312;&#34987;&#35757;&#32451;&#25104;&#20108;&#20998;&#31867;&#22120;&#65292;&#26088;&#22312;&#21306;&#20998;&#38451;&#24615;&#19982;&#38452;&#24615;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#20108;&#20803;&#35774;&#32622;&#19982;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#21307;&#23398;&#22330;&#26223;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#21160;&#25918;&#23556;&#23398;&#35786;&#26029;&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#38451;&#24615;&#31867;&#21035;&#30340;&#26696;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#24847;&#21619;&#30528;&#31616;&#21333;&#22320;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#38451;&#24615;&#20250;&#20007;&#22833;&#35768;&#22810;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#26500;&#24314;&#33021;&#22815;&#20174;&#21307;&#30103;&#22270;&#20687;&#20013;&#20256;&#36798;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#31934;&#32454;&#31890;&#24230;&#30693;&#35782;&#30340;AI&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#26032;&#30340;&#31934;&#32454;&#31890;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#30340;&#22522;&#20934;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#20998;&#21106;&#35268;&#21017;&#65292;&#23558;&#38451;&#24615;&#26696;&#20363;&#20998;&#20026;&#26356;&#32454;&#31890;&#24230;&#30340;&#23376;&#31867;&#21035;&#12290;&#36890;&#36807;&#22312;&#22823;&#22411;CXR&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#19968;&#31995;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20248;&#31168;&#27169;&#22411;&#33021;&#22815;&#36328;&#36234;&#29616;&#26377;&#30693;&#35782;&#36793;&#30028;&#65292;&#25429;&#33719;&#24182;&#20256;&#36882;&#20851;&#20110;&#30142;&#30149;&#29366;&#24577;&#30340;&#26356;&#22810;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#37325;&#35201;&#20294;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#21307;&#23398;&#22270;&#20687;&#30340;AI&#35786;&#26029;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02214v1 Announce Type: new  Abstract: With the advance of deep learning, much progress has been made in building powerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR) analysis. Most existing AI models are trained to be a binary classifier with the aim of distinguishing positive and negative cases. However, a large gap exists between the simple binary setting and complicated real-world medical scenarios. In this work, we reinvestigate the problem of automatic radiology diagnosis. We first observe that there is considerable diversity among cases within the positive class, which means simply classifying them as positive loses many important details. This motivates us to build AI models that can communicate fine-grained knowledge from medical images like human experts. To this end, we first propose a new benchmark on fine granularity learning from medical images. Specifically, we devise a division rule based on medical knowledge to divide positive cases i
&lt;/p&gt;</description></item><item><title>ExoViP&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39564;&#35777;&#27169;&#22359;&#30340;&#36741;&#21161;&#65292;&#33021;&#22815;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32534;&#31243;&#26041;&#26696;&#20013;&#35268;&#21010;&#19982;&#25191;&#34892;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#20219;&#21153;&#30340;&#27491;&#30830;&#25191;&#34892;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2408.02210</link><description>&lt;p&gt;
ExoViP: &#36880;&#27493;&#39564;&#35777;&#19982;&#25506;&#32034;&#19982;&#22806;&#39592;&#39612;&#27169;&#22359;&#32467;&#21512;&#30340;&#26500;&#25104;&#24615;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02210
&lt;/p&gt;
&lt;p&gt;
ExoViP&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39564;&#35777;&#27169;&#22359;&#30340;&#36741;&#21161;&#65292;&#33021;&#22815;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32534;&#31243;&#26041;&#26696;&#20013;&#35268;&#21010;&#19982;&#25191;&#34892;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#20219;&#21153;&#30340;&#27491;&#30830;&#25191;&#34892;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#37324;&#65292;arXiv:2408.02210v1 &#23459;&#24067;&#31867;&#22411;&#20026;&#26032;&#12290;&#25688;&#35201;&#65306;&#26500;&#25104;&#24615;&#35270;&#35273;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#22797;&#26434;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#21487;&#34892;&#30340;&#35270;&#35273;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#32452;&#21512;&#65292;&#24050;&#32463;&#22312;&#22797;&#26434;&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#25512;&#21160;&#19979;&#65292;&#36825;&#31181;&#22810;&#27169;&#24577;&#25361;&#25112;&#24050;&#32463;&#36890;&#36807;&#23558;LLM&#35270;&#20026;&#23569;&#26679;&#26412;/&#38646;&#26679;&#26412;&#35268;&#21010;&#32773;&#65292;&#21363;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#32534;&#31243;&#65292;&#36798;&#21040;&#19968;&#20010;&#26032;&#30340;&#38454;&#27573;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25317;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#23427;&#20204;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;LLM&#35268;&#21010;&#38169;&#35823;&#25110;&#35270;&#35273;&#25191;&#34892;&#27169;&#22359;&#30340;&#19981;&#20934;&#30830;&#65292;&#36825;&#20351;&#23427;&#20204;&#33853;&#21518;&#20110;&#38750;&#26500;&#25104;&#24615;&#27169;&#22411;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;ExoViP&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21453;&#30465;&#30340;&#39564;&#35777;&#26469;&#32416;&#27491;&#35268;&#21010;&#21644;&#31649;&#29702;&#38454;&#27573;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#21033;&#29992;&#39564;&#35777;&#27169;&#22359;&#20316;&#20026;&#8220;&#22806;&#39592;&#39612;&#8221;&#26469;&#22686;&#24378;&#24403;&#21069;&#30340;VL&#32534;&#31243;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#39564;&#35777;&#27169;&#22359;&#20351;&#29992;&#20102;&#19977;&#31181;&#23376;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22806;&#39592;&#39612;&#27169;&#22359;&#65292;&#19981;&#20165;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#32780;&#19988;&#22312;&#22806;&#39592;&#39612;&#27169;&#22359;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#37325;&#26032;&#35774;&#35745;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;VL&#32534;&#31243;&#26041;&#26696;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;ExoViP&#26041;&#27861;&#22312;&#27491;&#30830;&#25191;&#34892;&#25351;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#21644;&#22788;&#29702;&#35268;&#21010;&#38169;&#35823;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;ExoViP&#19981;&#20165;&#21487;&#20197;&#24212;&#29992;&#20110;&#35270;&#35273;&#25512;&#29702;&#65292;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20174;&#32780;&#20026;&#35270;&#35273;&#35821;&#35328;&#32534;&#31243;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#36171;&#33021;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02210v1 Announce Type: new  Abstract: Compositional visual reasoning methods, which translate a complex query into a structured composition of feasible visual tasks, have exhibited a strong potential in complicated multi-modal tasks. Empowered by recent advances in large language models (LLMs), this multi-modal challenge has been brought to a new stage by treating LLMs as few-shot/zero-shot planners, i.e., vision-language (VL) programming. Such methods, despite their numerous merits, suffer from challenges due to LLM planning mistakes or inaccuracy of visual execution modules, lagging behind the non-compositional models. In this work, we devise a "plug-and-play" method, ExoViP, to correct errors in both the planning and execution stages through introspective verification. We employ verification modules as "exoskeletons" to enhance current VL programming schemes. Specifically, our proposed verification module utilizes a mixture of three sub-verifiers to validate predictions a
&lt;/p&gt;</description></item><item><title>PanoFree&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#20840;&#26223;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21464;&#24418;&#21644;&#19978;&#33394;&#25216;&#26415;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#35299;&#20915;&#22810;&#35270;&#35282;&#22270;&#24418;&#29983;&#25104;&#20013;&#30340;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02157</link><description>&lt;p&gt;
PanoFree: &#26080;&#38656;&#35843;&#21442;&#30340;&#20840;&#23616;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#36328;&#35270;&#35282;&#33258;&#25105;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02157
&lt;/p&gt;
&lt;p&gt;
PanoFree&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#20840;&#26223;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21464;&#24418;&#21644;&#19978;&#33394;&#25216;&#26415;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#35299;&#20915;&#22810;&#35270;&#35282;&#22270;&#24418;&#29983;&#25104;&#20013;&#30340;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02157v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#27785;&#28024;&#24335;&#30340;&#22330;&#26223;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#21019;&#24314;&#20840;&#26223;&#22270;&#26041;&#38754;&#65292;&#20174;&#20026;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#21463;&#30410;&#21290;&#27973;&#12290;&#30001;&#20110;&#33719;&#21462;&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#20542;&#21521;&#20110;&#26080;&#35843;&#25972;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38480;&#20110;&#31616;&#21333;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#35201;&#20040;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#24494;&#35843;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PanoFree&#30340;&#26080;&#38656;&#35843;&#21442;&#30340;&#20840;&#23616;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#24191;&#27867;&#30340;&#19981;&#21516;&#23545;&#24212;&#20851;&#31995;&#30340;&#29983;&#25104;&#12290;PanoFree&#20351;&#29992;&#36845;&#20195;&#21464;&#24418;&#21644;&#19978;&#33394;&#25216;&#26415;&#65292;&#19981;&#20381;&#36182;&#20110;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#32047;&#31215;&#35823;&#24046;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644; artifacts &#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#24378;&#36328;&#35270;&#35282;&#24863;&#30693;&#21644;&#36890;&#36807;&#36328;&#35270;&#35282;&#25351;&#23548;&#12289;&#39118;&#38505;&#21306;&#22495;&#20272;&#35745;&#21644;&#25830;&#38500;&#20197;&#21450;&#23545;&#31216;&#21452;&#21521;&#24341;&#23548;&#29983;&#25104;&#30340;&#36827;&#27493;&#65292;&#23427;&#25913;&#36827;&#20102;&#32047;&#31215;&#35823;&#24046;&#30340;&#29366;&#20917;&#12290;&#23427;&#36824;&#36890;&#36807;&#22312;&#36827;&#21270;&#36807;&#31243;&#20013;&#20462;&#27491;&#21464;&#24418;&#21644;&#19978;&#33394;&#36807;&#31243;&#26469;&#32454;&#21270;&#36825;&#20123;&#36807;&#31243;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#37027;&#20123;&#22312;&#29983;&#25104;&#21021;&#26399;&#20986;&#29616;&#30340;&#19981;&#21305;&#37197;&#12290;&#36890;&#36807;&#23558;&#22810;&#35270;&#35282;&#24863;&#30693;&#32435;&#20837;&#36845;&#20195;&#36807;&#31243;&#65292;PanoFree&#26080;&#38656;&#27714;&#21161;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;&#24494;&#35843;&#25110;&#39044;&#35757;&#32451;&#26435;&#37325;&#65292;&#23601;&#21487;&#20197;&#29983;&#25104;&#31283;&#23450;&#32780;&#19968;&#33268;&#30340;&#12289;&#19982;&#22810;&#35270;&#35282;&#25968;&#25454;&#20860;&#23481;&#30340;&#20840;&#26223;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02157v1 Announce Type: new  Abstract: Immersive scene generation, notably panorama creation, benefits significantly from the adaptation of large pre-trained text-to-image (T2I) models for multi-view image generation. Due to the high cost of acquiring multi-view images, tuning-free generation is preferred. However, existing methods are either limited to simple correspondences or require extensive fine-tuning to capture complex ones. We present PanoFree, a novel method for tuning-free multi-view image generation that supports an extensive array of correspondences. PanoFree sequentially generates multi-view images using iterative warping and inpainting, addressing the key issues of inconsistency and artifacts from error accumulation without the need for fine-tuning. It improves error accumulation by enhancing cross-view awareness and refines the warping and inpainting processes via cross-view guidance, risky area estimation and erasing, and symmetric bidirectional guided genera
&lt;/p&gt;</description></item><item><title>VidModEx&#26041;&#27861;&#20351;&#29992;SHAP&#22686;&#24378;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02140</link><description>&lt;p&gt;
VidModEx: &#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#39640;&#25928;&#30340;&#40657;&#33394;&#30418;&#23376;&#27169;&#22411;&#25552;&#21462;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02140
&lt;/p&gt;
&lt;p&gt;
VidModEx&#26041;&#27861;&#20351;&#29992;SHAP&#22686;&#24378;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02140v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22312;&#40657;&#31665;&#27169;&#22411;&#25552;&#21462;&#39046;&#22495;&#65292;&#20381;&#36182;&#20110;&#36719;&#26631;&#31614;&#25110;&#26367;&#20195;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#21644;&#22788;&#29702;&#22823;&#37327;&#30456;&#20851;&#32852;&#30340;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;SHAP&#37327;&#21270;&#20102;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#36755;&#20986;&#25152;&#20570;&#30340;&#20010;&#20307;&#36129;&#29486;&#65292;&#36825;&#26377;&#21161;&#20110;&#20248;&#21270;&#19968;&#20010;&#22522;&#20110;&#33021;&#37327;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#36798;&#21040;&#19968;&#20010;&#29702;&#24819;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;16.45%&#65292;&#24182;&#19988;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#65292;&#22312;UCF11&#12289;UCF101&#12289;Kinetics 400&#12289;Kinetics 600&#21644;Something-Something V2&#31561;&#38590;&#20197;&#25361;&#25112;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;26.11%&#65292;&#26368;&#39640;&#25552;&#21319;&#20102;33.36%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#23454;&#38469;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of
&lt;/p&gt;</description></item><item><title>RICA^2&#27169;&#22411;&#26159;&#19968;&#31181;&#28145;&#23618;&#27010;&#29575;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;&#35780;&#20998;&#22823;&#32434;&#24182;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#32426;&#24405;&#65292;&#29305;&#21035;&#26159;&#22312;FineDiving&#12289;MTL-AQA&#21644;JIGSAWS&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2408.02138</link><description>&lt;p&gt;
RICA^2&#65306;&#22522;&#20110;&#35780;&#20998;&#22823;&#32434;&#30340;&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RICA^2: Rubric-Informed, Calibrated Assessment of Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02138
&lt;/p&gt;
&lt;p&gt;
RICA^2&#27169;&#22411;&#26159;&#19968;&#31181;&#28145;&#23618;&#27010;&#29575;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;&#35780;&#20998;&#22823;&#32434;&#24182;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#32426;&#24405;&#65292;&#29305;&#21035;&#26159;&#22312;FineDiving&#12289;MTL-AQA&#21644;JIGSAWS&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35270;&#35273;&#31038;&#21306;&#20013;&#21560;&#24341;&#36817;&#26399;&#20852;&#36259;&#30340;&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;(AQA)&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#20154;&#31867;&#19987;&#23478;&#20351;&#29992;&#30340;&#35780;&#20998;&#22823;&#32434;&#65292;&#24182;&#19988;&#26410;&#33021;&#37327;&#21270;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#23618;&#27010;&#29575;&#27169;&#22411;RICA^2&#65292;&#23427;&#25972;&#21512;&#20102;&#35780;&#20998;&#22823;&#32434;&#65292;&#24182;&#23545;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#21160;&#20316;&#27493;&#39588;&#30340;&#38543;&#26426;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#23450;&#20041;&#22312;&#19968;&#20010;&#32534;&#30721;&#35780;&#20998;&#22823;&#32434;&#30340;&#22270;&#20013;&#12290;&#36825;&#20123;&#23884;&#20837;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20998;&#25955;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#19988;&#20801;&#35768;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#31034;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#22270;&#32534;&#30721;&#20102;&#35780;&#20998;&#20934;&#21017;&#65292;&#22522;&#20110;&#36825;&#20123;&#20934;&#21017;&#65292;&#21487;&#20197;&#35299;&#30721;&#20986;&#36136;&#37327;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;FineDiving&#12289;MTL-AQA&#21644;JIGSAWS&#22312;&#20869;&#30340;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#35760;&#24405;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02138v1 Announce Type: new  Abstract: The ability to quantify how well an action is carried out, also known as action quality assessment (AQA), has attracted recent interest in the vision community. Unfortunately, prior methods often ignore the score rubric used by human experts and fall short of quantifying the uncertainty of the model prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model that integrates score rubric and accounts for prediction uncertainty for AQA. Central to our method lies in stochastic embeddings of action steps, defined on a graph structure that encodes the score rubric. The embeddings spread probabilistic density in the latent space and allow our method to represent model uncertainty. The graph encodes the scoring criteria, based on which the quality scores can be decoded. We demonstrate that our method establishes new state of the art on public benchmarks, including FineDiving, MTL-AQA, and JIGSAWS, with superior performance in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;-&#32034;&#24067;&#21015;&#22827;&#32423;&#25968;&#23545;&#25968;&#23383;&#22696;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#21487;&#33021;&#22312;&#26576;&#20123;&#26041;&#38754;&#20248;&#20110;&#25289;&#26684;&#26391;&#26085;-&#32034;&#24067;&#21015;&#22827;&#32423;&#25968;&#23545;&#25968;&#23383;&#22696;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2408.02135</link><description>&lt;p&gt;
&#20851;&#20110;&#20999;&#27604;&#38634;&#22827;-&#32034;&#24067;&#21015;&#22827;&#32423;&#25968;&#23545;&#25968;&#23383;&#22696;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A First Look at Chebyshev-Sobolev Series for Digital Ink
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;-&#32034;&#24067;&#21015;&#22827;&#32423;&#25968;&#23545;&#25968;&#23383;&#22696;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#21487;&#33021;&#22312;&#26576;&#20123;&#26041;&#38754;&#20248;&#20110;&#25289;&#26684;&#26391;&#26085;-&#32034;&#24067;&#21015;&#22827;&#32423;&#25968;&#23545;&#25968;&#23383;&#22696;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02135v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#23558;&#25968;&#23383;&#22696;&#35270;&#20026;&#24179;&#38754;&#26354;&#32447;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#31614;&#21517;&#39564;&#35777;&#12289;&#31508;&#35760;&#32534;&#20889;&#21644;&#25968;&#23398;&#25163;&#20889;&#35782;&#21035;&#12290;&#36825;&#20123;&#24179;&#38754;&#26354;&#32447;&#21487;&#20197;&#36890;&#36807;&#37319;&#26679;&#28857;&#30830;&#23450;&#30340;&#31616;&#21270;&#32423;&#25968;&#30340;&#21442;&#25968;&#21270;&#23545; (x(s), y(s)) &#33719;&#24471;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#36825;&#20123;&#31616;&#21270;&#32423;&#25968;&#65288;&#22810;&#39033;&#24335;&#65289;&#20013;&#37319;&#29992;&#25289;&#26684;&#26391;&#26085;&#25110;&#25289;&#26684;&#26391;&#26085;-&#32034;&#24067;&#21015;&#22827;&#22522;&#20855;&#26377;&#35768;&#22810;&#29702;&#24819;&#30340;&#23646;&#24615;&#12290;&#36825;&#20123;&#23646;&#24615;&#21253;&#25324;&#25968;&#25454;&#34920;&#31034;&#30340;&#32039;&#20945;&#24615;&#12289;&#21516;&#31181;&#31526;&#21495;&#22312;&#22810;&#39033;&#24335;&#31995;&#25968;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#26377;&#24847;&#20041;&#32858;&#31867;&#12289;&#35813;&#31354;&#38388;&#20013;&#31867;&#38388;&#30340;&#32447;&#24615;&#21487;&#20998;&#31163;&#24615;&#65292;&#20197;&#21450;&#35813;&#31354;&#38388;&#20013;&#26354;&#32447;&#20043;&#38388;&#21464;&#24322;&#24615;&#30340;&#39640;&#25928;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#25506;&#32034;&#20999;&#27604;&#38634;&#22827;-&#32034;&#24067;&#21015;&#22827;&#32423;&#25968;&#22312;&#31526;&#21495;&#35782;&#21035;&#20013;&#24212;&#29992;&#30340;&#31532;&#19968;&#27493;&#12290;&#26089;&#26399;&#30340;&#36857;&#35937;&#34920;&#26126;&#65292;&#36825;&#31181;&#34920;&#31034;&#23545;&#20110;&#26576;&#20123;&#30446;&#30340;&#26469;&#35828;&#21487;&#33021;&#20248;&#20110;&#25289;&#26684;&#26391;&#26085;-&#32034;&#24067;&#21015;&#22827;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02135v1 Announce Type: new  Abstract: Considering digital ink as plane curves provides a valuable framework for various applications, including signature verification, note-taking, and mathematical handwriting recognition. These plane curves can be obtained as parameterized pairs of approximating truncated series (x(s), y(s)) determined by sampled points. Earlier work has found that representing these truncated series (polynomials) in a Legendre or Legendre-Sobolev basis has a number of desirable properties. These include compact data representation, meaningful clustering of like symbols in the vector space of polynomial coefficients, linear separability of classes in this space, and highly efficient calculation of variation between curves. In this work, we take a first step at examining the use of Chebyshev-Sobolev series for symbol recognition. The early indication is that this representation may be superior to Legendre-Sobolev representation for some purposes.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.02100</link><description>&lt;p&gt;
&#22312;&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
View-consistent Object Removal in Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02100
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02100v1 Announce Type: new  Abstract: Radiance Fields (RFs) have emerged as a crucial technology for 3D scene representation, enabling the synthesis of novel views with remarkable realism. However, as RFs become more widely used, the need for effective editing techniques that maintain coherence across different perspectives becomes evident. Current methods primarily depend on per-frame 2D image inpainting, which often fails to maintain consistency across views, thus compromising the realism of edited RF scenes. In this work, we introduce a novel RF editing pipeline that significantly enhances consistency by requiring the inpainting of only a single reference image. This image is then projected across multiple views using a depth-based approach, effectively reducing the inconsistencies observed with per-frame inpainting. However, projections typically assume photometric consistency across views, which is often impractical in real-world settings. To accommodate realistic varia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#35780;&#20272;&#19982;&#36873;&#25321;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35757;&#32451;&#30340;&#25968;&#25454;&#26041;&#27861;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#65292;&#26088;&#22312;&#20026;&#26368;&#20248;&#30340;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
&#26631;&#39064;&#65306;&#37322;&#25918;&#25968;&#25454;&#24040;&#28010;&#30340;&#21147;&#37327;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35757;&#32451;&#30340;&#25968;&#25454;&#35780;&#20215;&#19982;&#36873;&#25321;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#35780;&#20272;&#19982;&#36873;&#25321;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35757;&#32451;&#30340;&#25968;&#25454;&#26041;&#27861;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#65292;&#26088;&#22312;&#20026;&#26368;&#20248;&#30340;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;arXiv:2408.02085v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25991;&#25688;&#35201;&#65306;&#25351;&#20196;&#35757;&#32451;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#21916;&#22909;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#24320;&#25918;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20294;&#30450;&#30446;&#22320;&#22312;&#25152;&#26377;&#29616;&#26377;&#25351;&#20196;&#19978;&#35757;&#32451;&#19968;&#20010;LLM&#21487;&#33021;&#24182;&#19981;&#29702;&#24819;&#19988;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#26377;&#21033;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#25351;&#20196;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#30693;&#35782;&#24046;&#36317;&#65292;&#21363;&#21738;&#20123;&#25968;&#25454;&#35780;&#20272;&#25351;&#26631;&#21487;&#20197;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#26159;&#22914;&#20309;&#34701;&#20837;&#36873;&#25321;&#26426;&#21046;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#29992;&#20110;&#25351;&#20196;&#35757;&#32451;&#30340;LLMs&#30340;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#29616;&#26377;&#25991;&#29486;&#30340;&#20840;&#38754;&#22238;&#39038;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#25152;&#26377;&#36866;&#29992;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22522;&#20110;&#36136;&#37327;&#30340;&#12289;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#21644;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#19977;&#31867;&#65292;&#20854;&#20013;&#32454;&#21270;&#20102;&#31934;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#23545;&#20110;&#27599;&#31181;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#26410;&#26469;&#30740;&#31350;&#21487;&#33021;&#30340;&#31354;&#30333;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#21644;&#26032;&#20852;&#25216;&#26415;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#25105;&#20204;&#30456;&#20449;&#21487;&#20197;&#20026;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#31350;&#19971;&#31181;&#39044;&#20551;&#35270;&#35273;&#20219;&#21153;&#30340;&#22810;&#35270;&#22270;&#29305;&#24449;&#20808;&#39564;&#26469;&#25913;&#36827;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#65292;&#24182;&#21033;&#29992;&#36845;&#20195;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.02079</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35270;&#22270;&#22270;&#20687;&#29305;&#24449;&#20808;&#39564;&#25913;&#36827;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Improving Neural Surface Reconstruction with Feature Priors from Multi-View Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#31350;&#19971;&#31181;&#39044;&#20551;&#35270;&#35273;&#20219;&#21153;&#30340;&#22810;&#35270;&#22270;&#29305;&#24449;&#20808;&#39564;&#26469;&#25913;&#36827;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#65292;&#24182;&#21033;&#29992;&#36845;&#20195;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#39064;: arXiv:2408.02079v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#26368;&#36817;&#22312;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;(NSR)&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#26174;&#30528;&#25552;&#39640;&#20102;&#19982;&#20307;&#31215;&#28210;&#26579;&#30456;&#32467;&#21512;&#30340;&#22810;&#35270;&#22270;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#20165;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#20381;&#36182;&#20809;&#24230;&#19968;&#33268;&#24615;&#24182;&#19981;&#33021;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#20986;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#21253;&#25324;&#36974;&#25377;&#21644;&#38750;&#27931;&#32500;&#22467;&#34920;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#23545;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#25439;&#22833;&#30340;&#35843;&#26597;&#65292;&#30446;&#30340;&#26159;&#21033;&#29992;&#22810;&#31181;&#39044;&#20551;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26377;&#20215;&#20540;&#29305;&#24449;&#20808;&#39564;&#65292;&#24182;&#20811;&#26381;&#30446;&#21069;&#30340;&#38480;&#21046;&#12290;&#38656;&#35201;&#25351;&#20986;&#30340;&#26159;&#65292;&#30446;&#21069;&#23384;&#22312;&#30528;&#30830;&#23450;&#26368;&#26377;&#25928;&#39044;&#20551;&#35270;&#35273;&#20219;&#21153;&#20197;&#22686;&#24378;NSR&#25152;&#38656;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#25506;&#32034;&#20102;&#19971;&#39033;&#39044;&#20551;&#35270;&#35273;&#20219;&#21153;&#30340;&#22810;&#35270;&#22270;&#29305;&#24449;&#20808;&#39564;&#65292;&#21253;&#25324;&#21313;&#19977;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#24191;&#27867;&#30340;&#21487;&#33021;&#24615;&#19979;&#24378;&#21270;NSR&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29305;&#24449;&#20998;&#36776;&#29575;&#30340;&#21464;&#21270;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#25152;&#26377;&#39033;&#30446;&#30340;&#24615;&#33021;&#21644;&#27979;&#37327;&#30340;&#25968;&#37327;&#32423;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36845;&#20195;&#35757;&#32451;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#26041;&#27861;&#20108;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#24615;&#33021;&#22522;&#20934;&#65292;&#23545;&#20110;&#29702;&#35299;NSR&#30340;&#26368;&#20339;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02079v1 Announce Type: new  Abstract: Recent advancements in Neural Surface Reconstruction (NSR) have significantly improved multi-view reconstruction when coupled with volume rendering. However, relying solely on photometric consistency in image space falls short of addressing complexities posed by real-world data, including occlusions and non-Lambertian surfaces. To tackle these challenges, we propose an investigation into feature-level consistent loss, aiming to harness valuable feature priors from diverse pretext visual tasks and overcome current limitations. It is crucial to note the existing gap in determining the most effective pretext visual task for enhancing NSR. In this study, we comprehensively explore multi-view feature priors from seven pretext visual tasks, comprising thirteen methods. Our main goal is to strengthen NSR training by considering a wide range of possibilities. Additionally, we examine the impact of varying feature resolutions and evaluate both pi
&lt;/p&gt;</description></item><item><title>LDFaceNet&#26159;&#20351;&#29992;&#25351;&#23548;&#28508;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38754;&#37096;&#20132;&#25442;&#30340;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;Deepfake&#35270;&#39057;&#12290;</title><link>https://arxiv.org/abs/2408.02078</link><description>&lt;p&gt;
LDFaceNet: &#22522;&#20110;&#28508;&#25193;&#25955;&#32593;&#32476;&#30340;&#39640;&#20445;&#30495;&#24230;Deepfake&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02078
&lt;/p&gt;
&lt;p&gt;
LDFaceNet&#26159;&#20351;&#29992;&#25351;&#23548;&#28508;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38754;&#37096;&#20132;&#25442;&#30340;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;Deepfake&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22312;&#21512;&#25104;&#23186;&#20307;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24378;&#22823;&#26041;&#27861;&#30340;&#25512;&#21160;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38750;&#24179;&#34913;&#28909;&#21147;&#23398;&#21551;&#21457;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#65292;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#36890;&#36807;&#23427;&#20204;&#30340;&#38543;&#26426;&#25277;&#26679;&#36807;&#31243;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#36924;&#30495;&#21644;&#38750;&#21516;&#36136;&#22270;&#20687;&#26041;&#38754;&#30340;&#26480;&#20986;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDFaceNet&#65288;&#28508;&#25193;&#25955;&#22522;&#20110;&#38754;&#37096;&#20132;&#25442;&#32593;&#32476;&#65289;&#30340;&#26032;&#22411;&#38754;&#37096;&#20132;&#25442;&#27169;&#22359;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#25351;&#23548;&#28508;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#38754;&#37096;&#20998;&#21106;&#21644;&#38754;&#37096;&#35782;&#21035;&#27169;&#22359;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#21435;&#22122;&#36807;&#31243;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#29420;&#29305;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20026;&#25193;&#25955;&#36807;&#31243;&#25552;&#20379;&#26041;&#21521;&#24615;&#25351;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LDFaceNet&#21487;&#20197;&#32467;&#21512;&#34917;&#20805;&#38754;&#37096;&#25351;&#23548;&#20449;&#24687;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;Deepfake&#35270;&#39057;&#29983;&#25104;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#29420;&#29305;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20026;&#25193;&#25955;&#36807;&#31243;&#25552;&#20379;&#26041;&#21521;&#24615;&#25351;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LDFaceNet&#21487;&#20197;&#32467;&#21512;&#34917;&#20805;&#38754;&#37096;&#25351;&#23548;&#20449;&#24687;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;Deepfake&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02078v1 Announce Type: new  Abstract: Over the past decade, there has been tremendous progress in the domain of synthetic media generation. This is mainly due to the powerful methods based on generative adversarial networks (GANs). Very recently, diffusion probabilistic models, which are inspired by non-equilibrium thermodynamics, have taken the spotlight. In the realm of image generation, diffusion models (DMs) have exhibited remarkable proficiency in producing both realistic and heterogeneous imagery through their stochastic sampling procedure. This paper proposes a novel facial swapping module, termed as LDFaceNet (Latent Diffusion based Face Swapping Network), which is based on a guided latent diffusion model that utilizes facial segmentation and facial recognition modules for a conditioned denoising process. The model employs a unique loss function to offer directional guidance to the diffusion process. Notably, LDFaceNet can incorporate supplementary facial guidance fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#20799;&#31461;&#21457;&#23637;&#36831;&#32531;&#35786;&#26029;&#31579;&#26597;&#26041;&#27861;&#65292;&#26088;&#22312;&#26089;&#21457;&#29616;&#12289;&#26089;&#24178;&#39044;&#65292;&#20943;&#23569;&#21307;&#30103;&#21644;&#31038;&#20250;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02073</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#20799;&#31461;&#21457;&#23637;&#36831;&#32531;&#35786;&#26029;&#31579;&#26597;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Case-based reasoning approach for diagnostic screening of children with developmental delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#20799;&#31461;&#21457;&#23637;&#36831;&#32531;&#35786;&#26029;&#31579;&#26597;&#26041;&#27861;&#65292;&#26088;&#22312;&#26089;&#21457;&#29616;&#12289;&#26089;&#24178;&#39044;&#65292;&#20943;&#23569;&#21307;&#30103;&#21644;&#31038;&#20250;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#30340;&#25968;&#25454;&#65292;&#20840;&#29699;&#32422;&#26377;6%&#33267;9%&#30340;&#20154;&#21475;&#24739;&#26377;&#21457;&#23637;&#36831;&#32531;&#12290;&#22522;&#20110;&#20013;&#22269;&#23433;&#24509;&#30465;&#28142;&#21271;&#24066;2023&#24180;&#26032;&#29983;&#20799;&#30340;&#25968;&#37327;&#65288;94,420&#65289;&#65292;&#25105;&#20204;&#20272;&#35745;&#27599;&#24180;&#26377;&#22823;&#32422;7,500&#20363;&#65288;&#30097;&#20284;&#21457;&#23637;&#36831;&#32531;&#30149;&#20363;&#65289;&#30340;&#30097;&#34385;&#30149;&#20363;&#12290;&#23545;&#36825;&#20123;&#20799;&#31461;&#36827;&#34892;&#26089;&#26399;&#35782;&#21035;&#24182;&#36827;&#34892;&#36866;&#24403;&#30340;&#26089;&#26399;&#24178;&#39044;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21307;&#30103;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#31038;&#20250;&#25104;&#26412;&#12290;&#22269;&#38469;&#30740;&#31350;&#25351;&#20986;&#65292;&#21457;&#23637;&#36831;&#32531;&#20799;&#31461;&#30340;&#26368;&#20339;&#24178;&#39044;&#26399;&#26159;&#20845;&#23681;&#21069;&#65292;&#26368;&#22909;&#26159;&#22312;&#19977;&#23681;&#21322;&#20043;&#21069;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25509;&#21463;&#26089;&#26399;&#24178;&#39044;&#30340;&#21457;&#23637;&#36831;&#32531;&#20799;&#31461;&#30151;&#29366;&#26126;&#26174;&#25913;&#21892;&#65292;&#29978;&#33267;&#26377;&#20123;&#20799;&#31461;&#21487;&#33021;&#23436;&#20840;&#24247;&#22797;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#32467;&#21512;CNN-Trans&#30340;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02073v1 Announce Type: new  Abstract: According to the World Health Organization, the population of children with developmental delays constitutes approximately 6% to 9% of the total population. Based on the number of newborns in Huaibei, Anhui Province, China, in 2023 (94,420), it is estimated that there are about 7,500 cases (suspected cases of developmental delays) of suspicious cases annually. Early identification and appropriate early intervention for these children can significantly reduce the wastage of medical resources and societal costs. International research indicates that the optimal period for intervention in children with developmental delays is before the age of six, with the golden treatment period being before three and a half years of age. Studies have shown that children with developmental delays who receive early intervention exhibit significant improvement in symptoms; some may even fully recover. This research adopts a hybrid model combining a CNN-Tran
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340;NLP&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23454;&#26102;&#25512;&#33616;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26368;&#20248;&#21435;&#22122;&#27493;&#39588;&#25968;&#65292;&#26174;&#33879;&#33410;&#32422;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2408.02054</link><description>&lt;p&gt;
&#27493;&#36895;&#22120;&#65306;&#39044;&#27979;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#25152;&#38656;&#30340;&#26368;&#23567;&#21435;&#22122;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02054
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340;NLP&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23454;&#26102;&#25512;&#33616;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26368;&#20248;&#21435;&#22122;&#27493;&#39588;&#25968;&#65292;&#26174;&#33879;&#33410;&#32422;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#36807;&#31934;&#24515;&#35843;&#20248;&#30340;NLP&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#30830;&#23450;&#20219;&#20309;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#25152;&#38656;&#30340;&#26368;&#23567;&#21435;&#22122;&#27493;&#39588;&#25968;&#12290;&#36825;&#20010;&#39640;&#32423;&#27169;&#22411;&#26159;&#19968;&#20010;&#23454;&#26102;&#24037;&#20855;&#65292;&#23427;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25512;&#33616;&#29702;&#24819;&#30340;&#21435;&#22122;&#27493;&#39588;&#65292;&#23427;&#19982;&#25193;&#25955;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#65292;&#30830;&#20445;&#22312;&#26368;&#30701;&#30340;&#26102;&#38388;&#20869;&#29983;&#20135;&#20986;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#37325;&#28857;&#25918;&#22312;&#20102;DDIM&#35843;&#24230;&#22120;&#19978;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#20197;&#36866;&#24212;&#21644;&#24212;&#29992;&#20110;&#21508;&#31181;&#20854;&#20182;&#35843;&#24230;&#22120;&#65292;&#27604;&#22914;&#27431;&#25289;&#12289;&#27431;&#25289;&#31062;&#20808;&#12289;&#26898;&#30416;&#27861;&#12289;DPM2 Karras&#12289;UniPC&#31561;&#31561;&#12290;&#36890;&#36807;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#23458;&#25143;&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#25191;&#34892;&#20135;&#29983;&#26368;&#20248;&#22270;&#20687;&#36136;&#37327;&#25152;&#38656;&#30340;&#26368;&#23569;&#21435;&#22122;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02054v1 Announce Type: new  Abstract: In this paper, we introduce an innovative NLP model specifically fine-tuned to determine the minimal number of denoising steps required for any given text prompt. This advanced model serves as a real-time tool that recommends the ideal denoise steps for generating high-quality images efficiently. It is designed to work seamlessly with the Diffusion model, ensuring that images are produced with superior quality in the shortest possible time. Although our explanation focuses on the DDIM scheduler, the methodology is adaptable and can be applied to various other schedulers like Euler, Euler Ancestral, Heun, DPM2 Karras, UniPC, and more. This model allows our customers to conserve costly computing resources by executing the fewest necessary denoising steps to achieve optimal quality in the produced images.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;HVTrack&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#20013;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#38382;&#39064;&#65292;&#36890;&#36807;&#30456;&#23545;&#23039;&#24577;&#24863;&#30693;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#24352;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.02049</link><description>&lt;p&gt;
&#28857;&#20113;&#20013;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
3D Single-object Tracking in Point Clouds with High Temporal Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02049
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HVTrack&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#20013;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#38382;&#39064;&#65292;&#36890;&#36807;&#30456;&#23545;&#23039;&#24577;&#24863;&#30693;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#24352;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02049v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#65288;3D SOT&#65289;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#28857;&#20113;&#30340;&#39640;&#26102;&#24207;&#21464;&#21270;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#28857;&#20113;&#30340;&#24418;&#29366;&#21464;&#21270;&#21644;&#23545;&#35937;&#22312;&#30456;&#37051;&#24103;&#20043;&#38388;&#30340;&#36816;&#21160;&#26159;&#24179;&#28369;&#30340;&#65292;&#26080;&#27861;&#24212;&#23545;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#20013;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#38382;&#39064;&#65292;&#31216;&#20026;HVTrack&#12290;HVTrack&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#32452;&#20214;&#26469;&#22788;&#29702;&#39640;&#26102;&#24207;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#65306;1&#65289;&#30456;&#23545;&#23039;&#24577;&#24863;&#30693;&#35760;&#24518;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#24207;&#21464;&#21270;&#30340;&#28857;&#20113;&#24418;&#29366;&#65307;2&#65289;&#22522;&#25193;&#24352;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#25193;&#23637;&#30340;&#25628;&#32034;&#21306;&#22495;&#20869;&#19982;&#23545;&#35937;&#30456;&#20284;&#30340;&#24178;&#25200;&#65307;3&#65289;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;KITTI-HV&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#19981;&#21516;&#30340;&#24103;&#38388;&#38548;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02049v1 Announce Type: new  Abstract: The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Pixel-Level Domain Adaptation (PLDA) &#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#22312;&#29289;&#20307;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20687;&#32032;&#32423;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#20174;&#22270;&#20687;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#32972;&#26223;&#19979;&#65292;&#29983;&#25104;&#26356;&#28165;&#26224;&#12289;&#26356;&#31934;&#30830;&#30340;&#20266; mask &#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2408.02039</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#22495;&#36866;&#24212;&#65306;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Pixel-Level Domain Adaptation (PLDA) &#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#22312;&#29289;&#20307;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20687;&#32032;&#32423;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#20174;&#22270;&#20687;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#32972;&#26223;&#19979;&#65292;&#29983;&#25104;&#26356;&#28165;&#26224;&#12289;&#26356;&#31934;&#30830;&#30340;&#20266; mask &#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02039v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#26368;&#36817;&#65292;&#24050;&#32463;&#26377;&#20154;&#20851;&#27880;&#20174;&#22270;&#20687;&#26631;&#31614;&#20013;&#23398;&#20064; semantic segmentation &#27169;&#22411;&#65292;&#36825;&#19968;&#33539;&#24335;&#34987;&#31216;&#20026; image-level Weakly Supervised Semantic Segmentation (WSSS)&#12290;&#29616;&#26377;&#23581;&#35797;&#37319;&#29992; Class Activation Maps (CAMs) &#20316;&#20026;&#20808;&#39564;&#26469;&#25366;&#25496;&#29289;&#20307;&#21306;&#22495;&#65292;&#20294;&#35266;&#23519;&#21040;&#28608;&#27963;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21482;&#26377;&#26368;&#20855;&#26377;&#36776;&#21035;&#24615;&#30340;&#29289;&#20307;&#37096;&#20998;&#34987;&#23450;&#20301;&#22312;&#27492;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#20174;&#22270;&#20687;&#26631;&#31614;&#23398;&#20064;&#30340;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#23436;&#25972;&#21644;&#31934;&#30830;&#30340;&#20266; mask &#20570;&#20026; ground truths&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; Pixel-Level Domain Adaptation (PLDA) &#26041;&#27861;&#65292;&#20197;&#27492;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#20687;&#32032;&#32423;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#30340;&#22810;&#22836;&#22495;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#29305;&#24449;&#25552;&#21462;&#22120;&#19968;&#36215;&#35757;&#32451;&#65292;&#20197;&#20419;&#36827;&#29983;&#25104;&#19982;&#22495;&#26080;&#20851;&#30340;&#20687;&#32032;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#22810;&#23610;&#24230;&#32454;&#21270;&#21644;&#29305;&#24449;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#20266; mask &#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340; PLDA &#26041;&#27861;&#65292;&#20854;&#22312; image-level WSSS &#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02039v1 Announce Type: new  Abstract: Recent attention has been devoted to the pursuit of learning semantic segmentation models exclusively from image tags, a paradigm known as image-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts adopt the Class Activation Maps (CAMs) as priors to mine object regions yet observe the imbalanced activation issue, where only the most discriminative object parts are located. In this paper, we argue that the distribution discrepancy between the discriminative and the non-discriminative parts of objects prevents the model from producing complete and precise pseudo masks as ground truths. For this purpose, we propose a Pixel-Level Domain Adaptation (PLDA) method to encourage the model in learning pixel-wise domain-invariant features. Specifically, a multi-head domain classifier trained adversarially with the feature extraction is introduced to promote the emergence of pixel features that are invariant with respect to the sh
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEGO&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#20219;&#21153;&#25429;&#25417;&#22797;&#26434;&#32467;&#26500;&#65292;&#25552;&#21319;&#25991;&#26412;&#35782;&#21035;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#26426;&#22120;&#35270;&#35273;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.02036</link><description>&lt;p&gt;
LEGO: &#33258;&#30417;&#30563;&#20195;&#34920;&#23398;&#20064;&#26041;&#27861;&#22312;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LEGO: Self-Supervised Representation Learning for Scene Text Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02036
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEGO&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#20219;&#21153;&#25429;&#25417;&#22797;&#26434;&#32467;&#26500;&#65292;&#25552;&#21319;&#25991;&#26412;&#35782;&#21035;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#26426;&#22120;&#35270;&#35273;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02036v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#27880;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36827;&#19968;&#27493;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#26159;&#19968;&#31181;&#24456;&#26377;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;NLP&#21644;CV&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#24207;&#21015;&#29305;&#24615;&#65292;&#23545;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#36890;&#29992;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Local Explicit and Global Order-aware self-supervised representation learning method&#65288;LEGO&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#29305;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#21333;&#35789;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#39640;&#32423;&#30340;&#35748;&#30693;&#21160;&#26426;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#24341;&#20837;&#19968;&#31995;&#21015;&#33258;&#30417;&#30563;&#20219;&#21153;&#65292;&#25104;&#21151;&#22320;&#25429;&#25417;&#20102;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#19981;&#21516;&#35282;&#24230;&#21644;&#35270;&#35282;&#30340;&#20449;&#24687;&#65292;&#29992;&#20197;&#39640;&#25928;&#22320;&#25429;&#25417;&#25991;&#26412;&#25991;&#26412;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#25991;&#26412;&#30452;&#36830;&#32493;&#24615;&#21644;&#25991;&#26412;&#22359;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#36890;&#36807;&#36825;&#20123;&#29305;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#19968;&#20123;&#38590;&#20197;&#32534;&#36753;&#30340;&#25991;&#26412;&#22270;&#20687;&#19978;&#12290;&#20026;&#20102;&#22686;&#24378;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#65292;&#36824;&#33021;&#20026;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26426;&#22120;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#21644;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02036v1 Announce Type: new  Abstract: In recent years, significant progress has been made in scene text recognition by data-driven methods. However, due to the scarcity of annotated real-world data, the training of these methods predominantly relies on synthetic data. The distribution gap between synthetic and real data constrains the further performance improvement of these methods in real-world applications. To tackle this problem, a highly promising approach is to utilize massive amounts of unlabeled real data for self-supervised training, which has been widely proven effective in many NLP and CV tasks. Nevertheless, generic self-supervised methods are unsuitable for scene text images due to their sequential nature. To address this issue, we propose a Local Explicit and Global Order-aware self-supervised representation learning method (LEGO) that accounts for the characteristics of scene text images. Inspired by the human cognitive process of learning words, which involve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#35270;&#21548;&#34701;&#21512;&#30340;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02033</link><description>&lt;p&gt;
&#22686;&#24378;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35270;&#21548;&#34701;&#21512;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human Action Recognition and Violence Detection Through Deep Learning Audiovisual Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#35270;&#21548;&#34701;&#21512;&#30340;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#31181;&#19981;&#21516;&#23186;&#20307;&#25968;&#25454;&#30340;&#34701;&#21512;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#38899;&#39057;&#21644;&#35270;&#39057;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#35270;&#21548;&#34701;&#21512;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26202;&#26399;&#34701;&#21512;&#12289;&#20013;&#26399;&#34701;&#21512;&#21644;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30001;&#20110;&#30446;&#26631;&#26159;&#22312;&#20844;&#20849;&#22330;&#25152;&#26816;&#27979;&#21644;&#35782;&#21035;&#26292;&#21147;&#65292;&#25105;&#20204;&#23545;&#8220;&#29616;&#23454;&#29983;&#27963;&#26292;&#21147;&#24773;&#26223;&#8221;(RLVS)&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#24182;&#29992;&#20110;&#23454;&#39564;&#12290;&#23545;HFBDL&#65288;&#28151;&#21512;&#34701;&#21512;&#22411;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;96.67%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#27492;&#25968;&#25454;&#38598;&#19978;&#30340;&#20854;&#20182;&#20808;&#36827;&#30340;&#26041;&#27861;&#35201;&#31934;&#30830;&#24471;&#22810;&#12290;&#20026;&#20102;&#23637;&#31034;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#24405;&#21046;&#20102;54&#27573;&#26377;&#22768;&#38899;&#30340;&#35270;&#39057;&#65292;&#26082;&#26377;&#26292;&#21147;&#22330;&#26223;&#20063;&#26377;&#38750;&#26292;&#21147;&#22330;&#26223;&#12290;&#27169;&#22411;&#25104;&#21151;&#27491;&#30830;&#22320;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;52&#27573;&#35270;&#39057;&#12290;&#25552;&#20986;&#30340;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20154;&#31867;&#34892;&#21160;&#35782;&#21035;&#21644;&#26292;&#21147;&#26816;&#27979;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34701;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26292;&#21147;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02033v1 Announce Type: new  Abstract: This paper proposes a hybrid fusion-based deep learning approach based on two different modalities, audio and video, to improve human activity recognition and violence detection in public places. To take advantage of audiovisual fusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning (HFBDL) are used and compared. Since the objective is to detect and recognize human violence in public places, Real-life violence situation (RLVS) dataset is expanded and used. Simulating results of HFBDL show 96.67\% accuracy on validation data, which is more accurate than the other state-of-the-art methods on this dataset. To showcase our model's ability in real-world scenarios, another dataset of 54 sounded videos of both violent and non-violent situations was recorded. The model could successfully detect 52 out of 54 videos correctly. The proposed method shows a promising performance on real scenarios. Thus, it can be used for hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;AutoEncoder&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22312;&#20010;&#20307;&#22522;&#30784;&#19978;&#36827;&#34892;MRI&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#29305;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02018</link><description>&lt;p&gt;
&#20010;&#20307;&#21270;&#22810;&#26102;&#38388;&#32447;MRI&#36712;&#36857;&#39044;&#27979;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;AutoEncoder&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22312;&#20010;&#20307;&#22522;&#30784;&#19978;&#36827;&#34892;MRI&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36864;&#34892;&#24615;&#25913;&#21464;&#36890;&#36807;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26469;&#27979;&#37327;&#65292;&#36825;&#26159;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#19968;&#31181;&#28508;&#22312;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20294;&#20854;&#26222;&#36941;&#35748;&#20026;&#19981;&#22914;&#28096;&#31881;&#26679;&#34507;&#30333;&#25110;tau&#29983;&#29289;&#26631;&#24535;&#29289;&#29305;&#24322;&#24615;&#12290;&#30001;&#20110;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#33041;&#35299;&#21078;&#32467;&#26500;&#30340;&#24046;&#24322;&#24456;&#22823;&#65292;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;MRI&#26102;&#38388;&#24207;&#21015;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#29305;&#24322;&#24615;&#65292;&#21363;&#23558;&#27599;&#20010;&#24739;&#32773;&#35270;&#20026;&#20854;&#33258;&#36523;&#30340;&#22522;&#32447;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36716;&#21521;&#26465;&#20214;&#21464;&#20998; Autoencoder &#26469;&#29983;&#25104;&#20010;&#20307;&#21270;&#30340;MRI&#39044;&#27979;&#65292;&#36825;&#20123;&#39044;&#27979;&#22522;&#20110;&#24739;&#32773;&#30340;&#24180;&#40836;&#12289;&#30142;&#30149;&#29366;&#24577;&#21644;&#20043;&#21069;&#30340;&#25195;&#25551;&#12290;&#20351;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#20513;&#35758;&#65288;ADNI&#65289;&#30340;&#36830;&#32493;&#25104;&#20687;&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#26550;&#26500;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#65292;&#21487;&#20197;&#20174;&#35813;&#20998;&#24067;&#20013;&#25277;&#26679;&#29983;&#25104;&#35299;&#21078;&#32467;&#26500;&#21464;&#21270;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#36229;&#20986;&#25968;&#25454;&#38598;&#30340;&#33539;&#22260;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;MRI&#36827;&#34892;&#39044;&#27979;&#65292;&#26368;&#22810;&#21487;&#36798;10&#24180;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20445;&#30041;&#30340;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#20174;&#26410;&#34987;&#29992;&#26469;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02018v1 Announce Type: new  Abstract: Neurodegeneration as measured through magnetic resonance imaging (MRI) is recognized as a potential biomarker for diagnosing Alzheimer's disease (AD), but is generally considered less specific than amyloid or tau based biomarkers. Due to a large amount of variability in brain anatomy between different individuals, we hypothesize that leveraging MRI time series can help improve specificity, by treating each patient as their own baseline. Here we turn to conditional variational autoencoders to generate individualized MRI predictions given the subject's age, disease status and one previous scan. Using serial imaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a novel architecture to build a latent space distribution which can be sampled from to generate future predictions of changing anatomy. This enables us to extrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated the model on a held-out set fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35780;&#20272;&#21644;&#20248;&#20808;&#22788;&#29702;&#32925;&#33039;&#21019;&#20260;&#30149;&#20363;&#65292;&#20197;&#20943;&#23569;&#27835;&#30103;&#25104;&#26412;&#21644;&#32487;&#21457;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2408.02012</link><description>&lt;p&gt;
&#32925;&#33039;&#21019;&#20260;&#20248;&#20808;&#22788;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Decision Support System to triage of liver trauma
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35780;&#20272;&#21644;&#20248;&#20808;&#22788;&#29702;&#32925;&#33039;&#21019;&#20260;&#30149;&#20363;&#65292;&#20197;&#20943;&#23569;&#27835;&#30103;&#25104;&#26412;&#21644;&#32487;&#21457;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02012v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#20840;&#29699;&#20581;&#24247;&#21463;&#21040;&#21019;&#20260;&#30340;&#20005;&#37325;&#24433;&#21709;&#65292;&#27599;&#24180;&#36229;&#36807;500&#19975;&#20154;&#27515;&#20110;&#21019;&#20260;&#65292;&#36825;&#19982;&#32467;&#26680;&#30149;&#12289;&#33406;&#28363;&#30149;&#21644;&#30111;&#30142;&#31561;&#30142;&#30149;&#30340;&#27515;&#20129;&#29575;&#30456;&#24403;&#12290;&#22312;&#20234;&#26391;&#65292;&#20132;&#36890;&#20107;&#25925;&#30340;&#32463;&#27982;&#21518;&#26524;&#27599;&#24180;&#32422;&#21344;&#22269;&#23478;&#22269;&#27665;&#29983;&#20135;&#24635;&#20540;&#65288;GNP&#65289;&#30340;2%&#12290;&#21019;&#20260;&#24739;&#32773;&#22312;&#25509;&#21463;&#20260;&#23475;&#21518;24&#23567;&#26102;&#20869;&#30340;&#20986;&#34880;&#26159;&#33268;&#27515;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#22240;&#27492;&#65292;&#36805;&#36895;&#35786;&#26029;&#21644;&#35780;&#20272;&#30142;&#30149;&#30340;&#20005;&#37325;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#24739;&#32773;&#38656;&#35201;&#23545;&#25152;&#26377;&#22120;&#23448;&#36827;&#34892;&#20840;&#38754;&#30340;&#25195;&#25551;&#65292;&#36825;&#20250;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#12290;&#23545;&#20840;&#36523;&#36827;&#34892;CT&#22270;&#20687;&#35780;&#20272;&#26082;&#32791;&#26102;&#21448;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#31361;&#26174;&#20102;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#26377;&#25928;&#31649;&#29702;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;&#12290;&#39640;&#25928;&#30340;&#35786;&#26029;&#27969;&#31243;&#33021;&#22815;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#20943;&#23569;&#32487;&#21457;&#24182;&#21457;&#30151;&#30340;&#21457;&#29983;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;D
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02012v1 Announce Type: cross  Abstract: Trauma significantly impacts global health, accounting for over 5 million deaths annually, which is comparable to mortality rates from diseases such as tuberculosis, AIDS, and malaria. In Iran, the financial repercussions of road traffic accidents represent approximately 2% of the nation's Gross National Product each year. Bleeding is the leading cause of mortality in trauma patients within the first 24 hours following an injury, making rapid diagnosis and assessment of severity crucial. Trauma patients require comprehensive scans of all organs, generating a large volume of data. Evaluating CT images for the entire body is time-consuming and requires significant expertise, underscoring the need for efficient time management in diagnosis. Efficient diagnostic processes can significantly reduce treatment costs and decrease the likelihood of secondary complications. In this context, the development of a reliable Decision Support System (D
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;SAM&#21644;Detic&#25216;&#26415;&#21019;&#24314;&#20165;&#21253;&#21547;&#21069;&#26223;&#25968;&#25454;&#30340;&#24037;&#31243;&#21270;&#31649;&#36947;&#65292;&#20197;&#20419;&#36827;&#24494;&#32454;&#20998;&#20219;&#21153;&#20013;&#24573;&#35270;&#32972;&#26223;&#22122;&#22768;&#24433;&#21709;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2408.01998</link><description>&lt;p&gt;
&#27809;&#26377;&#32972;&#26223;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#20026;&#24494;&#32454;&#20998;&#20219;&#21153;&#26500;&#24314;&#20165;&#21253;&#21547;&#21069;&#26223;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What Happens Without Background? Constructing Foreground-Only Data for Fine-Grained Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;SAM&#21644;Detic&#25216;&#26415;&#21019;&#24314;&#20165;&#21253;&#21547;&#21069;&#26223;&#25968;&#25454;&#30340;&#24037;&#31243;&#21270;&#31649;&#36947;&#65292;&#20197;&#20419;&#36827;&#24494;&#32454;&#20998;&#20219;&#21153;&#20013;&#24573;&#35270;&#32972;&#26223;&#22122;&#22768;&#24433;&#21709;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01998v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032;&#28040;&#24687; &#25688;&#35201;&#65306;&#22312;&#35270;&#35273;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#24494;&#32454;&#20998;&#35782;&#21035;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#26679;&#26412;&#20013;&#23384;&#22312;&#30340;&#37492;&#21035;&#24615;&#20449;&#24687;&#65292;&#21306;&#20998;&#30456;&#20284;&#30340;&#19979;&#20301;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38169;&#35823;&#22320;&#20851;&#27880;&#32972;&#26223;&#21306;&#22495;&#65292;&#24573;&#35270;&#20102;&#23545;&#20027;&#20307;&#26377;&#25928;&#37492;&#21035;&#20449;&#24687;&#30340;&#25429;&#25417;&#65292;&#36825;&#38459;&#30861;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#20419;&#36827;&#23545;&#27169;&#22411;&#32972;&#26223;&#22122;&#22768;&#24433;&#21709;&#30340;&#31185;&#23398;&#30740;&#31350;&#65292;&#24182;&#22686;&#24378;&#20854;&#23545;&#20027;&#20307;&#37492;&#21035;&#29305;&#24449;&#30340;&#38598;&#20013;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#31243;&#21270;&#31649;&#36947;&#65292;&#23427;&#21033;&#29992;SAM&#21644;Detic&#30340;&#33021;&#21147;&#65292;&#21019;&#24314;&#20102;&#20165;&#21253;&#21547;&#21069;&#26223;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#19981;&#21253;&#21547;&#32972;&#26223;&#12290;&#24191;&#27867;&#30340;&#20132;&#21449;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#21069;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26377;&#25928;&#24615;&#65292;&#22686;&#24378;&#20102;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#20026;&#25968;&#25454;&#27169;&#24335;&#30340;&#36827;&#19968;&#27493;&#25193;&#23637;&#22880;&#23450;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01998v1 Announce Type: new  Abstract: Fine-grained recognition, a pivotal task in visual signal processing, aims to distinguish between similar subclasses based on discriminative information present in samples. However, prevailing methods often erroneously focus on background areas, neglecting the capture of genuinely effective discriminative information from the subject, thus impeding practical application. To facilitate research into the impact of background noise on models and enhance their ability to concentrate on the subject's discriminative features, we propose an engineered pipeline that leverages the capabilities of SAM and Detic to create fine-grained datasets with only foreground subjects, devoid of background. Extensive cross-experiments validate this approach as a preprocessing step prior to training, enhancing algorithmic performance and holding potential for further modal expansion of the data.
&lt;/p&gt;</description></item><item><title>SR-CIS&#36890;&#36807;&#32467;&#21512;&#23567;&#27169;&#22411;&#24555;&#36895;&#25512;&#29702;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;CA-OAD&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#23384;&#19982;&#25512;&#29702;&#35299;&#32806;&#30340;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#23398;&#20064;&#26426;&#21046;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.01970</link><description>&lt;p&gt;
SR-CIS: &#33258;&#25105;&#21453;&#24605;&#22686;&#37327;&#31995;&#32479;&#19982;&#35299;&#32806;&#35760;&#24518;&#19982;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01970
&lt;/p&gt;
&lt;p&gt;
SR-CIS&#36890;&#36807;&#32467;&#21512;&#23567;&#27169;&#22411;&#24555;&#36895;&#25512;&#29702;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;CA-OAD&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#23384;&#19982;&#25512;&#29702;&#35299;&#32806;&#30340;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#23398;&#20064;&#26426;&#21046;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01970v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20154;&#31867;&#24555;&#36895;&#23398;&#20064;&#26032;&#30693;&#35782;&#21516;&#26102;&#20445;&#30041;&#26087;&#35760;&#24518;&#30340;&#33021;&#21147;&#20026;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35760;&#24518;&#21644;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24335;&#20114;&#34917;&#22686;&#37327;&#31995;&#32479;&#65288;SR-CIS&#65289;&#12290;SR-CIS&#30001;&#35299;&#26500;&#21270;&#30340;&#20114;&#34917;&#25512;&#26029;&#27169;&#22359;&#65288;CIM&#65289;&#21644;&#20114;&#34917;&#35760;&#24518;&#27169;&#22359;&#65288;CMM&#65289;&#32452;&#25104;&#65292;&#20854;&#20013;CIM&#36890;&#36807;&#32622;&#20449;&#24230;&#33258;&#36866;&#24212;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65288;CA-OAD&#65289;&#26426;&#21046;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#23567;&#27169;&#22411;&#65292;&#32780;CIM&#21017;&#30001;Confidence-Aware Online Anomaly Detection&#65288;CA-OAD&#65289;&#26426;&#21046;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#27169;&#22411;&#12290;CMM&#30001;&#20219;&#21153;&#29305;&#23450;&#30340;&#30701;&#26399;&#35760;&#24518;&#65288;STM&#65289;&#21306;&#22495;&#21644;&#36890;&#29992;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#21306;&#22495;&#32452;&#25104;&#12290;&#36890;&#36807;&#35774;&#23450;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#21644;&#30456;&#20851;&#21407;&#22411;&#26435;&#37325;&#21644;&#20559;&#24046;&#65292;&#23427;&#20026;&#21442;&#25968;&#21644;&#34920;&#31034;&#35760;&#24518;&#24314;&#31435;&#20102;&#22806;&#37096;&#23384;&#20648;&#23454;&#20363;&#65292;&#20174;&#32780;&#35299;&#26500;&#20102;&#35760;&#24518;&#27169;&#22359;&#19982;&#25512;&#29702;&#27169;&#22359;&#65292;&#24182;&#23454;&#29616;&#20102;&#35760;&#24518;&#30340;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#26041;&#38754;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#31038;&#20250;&#19968;&#33268;&#24615;&#20250;&#20013;&#20171;&#27169;&#22411;&#21453;&#26144;&#30340;&#20154;&#31867;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38598;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20934;&#30830;&#24418;&#25104;&#37027;&#20123;&#20381;&#36182;&#20110;&#19981;&#21487;&#35265;&#23646;&#24615;&#30340;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2408.01959</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#31038;&#20250;&#19968;&#33268;&#24615;&#20013;&#20171;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#30340;&#35270;&#35273;&#35821;&#35328;AI
&lt;/p&gt;
&lt;p&gt;
Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01959
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#26041;&#38754;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#31038;&#20250;&#19968;&#33268;&#24615;&#20250;&#20013;&#20171;&#27169;&#22411;&#21453;&#26144;&#30340;&#20154;&#31867;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38598;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20934;&#30830;&#24418;&#25104;&#37027;&#20123;&#20381;&#36182;&#20110;&#19981;&#21487;&#35265;&#23646;&#24615;&#30340;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01959v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#22312;&#21253;&#25324;&#33258;&#21160;&#22270;&#20687;&#23383;&#24149;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#29978;&#33267;&#23545;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#29992;&#25143;&#30340;&#23433;&#20840;&#35775;&#38382;&#24212;&#29992;&#31243;&#24207;&#20063;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#23545;&#20559;&#35265;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#21644;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#65292;&#24182;&#19988;&#25105;&#20204;&#39318;&#27425;&#21457;&#29616;&#65292;&#36825;&#20123;&#20559;&#35265;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;CLIP&#27169;&#22411;&#23478;&#26063;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;&#25105;&#20204;&#36824;&#39318;&#27425;&#23637;&#31034;&#20102;&#65292;&#19968;&#20010;&#20559;&#35265;&#30340;&#31243;&#24230;&#22312;&#31038;&#20250;&#20013;&#20849;&#20139;&#65292;&#39044;&#27979;&#20102;&#23427;&#22312;CLIP&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#31243;&#24230;&#30340;&#31243;&#24230;&#12290;&#22312;&#20165;&#23545;&#26368;&#22823;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#20154;&#31867;&#20284;&#30340;&#38754;&#37096;&#21360;&#35937;&#25165;&#20250;&#20986;&#29616;&#65292;&#36825;&#34920;&#26126;&#19982;&#26410;&#32463;&#23457;&#26680;&#30340;&#25991;&#21270;&#30340;&#26356;&#22909;&#21305;&#37197;&#23545;&#20110;&#22797;&#21046;&#36234;&#26469;&#36234;&#24494;&#22937;&#30340;&#31038;&#20132;&#20559;&#35265;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#23545;&#22270;&#20687;&#30340;&#26576;&#20123;&#19981;&#21487;&#35265;&#23646;&#24615;&#65288;&#22914;&#21487;&#20449;&#36182;&#24615;&#21644;&#24615;&#21462;&#21521;&#65289;&#24418;&#25104;&#21360;&#35937;&#26102;&#65292;&#21482;&#26377;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#19979;&#25991;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#25165;&#26356;&#21152;&#20934;&#30830;&#12290;&#36825;&#21487;&#33021;&#24847;&#21619;&#30528;&#22312;&#29983;&#25104;&#19982;&#31038;&#20250;&#20559;&#22909;&#39640;&#24230;&#19968;&#33268;&#30340;&#25991;&#26412;&#25551;&#36848;&#26041;&#38754;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#36828;&#31163;&#23545;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#26144;&#23556;&#65292;&#24182;&#24320;&#22987;&#23398;&#20064;&#22914;&#20309;&#20102;&#35299;&#21644;&#27169;&#20223;&#20154;&#31867;&#30340;&#31038;&#20250;&#24418;&#24577;&#30693;&#35273;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#25581;&#31034;&#36825;&#20123;&#20559;&#35265;&#23545;&#20915;&#31574;&#21046;&#23450;&#30340;&#28508;&#22312;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;EqvAfford&#26694;&#26550;&#36890;&#36807;&#30830;&#20445;&#28857;&#32423;affordance&#23398;&#20064;&#30340;SE(3)&#31561;&#21464;&#24615;&#65292;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#35770;&#29289;&#20307;&#23039;&#24577;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2408.01953</link><description>&lt;p&gt;
EqvAfford: SE(3)&#31561;&#21464;&#24615;&#23545;&#28857;&#32423; affordance&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01953
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;EqvAfford&#26694;&#26550;&#36890;&#36807;&#30830;&#20445;&#28857;&#32423;affordance&#23398;&#20064;&#30340;SE(3)&#31561;&#21464;&#24615;&#65292;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#35770;&#29289;&#20307;&#23039;&#24577;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01953v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#20154;&#31867;&#22312;&#24863;&#30693;&#21644;&#19982;&#19990;&#30028;&#20114;&#21160;&#26102;&#20855;&#26377;&#31561;&#21464;&#24615;&#24847;&#35782;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#29289;&#20307;&#23039;&#24577;&#19979;&#36827;&#34892;&#25805;&#20316;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#31561;&#21464;&#24615;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20063;&#26159;&#23384;&#22312;&#30340;&#12290;&#20363;&#22914;&#65292;&#26080;&#35770;&#25277;&#23625;&#30340;&#23039;&#24577;&#22914;&#20309;&#65288;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#20542;&#26012;&#65289;&#65292;&#25805;&#20316;&#31574;&#30053;&#37117;&#26159;&#19968;&#33268;&#30340;&#65288;&#25235;&#20303;&#25226;&#25163;&#24182;&#27839;&#30452;&#32447;&#25289;&#21160;&#65289;&#12290;&#23613;&#31649;&#20256;&#32479;&#27169;&#22411;&#36890;&#24120;&#19981;&#20855;&#26377;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25552;&#20379;&#31561;&#21464;&#24615;&#30340;&#24847;&#35782;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35757;&#32451;&#25152;&#38656;&#30340;&#25968;&#25454;&#36807;&#22810;&#65292;&#24182;&#19988;&#22312;&#26032;&#30340;&#29289;&#20307;&#23039;&#24577;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#25105;&#20204;&#30340;EqvAfford&#26694;&#26550;&#65292;&#20855;&#26377;&#26032;&#30340;&#35774;&#35745;&#26469;&#20445;&#35777;&#28857;&#32423;affordance&#23398;&#20064;&#20013;&#30340;&#31561;&#21464;&#24615;&#65292;&#23545;&#20110;&#19979;&#28216;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;&#20195;&#34920;&#24615;&#30340;&#29289;&#20307;&#19978;&#20855;&#26377;&#22312;&#22810;&#26679;&#23039;&#24577;&#19979;&#34920;&#29616;&#30340;&#20248;&#24322;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01953v1 Announce Type: new  Abstract: Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;PnP&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#23039;&#21183;&#20272;&#35745;&#22330;&#26223;&#20013;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2408.01945</link><description>&lt;p&gt;
&#20223;&#23556;n&#28857;&#38382;&#39064;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generalized Maximum Likelihood Estimation for Perspective-n-Point Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;PnP&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#23039;&#21183;&#20272;&#35745;&#22330;&#26223;&#20013;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01945v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#35270;&#22270;n&#20010;&#28857;(PnP)&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#23039;&#24577;&#20272;&#35745;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#35266;&#27979;&#25968;&#25454;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#24573;&#35270;&#21487;&#33021;&#23548;&#33268;&#22312;&#23384;&#22312;&#22122;&#22768;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27425;&#20248;&#21644;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;PnP&#27714;&#35299;&#22120;&#65292;&#21517;&#20026;GMLPnP&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;GLS&#31243;&#24207;&#26469;&#21516;&#26102;&#20272;&#35745;&#23039;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#30456;&#26426;&#27169;&#22411;&#26080;&#20851;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#35265;&#30340;&#23039;&#24577;&#20272;&#35745;&#24773;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#31934;&#24230;&#65292;&#19982;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;GMLPnP&#22312;TUM-RGBD&#21644;KITTI-360&#25968;&#25454;&#38598;&#19978;&#30340;&#26059;&#36716;/&#24179;&#31227;&#31934;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;4.7%/2.0%&#21644;18.6%/18.4%&#12290;&#23427;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#26356;&#22909;&#65292;&#23545;&#21442;&#25968;&#30340;&#20272;&#35745;&#20063;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01945v1 Announce Type: cross  Abstract: The Perspective-n-Point (PnP) problem has been widely studied in the literature and applied in various vision-based pose estimation scenarios. However, existing methods ignore the anisotropy uncertainty of observations, as demonstrated in several real-world datasets in this paper. This oversight may lead to suboptimal and inaccurate estimation, particularly in the presence of noisy observations. To this end, we propose a generalized maximum likelihood PnP solver, named GMLPnP, that minimizes the determinant criterion by iterating the GLS procedure to estimate the pose and uncertainty simultaneously. Further, the proposed method is decoupled from the camera model. Results of synthetic and real experiments show that our method achieves better accuracy in common pose estimation scenarios, GMLPnP improves rotation/translation accuracy by 4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best baseline. It is more ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#22810;&#23610;&#24230;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#21442;&#25968;&#20272;&#35745;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#23545;&#26410;&#30693;&#27979;&#35797;&#26679;&#21697;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27979;&#35797;&#25193;&#25955;&#26041;&#21521;&#19982;&#35757;&#32451;&#26041;&#21521;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2408.01944</link><description>&lt;p&gt;
RobNODDI: &#33258;&#36866;&#24212;&#37319;&#26679;&#19979;&#36830;&#32493;&#34920;&#31034;&#19979;NODDI&#21442;&#25968;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under Continuous Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#22810;&#23610;&#24230;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#21442;&#25968;&#20272;&#35745;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#23545;&#26410;&#30693;&#27979;&#35797;&#26679;&#21697;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27979;&#35797;&#25193;&#25955;&#26041;&#21521;&#19982;&#35757;&#32451;&#26041;&#21521;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01944v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032; Abstract: &#31070;&#32463;&#32420;&#32500;&#21462;&#21521;&#20998;&#25955;&#19982;&#23494;&#24230;&#25104;&#20687;&#65288;NODDI&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#33041;&#32452;&#32455;&#24494;&#35266;&#32467;&#26500;&#30340;&#37325;&#35201;&#25104;&#20687;&#25216;&#26415;&#65292;&#23545;&#20110;&#21457;&#29616;&#21644;&#27835;&#30103;&#21508;&#31181;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#30446;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#65292;&#20351;&#29992;&#25968;&#37327;&#36739;&#23569;&#30340;&#25193;&#25955;&#26799;&#24230;&#12290;&#36825;&#20123;&#26041;&#27861;&#21152;&#36895;&#20102;&#21442;&#25968;&#30340;&#20272;&#35745;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#26399;&#38388;&#20351;&#29992;&#30340;&#25193;&#25955;&#26041;&#21521;&#24517;&#39035;&#22312;&#20005;&#26684;&#30340;&#24847;&#20041;&#19978;&#19982;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#25193;&#25955;&#26041;&#21521;&#19968;&#33268;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;dMRI&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#19981;&#20339;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#39564;&#35777;&#20102;&#24403;&#27979;&#35797;&#26102;&#30340;&#25193;&#25955;&#26041;&#21521;&#19982;&#35757;&#32451;&#26102;&#30340;&#25193;&#25955;&#26041;&#21521;&#19981;&#19968;&#33268;&#26102;&#65292;&#30446;&#21069;&#30340;&#20027;&#27969;&#26041;&#27861;&#22312;dMRI&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#24615;&#33021;&#23558;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#22810;&#23610;&#24230;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#30340;&#25193;&#25955;&#26041;&#21521;&#24182;&#32452;&#32455;&#30340;&#21462;&#26679;&#36807;&#31243;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#26410;&#30693;&#27979;&#35797;&#26679;&#21697;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;dMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#26126;&#20102;&#23427;&#22312;&#22788;&#29702;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#30340;&#25361;&#25112;&#26102;&#65292;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01944v1 Announce Type: new  Abstract: Neurite Orientation Dispersion and Density Imaging (NODDI) is an important imaging technology used to evaluate the microstructure of brain tissue, which is of great significance for the discovery and treatment of various neurological diseases. Current deep learning-based methods perform parameter estimation through diffusion magnetic resonance imaging (dMRI) with a small number of diffusion gradients. These methods speed up parameter estimation and improve accuracy. However, the diffusion directions used by most existing deep learning models during testing needs to be strictly consistent with the diffusion directions during training. This results in poor generalization and robustness of deep learning models in dMRI parameter estimation. In this work, we verify for the first time that the parameter estimation performance of current mainstream methods will significantly decrease when the testing diffusion directions and the training diffus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19987;&#38376;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#30340;&#22810;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#25915;&#20987;&#35780;&#20215;&#25351;&#26631;&#30340;&#22238;&#39038;&#65292;&#24182;&#23545;&#22810;&#31181;&#24320;&#28304;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2408.01934</link><description>&lt;p&gt;
&#12298;&#23545;&#25239;&#25915;&#20987;&#22312;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#35843;&#26597;&#19982;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey and Evaluation of Adversarial Attacks for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19987;&#38376;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#30340;&#22810;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#25915;&#20987;&#35780;&#20215;&#25351;&#26631;&#30340;&#22238;&#39038;&#65292;&#24182;&#23545;&#22810;&#31181;&#24320;&#28304;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#19987;&#38376;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#35780;&#20215;&#25351;&#26631;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#24320;&#28304;&#25915;&#20987;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20851;&#38190;&#35266;&#23519;&#65292;&#20197;&#25552;&#39640;&#23545;&#25915;&#20987;&#26377;&#25928;&#24615;&#21644;&#30456;&#24212;&#23545;&#31574;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#29992;&#20197;&#25351;&#23548;&#26410;&#26469;&#23433;&#20840;&#33258;&#21160;&#21270;&#23545;&#35937;&#26816;&#27979;&#31995;&#32479;&#30740;&#31350;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01934v1 Announce Type: new  Abstract: Deep learning models excel in various computer vision tasks but are susceptible to adversarial examples-subtle perturbations in input data that lead to incorrect predictions. This vulnerability poses significant risks in safety-critical applications such as autonomous vehicles, security surveillance, and aircraft health monitoring. While numerous surveys focus on adversarial attacks in image classification, the literature on such attacks in object detection is limited. This paper offers a comprehensive taxonomy of adversarial attacks specific to object detection, reviews existing adversarial robustness evaluation metrics, and systematically assesses open-source attack methods and model robustness. Key observations are provided to enhance the understanding of attack effectiveness and corresponding countermeasures. Additionally, we identify crucial research challenges to guide future efforts in securing automated object detection systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20493;&#29575;&#20449;&#24687;&#22788;&#29702;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;H&amp;E&#26579;&#33394;&#20999;&#29255;&#21512;&#25104;IHC-HER2&#20999;&#29255;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22788;&#29702;&#22810;&#20493;&#29575;&#30149;&#29702;&#22270;&#20687;&#21644;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#20851;&#27880;&#37325;&#35201;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01929</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#20013;H&amp;E&#26579;&#33394;&#33267;IHC&#26579;&#33394;&#32763;&#35793;&#30340;&#36827;&#23637;&#65306;&#22522;&#20110;&#22810;&#20493;&#29575;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing H&amp;E-to-IHC Stain Translation in Breast Cancer: A Multi-Magnification and Attention-Based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20493;&#29575;&#20449;&#24687;&#22788;&#29702;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;H&amp;E&#26579;&#33394;&#20999;&#29255;&#21512;&#25104;IHC-HER2&#20999;&#29255;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22788;&#29702;&#22810;&#20493;&#29575;&#30149;&#29702;&#22270;&#20687;&#21644;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#20851;&#27880;&#37325;&#35201;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#20840;&#29699;&#33539;&#22260;&#20869;&#37325;&#35201;&#30340;&#20581;&#24247;&#25361;&#25112;&#65292;&#38656;&#35201;&#31934;&#30830;&#30340;&#35786;&#26029;&#21644;&#26377;&#25928;&#30340;&#27835;&#30103;&#31574;&#30053;&#65292;&#20854;&#20013;&#32452;&#32455;&#30149;&#29702;&#23398;&#23545;Hematoxylin&#21644;Eosin&#65288;H&amp;E&#65289;&#26579;&#33394;&#30340;&#32452;&#32455;&#20999;&#29255;&#36215;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#30340;&#20316;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#35780;&#20272;&#20687;&#34920;&#30382;&#29983;&#38271;&#22240;&#23376;&#21463;&#20307;2&#65288;HER2&#65289;&#36825;&#26679;&#30340;&#29305;&#23450;&#29983;&#29289;&#26631;&#35760;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#20173;&#28982;&#21463;&#21040;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#26368;&#36817;&#30340;&#19968;&#20123;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#38754;&#65292;&#20026;&#20174;H&amp;E&#26579;&#33394;&#20999;&#29255;&#21512;&#25104;IHC-HER2&#20999;&#29255;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#22788;&#29702;&#30149;&#29702;&#22270;&#20687;&#30340;&#22810;&#20493;&#29575;&#21644;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#23545;&#37325;&#35201;&#20449;&#24687;&#30340;&#19981;&#36275;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20493;&#29575;&#20449;&#24687;&#22788;&#29702;&#30340;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01929v1 Announce Type: cross  Abstract: Breast cancer presents a significant healthcare challenge globally, demanding precise diagnostics and effective treatment strategies, where histopathological examination of Hematoxylin and Eosin (H&amp;E) stained tissue sections plays a central role. Despite its importance, evaluating specific biomarkers like Human Epidermal Growth Factor Receptor 2 (HER2) for personalized treatment remains constrained by the resource-intensive nature of Immunohistochemistry (IHC). Recent strides in deep learning, particularly in image-to-image translation, offer promise in synthesizing IHC-HER2 slides from H\&amp;E stained slides. However, existing methodologies encounter challenges, including managing multiple magnifications in pathology images and insufficient focus on crucial information during translation. To address these issues, we propose a novel model integrating attention mechanisms and multi-magnification information processing. Our model employs a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#26550;&#26500;&#30340;CAF-YOLO&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#30149;&#21464;&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;ACFM&#21644;SFM&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#24322;&#24120;&#32454;&#32990;&#21644;&#32954;&#32467;&#33410;&#31561;&#32454;&#23567;&#32467;&#26500;&#30340;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01897</link><description>&lt;p&gt;
CAF-YOLO&#65306;&#19968;&#31181;&#22312;&#29983;&#29289;&#21307;&#23398;&#24433;&#20687;&#20013;&#22810;&#23610;&#24230;&#30149;&#21464;&#26816;&#27979;&#30340;&#22362;&#22266;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in Biomedical Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#26550;&#26500;&#30340;CAF-YOLO&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#30149;&#21464;&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;ACFM&#21644;SFM&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#24322;&#24120;&#32454;&#32990;&#21644;&#32954;&#32467;&#33410;&#31561;&#32454;&#23567;&#32467;&#26500;&#30340;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01897v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30446;&#26631;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30149;&#21464;&#23450;&#20301;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#21644;&#26631;&#35760;&#30149;&#21464;&#26041;&#38754;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#26816;&#27979;&#26497;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#65288;&#22914;&#24322;&#24120;&#32454;&#32990;&#12289;&#30452;&#24452;&#23567;&#20110;3&#27627;&#31859;&#30340;&#32954;&#32467;&#33410;&#65289;&#30340;&#31934;&#30830;&#24615;&#65292;&#36825;&#22312;&#36139;&#34880;&#21644;&#32954;&#30149;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;YOLOv8&#26550;&#26500;&#30340;CAF-YOLO&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#22362;&#22266;&#30340;&#21307;&#23398;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#21464;&#25442;&#22120;&#30340;&#24378;&#22823;&#21151;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#21367;&#31215;&#26680;&#30340;&#23616;&#38480;&#24615;&#65292;&#21367;&#31215;&#26680;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#19982;&#36828;&#31163;&#30340;&#20449;&#24687;&#20114;&#21160;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#21644;&#21367;&#31215;&#34701;&#21512;&#27169;&#22359;&#65288;ACFM&#65289;&#12290;&#36825;&#20010;&#27169;&#22359;&#22686;&#24378;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#25429;&#33719;&#20102;&#38271;&#26399;&#29305;&#24449;&#20381;&#36182;&#24615;&#21644;&#31354;&#38388;&#33258;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26102;&#31354;&#34701;&#21512;&#27169;&#22359;&#65288;SFM&#65289;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;CAF-YOLO&#22312;&#22810;&#23610;&#24230;&#30149;&#21464;&#26816;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#21306;&#20998;&#30495;&#23454;&#30149;&#21464;&#19982;&#22122;&#22768;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#30149;&#21464;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01897v1 Announce Type: new  Abstract: Object detection is of paramount importance in biomedical image analysis, particularly for lesion identification. While current methodologies are proficient in identifying and pinpointing lesions, they often lack the precision needed to detect minute biomedical entities (e.g., abnormal cells, lung nodules smaller than 3 mm), which are critical in blood and lung pathology. To address this challenge, we propose CAF-YOLO, based on the YOLOv8 architecture, a nimble yet robust method for medical object detection that leverages the strengths of convolutional neural networks (CNNs) and transformers. To overcome the limitation of convolutional kernels, which have a constrained capacity to interact with distant information, we introduce an attention and convolution fusion module (ACFM). This module enhances the modeling of both global and local features, enabling the capture of long-term feature dependencies and spatial autocorrelation. Additiona
&lt;/p&gt;</description></item><item><title>This study introduces FBINeRF, a novel technique integrating feature-based recurrent neural networks with adaptive GRUs for improved camera pose adjustment in capturing 3D scenes, especially for fisheye cameras, offering faster convergence and higher reconstruction quality relative to existing methods.</title><link>https://arxiv.org/abs/2408.01878</link><description>&lt;p&gt;
FBINeRF: &#22522;&#20110;&#29305;&#24449;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#40060;&#30524;&#21644;&#26631;&#22836;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and Fisheye Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01878
&lt;/p&gt;
&lt;p&gt;
This study introduces FBINeRF, a novel technique integrating feature-based recurrent neural networks with adaptive GRUs for improved camera pose adjustment in capturing 3D scenes, especially for fisheye cameras, offering faster convergence and higher reconstruction quality relative to existing methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24050;&#32463;&#22312;&#21069;&#38754;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20248;&#21270;&#21644;&#35843;&#25972;&#30456;&#26426;&#23039;&#21183;&#30340;&#33021;&#21147;&#65292;&#27604;&#22914;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;BARF&#21644;DBARF&#26041;&#27861;&#65292;&#20026;&#26631;&#22836;&#30456;&#26426;&#23039;&#21183;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#21253;&#25324;&#40060;&#30524;&#30456;&#26426;&#22312;&#20869;&#30340;&#21508;&#31181;&#30456;&#26426;&#31867;&#22411;&#30340;&#22270;&#20687;&#22833;&#30495;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;DBARF&#26041;&#27861;&#20013;&#65292;&#21021;&#22987;&#28145;&#24230;&#20540;&#30340;&#35823;&#24046;&#20250;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#30340;&#25910;&#25947;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;GRU&#21333;&#20803;&#20197;&#21450;&#19968;&#31181;&#28789;&#27963;&#30340;&#35843;&#25972;&#26041;&#27861;&#26469;&#35299;&#20915;&#26012;&#24452;&#22833;&#30495;&#31561;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#20174;&#40060;&#30524;&#30456;&#26426;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#36830;&#32493;&#30340;&#26032;&#22270;&#20687;&#12290;&#20854;&#20182;&#38024;&#23545;&#40060;&#30524;&#30456;&#26426;&#22270;&#20687;&#30340;NeRF&#26041;&#27861;&#65292;&#22914;SCNeRF&#21644;OMNI-NeRF&#65292;&#20351;&#29992;&#20102;&#25237;&#24433;&#20809;&#26463;&#36317;&#31163;&#25439;&#22833;&#26469;&#32454;&#21270;&#22833;&#30495;&#23039;&#21183;&#65292;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#22270;&#20687;&#22833;&#30495;&#12289;&#36739;&#38271;&#30340;&#28210;&#26579;&#26102;&#38388;&#21644;&#38590;&#20197;&#29702;&#35299;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23553;&#35013;&#20809;&#26463;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26012;&#24452;&#22833;&#30495;&#65292;&#32780;&#19988;&#33021;&#22815;&#36890;&#36807;&#36882;&#24402;&#32593;&#32476;&#26377;&#25928;&#22320;&#38598;&#25104;&#29305;&#24449;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#22270;&#20687;&#32454;&#33410;&#30340;&#21516;&#26102;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01878v1 Announce Type: new  Abstract: Previous studies aiming to optimize and bundle-adjust camera poses using Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated impressive capabilities in 3D scene reconstruction. However, these approaches have been designed for pinhole-camera pose optimization and do not perform well under radial image distortions such as those in fisheye cameras. Furthermore, inaccurate depth initialization in DBARF results in erroneous geometric information affecting the overall convergence and quality of results. In this paper, we propose adaptive GRUs with a flexible bundle-adjustment method adapted to radial distortions and incorporate feature-based recurrent neural networks to generate continuous novel views from fisheye datasets. Other NeRF methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray distance loss for distorted pose refinement, causing severe artifacts, long rendering time, and are difficult to u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23454;&#20307;&#22320;&#38754;&#20195;&#29702;&#21644;&#31354;&#20013;&#20195;&#29702;&#20043;&#38388;&#23454;&#26045;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#36890;&#20449;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#30450;&#30446;&#25506;&#32034;&#30340;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2408.01877</link><description>&lt;p&gt;
&#23454;&#20307;&#20195;&#29702;&#38388;&#30340;&#29983;&#25104;&#24335;&#36890;&#20449;&#23545;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23454;&#20307;&#22320;&#38754;&#20195;&#29702;&#21644;&#31354;&#20013;&#20195;&#29702;&#20043;&#38388;&#23454;&#26045;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#36890;&#20449;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#30450;&#30446;&#25506;&#32034;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#20013;&#65292;&#23454;&#20307;&#22320;&#38754;&#20195;&#29702;&#34987;&#26399;&#26395;&#22312;&#27809;&#26377;&#36827;&#34892;&#29305;&#23450;&#29615;&#22659;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#27839;&#30528;&#30446;&#26631;&#23545;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#26631;&#31614;&#23548;&#33322;&#12290;&#37492;&#20110;&#22320;&#38754;&#20195;&#29702;&#30340;&#26377;&#38480;&#35270;&#37326;&#21644;&#29420;&#31435;&#25506;&#32034;&#34892;&#20026;&#65292;&#36825;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#26377;&#38480;&#20840;&#23616;&#35270;&#37326;&#30340;&#31354;&#20013;&#20195;&#29702;&#65292;&#24182;&#19982;&#22320;&#38754;&#20195;&#29702;&#19968;&#36215;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#23548;&#33322;&#26041;&#26696;&#65292;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30830;&#31435;&#20102;&#20855;&#26377;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#23454;&#20307;&#20195;&#29702;&#20043;&#38388;&#30340;&#29983;&#25104;&#24615;&#36890;&#20449;&#65288;GC&#65289;&#22312;&#25913;&#36827;&#38646;&#26679;&#26412;ObjectNav&#20013;&#30340;&#24433;&#21709;&#65292;&#22312;&#27169;&#25311;&#20013;&#65292;&#30456;&#23545;&#20110;&#19968;&#20010;&#27809;&#26377;&#21327;&#21161;&#30340;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#22320;&#38754;&#20195;&#29702;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#30340;10%&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#23545;GC&#30340;&#20998;&#26512;&#65292;&#20026;&#37327;&#21270;&#24187;&#35273;&#21644;&#21512;&#20316;&#30340;&#23384;&#22312;&#30830;&#23450;&#20102;&#29420;&#29305;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#8220;&#39044;&#38450;&#24615;&#24187;&#35273;&#34892;&#20026;&#8221;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#36825;&#31181;&#34892;&#20026;&#26377;&#21161;&#20110;&#22320;&#38754;&#20195;&#29702;&#26356;&#26089;&#22320;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#65292;&#32780;&#19981;&#24517;&#30450;&#30446;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;GC&#36890;&#36807;&#21152;&#28145;&#23545;&#31354;&#20013;&#20195;&#29702;&#36755;&#20986;&#30340;&#29702;&#35299;&#65292;&#25552;&#39640;&#20102;&#23545;&#30446;&#26631;&#23545;&#35937;&#20301;&#32622;&#30340;&#39044;&#27979;&#31934;&#30830;&#24230;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#22320;&#38754;&#20195;&#29702;&#30340;&#25628;&#32034;&#31574;&#30053;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#21512;&#36866;&#30340;&#31574;&#30053;&#26469;&#38598;&#25104;GC&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23454;&#20307;&#20195;&#29702;&#22312;&#26080;&#30417;&#30563;&#19979;&#23548;&#33322;&#21040;&#30446;&#26631;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01877v1 Announce Type: cross  Abstract: In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a target object specified by a natural language label without any environment-specific fine-tuning. This is challenging, given the limited view of a ground agent and its independent exploratory behavior. To address these issues, we consider an assistive overhead agent with a bounded global view alongside the ground agent and present two coordinated navigation schemes for judicious exploration. We establish the influence of the Generative Communication (GC) between the embodied agents equipped with Vision-Language Models (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in the ground agent's ability to find the target object in comparison with an unassisted setup in simulation. We further analyze the GC for unique traits quantifying the presence of hallucination and cooperation. In particular, we identify a unique trait of "preemptive hallucin
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2408.01872</link><description>&lt;p&gt;
&#20351;&#29992;&#19982;&#27491;&#26679;&#26412;&#30456;&#21516;&#30340;&#20869;&#20998;&#24067;&#25968;&#25454;&#23454;&#29616;&#23433;&#20840;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01872
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01872v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#24403;&#21482;&#26377;&#23569;&#37327;&#30340;&#26631;&#31614;&#21487;&#29992;&#26102;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#26631;&#27880;&#21644;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#31867;&#20998;&#24067;&#30456;&#31561;&#65307;&#28982;&#32780;&#65292;&#24403;&#26410;&#26631;&#27880;&#25968;&#25454;&#20013;&#23384;&#22312;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20197;&#24448;&#30340;&#23433;&#20840;&#21322;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#24050;&#32463;&#36890;&#36807;&#22522;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31574;&#30053;&#26377;&#25928;&#38477;&#20302;&#20102;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36825;&#20123;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#22320;&#31579;&#36873;&#20986;&#19981;&#24517;&#35201;&#30340;&#20986;&#20998;&#24067;&#25968;&#25454;&#65292;&#20182;&#20204;&#20063;&#21487;&#33021;&#20250;&#22833;&#21435;&#25968;&#25454;&#20043;&#38388;&#20849;&#20139;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#32780;&#19981;&#31649;&#20854;&#31867;&#21035;&#22914;&#20309;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#31995;&#25968;&#35843;&#24230;&#27604;&#20363;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#32858;&#21512;&#20316;&#20026;&#38170;&#23450;&#30340;&#26631;&#27880;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#30830;&#20445;&#25152;&#26377;&#25968;&#25454;&#22312;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#36991;&#20813;&#22240;&#36807;&#28388;&#20986;&#20998;&#24067;&#25968;&#25454;&#32780;&#25439;&#22833;&#22522;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#21487;&#35265;&#22495;&#21040;&#32418;&#22806;&#22495;&#30340;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#22270;&#20687;&#32763;&#35793;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;&#29289;&#20307;&#26816;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#38598;&#25104;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#26082;&#33021;&#20445;&#30041;&#21487;&#35265;&#22270;&#20687;&#30340;&#32467;&#26500;&#32454;&#33410;&#21448;&#33021;&#20445;&#25345;&#32418;&#22806;&#22270;&#20687;&#32441;&#29702;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#34987;&#29992;&#20110;&#35757;&#32451;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01843</link><description>&lt;p&gt;
&#30001;&#21487;&#35265;&#22495;&#21040;&#32418;&#22806;&#22495;&#30340;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Supervised Image Translation from Visible to Infrared Domain for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#21487;&#35265;&#22495;&#21040;&#32418;&#22806;&#22495;&#30340;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#22270;&#20687;&#32763;&#35793;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;&#29289;&#20307;&#26816;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#38598;&#25104;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#26082;&#33021;&#20445;&#30041;&#21487;&#35265;&#22270;&#20687;&#30340;&#32467;&#26500;&#32454;&#33410;&#21448;&#33021;&#20445;&#25345;&#32418;&#22806;&#22270;&#20687;&#32441;&#29702;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#34987;&#29992;&#20110;&#35757;&#32451;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23398;&#20064;&#19968;&#31181;&#20174;&#21487;&#35265;&#22495;&#21040;&#32418;&#22806;&#22495;&#22270;&#20687;&#30340;&#32763;&#35793;&#26041;&#27861;&#65292;&#24357;&#21512;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#20197;&#25552;&#39640;&#21253;&#25324;&#29289;&#20307;&#26816;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153; accuracy&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25110;&#31471;&#21040;&#31471;&#30340;&#28145;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#21452;&#22495;&#29305;&#24449;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#31867;&#20284;&#20110;&#22270;&#20687;&#32763;&#35793;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;&#32763;&#35793;&#27169;&#22411;&#22312;&#23398;&#20064;&#20445;&#30041;&#21487;&#35265;&#22270;&#20687;&#30340;&#32467;&#26500;&#32454;&#33410;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#32418;&#22806;&#22270;&#20687;&#30340;&#32441;&#29702;&#21644;&#20854;&#20182;&#29305;&#24449;&#12290;&#25152;&#29983;&#25104;&#30340;&#22270;&#20687;&#34987;&#29992;&#20110;&#35757;&#32451;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;Yolov5&#12289;Mask&#21644;Faster R-CNN&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22312;&#25105;&#20204;&#30340;&#31649;&#36947;&#20013;&#38598;&#25104;&#36229;&#20998;&#36776;&#29575;&#27493;&#39588;&#30340;&#26377;&#29992;&#24615;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#23454;&#29616;&#26174;&#30528;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01843v1 Announce Type: new  Abstract: This study aims to learn a translation from visible to infrared imagery, bridging the domain gap between the two modalities so as to improve accuracy on downstream tasks including object detection. Previous approaches attempt to perform bi-domain feature fusion through iterative optimization or end-to-end deep convolutional networks. However, we pose the problem as similar to that of image translation, adopting a two-stage training strategy with a Generative Adversarial Network and an object detection model. The translation model learns a conversion that preserves the structural detail of visible images while preserving the texture and other characteristics of infrared images. Images so generated are used to train standard object detection frameworks including Yolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating a super-resolution step into our pipeline to further improve model accuracy, and achieve an improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#30452;&#25509;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#31471;&#21040;&#31471;&#22320;&#20943;&#23569;Vibroseis&#25968;&#25454;&#30340;&#20018;&#32852;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#36339;&#36807;&#36830;&#25509;&#20445;&#25345;&#25968;&#25454;&#30340;&#32454;&#33410;&#65292;&#20174;&#32780;&#26377;&#25928;&#38477;&#20302;&#20102;&#20018;&#32852;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2408.01831</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;Vibroseis&#25968;&#25454;&#20018;&#32852;&#25928;&#24212;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
A Deep CNN Model for Ringing Effect Attenuation of Vibroseis Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#30452;&#25509;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#31471;&#21040;&#31471;&#22320;&#20943;&#23569;Vibroseis&#25968;&#25454;&#30340;&#20018;&#32852;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#36339;&#36807;&#36830;&#25509;&#20445;&#25345;&#25968;&#25454;&#30340;&#32454;&#33410;&#65292;&#20174;&#32780;&#26377;&#25928;&#38477;&#20302;&#20102;&#20018;&#32852;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01831v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22312;&#21208;&#25506;&#22320;&#29699;&#29289;&#29702;&#39046;&#22495;&#65292;&#22320;&#38663;&#25391;&#21160;&#22120;&#26159;&#29992;&#20110;&#37319;&#38598;&#22320;&#38663;&#25968;&#25454;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#22320;&#38663;&#28304;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;vibroseis&#12290;&#25391;&#21160;&#22120;&#25968;&#25454;&#30340;&#8220;&#20018;&#32852;&#25928;&#24212;&#8221;&#26159;&#30001;&#20110;&#25391;&#21160;&#22120;&#30340;&#39057;&#29575;&#24102;&#23485;&#26377;&#38480;&#32780;&#24120;&#35265;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#31532;&#19968;&#31361;&#30772;&#25361;&#36873;&#30340;&#24615;&#33021;&#12290;&#22312;&#27492;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;Vibroseis&#25968;&#25454;&#21435;&#22122;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#31574;&#30053;&#30452;&#25509;&#33719;&#21462;&#21435;&#22122;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#36339;&#36807;&#36830;&#25509;&#26469;&#25913;&#36827;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#30041;Vibroseis&#25968;&#25454;&#30340;&#32454;&#33410;&#12290;&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;Vibroseis&#21435;&#22122;&#20219;&#21153;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;Vibroseis&#25968;&#25454;&#20013;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#30456;&#24212;&#30340;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#28145;&#24230;CNN&#27169;&#22411;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;Vibroseis&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;CNN&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#20018;&#32852;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01831v1 Announce Type: new  Abstract: In the field of exploration geophysics, seismic vibrator is one of the widely used seismic sources to acquire seismic data, which is usually named vibroseis. "Ringing effect" is a common problem in vibroseis data processing due to the limited frequency bandwidth of the vibrator, which degrades the performance of first-break picking. In this paper, we proposed a novel deringing model for vibroseis data using deep convolutional neural network (CNN). In this model we use end-to-end training strategy to obtain the deringed data directly, and skip connections to improve model training process and preserve the details of vibroseis data. For real vibroseis deringing task we synthesize training data and corresponding labels from real vibroseis data and utilize them to train the deep CNN model. Experiments are conducted both on synthetic data and real vibroseis data. The experiment results show that deep CNN model can attenuate the ringing effect
&lt;/p&gt;</description></item><item><title>GLDiTalker&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#20316;&#20808;&#39564;&#21644;&#38543;&#26426;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01826</link><description>&lt;p&gt;
GLDiTalker:&#22522;&#20110;&#35821;&#38899;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#29983;&#25104;&#19982;&#22270;&#35889;&#28508;&#22312;&#25193;&#25955;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01826
&lt;/p&gt;
&lt;p&gt;
GLDiTalker&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#20316;&#20808;&#39564;&#21644;&#38543;&#26426;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01826v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#22522;&#20110;&#35821;&#38899;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#29983;&#25104;&#22312;&#24037;&#19994;&#24212;&#29992;&#21644;&#23398;&#26415;&#30740;&#31350;&#20013;&#24050;&#21463;&#21040;&#22823;&#37327;&#20851;&#27880;&#12290;&#30001;&#20110;&#29616;&#23454;&#20013;&#38754;&#37096;&#34920;&#24773;&#30340;&#38750;&#35328;&#35821;&#32447;&#32034;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#29983;&#25104;&#32467;&#26524;&#24212;&#24403;&#20855;&#22791;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#26159;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#26080;&#27861;&#23398;&#20064;&#38899;&#39057;&#19982;&#38754;&#37096;&#21160;&#20316;&#30340;&#35768;&#22810;&#23545;&#35768;&#22810;&#26144;&#23556;&#65292;&#20197;&#33267;&#20110;&#26080;&#27861;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#38754;&#37096;&#21160;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLDiTalker&#65292;&#23427;&#24341;&#20837;&#20102;&#21160;&#20316;&#20808;&#39564;&#20197;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#38543;&#26426;&#24615;&#65292;&#20197;&#20943;&#23569;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#22686;&#21152;&#38754;&#37096;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#22810;&#26679;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;GLDiTalker&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;VQ-VAE&#23558;&#38754;&#37096;&#21160;&#20316;&#32593;&#26684;&#24207;&#21015;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#36845;&#20195;&#22320;&#28155;&#21152;&#21644;&#21435;&#38500;&#22122;&#22768;&#21040;&#28508;&#38754;&#37096;&#21160;&#20316;&#29305;&#24449;&#20013;&#12290;&#20026;&#20102;&#25972;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#36824;&#20351;&#29992;&#20102;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#26469;&#25551;&#36848;&#31354;&#38388;&#19978;&#19979;&#25991;&#21450;&#20854;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21644;&#23545;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;GLDiTalker&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#35821;&#22768;&#21040;&#34920;&#24773;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#21160;&#30011;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info
&lt;/p&gt;</description></item><item><title>MiniCPM-V&#26159;&#19968;&#27454;&#33021;&#23454;&#29616;&#22312;&#25163;&#26426;&#19978;&#36816;&#34892;&#30340;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;GPT-4V-1106&#65292;Gemini Pro&#21644;Claudrat's Pluribus&#65292;&#24182;&#19988;&#22823;&#22823;&#20943;&#23569;&#20102;&#33021;&#32791;&#21644;&#25104;&#26412;&#65292;&#21487;&#20197;&#20026;&#31227;&#21160;&#24212;&#29992;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#20445;&#25252;&#20102;&#29992;&#25143;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2408.01800</link><description>&lt;p&gt;
MiniCPM-V: &#19968;&#27454;&#21487;&#22312;&#24744;&#25163;&#26426;&#19978;&#36816;&#34892;&#30340;GPT-4V&#32423;&#21035;&#30340;MLLM
&lt;/p&gt;
&lt;p&gt;
MiniCPM-V: A GPT-4V Level MLLM on Your Phone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01800
&lt;/p&gt;
&lt;p&gt;
MiniCPM-V&#26159;&#19968;&#27454;&#33021;&#23454;&#29616;&#22312;&#25163;&#26426;&#19978;&#36816;&#34892;&#30340;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;GPT-4V-1106&#65292;Gemini Pro&#21644;Claudrat's Pluribus&#65292;&#24182;&#19988;&#22823;&#22823;&#20943;&#23569;&#20102;&#33021;&#32791;&#21644;&#25104;&#26412;&#65292;&#21487;&#20197;&#20026;&#31227;&#21160;&#24212;&#29992;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#20445;&#25252;&#20102;&#29992;&#25143;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01800v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#20852;&#36215;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#34892;&#19994;&#30340;&#21457;&#23637;&#65292;&#24182;&#29031;&#20142;&#20102;&#19968;&#26465;&#36890;&#24448;&#19979;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#37324;&#31243;&#30865;&#30340;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#38459;&#27490;MLLM&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21464;&#24471;&#23454;&#29992;&#30340;&#37325;&#22823;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#36816;&#34892;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;MLLM&#25152;&#38656;&#30340;&#39640;&#26114;&#25104;&#26412;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;MLLM&#38656;&#35201;&#22312;&#39640;&#24615;&#33021;&#30340;&#20113;&#26381;&#21153;&#22120;&#19978;&#36816;&#34892;&#65292;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#22914;&#31227;&#21160;&#12289;&#31163;&#32447;&#12289;&#33021;&#28304;&#25935;&#24863;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;MiniCPM-V&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#27454;&#21487;&#20197;&#22312;&#35774;&#22791;&#31471;&#37096;&#32626;&#30340;&#25928;&#29575;MLLM&#12290;&#36890;&#36807;&#22312;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21644;&#26657;&#20934;&#26041;&#38754;&#38598;&#25104;&#26368;&#26032;&#30340;MLLM&#25216;&#26415;&#65292;&#26368;&#26032;&#30340;MiniCPM-Llama3-V 2.5&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4V-1106&#12289;Gemini Pro&#21644;Claudrat&#8217;s Pluribus&#12290;&#65288;2&#65289;&#39640;&#25928;&#33021;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#27169;&#22411;&#36816;&#34892;&#30340;&#33021;&#32791;&#21644;&#25104;&#26412;&#12290;&#65288;3&#65289;&#26131;&#37096;&#32626;&#65292;&#21487;&#20197;&#34987;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01800v1 Announce Type: new  Abstract: The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain preventing MLLMs from being practical in real-world applications. The most notable challenge comes from the huge cost of running an MLLM with a massive number of parameters and extensive computation. As a result, most MLLMs need to be deployed on high-performing cloud servers, which greatly limits their application scopes such as mobile, offline, energy-sensitive, and privacy-protective scenarios. In this work, we present MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By integrating the latest MLLM techniques in architecture, pretraining and alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1) Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claud
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#39640;&#25928;&#30340;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#957;Lite&#65292;&#19982;SOTA&#27169;&#22411;CellViT&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#21644;GFlops&#26174;&#33879;&#20943;&#23569;&#65292;&#20026;&#30149;&#29702;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01797</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#957;Lite -- &#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#24555;&#36895;&#30340;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01797
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#39640;&#25928;&#30340;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#957;Lite&#65292;&#19982;SOTA&#27169;&#22411;CellViT&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#21644;GFlops&#26174;&#33879;&#20943;&#23569;&#65292;&#20026;&#30149;&#29702;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01797v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#30149;&#29702;&#23398;&#20013;&#65292;&#23545;HE&#26579;&#33394;&#65288;H&amp;E&#65289;&#29627;&#29255;&#36827;&#34892;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20998;&#26512;&#23545;&#20110;&#21450;&#26102;&#21644;&#26377;&#25928;&#22320;&#36827;&#34892;&#30284;&#30151;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#29992;&#20110;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20276;&#38543;&#30528;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#38656;&#27714;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#957;Lite&#65292;&#36825;&#26159;&#19968;&#27454;&#22522;&#20110;Fast-ViT&#30340;U-Net&#31867;&#20284;&#26550;&#26500;&#65292;Fast-ViT&#26159;&#19968;&#27454;&#26368;&#26032;&#30340;&#65288;SOTA&#65289;&#36731;&#37327;&#32423;CNN&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#19977;&#31181;&#29256;&#26412;&#65292;&#21363;&#957;Lite-S&#12289;&#957;Lite-M&#21644;&#957;Lite-H&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Panoptic Quality&#21644;&#26816;&#27979;&#26041;&#38754;&#19982;CellViT&#65288;SOTA&#65289;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340; lightest&#27169;&#22411;&#65292;&#21363;NuLite-S&#65292;&#22312;&#21442;&#25968;&#19978;&#27604;CellViT&#23567;40&#20493;&#65292;&#22312;GFlops&#19978;&#22823;&#32422;&#23567;&#20102;8&#20493;&#12290;&#25105;&#20204;&#30340;&#26368;&#37325;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#19978;&#30053;&#24494;&#26356;&#23567;&#65292;&#20294;&#22312;GFlops&#19978;&#19982;CellViT&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#30149;&#29702;&#23398;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#21644;&#39640;&#25928;&#24615;&#30340;&#34701;&#21512;&#65292;&#24182;&#20026;&#21307;&#30103;&#24433;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#24320;&#36767;&#20102;&#26032;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01797v1 Announce Type: cross  Abstract: In pathology, accurate and efficient analysis of Hematoxylin and Eosin (H\&amp;E) slides is crucial for timely and effective cancer diagnosis. Although many deep learning solutions for nuclei instance segmentation and classification exist in the literature, they often entail high computational costs and resource requirements, thus limiting their practical usage in medical applications. To address this issue, we introduce a novel convolutional neural network, NuLite, a U-Net-like architecture designed explicitly on Fast-ViT, a state-of-the-art (SOTA) lightweight CNN. We obtained three versions of our model, NuLite-S, NuLite-M, and NuLite-H, trained on the PanNuke dataset. The experimental results prove that our models equal CellViT (SOTA) in terms of panoptic quality and detection. However, our lightest model, NuLite-S, is 40 times smaller in terms of parameters and about 8 times smaller in terms of GFlops, while our heaviest model is 17 ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#25216;&#26415;&#20197;&#35774;&#35745;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#21644;&#19977;&#32500;&#23884;&#20837;&#25216;&#26415;&#30340;&#23637;&#31034;&#26696;&#20363;&#12290;</title><link>https://arxiv.org/abs/2408.01767</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Embedded Spaces for Deep Learning Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#25216;&#26415;&#20197;&#35774;&#35745;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#21644;&#19977;&#32500;&#23884;&#20837;&#25216;&#26415;&#30340;&#23637;&#31034;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01767v1 &#20844;&#21578;&#31867;&#22411;&#65306;cross  &#25688;&#35201;&#65306;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#23884;&#20837;&#31354;&#38388;&#26159;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#12290;&#33391;&#22909;&#30340;&#23884;&#20837;&#31354;&#38388;&#33021;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#20998;&#31867;&#20197;&#21450;&#35832;&#22914;&#24320;&#25918;&#24335;&#35782;&#21035;&#12289;&#23567;&#25209;&#37327;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#39640;&#32423;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#19981;&#21516;&#25216;&#26415;&#35774;&#35745;&#30340;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#20998;&#31867;&#30340;&#32039;&#20945;&#27010;&#36848;&#12290;&#23427;&#26681;&#25454;&#32593;&#32476;&#21442;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#32422;&#26463;&#65292;&#23545;&#27604;&#20102;&#19981;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#21487;&#36798;&#20960;&#20309;&#32467;&#26500;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;MNIST&#12289;Fashion MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20108;&#32500;&#21644;&#19977;&#32500;&#23884;&#20837;&#30340;&#28436;&#31034;&#65292;&#20801;&#35768;&#26816;&#26597;&#23884;&#20837;&#31354;&#38388;&#30340;&#35270;&#35273;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01767v1 Announce Type: cross  Abstract: Embedded spaces are a key feature in deep learning. Good embedded spaces represent the data well to support classification and advanced techniques such as open-set recognition, few-short learning and explainability. This paper presents a compact overview of different techniques to design embedded spaces for classification. It compares different loss functions and constraints on the network parameters with respect to the achievable geometric structure of the embedded space. The techniques are demonstrated with two and three-dimensional embeddings for the MNIST, Fashion MNIST and CIFAR-10 datasets, allowing visual inspection of the embedded spaces.
&lt;/p&gt;</description></item><item><title>MultiFuser&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#65292;&#36890;&#36807;&#20107;&#20214;&#20998;&#35299;&#27169;&#22359;&#21644;&#27169;&#24577;&#21512;&#25104;&#22120;&#38598;&#25104;&#22810;&#31181;&#25668;&#20687;&#22836;&#25968;&#25454;&#26469;&#25552;&#21319;&#34920;&#31034;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01766</link><description>&lt;p&gt;
MultiFuser: &#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#25442;&#22120;&#29992;&#20110;&#22686;&#24378;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MultiFuser: Multimodal Fusion Transformer for Enhanced Driver Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01766
&lt;/p&gt;
&lt;p&gt;
MultiFuser&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#65292;&#36890;&#36807;&#20107;&#20214;&#20998;&#35299;&#27169;&#22359;&#21644;&#27169;&#24577;&#21512;&#25104;&#22120;&#38598;&#25104;&#22810;&#31181;&#25668;&#20687;&#22836;&#25968;&#25454;&#26469;&#25552;&#21319;&#34920;&#31034;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#23545;&#20110;&#25552;&#39640;&#39550;&#39542;&#21592;&#19982;&#36710;&#36742;&#20043;&#38388;&#30340;&#20114;&#21160;&#21644;&#23433;&#20840;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#19968;&#33324;&#30340;&#21160;&#20316;&#35782;&#21035;&#19981;&#21516;&#65292;&#39550;&#39542;&#21592;&#30340;&#29615;&#22659;&#24448;&#24448;&#24456;&#22256;&#38590;&#65292;&#20809;&#32447;&#26127;&#26263;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#30340;&#21457;&#23637;&#65292;&#21508;&#31181;&#25668;&#20687;&#22836;&#65292;&#22914;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#65292;&#34987;&#29992;&#20110;&#20998;&#26512;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#25442;&#22120;&#65292;&#21517;&#20026;MultiFuser&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#39550;&#39542;&#21592;&#36710;&#21410;&#35270;&#39057;&#20013;&#38598;&#25104;&#22810;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#24471;&#21040;&#25913;&#36827;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MultiFuser&#21253;&#25324;&#20998;&#23618;&#30340;&#20107;&#20214;&#20998;&#35299;&#27169;&#22359;&#26469;&#24314;&#27169;&#26102;&#31354;&#29305;&#24449;&#65292;&#20197;&#21450;&#19968;&#31181;&#27169;&#24577;&#21512;&#25104;&#22120;&#29992;&#20110;&#38598;&#25104;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#27599;&#20010;&#20107;&#20214;&#20998;&#35299;&#27169;&#22359;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#29305;&#24449;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;&#22270;&#22359;&#30340;&#36866;&#24212;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01766v1 Announce Type: new  Abstract: Driver action recognition, aiming to accurately identify drivers' behaviours, is crucial for enhancing driver-vehicle interactions and ensuring driving safety. Unlike general action recognition, drivers' environments are often challenging, being gloomy and dark, and with the development of sensors, various cameras such as IR and depth cameras have emerged for analyzing drivers' behaviors. Therefore, in this paper, we propose a novel multimodal fusion transformer, named MultiFuser, which identifies cross-modal interrelations and interactions among multimodal car cabin videos and adaptively integrates different modalities for improved representations. Specifically, MultiFuser comprises layers of Bi-decomposed Modules to model spatiotemporal features, with a modality synthesizer for multimodal features integration. Each Bi-decomposed Module includes a Modal Expertise ViT block for extracting modality-specific features and a Patch-wise Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#30740;&#31350;&#19987;&#27880;&#20110;&#24314;&#31435;&#36731;&#37327;&#32423;CNN&#27169;&#22411;&#65292;&#22914;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#30340;&#39640;&#25928;&#21644;&#20934;&#30830;&#35782;&#21035;&#12290;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#32477;&#23545;&#24046;&#24322;&#25805;&#20316;&#12289;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#21644;&#25913;&#36827;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30149;&#23475;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01752</link><description>&lt;p&gt;
&#25552;&#21319;&#32511;&#33394;AI: &#20026;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#35782;&#21035;&#35774;&#35745;&#30340;&#26377;&#25928;&#21644;&#39640;&#31934;&#24230;&#30340;&#36731;&#37327;&#32423;CNN
&lt;/p&gt;
&lt;p&gt;
Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#30740;&#31350;&#19987;&#27880;&#20110;&#24314;&#31435;&#36731;&#37327;&#32423;CNN&#27169;&#22411;&#65292;&#22914;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#30340;&#39640;&#25928;&#21644;&#20934;&#30830;&#35782;&#21035;&#12290;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#32477;&#23545;&#24046;&#24322;&#25805;&#20316;&#12289;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#21644;&#25913;&#36827;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30149;&#23475;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#27700;&#31291;&#26159;&#20840;&#29699;&#19968;&#21322;&#20197;&#19978;&#20154;&#21475;&#30340;&#20027;&#35201;&#39135;&#29289;&#26469;&#28304;&#65292;&#20854;&#20135;&#37327;&#23545;&#20110;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27700;&#31291;&#26685;&#22521;&#32463;&#24120;&#21463;&#21040;&#22810;&#31181;&#30142;&#30149;&#30340;&#20405;&#25200;&#65292;&#36825;&#20123;&#30142;&#30149;&#21487;&#20197;&#20005;&#37325;&#38477;&#20302;&#20135;&#37327;&#21644;&#21697;&#36136;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#21644;&#20934;&#30830;&#22320;&#26816;&#27979;&#27700;&#31291;&#30142;&#30149;&#23545;&#20110;&#38450;&#27490;&#30142;&#30149;&#34067;&#24310;&#21644;&#20943;&#36731;&#20316;&#29289;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#27425;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#65292;&#29992;&#20110;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#20998;&#31867;&#12290;&#36873;&#25321;&#36825;&#20123;&#27169;&#22411;&#26159;&#22240;&#20026;&#23427;&#20204;&#19982;&#31227;&#21160;&#35774;&#22791;&#20860;&#23481;&#65292;&#19982;&#20854;&#20182;CNN&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#38656;&#35201;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20869;&#23384;&#36739;&#23569;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#19977;&#27454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#28155;&#21152;&#20102;&#20004;&#20010;&#20840;&#36830;&#25509;&#23618;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#20010;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#30340;dropout&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#26089;&#20572;&#26041;&#27861;&#26469;&#38450;&#27490;&#27169;&#22411;&#21457;&#29983;&#36807;&#25311;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#24615;&#33021;&#26469;&#33258;&#8230;&#8230;&#65288;&#30001;&#20110;&#20869;&#23481;&#25972;&#21512;&#20013;&#33521;&#25991;&#25688;&#35201;&#36739;&#38271;&#65292;&#24314;&#35758;&#22312;&#29983;&#25104;&#25688;&#35201;&#26102;&#36827;&#34892;&#36866;&#24403;&#30340;&#31934;&#31616;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01752v1 Announce Type: new  Abstract: Rice plays a vital role as a primary food source for over half of the world's population, and its production is critical for global food security. Nevertheless, rice cultivation is frequently affected by various diseases that can severely decrease yield and quality. Therefore, early and accurate detection of rice diseases is necessary to prevent their spread and minimize crop losses. In this research, we explore three mobile-compatible CNN architectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice leaf disease classification. These models are selected due to their compatibility with mobile devices, as they demand less computational power and memory compared to other CNN models. To enhance the performance of the three models, we added two fully connected layers separated by a dropout layer. We used early stop creation to prevent the model from being overfiting. The results of the study showed that the best performance wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22495;&#24809;&#32602;&#26694;&#26550;&#65292;&#29992;&#20110;&#24179;&#34913;&#22810;&#28304;&#22495;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#26816;&#27979;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;GWHD 2021&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01746</link><description>&lt;p&gt;
&#22495;&#24809;&#32602;&#25913;&#36827;&#30340;&#30028;&#38480;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain penalisation for improved Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22495;&#24809;&#32602;&#26694;&#26550;&#65292;&#29992;&#20110;&#24179;&#34913;&#22810;&#28304;&#22495;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#26816;&#27979;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;GWHD 2021&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01746v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#23545;&#35937;&#26816;&#27979;&#39046;&#22495;&#65292;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#23545;&#24212;&#20110;&#24863;&#20852;&#36259;&#23545;&#35937;&#30340;&#22810;&#20010;&#28304;&#22495;&#20013;&#22266;&#26377;&#30340;&#31283;&#20581;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#30830;&#20445;&#22312;&#19981;&#21516;&#21644;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#20013;&#34920;&#29616;&#20986;&#20581;&#22766;&#24615;&#33021;&#12290;&#23613;&#31649;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#24050;&#32463;&#26377;&#35768;&#22810;&#22522;&#20110;DG&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#23545;&#35937;&#26816;&#27979;&#39046;&#22495;&#65292;&#30456;&#20851;&#24037;&#20316;&#21364;&#30456;&#24403;&#23569;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22495;&#24809;&#32602;&#65288;DP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#26694;&#26550;&#19979;&#65292;&#25968;&#25454;&#26159;&#20174;&#22810;&#20010;&#28304;&#22495;&#20013;&#25277;&#21462;&#30340;&#65292;&#24182;&#19988;&#22312;&#23436;&#20840;&#26410;&#30693;&#30340;&#30446;&#26631;&#27979;&#35797;&#22495;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#22495;&#20998;&#37197;&#24809;&#32602;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#23558;&#26681;&#25454;&#26816;&#27979;&#32593;&#32476;&#22312;&#21508;&#33258;&#28304;&#22495;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#30340;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;GWHD 2021&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26159;Wathlon Object Detection Challenge&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01746v1 Announce Type: new  Abstract: In the field of object detection, domain generalisation (DG) aims to ensure robust performance across diverse and unseen target domains by learning the robust domain-invariant features corresponding to the objects of interest across multiple source domains. While there are many approaches established for performing DG for the task of classification, there has been a very little focus on object detection. In this paper, we propose a domain penalisation (DP) framework for the task of object detection, where the data is assumed to be sampled from multiple source domains and tested on completely unseen test domains. We assign penalisation weights to each domain, with the values updated based on the detection networks performance on the respective source domains. By prioritising the domains that needs more attention, our approach effectively balances the training process. We evaluate our solution on the GWHD 2021 dataset, a component of the W
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;&#38899;&#39057;&#21516;&#27493;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#35270;&#39057;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21767;&#24418;&#21516;&#27493;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2408.01732</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#35828;&#35805;&#20154;&#22836;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01732
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;&#38899;&#39057;&#21516;&#27493;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#35270;&#39057;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21767;&#24418;&#21516;&#27493;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01732v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#29983;&#25104;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#34394;&#25311;&#22836;&#20687;&#12289;&#30005;&#24433;&#21046;&#20316;&#21644;&#22312;&#32447;&#20250;&#35758;&#22312;&#20869;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#36807;&#20998;&#24378;&#35843;&#29983;&#25104;&#19982;&#35821;&#38899;&#21516;&#27493;&#30340;&#21767;&#24418;&#65292;&#32780;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;&#24103;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#32780;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#21017;&#36807;&#20998;&#24378;&#35843;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24103;&#65292;&#24573;&#35270;&#20102;&#21767;&#24418;&#21305;&#37197;&#65292;&#23548;&#33268;&#22068;&#37096;&#36816;&#21160;&#20986;&#29616;&#25238;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;&#31532;&#19968;&#38454;&#27573;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#35821;&#38899;&#29983;&#25104;&#21516;&#27493;&#30340;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#29305;&#24449;&#28857;&#20316;&#20026;&#26465;&#20214;&#22312;&#21435;&#22122;&#36807;&#31243;&#20013;&#20351;&#29992;&#65292;&#26088;&#22312;&#20248;&#21270;&#22068;&#37096;&#25238;&#21160;&#38382;&#39064;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#21516;&#27493;&#21644;&#22312;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#36136;&#37327;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20154;&#31867;&#35780;&#20215;&#21644;&#27169;&#22411;&#24615;&#33021;&#24230;&#37327;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01723</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#36136;&#37327;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Evaluation Framework for Image2Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#36136;&#37327;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20154;&#31867;&#35780;&#20215;&#21644;&#27169;&#22411;&#24615;&#33021;&#24230;&#37327;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01723v1 &#20844;&#21578;&#31867;&#22411;: &#26032; Abstract: &#33258;&#21160;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#30340;&#36136;&#37327;&#35780;&#20272;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#23427;&#38656;&#35201;&#21508;&#31181;&#26041;&#38754;&#22914;&#21477;&#27861;&#12289;&#35206;&#30422;&#12289;&#27491;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#34429;&#28982;&#20154;&#24037;&#35780;&#20272;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#26102;&#38388;&#28040;&#32791;&#30340;&#33258;&#28982;&#38480;&#21046;&#20102;&#20854;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#22914;BLEU&#12289;ROUGE&#12289;METEOR&#21644;CIDEr&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#24448;&#24448;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#32852;&#31995;&#36739;&#24369;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22914;GPT-4&#25110;Gemini&#65292;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#12290;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36755;&#20837;&#22270;&#20687;&#21890;&#20837;&#25351;&#23450;&#30340;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#21518;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#12290;&#20351;&#29992;&#36825;&#20010;&#25551;&#36848;&#65292;LLM&#28982;&#21518;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#20174;&#21407;&#22987;&#22270;&#20687;&#21644;&#30001;LLM&#21019;&#24314;&#30340;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25105;&#20204;&#20351;&#29992;&#25351;&#23450;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#20004;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20154;&#31867;&#35780;&#20215;&#32773;&#30340;&#21453;&#39304;&#20316;&#20026;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#20840;&#38754;&#34913;&#37327;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#32773;&#21644;&#29616;&#20195;LLM&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#33258;&#21160;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#30340;&#36136;&#37327;&#35780;&#20272;&#25552;&#20379;&#19968;&#20010;&#26356;&#31934;&#30830;&#12289;&#26356;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01723v1 Announce Type: new  Abstract: Evaluating the quality of automatically generated image descriptions is challenging, requiring metrics that capture various aspects such as grammaticality, coverage, correctness, and truthfulness. While human evaluation offers valuable insights, its cost and time-consuming nature pose limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr aim to bridge this gap but often show weak correlations with human judgment. We address this challenge by introducing a novel evaluation framework rooted in a modern large language model (LLM), such as GPT-4 or Gemini, capable of image generation. In our proposed framework, we begin by feeding an input image into a designated image captioning model, chosen for evaluation, to generate a textual description. Using this description, an LLM then creates a new image. By extracting features from both the original and LLM-created images, we measure their similarity using a designated simil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22810;&#31181;&#35270;&#35273;-&#24815;&#24615;SLAM&#31995;&#32479;&#36827;&#34892;&#20102;&#20892;&#19994;&#26426;&#22120;&#20154;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#38381;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2408.01716</link><description>&lt;p&gt;
&#20892;&#19994;&#26426;&#22120;&#20154;&#35270;&#35273;-&#24815;&#24615;SLAM: &#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#38381;&#29615;&#38381;&#21512;&#30340;&#22909;&#22788;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the Benefits and Computational Costs of Loop Closing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#31181;&#35270;&#35273;-&#24815;&#24615;SLAM&#31995;&#32479;&#36827;&#34892;&#20102;&#20892;&#19994;&#26426;&#22120;&#20154;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#38381;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01716v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: &#21516;&#27493;&#23450;&#20301;&#19982;&#22320;&#22270;&#26500;&#24314;(SLAM)&#23545;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#22312;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#23450;&#20301;&#31995;&#32479;&#30340;&#24773;&#20917;&#19979;&#65292;&#20801;&#35768;&#22312;&#21160;&#24577;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#25143;&#22806;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#12290;&#22312;&#20892;&#19994;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#29031;&#26126;&#25110;&#22825;&#27668;&#26465;&#20214;&#30340;&#21487;&#21464;&#24615;&#65292;&#35270;&#35273;-&#24815;&#24615;SLAM&#24050;&#25104;&#20026;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#23545;&#20960;&#31181;&#24320;&#28304;&#35270;&#35273;-&#24815;&#24615;SLAM&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;ORB-SLAM3&#12289;VINS-Fusion&#12289;OpenVINS&#12289;Kimera&#21644;SVO Pro&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20391;&#37325;&#20110;&#38381;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#23545;&#24103;&#29575;&#21464;&#21270;&#23545;&#23450;&#20301;&#31934;&#24230;&#24433;&#21709;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#31995;&#32479;&#22312;&#38451;&#20809;&#30452;&#23556;&#21644;&#36974;&#25377;&#19979;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#39640;&#37325;&#22797;&#24615;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#20892;&#19994;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#38381;&#29615;&#38381;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;&#23450;&#20301;&#31934;&#24230;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19981;&#21516;&#31995;&#32479;&#22312;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20026;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#28508;&#22312;&#30340;&#33021;&#28304;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20026;&#20892;&#19994;&#26426;&#22120;&#20154;&#20013;&#30340;&#35270;&#35273;-&#24815;&#24615;SLAM&#30340;&#36873;&#22411;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01716v1 Announce Type: new  Abstract: Simultaneous Localization and Mapping (SLAM) is essential for mobile robotics, enabling autonomous navigation in dynamic, unstructured outdoor environments without relying on external positioning systems. In agricultural applications, where environmental conditions can be particularly challenging due to variable lighting or weather conditions, Visual-Inertial SLAM has emerged as a potential solution. This paper benchmarks several open-source Visual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS, Kimera, and SVO Pro, to evaluate their performance in agricultural settings. We focus on the impact of loop closing on localization accuracy and computational demands, providing a comprehensive analysis of these systems' effectiveness in real-world environments and especially their application to embedded systems in agricultural robotics. Our contributions further include an assessment of varying frame rates on localization acc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#27454;&#36866;&#29992;&#20110;&#20108;&#20540;&#36793;&#32536;&#22270;&#20687;&#27169;&#31946;&#32467;&#26500;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36793;&#32536;&#36861;&#36394;&#25216;&#26415;&#65292;&#24182;&#35299;&#37322;&#20102;&#31639;&#27861;&#20197;&#23454;&#29616;&#21518;&#32493;&#20219;&#21153;&#30340;&#22788;&#29702;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.01712</link><description>&lt;p&gt;
&#20108;&#20540;&#36793;&#32536;&#22270;&#20687;&#30340;&#36890;&#29992;&#27169;&#31946;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36793;&#32536;&#36861;&#36394;&#21450;&#20854;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
A General Ambiguity Model for Binary Edge Images with Edge Tracing and its Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#27454;&#36866;&#29992;&#20110;&#20108;&#20540;&#36793;&#32536;&#22270;&#20687;&#27169;&#31946;&#32467;&#26500;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36793;&#32536;&#36861;&#36394;&#25216;&#26415;&#65292;&#24182;&#35299;&#37322;&#20102;&#31639;&#27861;&#20197;&#23454;&#29616;&#21518;&#32493;&#20219;&#21153;&#30340;&#22788;&#29702;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#27425;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20108;&#20540;&#36793;&#32536;&#22270;&#20687;&#20013;&#30340;&#20132;&#21449;&#28857;&#12289;&#25509;&#21512;&#21644;&#20854;&#20182;&#32467;&#26500;&#30340;&#19968;&#33324;&#19988;&#30452;&#35266;&#30340;&#27169;&#31946;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19982;&#36793;&#32536;&#36861;&#36394;&#25216;&#26415;&#32467;&#21512;&#65292;&#20854;&#20013;&#36793;&#32536;&#34987;&#30475;&#20316;&#26159;&#19968;&#31995;&#21015;&#36830;&#25509;&#30340;&#20687;&#32032;&#26377;&#24207;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#31181;&#36890;&#29992;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35832;&#22914;&#22270;&#24213;&#20998;&#21106;&#12289;&#29289;&#20307;&#35782;&#21035;&#12289;&#25299;&#25169;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21407;&#21017;&#65292;&#32467;&#26524;&#25551;&#36848;&#36215;&#26469;&#30452;&#35266;&#26131;&#25026;&#65292;&#26377;&#21161;&#20110;&#21518;&#32493;&#22788;&#29702;&#27493;&#39588;&#30340;&#23454;&#29616;&#65292;&#22914;&#22312;&#25509;&#21512;&#22788;&#35299;&#20915;&#36793;&#32536;&#36830;&#25509;&#30340;&#27169;&#31946;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#25193;&#22823;&#30340;&#36793;&#32536;&#22270;&#65292;&#21487;&#20197;&#21033;&#29992;&#24555;&#36895;&#30340;&#23616;&#37096;&#25628;&#32034;&#25805;&#20316;&#30452;&#25509;&#35775;&#38382;&#30456;&#37051;&#30340;&#36793;&#32536;&#12290;&#25105;&#20204;&#30340;&#36793;&#32536;&#36861;&#36394;&#31639;&#27861;&#37319;&#29992;&#20102;&#36882;&#24402;&#25216;&#26415;&#65292;&#36825;&#31616;&#21270;&#20102;&#32534;&#31243;&#20195;&#30721;&#12290;&#38543;&#21518;&#25105;&#20204;&#23558;&#29992;&#20266;&#20195;&#30721;&#35814;&#32454;&#20171;&#32461;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#19982;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#31616;&#21333;&#30340;&#27169;&#22359;&#21270;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#20248;&#21270;&#32467;&#26524;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#23436;&#25972;&#24615;&#27426;&#36814;&#26377;&#20852;&#36259;&#30340;&#29992;&#25143;&#21644;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01712v1 Announce Type: new  Abstract: We present a general and intuitive ambiguity model for intersections, junctions and other structures in binary edge images. The model is combined with edge tracing, where edges are ordered sequences of connected pixels. The objective is to provide a versatile preprocessing method for tasks such as figure-ground segmentation, object recognition, topological analysis, etc. By using only a small set of straightforward principles, the results are intuitive to describe. This helps to implement subsequent processing steps, such as resolving ambiguous edge connections at junctions. By using an augmented edge map, neighboring edges can be directly accessed using quick local search operations. The edge tracing uses recursion, which leads to compact programming code. We explain our algorithm using pseudocode, compare it with related methods, and show how simple modular postprocessing steps can be used to optimize the results. The complete algorith
&lt;/p&gt;</description></item><item><title>AVESFormer&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#38899;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;AVS&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.01708</link><description>&lt;p&gt;
AVESFormer&#65306;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#20998;&#21106;&#30340;&#39640;&#25928;&#21464;&#25442;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01708
&lt;/p&gt;
&lt;p&gt;
AVESFormer&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#38899;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;AVS&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01708v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#26368;&#36817;&#65292;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#22312;&#38899;&#39057;&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#29702;&#22312;&#23454;&#38469;&#20013;&#19981;&#21487;&#34892;&#12290;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#30340;&#27880;&#24847;&#22270;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;AVS&#27169;&#22411;&#30340;&#20004;&#20010;&#20851;&#38190;&#38556;&#30861;&#65306;1&#65289;&#27880;&#24847;&#25439;&#32791;&#65292;&#23545;&#24212;&#20110;&#22312;&#21463;&#38480;&#26694;&#26550;&#20869;&#30001;Softmax&#23548;&#33268;&#30340;&#27880;&#24847;&#21147;&#26435;&#20540;&#30340;&#36807;&#24230;&#38598;&#20013;&#65292;&#20197;&#21450;2&#65289;&#19981;&#39640;&#25928;&#30340;&#12289;&#27785;&#37325;&#30340;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#65292;&#26159;&#30001;&#26089;&#26399;&#38454;&#27573;&#20986;&#29616;&#30340;&#19981;&#24191;&#27867;&#20851;&#27880;&#27169;&#24335;&#24341;&#36215;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AVESFormer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#26377;&#25928;&#20998;&#21106;transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#39640;&#25928;&#30340;&#25552;&#31034;&#26597;&#35810;&#29983;&#25104;&#22120;&#26469;&#32416;&#27491;&#36328;&#27880;&#24847;&#21147;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;ELF&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20419;&#36827;&#36866;&#21512;&#23616;&#37096;&#29305;&#24449;&#30340;&#21367;&#31215;&#65292;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22823;&#25928;&#29575;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01708v1 Announce Type: new  Abstract: Recently, transformer-based models have demonstrated remarkable performance on audio-visual segmentation (AVS) tasks. However, their expensive computational cost makes real-time inference impractical. By characterizing attention maps of the network, we identify two key obstacles in AVS models: 1) attention dissipation, corresponding to the over-concentrated attention weights by Softmax within restricted frames, and 2) inefficient, burdensome transformer decoder, caused by narrow focus patterns in early stages. In this paper, we introduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation transformer that achieves fast, efficient and light-weight simultaneously. Our model leverages an efficient prompt query generator to correct the behaviour of cross-attention. Additionally, we propose ELF decoder to bring greater efficiency by facilitating convolutions suitable for local features to reduce computational burdens. Extensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20449;&#21495;-SGN&#30340;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#22270;&#21367;&#31215;&#29305;&#24615;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29305;&#24615;&#65292;&#26088;&#22312;&#26377;&#25928;&#30340;&#22788;&#29702;&#39592;&#39612;&#21160;&#20316;&#24207;&#21015;&#30340;&#26102;&#38388;&#32500;&#24230;&#12290;&#20449;&#21495;-SGN&#33021;&#22815;&#36890;&#36807;&#22312;&#21333;&#20010;&#24103;&#19978;&#36827;&#34892;&#22270;&#21367;&#31215;&#65292;&#24182;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#25429;&#33719;&#24103;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#65292;&#20197;&#21450;&#22312;&#39057;&#29575;&#31354;&#38388;&#22788;&#29702;&#26102;&#31354;&#39057;&#29575;&#21160;&#24577;&#65292;&#20197;&#23454;&#29616;&#21160;&#20316;&#35782;&#21035;&#30340;&#39640;&#25928;&#21644;&#31934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2408.01701</link><description>&lt;p&gt;
&#20449;&#21495;-SGN&#65306;&#19968;&#31181;&#29992;&#20110;&#36890;&#36807;&#23398;&#20064;&#26102;&#31354;&#39057;&#29575;&#21160;&#24577;&#30340;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20449;&#21495;-SGN&#30340;&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#22270;&#21367;&#31215;&#29305;&#24615;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29305;&#24615;&#65292;&#26088;&#22312;&#26377;&#25928;&#30340;&#22788;&#29702;&#39592;&#39612;&#21160;&#20316;&#24207;&#21015;&#30340;&#26102;&#38388;&#32500;&#24230;&#12290;&#20449;&#21495;-SGN&#33021;&#22815;&#36890;&#36807;&#22312;&#21333;&#20010;&#24103;&#19978;&#36827;&#34892;&#22270;&#21367;&#31215;&#65292;&#24182;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#25429;&#33719;&#24103;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#65292;&#20197;&#21450;&#22312;&#39057;&#29575;&#31354;&#38388;&#22788;&#29702;&#26102;&#31354;&#39057;&#29575;&#21160;&#24577;&#65292;&#20197;&#23454;&#29616;&#21160;&#20316;&#35782;&#21035;&#30340;&#39640;&#25928;&#21644;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01701v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;Abstract: &#22312;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#20013;&#65292;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;(GCNs)&#30340;&#26041;&#27861;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#39640;&#33021;&#37327;&#28040;&#32791;&#32780;&#23384;&#22312;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22240;&#20854;&#20302;&#33021;&#37327;&#28040;&#32791;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#29616;&#26377;&#32467;&#21512;GCNs&#21644;SNNs&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#39592;&#39612;&#24207;&#21015;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#23548;&#33268;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Signal-SGN(Spiking Graph Convolutional Network)&#30340;&#27169;&#22411;&#65292;&#23427;&#23558;&#39592;&#39612;&#24207;&#21015;&#30340;&#26102;&#38388;&#32500;&#24230;&#20316;&#20026;&#33033;&#20914;&#26102;&#38388;&#27493;&#38271;&#65292;&#24182;&#23558;&#29305;&#24449;&#35270;&#20026;&#31163;&#25955;&#30340;&#38543;&#26426;&#20449;&#21495;&#12290;&#32593;&#32476;&#30340;&#26680;&#24515;&#26159;&#30001;&#19968;&#20010;1D&#33033;&#20914;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;1D-SGN&#65289;&#21644;&#19968;&#20010;&#39057;&#29575;&#33033;&#20914;&#21367;&#31215;&#32593;&#32476;&#65288;FSN&#65289;&#32452;&#25104;&#30340;&#12290;SGN&#22312;&#21333;&#20010;&#24103;&#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#65292;&#24182;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29305;&#24615;&#26469;&#25429;&#33719;&#24103;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;FSN&#21017;&#22788;&#29702;&#36825;&#20123;&#26102;&#31354;&#39057;&#29575;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#39592;&#39612;&#21160;&#20316;&#30340;&#39640;&#25928;&#12289;&#31934;&#30830;&#35782;&#21035;&#12290;The proposed network effectively reduces energy consumption and computational costs while improving the spatial-temporal feature representation capability.&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;Signal-SGN&#22312;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01701v1 Announce Type: new  Abstract: In skeletal-based action recognition, Graph Convolutional Networks (GCNs) based methods face limitations due to their complexity and high energy consumption. Spiking Neural Networks (SNNs) have gained attention in recent years for their low energy consumption, but existing methods combining GCNs and SNNs fail to fully utilize the temporal characteristics of skeletal sequences, leading to increased storage and computational costs. To address this issue, we propose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the temporal dimension of skeletal sequences as the spiking timestep and treats features as discrete stochastic signals. The core of the network consists of a 1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking Convolutional Network (FSN). The SGN performs graph convolution on single frames and incorporates spiking network characteristics to capture inter-frame temporal relationships, while th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#29109;&#26469;&#20248;&#21270;&#23545;&#31232;&#30095;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20165;&#20351;&#29992;&#24456;&#23569;&#30340;&#20687;&#32032;&#26631;&#27880;&#21363;&#21487;&#36798;&#21040;&#30417;&#30563;&#35757;&#32451;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2408.01694</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Bayesian Active Learning for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01694
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#29109;&#26469;&#20248;&#21270;&#23545;&#31232;&#30095;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20165;&#20351;&#29992;&#24456;&#23569;&#30340;&#20687;&#32032;&#26631;&#27880;&#21363;&#21487;&#36798;&#21040;&#30417;&#30563;&#35757;&#32451;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01694v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#23436;&#20840;&#30417;&#30563;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#19988;&#22256;&#38590;&#65292;&#22240;&#20026;&#27599;&#24352;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#37117;&#38656;&#35201;&#36827;&#34892;&#26631;&#27880;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#31232;&#30095;&#20687;&#32032;&#32423;&#26631;&#27880;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#23545;&#27599;&#24352;&#22270;&#20687;&#20013;&#30340;&#23376;&#38598;&#20687;&#32032;&#36827;&#34892;&#26631;&#27880;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#24179;&#34913;&#29109;&#65288;Balanced Entropy&#65289;&#30340;&#20687;&#32032;&#32423;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20854;&#20013;Balanced Entropy&#26159;&#19968;&#31181;&#22522;&#20110;[84]&#30340;&#24179;&#34913;&#29109;&#65292;&#33021;&#22815;&#25429;&#25417;&#27169;&#22411;&#39044;&#27979;&#30340;&#20960;&#20309;&#20998;&#24067;&#21644;&#30456;&#20851;&#20687;&#32032;&#26631;&#31614;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;Balanced Entropy&#20855;&#26377;&#32447;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#26512;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#20854;&#20182;&#20687;&#32032;&#30340;&#24773;&#20917;&#19979;&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#20687;&#32032;&#30340;Balanced Entropy&#20540;&#12290;&#25105;&#20204;&#22312;&#25105;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#19978;&#22521;&#35757;&#20102;Cityscapes&#12289;Camvid&#12289;ADE20K&#21644;VOC2012&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20351;&#29992;&#20165;&#21344;&#20808;&#21069;&#25551;&#36848;&#25968;&#25454;&#38598;&#19968;&#23567;&#37096;&#20998;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#27700;&#24179;&#30456;&#20284;&#30340;mIoU&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312;Cityscapes&#12289;Camvid&#12289;ADE20K&#21644;VOC2012&#25968;&#25454;&#38598;&#20013;&#21021;&#27493;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01694v1 Announce Type: new  Abstract: Fully supervised training of semantic segmentation models is costly and challenging because each pixel within an image needs to be labeled. Therefore, the sparse pixel-level annotation methods have been introduced to train models with a subset of pixels within each image. We introduce a Bayesian active learning framework based on sparse pixel-level annotation that utilizes a pixel-level Bayesian uncertainty measure based on Balanced Entropy (BalEnt) [84]. BalEnt captures the information between the models' predicted marginalized probability distribution and the pixel labels. BalEnt has linear scalability with a closed analytical form and can be calculated independently per pixel without relational computations with other pixels. We train our proposed active learning framework for Cityscapes, Camvid, ADE20K and VOC2012 benchmark datasets and show that it reaches supervised levels of mIoU using only a fraction of labeled pixels while outpe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#22235;&#31181;&#20256;&#32479;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65288;U-Net&#12289;LinkNet&#12289;PSPNet&#21644;FPN&#65289;&#21644;ResNet50 backbone encoder&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#28369;&#22369;&#24182;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01692</link><description>&lt;p&gt;
&#22522;&#20110;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28369;&#22369;&#26816;&#27979;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of CNN-based Deep Learning Models for Landslide Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01692
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#22235;&#31181;&#20256;&#32479;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65288;U-Net&#12289;LinkNet&#12289;PSPNet&#21644;FPN&#65289;&#21644;ResNet50 backbone encoder&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#28369;&#22369;&#24182;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01692v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#28369;&#22369;&#23545;&#31038;&#20250;&#30340;&#32463;&#27982;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#20316;&#20026;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#39057;&#32321;&#21644;&#30772;&#22351;&#24615;&#33258;&#28982;&#28798;&#23475;&#30340;&#37325;&#35201;&#24615;&#12290;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#21271;&#37096;&#22320;&#21306;&#26368;&#36817;&#21457;&#29983;&#30340;&#28369;&#22369;&#36896;&#25104;&#20102;&#20005;&#37325;&#30340;&#30772;&#22351;&#65292;&#25439;&#22351;&#20102;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#23545;&#24403;&#22320;&#31038;&#21306;&#26500;&#25104;&#20102;&#23041;&#32961;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#21644;&#39640;&#32423;&#30340;CNN-based&#27169;&#22411;&#22312;&#28369;&#22369;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#31639;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#26356;&#35814;&#32454;&#22320;&#30740;&#31350;CNNs&#30340;&#28508;&#21147;&#65292;&#24182;&#30528;&#37325;&#20110;&#23545;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36827;&#34892;&#28369;&#22369;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#20256;&#32479;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65288;U-Net&#12289;LinkNet&#12289;PSPNet&#21644;FPN&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23558;ResNet50 backbone encoder&#24212;&#29992;&#20110;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#39564;&#20102;&#35832;&#22914;&#23398;&#20064;&#29575;&#12289;&#25209;&#22823;&#23567;&#21644;&#35757;&#32451; epoch &#31561;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;&#22810;&#20809;&#35889;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#28369;&#22369;&#24182;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01692v1 Announce Type: new  Abstract: Landslides inflict substantial societal and economic damage, underscoring their global significance as recurrent and destructive natural disasters. Recent landslides in northern parts of India and Nepal have caused significant disruption, damaging infrastructure and posing threats to local communities. Convolutional Neural Networks (CNNs), a type of deep learning technique, have shown remarkable success in image processing. Because of their sophisticated architectures, advanced CNN-based models perform better in landslide detection than conventional algorithms. The purpose of this work is to investigate CNNs' potential in more detail, with an emphasis on comparison of CNN based models for better landslide detection. We compared four traditional semantic segmentation models (U-Net, LinkNet, PSPNet, and FPN) and utilized the ResNet50 backbone encoder to implement them. Moreover, we have experimented with the hyperparameters such as learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#30340;&#27450;&#35784;&#26816;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2408.01690</link><description>&lt;p&gt;
IDNet: &#19968;&#20010;&#29992;&#20110;&#36523;&#20221;&#25991;&#26723;&#20998;&#26512;&#21644;&#27450;&#35784;&#26816;&#27979;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#30340;&#27450;&#35784;&#26816;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01690v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#25387;&#36133;&#36523;&#20221;&#30423;&#31363;&#24182;&#21152;&#24378;&#23433;&#20840;&#65292;&#26377;&#25928;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#21644;&#23545;&#25919;&#24220;&#21457;&#34892;&#30340;&#36523;&#20221;&#25991;&#26723;(&#22914;&#25252;&#29031;&#12289;&#39550;&#39542;&#25191;&#29031;&#21644;&#36523;&#20221;&#35777;)&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#35757;&#32451;&#20934;&#30830;&#30340;&#36523;&#20221;&#25991;&#26723;&#27450;&#35784;&#26816;&#27979;&#21644;&#20998;&#26512;&#24037;&#20855;&#20381;&#36182;&#20110;&#22823;&#37327;&#36523;&#20221;&#25991;&#26723;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#29992;&#20110;&#36523;&#20221;&#25991;&#26723;&#20998;&#26512;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;MIDV-500&#12289;MIDV-2020&#21644;FMIDV&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65306;&#23427;&#20204;&#25552;&#20379;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#28041;&#21450;&#30340;&#27450;&#35784;&#27169;&#24335;&#19981;&#36275;&#65292;&#32780;&#19988;&#24456;&#23569;&#21253;&#25324;&#23545;&#20851;&#38190;&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#23383;&#27573;(&#22914;&#32918;&#20687;&#22270;&#20687;)&#30340;&#26356;&#25913;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#30495;&#23454;&#29983;&#27963;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;  &#20026;&#20102;&#22238;&#24212;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#8212;&#8212;IDNet&#65292;&#26088;&#22312;&#25512;&#36827;&#38544;&#31169;&#20445;&#25252;&#30340;&#27450;&#35784;&#26816;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01690v1 Announce Type: new  Abstract: Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy.   In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#949;&#25511;&#21046;&#22240;&#23376;&#35843;&#33410;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21435;&#23398;&#20064;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2408.01689</link><description>&lt;p&gt;
&#20351;&#29992;&#949;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#25511;&#21046;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#949;&#25511;&#21046;&#22240;&#23376;&#35843;&#33410;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21435;&#23398;&#20064;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29983;&#25104;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#27844;&#38706;&#21644;&#20559;&#35265;&#31561;&#38382;&#39064;&#30340;&#25285;&#24551;&#12290;&#26426;&#22120;&#21435;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20363;&#22914;&#21547;&#26377;&#31169;&#20154;&#20449;&#24687;&#21644;&#20559;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Image-to-Image&#65288;I2I&#65289;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20165;&#25552;&#20379;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#24573;&#35270;&#20102;&#29992;&#25143;&#23545;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#25928;&#29992;&#20043;&#38388;&#30340;&#22810;&#26679;&#21270;&#26399;&#24453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#21046;&#21435;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25511;&#21046;&#31995;&#25968;&#949;&#26469;&#25511;&#21046;&#36825;&#31181;&#21462;&#33293;&#12290;&#25105;&#20204;&#23558;I2I&#29983;&#25104;&#27169;&#22411;&#21435;&#23398;&#20064;&#30340;&#21839;&#38988;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#949;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#25214;&#21040;&#38024;&#23545;&#36951;&#24536;&#30340;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#30340;&#26032;&#40092;&#24230;&#21644;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#30340;&#22330;&#26223;&#65292;&#22914;&#22270;&#24418;&#35774;&#35745;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#31561;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#25968;&#25454;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01689v1 Announce Type: cross  Abstract: While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\varepsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\varepsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearni
&lt;/p&gt;</description></item><item><title>SiamMo&#26159;&#19968;&#31181;&#26032;&#30340;3D&#29289;&#20307;&#36319;&#36394;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Siamese&#36816;&#21160;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;Siamese&#29305;&#24449;&#25552;&#21462;&#21644;Spatio-Temporal Feature Aggregation&#27169;&#22359;&#25913;&#21892;&#20102;&#36319;&#36394;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;Box-aware Feature Encoding&#27169;&#22359;&#26469;&#32534;&#30721;&#29289;&#20307;&#26694;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2408.01688</link><description>&lt;p&gt;
&#27888;&#31859;&#23572;&#33707;&#65306;&#22522;&#20110;Siamese&#36816;&#21160;&#20013;&#24515;3D&#29289;&#20307;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
SiamMo: Siamese Motion-Centric 3D Object Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01688
&lt;/p&gt;
&lt;p&gt;
SiamMo&#26159;&#19968;&#31181;&#26032;&#30340;3D&#29289;&#20307;&#36319;&#36394;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Siamese&#36816;&#21160;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;Siamese&#29305;&#24449;&#25552;&#21462;&#21644;Spatio-Temporal Feature Aggregation&#27169;&#22359;&#25913;&#21892;&#20102;&#36319;&#36394;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;Box-aware Feature Encoding&#27169;&#22359;&#26469;&#32534;&#30721;&#29289;&#20307;&#26694;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01688v1 &#39044;&#25253;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#24403;&#21069;&#30340;&#19977;&#32500;&#21333;&#19968;&#29289;&#20307;&#36319;&#36394;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;Siamese&#21305;&#37197;&#22522;&#30784;&#33539;&#24335;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#32570;&#20047;&#32441;&#29702;&#21644;&#19981;&#23436;&#25972;&#30340;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#24773;&#20917;&#19979;&#36935;&#21040;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#36816;&#21160;&#20013;&#24515;&#33539;&#24335;&#36991;&#20813;&#20102;&#22806;&#35266;&#21305;&#37197;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20854;&#22797;&#26434;&#30340;multistage&#31649;&#36947;&#21644;&#21333;&#20010;&#27969;&#26550;&#26500;&#30340;&#26377;&#38480;&#26102;&#38388;&#24314;&#27169;&#33021;&#21147;&#38480;&#21046;&#20102;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SiamMo&#30340;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;Siamese&#36816;&#21160;&#20013;&#24515;&#36319;&#36394;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#21333;&#27969;&#26550;&#26500;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;Siamese&#29305;&#24449;&#25552;&#21462;&#26469;&#36827;&#34892;&#36816;&#21160;&#20013;&#24515;&#36319;&#36394;&#12290;&#36825;&#31181;&#29305;&#24449;&#25552;&#21462;&#19982;&#26102;&#38388;&#34701;&#21512;&#30340;&#20998;&#31163;&#26174;&#33879;&#25552;&#39640;&#20102;&#36319;&#36394;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Spatio-Temporal Feature Aggregation&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#22810;&#23610;&#24230;&#19979;&#25972;&#21512;Siamese&#29305;&#24449;&#65292;&#26377;&#25928;&#25429;&#33719;&#36816;&#21160;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;Box-aware Feature Encoding&#27169;&#22359;&#65292;&#29992;&#20110;&#32534;&#30721;&#29289;&#20307;&#26694;&#30456;&#20851;&#29305;&#24449;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;Siamese&#32593;&#32476;&#32467;&#26500;&#21644;&#31934;&#31616;&#30340;&#27169;&#22359;&#35774;&#35745;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SiamMo&#22312;&#22810;&#31181;&#39640;&#38590;&#24230;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#38271;&#36317;&#31163;&#36319;&#36394;&#12289;&#21160;&#24577;&#22330;&#26223;&#21644;&#20855;&#26377;&#36974;&#25377;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;SiamMo&#22312;&#20943;&#23569;&#36319;&#36394;&#35823;&#24046;&#21644;&#21152;&#36895;&#36319;&#36394;&#36807;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#26080;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;3D&#29289;&#20307;&#36319;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01688v1 Announce Type: new  Abstract: Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode obj
&lt;/p&gt;</description></item><item><title>iControl3D&#26159;&#19968;&#20010;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#30830;&#25511;&#21046;&#24182;&#29983;&#25104;&#21487;&#23450;&#21046;3D&#22330;&#26223;&#30340;&#20132;&#20114;&#24335;&#31995;&#32479;&#12290;&#23427;&#20351;&#29992;3D&#32593;&#26684;&#20316;&#20026;&#20013;&#38388;&#20195;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#21512;&#24182;2D&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#21019;&#24314;&#36830;&#36143;&#32479;&#19968;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2408.01678</link><description>&lt;p&gt;
iControl3D&#65306;&#21487;&#25511;3D&#22330;&#26223;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
iControl3D: An Interactive System for Controllable 3D Scene Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01678
&lt;/p&gt;
&lt;p&gt;
iControl3D&#26159;&#19968;&#20010;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#30830;&#25511;&#21046;&#24182;&#29983;&#25104;&#21487;&#23450;&#21046;3D&#22330;&#26223;&#30340;&#20132;&#20114;&#24335;&#31995;&#32479;&#12290;&#23427;&#20351;&#29992;3D&#32593;&#26684;&#20316;&#20026;&#20013;&#38388;&#20195;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#21512;&#24182;2D&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#21019;&#24314;&#36830;&#36143;&#32479;&#19968;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01678v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;3D&#20869;&#23481;&#21019;&#24314;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#25216;&#33021;&#21644;&#36164;&#28304;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#21457;&#23637;&#20801;&#35768;&#22522;&#20110;&#25991;&#26412;&#30340;3D&#23545;&#35937;&#21644;&#22330;&#26223;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#22312;&#25552;&#20379;&#29983;&#25104;&#36807;&#31243;&#30340;&#36275;&#22815;&#25511;&#21046;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#65292;&#23548;&#33268;&#29992;&#25143;&#21019;&#24847;&#24895;&#26223;&#19982;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;iControl3D&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20197;&#31934;&#30830;&#30340;&#25511;&#21046;&#29983;&#25104;&#21644;&#28210;&#26579;&#21487;&#23450;&#21046;&#30340;3D&#22330;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;3D&#21019;&#36896;&#32773;&#30028;&#38754;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#21019;&#24314;&#36807;&#31243;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;3D&#32593;&#26684;&#20316;&#20026;&#20013;&#38388;&#20195;&#29702;&#65292;&#23558;&#20010;&#20307;2D&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#36845;&#20195;&#21512;&#24182;&#20026;&#36830;&#36143;&#19988;&#32479;&#19968;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#20026;&#20102;&#30830;&#20445;3D&#32593;&#26684;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25105;&#20204;&#22312;&#34701;&#21512;&#26032;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#21069;&#65292;&#25552;&#20986;&#20102;&#36793;&#30028;&#24863;&#30693;&#28145;&#24230;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01678v1 Announce Type: new  Abstract: 3D content creation has long been a complex and time-consuming process, often requiring specialized skills and resources. While recent advancements have allowed for text-guided 3D object and scene generation, they still fall short of providing sufficient control over the generation process, leading to a gap between the user's creative vision and the generated results. In this paper, we present iControl3D, a novel interactive system that empowers users to generate and render customizable 3D scenes with precise control. To this end, a 3D creator interface has been developed to provide users with fine-grained control over the creation process. Technically, we leverage 3D meshes as an intermediary proxy to iteratively merge individual 2D diffusion-generated images into a cohesive and unified 3D scene representation. To ensure seamless integration of 3D meshes, we propose to perform boundary-aware depth alignment before fusing the newly gener
&lt;/p&gt;</description></item><item><title>HIVE&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#20307;&#31215;&#32534;&#30721;&#21644;&#31232;&#30095;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#23545;&#19977;&#32500;&#24418;&#29366;&#30340;&#39640;&#31934;&#24230;&#37325;&#24314;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#32454;&#33410;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.01677</link><description>&lt;p&gt;
HIVE: &#23618;&#27425;&#21270;&#20307;&#31215;&#32534;&#30721;&#20687;&#32032;&#38544;&#24335;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
HIVE: HIerarchical Volume Encoding for Neural Implicit Surface Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01677
&lt;/p&gt;
&lt;p&gt;
HIVE&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#20307;&#31215;&#32534;&#30721;&#21644;&#31232;&#30095;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#23545;&#19977;&#32500;&#24418;&#29366;&#30340;&#39640;&#31934;&#24230;&#37325;&#24314;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#32454;&#33410;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01677v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#31070;&#32463;&#38544;&#24335;&#34920;&#38754;&#37325;&#24314;&#20174;&#22270;&#20687;&#20013;&#37325;&#24314;&#19977;&#32500;&#24418;&#29366;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#26041;&#27861;&#20013;&#65292;3D&#22330;&#26223;&#20165;&#30001;MLPs&#32534;&#30721;&#65292;&#36825;&#20123;MLPs&#27809;&#26377;&#26126;&#30830;&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#31034;3D&#24418;&#29366;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20307;&#31215;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#32534;&#30721;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#23618;&#27425;&#21270;&#20307;&#31215;&#65292;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#32534;&#30721;&#22330;&#26223;&#32467;&#26500;&#12290;&#39640;&#20998;&#36776;&#29575;&#20307;&#31215;&#25429;&#33719;&#39640;&#39057;&#20960;&#20309;&#32454;&#33410;&#65292;&#22240;&#20026;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;3D&#28857;&#23398;&#20064;&#31354;&#38388;&#21464;&#21270;&#29305;&#24449;&#65292;&#32780;&#20302;&#20998;&#36776;&#29575;&#20307;&#31215;&#20445;&#35777;&#20102;&#24418;&#29366;&#30340;&#24179;&#28369;&#24615;&#65292;&#22240;&#20026;&#30456;&#37051;&#20301;&#32622;&#20855;&#26377;&#30456;&#21516;&#30340;&#20302;&#20998;&#36776;&#29575;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#31232;&#30095;&#32467;&#26500;&#26469;&#20943;&#23569;&#22312;&#39640;&#20998;&#36776;&#29575;&#20307;&#31215;&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#65292;&#20197;&#21450;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#26469;&#22686;&#24378;&#32467;&#26524;&#30340;&#24179;&#28369;&#24615;&#12290;&#36825;&#31181;&#23618;&#27425;&#21270;&#20307;&#31215;&#32534;&#30721;&#21487;&#20197;&#38468;&#21152;&#21040;&#20219;&#20309;&#19977;&#32500;&#37325;&#24314;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#30340;&#19977;&#32500;&#24418;&#29366;&#30340;&#31934;&#30830;&#24230;&#21644;&#32454;&#33410;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01677v1 Announce Type: new  Abstract: Neural implicit surface reconstruction has become a new trend in reconstructing a detailed 3D shape from images. In previous methods, however, the 3D scene is only encoded by the MLPs which do not have an explicit 3D structure. To better represent 3D shapes, we introduce a volume encoding to explicitly encode the spatial information. We further design hierarchical volumes to encode the scene structures in multiple scales. The high-resolution volumes capture the high-frequency geometry details since spatially varying features could be learned from different 3D points, while the low-resolution volumes enforce the spatial consistency to keep the shape smooth since adjacent locations possess the same low-resolution feature. In addition, we adopt a sparse structure to reduce the memory consumption at high-resolution volumes, and two regularization terms to enhance results smoothness. This hierarchical volume encoding could be appended to any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MkfaNet &#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#30340;&#29305;&#24449;&#65292;&#25552;&#21319;&#20102;&#28145;&#24230;&#20551;&#36896;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01668</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22810;&#24773;&#22659;&#19982;&#39057;&#29575;&#32858;&#21512;&#32593;&#32476;&#23545;&#20110; deepfake &#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiple Contexts and Frequencies Aggregation Network forDeepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MkfaNet &#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#30340;&#29305;&#24449;&#65292;&#25552;&#21319;&#20102;&#28145;&#24230;&#20551;&#36896;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01668v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#25104;&#38271;&#65292;&#28145;&#24230;&#36896;&#20551;&#26816;&#27979;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#28145;&#24230;&#36896;&#20551;&#25216;&#26415;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20381;&#36182;&#20110;&#36890;&#36807;&#31354;&#38388;&#25110;&#39057;&#29575;&#22495;&#30340;&#25163;&#21160;&#29305;&#24449;&#32780;&#19981;&#26159;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#24314;&#27169;&#19968;&#33324;&#36896;&#20551;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#20102;&#31354;&#38388;&#21644;&#39057;&#29575;&#26816;&#27979;&#22120;&#30340;&#20004;&#20010;&#30452;&#35266;&#20808;&#39564;&#65292;&#21363;&#23398;&#20064;&#33021;&#22815;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#24314;&#27169;&#30495;&#23454;&#21644;&#20551;&#26679;&#26412;&#20043;&#38388;&#32454;&#24494;&#38754;&#37096;&#24046;&#24322;&#30340;&#31283;&#20581;&#31354;&#38388;&#23646;&#24615;&#21644;&#39057;&#29575;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38754;&#37096;&#36896;&#20551;&#26816;&#27979;&#32593;&#32476; MkfaNet&#65292;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#12290;&#23545;&#20110;&#31354;&#38388;&#24773;&#22659;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22810;&#20869;&#26680;&#32858;&#21512;&#22120;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#22810;&#31181;&#21367;&#31215;&#25552;&#21462;&#30340;&#32452;&#32455;&#29305;&#24449;&#26469;&#36866;&#24212;&#24615;&#22320;&#36873;&#25321;&#24314;&#27169;&#30495;&#23454;&#21644;&#20551;&#20154;&#33080;&#20043;&#38388;&#30340;&#32454;&#24494;&#38754;&#37096;&#24046;&#24322;&#12290;&#23545;&#20110;&#39057;&#29575;&#25104;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39057;&#29575;&#32858;&#21512;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#39057;&#29575;&#22495;&#30340;&#24402;&#19968;&#21270;&#21644;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#28145;&#24230;&#36896;&#20551;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#22810;&#26680;&#32441;&#29702;&#20998;&#26512;&#12289;&#22810;&#39057;&#29575;&#29305;&#24449;&#32858;&#21512;&#20197;&#21450;&#31471;&#23545;&#31471;&#35757;&#32451;&#25552;&#21319;&#20102;&#28145;&#24230;&#20551;&#36896;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01668v1 Announce Type: new  Abstract: Deepfake detection faces increasing challenges since the fast growth of generative models in developing massive and diverse Deepfake technologies. Recent advances rely on introducing heuristic features from spatial or frequency domains rather than modeling general forgery features within backbones. To address this issue, we turn to the backbone design with two intuitive priors from spatial and frequency detectors, \textit{i.e.,} learning robust spatial attributes and frequency distributions that are discriminative for real and fake samples. To this end, we propose an efficient network for face forgery detection named MkfaNet, which consists of two core modules. For spatial contexts, we design a Multi-Kernel Aggregator that adaptively selects organ features extracted by multiple convolutions for modeling subtle facial differences between real and fake faces. For the frequency components, we propose a Multi-Frequency Aggregator to process 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#22312;3D&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#35821;&#20041;&#23646;&#24615;&#32534;&#36753;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01664</link><description>&lt;p&gt;
SAT3D:&#22522;&#20110;&#22270;&#20687;&#30340;3D&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
SAT3D: Image-driven Semantic Attribute Transfer in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#22312;3D&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#35821;&#20041;&#23646;&#24615;&#32534;&#36753;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01664v1 Announce Type: &#26032;&#30340;&#25688;&#35201;: &#22522;&#20110;GAN&#30340;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#26088;&#22312;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#22270;&#20687;&#23646;&#24615;&#36827;&#34892;&#25805;&#20316;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;2D&#21644;3D&#24863;&#30693;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#38590;&#20197;&#20998;&#36776;&#30340;&#35821;&#20041;&#25110;&#21306;&#22495;&#32534;&#36753;&#23646;&#24615;&#65292;&#36825;&#26080;&#27861;&#23454;&#29616;&#25668;&#24433;&#39118;&#26684;&#30340;&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#65292;&#22914;&#20174;&#19968;&#20010;&#30007;&#20154;&#30340;&#29031;&#29255;&#20013;&#36716;&#31227;&#32993;&#39035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#26041;&#27861;(SAT3D)&#65292;&#36890;&#36807;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#32534;&#36753;&#35821;&#20041;&#23646;&#24615;&#12290;&#23545;&#20110;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#26159;&#22312;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;3D&#24863;&#30693;StyleGAN&#22522;generator&#30340;&#26679;&#24335;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#36890;&#36807;&#23398;&#20064;&#19982;&#26679;&#24335;&#20195;&#30721;&#36890;&#36947;&#30456;&#20851;&#30340;&#35821;&#20041;&#23646;&#24615;&#21644;&#26679;&#24335;&#30721;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#25351;&#23548;&#65292;&#25105;&#20204;&#19982;&#19968;&#32452;&#22522;&#20110;&#30701;&#35821;&#30340;&#25551;&#36848;&#31526;&#32452;&#20851;&#32852;&#27599;&#20010;&#23646;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#27979;&#37327;&#27169;&#22359;(QMM)&#65292;&#20197;&#22522;&#20110;&#25551;&#36848;&#31526;&#32452;&#22312;&#22270;&#20687;&#20013;&#23450;&#37327;&#25551;&#36848;&#23646;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01664v1 Announce Type: new  Abstract: GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor group
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28145;&#24230;&#34917;&#19969;&#35270;&#35273;SLAM&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#21333;GPU&#19978;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21333;&#30446;&#35270;&#35273;SLAM&#65292;&#21363;&#20351;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#20445;&#25345;&#25509;&#36817;&#30495;&#23454;&#26102;&#38388;&#30340;&#24103;&#29575;&#65292;&#21516;&#26102;&#20869;&#23384;&#28040;&#32791;&#20165;&#20026;&#29616;&#26377;&#31995;&#32479;&#30340;&#24456;&#23567;&#19968;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2408.01654</link><description>&lt;p&gt;
&#28145;&#24230;&#34917;&#19969;&#35270;&#35273;SLAM
&lt;/p&gt;
&lt;p&gt;
Deep Patch Visual SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01654
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28145;&#24230;&#34917;&#19969;&#35270;&#35273;SLAM&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#21333;GPU&#19978;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21333;&#30446;&#35270;&#35273;SLAM&#65292;&#21363;&#20351;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#20445;&#25345;&#25509;&#36817;&#30495;&#23454;&#26102;&#38388;&#30340;&#24103;&#29575;&#65292;&#21516;&#26102;&#20869;&#23384;&#28040;&#32791;&#20165;&#20026;&#29616;&#26377;&#31995;&#32479;&#30340;&#24456;&#23567;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01654v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#21457;&#34920;  &#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;SLAM&#39046;&#22495;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#39592;&#24178;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#20934;&#30830;&#24615;&#20248;&#24322;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36816;&#34892;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#27874;&#21160;&#36739;&#22823;&#65292;&#32780;&#21069;&#31471;&#30340;&#31454;&#20105;&#20063;&#20351;&#24471;GPU&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#34917;&#19969;&#35270;&#35273;&#65288;DPV&#65289;SLAM&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21333;GPU&#19978;&#30340;&#21333;&#30446;&#35270;&#35273;SLAM&#25216;&#26415;&#12290;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;SLAM&#31995;&#32479;&#30456;&#27604;&#65292;DPV-SLAM&#22312;&#20445;&#25345;&#39640;&#26368;&#20302;&#24103;&#29575;&#30340;&#21516;&#20107;&#65292;&#25317;&#26377;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#65288;5-7G&#65289;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#65292;DPV-SLAM&#30340;&#24103;&#29575;&#21487;&#20197;&#36798;&#21040;1x&#21040;4x&#30340;&#23454;&#38469;&#24103;&#29575;&#12290;&#25105;&#20204;&#22312;EuRoC&#21644;TartanAir&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;DROID-SLAM&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#20351;&#29992;&#30340;&#26159;&#20854;&#20869;&#23384;&#28040;&#32791;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#26159;&#20854;2.5&#20493;&#12290;DPV-SLAM&#26159;DPVO&#35270;&#35273;&#36816;&#21160;&#31995;&#32479;&#30340;&#19968;&#20010;&#25193;&#23637;&#65307;&#23427;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#21516;&#19968;&#20179;&#24211;&#20013;&#25214;&#21040;&#65306;https://github.com/princeton-vl/DPVO
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01654v1 Announce Type: new  Abstract: Recent work in visual SLAM has shown the effectiveness of using deep network backbones. Despite excellent accuracy, however, such approaches are often expensive to run or do not generalize well zero-shot. Their runtime can also fluctuate wildly while their frontend and backend fight for access to GPU resources. To address these problems, we introduce Deep Patch Visual (DPV) SLAM, a method for monocular visual SLAM on a single GPU. DPV-SLAM maintains a high minimum framerate and small memory overhead (5-7G) compared to existing deep SLAM systems. On real-world datasets, DPV-SLAM runs at 1x-4x real-time framerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir while running 2.5x faster using a fraction of the memory. DPV-SLAM is an extension to the DPVO visual odometry system; its code can be found in the same repository: https://github.com/princeton-vl/DPVO
&lt;/p&gt;</description></item><item><title>MCPDepth&#26159;&#19968;&#31181;&#20351;&#29992;&#22278;&#26609;&#20840;&#26223;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;&#26469;&#20272;&#35745;&#20840;&#26223;&#28145;&#24230;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#22312;&#19981;&#21516;&#35270;&#22270;&#30340;&#28145;&#24230;&#22270;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#25216;&#26415;&#29305;&#21035;&#35774;&#35745;&#20026;&#31616;&#21270;&#23545;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#37096;&#32626;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01653</link><description>&lt;p&gt;
&#22810;&#22278;&#26609;&#20840;&#26223;&#28145;&#24230;&#20272;&#35745;&#65306;&#36890;&#36807;&#22810;&#22278;&#26609;&#20840;&#26223;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MCPDepth: Omnidirectional Depth Estimation via Stereo Matching from Multi-Cylindrical Panoramas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01653
&lt;/p&gt;
&lt;p&gt;
MCPDepth&#26159;&#19968;&#31181;&#20351;&#29992;&#22278;&#26609;&#20840;&#26223;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;&#26469;&#20272;&#35745;&#20840;&#26223;&#28145;&#24230;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#22312;&#19981;&#21516;&#35270;&#22270;&#30340;&#28145;&#24230;&#22270;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#25216;&#26415;&#29305;&#21035;&#35774;&#35745;&#20026;&#31616;&#21270;&#23545;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#37096;&#32626;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Multi-Cylindrical Panoramic Depth Estimation&#65288;MCPDepth&#65289;&#30340;&#20108;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20010;&#22278;&#26609;&#20840;&#26223;&#30340;&#31435;&#20307;&#21305;&#37197;&#36827;&#34892;&#20840;&#26223;&#28145;&#24230;&#20272;&#35745;&#12290;MCPDepth&#21033;&#29992;&#22810;&#20010;&#22278;&#26609;&#20840;&#26223;&#36827;&#34892;&#21021;&#22987;&#30340;&#31435;&#20307;&#21305;&#37197;&#65292;&#28982;&#21518;&#23558;&#28145;&#24230;&#22270;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#34701;&#21512;&#12290;MCPDepth&#20351;&#29992;&#20102;&#19968;&#31181;&#29615;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#20811;&#26381;&#22402;&#30452;&#36724;&#19978;&#30340;&#22833;&#30495;&#38382;&#39064;&#12290;MCPDepth&#20165;&#20351;&#29992;&#26631;&#20934;&#32593;&#32476;&#32452;&#20214;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#37096;&#32626;&#21040;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#65292;&#24182;&#19988;&#22312;&#20840;&#29699;&#39046;&#20808;&#30340;&#26041;&#27861;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#29305;&#27530;&#30340;&#31639;&#23376;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#20004;&#20010;&#26041;&#38754;&#23545;&#27604;&#20102;&#31435;&#20307;&#21305;&#37197;&#20013;&#30340;&#29699;&#20307;&#25237;&#24433;&#21644;&#22278;&#26609;&#25237;&#24433;&#65292;&#24378;&#35843;&#20102;&#22278;&#26609;&#25237;&#24433;&#30340;&#20248;&#28857;&#12290;MCPDepth&#22312;Deep360&#23460;&#22806;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;3D60&#23460;&#20869;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;18.8%&#21644;19.9%&#30340;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01653v1 Announce Type: new  Abstract: We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a two-stage framework for omnidirectional depth estimation via stereo matching between multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for initial stereo matching and then fuses the resulting depth maps across views. A circular attention module is employed to overcome the distortion along the vertical axis. MCPDepth exclusively utilizes standard network components, simplifying deployment to embedded devices and outperforming previous methods that require custom kernels. We theoretically and experimentally compare spherical and cylindrical projections for stereo matching, highlighting the advantages of the cylindrical projection. MCPDepth achieves state-of-the-art performance with an 18.8% reduction in mean absolute error (MAE) for depth on the outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor real-scene dataset 3D60.
&lt;/p&gt;</description></item><item><title>&#26368;&#26032;&#19968;&#20195;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;Segmen Anything Model 2&#22312;&#38646;&#26679;&#26412;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#25163;&#26415;&#35270;&#39057;&#20013;&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25163;&#26415;&#31867;&#22411;&#21644;&#24037;&#20855;&#38271;&#24230;&#30340;&#22797;&#26434;&#32972;&#26223;&#65292;&#24182;&#19988;&#24615;&#33021;&#24471;&#21040;&#20102;&#33391;&#22909;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.01648</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25163;&#26415;&#24037;&#20855;&#22312;&#21333;&#30446;&#35270;&#39057;&#20013;&#20351;&#29992;Segmen Anything Model 2&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Surgical Tool Segmentation in Monocular Video Using Segment Anything Model 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01648
&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#19968;&#20195;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;Segmen Anything Model 2&#22312;&#38646;&#26679;&#26412;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#25163;&#26415;&#35270;&#39057;&#20013;&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25163;&#26415;&#31867;&#22411;&#21644;&#24037;&#20855;&#38271;&#24230;&#30340;&#22797;&#26434;&#32972;&#26223;&#65292;&#24182;&#19988;&#24615;&#33021;&#24471;&#21040;&#20102;&#33391;&#22909;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01648v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: Segment Anything Model 2 (SAM 2)&#26159;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#21106;&#30340;&#26368;&#26032;&#19968;&#20195;&#22522;&#30784;&#27169;&#22411;&#12290;&#22312;&#24222;&#22823;&#30340;Segment Anything Video&#65288;SA-V&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;50.9K&#35270;&#39057;&#20013;&#30340;3550&#19975;&#20010;&#25513;&#30721;&#65292;SAM 2&#36890;&#36807;&#25903;&#25345;&#21508;&#31181;&#25552;&#31034;&#65288;&#20363;&#22914;&#28857;&#12289;&#26694;&#21644;&#25513;&#30721;&#65289;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#25512;&#36827;&#20102;&#20854;&#21069;&#36523;&#30340;&#24615;&#33021;&#12290;&#20854;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#20869;&#23384;&#20351;&#29992;&#20351;&#20854;&#22312;&#25163;&#26415;&#24037;&#20855;&#35270;&#39057;&#20998;&#21106;&#26041;&#38754;&#29305;&#21035;&#21560;&#24341;&#20154;&#65292;&#23588;&#20854;&#26159;&#22312;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#25163;&#26415;&#31243;&#24207;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;SAM 2&#27169;&#22411;&#22312;&#22810;&#31181;&#25163;&#26415;&#31867;&#22411;&#65288;&#21253;&#25324;&#20869;&#31397;&#26415;&#21644;&#26174;&#24494;&#26415;&#65289;&#20013;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#20998;&#21106;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#23545;&#20854;&#22312;&#21253;&#21547;&#21333;&#24037;&#20855;&#21644;&#22810;&#24037;&#20855;&#65288;&#38271;&#24230;&#19981;&#21516;&#65289;&#30340;&#35270;&#39057;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;SAM 2&#22312;&#25163;&#26415;&#39046;&#22495;&#24212;&#29992;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SAM 2&#22312;&#38646;&#26679;&#26412;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#26041;&#38754;&#30340;&#24615;&#33021;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#22797;&#26434;&#32972;&#26223;&#12290;&#22312;&#22810;&#31181;&#25163;&#26415;&#21644;&#24037;&#20855;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;SAM 2&#30340;&#24615;&#33021;&#22343;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01648v1 Announce Type: cross  Abstract: The Segment Anything Model 2 (SAM 2) is the latest generation foundation model for image and video segmentation. Trained on the expansive Segment Anything Video (SA-V) dataset, which comprises 35.5 million masks across 50.9K videos, SAM 2 advances its predecessor's capabilities by supporting zero-shot segmentation through various prompts (e.g., points, boxes, and masks). Its robust zero-shot performance and efficient memory usage make SAM 2 particularly appealing for surgical tool segmentation in videos, especially given the scarcity of labeled data and the diversity of surgical procedures. In this study, we evaluate the zero-shot video segmentation performance of the SAM 2 model across different types of surgeries, including endoscopy and microscopy. We also assess its performance on videos featuring single and multiple tools of varying lengths to demonstrate SAM 2's applicability and effectiveness in the surgical domain. We found tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28040;&#36153;&#22411;&#36710;&#36742;&#19978;&#30340;GNSS&#25968;&#25454;&#21644;&#25668;&#20687;&#22836;&#22270;&#20687;&#36827;&#34892;&#36947;&#36335;&#22270;&#33258;&#21160;&#21019;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#21019;&#24314;&#36947;&#36335;&#22270;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.01640</link><description>&lt;p&gt;
&#21033;&#29992;&#28040;&#36153;&#22411;&#36710;&#36742;&#19978;GNSS&#21644;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#36947;&#36335;&#32593;&#32476;&#20272;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28040;&#36153;&#22411;&#36710;&#36742;&#19978;&#30340;GNSS&#25968;&#25454;&#21644;&#25668;&#20687;&#22836;&#22270;&#20687;&#36827;&#34892;&#36947;&#36335;&#22270;&#33258;&#21160;&#21019;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#21019;&#24314;&#36947;&#36335;&#22270;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01640v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#22320;&#22270;&#23545;&#20110;&#35832;&#22914;&#36710;&#36742;&#23548;&#33322;&#21644;&#33258;&#21160;&#39550;&#39542;&#26426;&#22120;&#20154;&#31561;&#20247;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#24212;&#29992;&#37117;&#38656;&#35201;&#26377;&#25928;&#30340;&#36335;&#32447;&#35268;&#21010;&#21644;&#23450;&#20301;&#31354;&#38388;&#27169;&#22411;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26500;&#24314;&#36947;&#36335;&#22270;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#27493;&#65292;&#20294;&#21019;&#24314;&#19968;&#20010;&#20934;&#30830;&#30340;&#36947;&#36335;&#22270;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#23454;&#29616;&#23436;&#20840;&#30340;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#36712;&#36857;&#21644;&#20174;&#29616;&#20195;&#36710;&#36742;&#19978;&#26631;&#20934;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#22522;&#26412;&#22270;&#20687;&#25968;&#25454;&#33258;&#21160;&#19988;&#20934;&#30830;&#22320;&#29983;&#25104;&#27492;&#31867;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#26694;&#26550;&#20026;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#26469;&#35299;&#20915;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20174;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#21435;&#22122;&#25968;&#25454;&#20197;&#20943;&#23569;&#19981;&#30456;&#20851;&#30340;&#36793;&#32536;&#21644;&#22122;&#28857;&#12290;&#22312;&#36710;&#36742;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#23545;&#27604;&#65292;&#35828;&#26126;&#20102;&#20351;&#29992;&#24314;&#35758;&#30340;&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#25968;&#25454;&#25910;&#38598;&#21644;&#33258;&#21160;&#21270;&#36947;&#36335;&#22270;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#21162;&#21147;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01640v1 Announce Type: cross  Abstract: Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today's advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data's time 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;JambaTalk&#30340;3D&#35828;&#35805;&#22836;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#21160;&#20316;&#22810;&#26679;&#24615;&#23454;&#29616;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#24182;&#22312;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2408.01627</link><description>&lt;p&gt;
JambaTalk:&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35828;&#35805;&#22836;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;JambaTalk&#30340;3D&#35828;&#35805;&#22836;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#21160;&#20316;&#22810;&#26679;&#24615;&#23454;&#29616;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#24182;&#22312;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01627v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032; Abstract: &#36817;&#24180;&#26469;&#65292;&#35828;&#35805;&#22836;&#20687;&#29983;&#25104;&#24050;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#20154;&#20204;&#25237;&#20837;&#20102;&#22823;&#37327;&#31934;&#21147;&#26469;&#25913;&#36827;&#21767;&#21516;&#27493;&#36816;&#21160;&#12289;&#25429;&#25417;&#38754;&#37096;&#34920;&#24773;&#12289;&#29983;&#25104;&#33258;&#28982;&#22836;&#37096;&#23039;&#24577;&#65292;&#20197;&#21450;&#23454;&#29616;&#39640;&#28165;&#35270;&#39057;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#33021;&#22815;&#22312;&#25152;&#26377;&#36825;&#20123;&#25351;&#26631;&#19978;&#23454;&#29616;&#22343;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#20351;&#29992;Jamba&#65292;&#19968;&#31181;&#28151;&#21512;Transformer-Mamba&#27169;&#22411;&#30340;3D&#33080;&#37096;&#21160;&#30011;&#12290;Mamba&#65292;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26550;&#26500;&#65292;&#34987;&#35774;&#35745;&#29992;&#26469;&#35299;&#20915;&#20256;&#32479;Transformer&#26550;&#26500;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#26377;&#19968;&#20123;&#32570;&#28857;&#12290;Jamba&#34701;&#21512;&#20102;Transformer&#21644;Mamba&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;Jamba&#30340;&#22522;&#26412;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JambaTalk&#65292;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#25972;&#21512;&#22686;&#24378;&#21160;&#20316;&#30340;&#22810;&#26679;&#24615;&#21644;&#36895;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01627v1 Announce Type: new  Abstract: In recent years, talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high video quality. However, no single model has yet achieved equivalence across all these metrics. This paper aims to animate a 3D face using Jamba, a hybrid Transformers-Mamba model. Mamba, a pioneering Structured State Space Model (SSM) architecture, was designed to address the constraints of the conventional Transformer architecture. Nevertheless, it has several drawbacks. Jamba merges the advantages of both Transformer and Mamba approaches, providing a holistic solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and speed through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
&lt;/p&gt;</description></item><item><title>MedUHIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#19982;&#20154;&#31867;&#21442;&#19982;&#24490;&#29615;&#30340;&#21307;&#30103;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#21307;&#30103;&#22270;&#20687;&#20013;&#23454;&#29616;&#23450;&#37327;&#20998;&#21106;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#36890;&#36807;&#19982;&#21307;&#29983;&#30340;&#20114;&#21160;&#19981;&#26029;&#25913;&#36827;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01620</link><description>&lt;p&gt;
MedUHIP: &#36808;&#21521;&#20154;&#31867;&#21442;&#19982;&#30340;&#21307;&#30103;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MedUHIP: Towards Human-In-the-Loop Medical Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01620
&lt;/p&gt;
&lt;p&gt;
MedUHIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#19982;&#20154;&#31867;&#21442;&#19982;&#24490;&#29615;&#30340;&#21307;&#30103;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#21307;&#30103;&#22270;&#20687;&#20013;&#23454;&#29616;&#23450;&#37327;&#20998;&#21106;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#36890;&#36807;&#19982;&#21307;&#29983;&#30340;&#20114;&#21160;&#19981;&#26029;&#25913;&#36827;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#24050;&#32463;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#12290;&#30001;&#20110;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#23384;&#22312;&#65292;&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#23588;&#20854;&#22797;&#26434;&#12290;&#20363;&#22914;&#65292;&#32452;&#32455;&#36793;&#30028;&#30340;&#19981;&#26126;&#30830;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#21307;&#29983;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#30340;&#20294;&#21512;&#29702;&#30340;&#26631;&#27880;&#12290;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#22312;&#20020;&#24202;&#35299;&#37322;&#19978;&#36896;&#25104;&#20102;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#24433;&#21709;&#20102;&#38543;&#21518;&#30340;&#21307;&#30103;&#24178;&#39044;&#12290;&#22240;&#27492;&#65292;&#20174;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#21307;&#30103;&#22270;&#20687;&#20013;&#23454;&#29616;&#23450;&#37327;&#20998;&#21106;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#8220;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#8221;&#21644;&#8220;&#20154;&#31867;&#21442;&#19982;&#30340;&#24490;&#29615;&#8221;&#20114;&#21160;&#12290;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#20998;&#21106;&#20197;&#24212;&#23545;&#21307;&#30103;&#22270;&#20687;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#20154;&#31867;&#21442;&#19982;&#30340;&#24490;&#29615;&#20114;&#21160;&#21017;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#19981;&#26029;&#23545;&#20998;&#21106;&#32467;&#26524;&#36827;&#34892;&#36845;&#20195;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01620v1 Announce Type: cross  Abstract: Although segmenting natural images has shown impressive performance, these techniques cannot be directly applied to medical image segmentation. Medical image segmentation is particularly complicated by inherent uncertainties. For instance, the ambiguous boundaries of tissues can lead to diverse but plausible annotations from different clinicians. These uncertainties cause significant discrepancies in clinical interpretations and impact subsequent medical interventions. Therefore, achieving quantitative segmentations from uncertain medical images becomes crucial in clinical practice. To address this, we propose a novel approach that integrates an \textbf{uncertainty-aware model} with \textbf{human-in-the-loop interaction}. The uncertainty-aware model proposes several plausible segmentations to address the uncertainties inherent in medical images, while the human-in-the-loop interaction iteratively modifies the segmentation under clinici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#30340;OBIA&#20219;&#21153;&#12289;&#38754;&#20020;&#30340;&#25361;&#25112;&#20197;&#21450;&#24212;&#23545;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2408.01607</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;OBIA&#30340;&#32467;&#21512;&#65306;&#20219;&#21153;&#12289;&#25361;&#25112;&#12289;&#31574;&#30053;&#21644;&#35270;&#37326;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Meets OBIA: Tasks, Challenges, Strategies, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#30340;OBIA&#20219;&#21153;&#12289;&#38754;&#20020;&#30340;&#25361;&#25112;&#20197;&#21450;&#24212;&#23545;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01607v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#20687;&#32032;&#32423;&#25110;&#22359;&#32423;&#24212;&#29992;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#23581;&#35797;&#23558;&#28145;&#24230;&#23398;&#20064;&#34701;&#20837;&#21040;&#23545;&#35937;&#20026;&#22522;&#30784;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#20013;&#65292;&#20294;&#23427;&#30340;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#24320;&#21457;&#12290;&#26412;&#25991;&#23545;&#36965;&#24863;&#39046;&#22495;&#20013;OBIA&#30340;&#20351;&#29992;&#26085;&#30410;&#24191;&#27867;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#21644;&#25299;&#23637;&#65292;&#21253;&#25324;&#20854;&#20219;&#21153;&#23376;&#39046;&#22495;&#65292;&#26080;&#35770;&#26159;&#21542;&#25972;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24050;&#35782;&#21035;&#21644;&#24635;&#32467;&#20102;&#20116;&#31181;&#27969;&#34892;&#30340;&#31574;&#30053;&#26469;&#24212;&#23545;&#22312;OBIA&#20013;&#30452;&#25509;&#22788;&#29702;&#23545;&#35937;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#30340;&#22238;&#39038;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#20540;&#24471;&#27880;&#24847;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#28608;&#21457;&#22312;&#36825;&#20010;&#26377;&#36259;&#30340;&#20294;&#34987;&#24573;&#35270;&#30340;&#39046;&#22495;&#20013;&#26356;&#22810;&#30340;&#25506;&#32034;&#65292;&#24182;&#20419;&#36827;&#28145;&#24230;&#23398;&#20064;&#22312;OBIA&#22788;&#29702;&#27969;&#31243;&#20013;&#26356;&#21152;&#24191;&#27867;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01607v1 Announce Type: new  Abstract: Deep learning has gained significant attention in remote sensing, especially in pixel- or patch-level applications. Despite initial attempts to integrate deep learning into object-based image analysis (OBIA), its full potential remains largely unexplored. In this article, as OBIA usage becomes more widespread, we conducted a comprehensive review and expansion of its task subdomains, with or without the integration of deep learning. Furthermore, we have identified and summarized five prevailing strategies to address the challenge of deep learning's limitations in directly processing unstructured object data within OBIA, and this review also recommends some important future research directions. Our goal with these endeavors is to inspire more exploration in this fascinating yet overlooked area and facilitate the integration of deep learning into OBIA processing workflows.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25668;&#20687;&#26426;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25668;&#20687;&#26426;&#20869;&#37096;&#32467;&#26500;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.01565</link><description>&lt;p&gt;
&#22522;&#20110;&#25668;&#20687;&#26426;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Depth Estimation Based on Camera Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25668;&#20687;&#26426;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25668;&#20687;&#26426;&#20869;&#37096;&#32467;&#26500;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01565v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; Abstract: &#23545;&#20110;&#26426;&#22120;&#20154;&#23398;&#21644;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#32780;&#35328;&#65292;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#22312;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#20013;&#65292;&#30456;&#36739;&#20110;&#38656;&#35201;&#26114;&#36149;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#65292;&#30001;&#20110;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#25104;&#26412;&#65292;&#33258;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#28145;&#24230;&#20272;&#35745;&#24615;&#33021;&#19978;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#21333;&#30446;&#26080;&#30417;&#30563;&#28145;&#24230;&#20272;&#35745;&#32780;&#35328;&#65292;&#23610;&#24230;&#38382;&#39064;&#26159;&#21478;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#32416;&#27491;&#28145;&#24230;&#20272;&#35745;&#65292;&#21333;&#30446;&#26080;&#30417;&#30563;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#26469;&#33258;GPS&#12289;LiDAR&#25110;&#20854;&#20182;&#29616;&#26377;&#22320;&#22270;&#30340;&#22320;&#38754;&#30495;&#23454;&#23610;&#24230;&#25968;&#25454;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25506;&#32034;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#35757;&#32451;&#26080;&#30417;&#30563;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#25668;&#20687;&#26426;&#33258;&#36523;&#25552;&#20379;&#30340;&#22522;&#26412;&#20449;&#24687;&#34987;&#26222;&#36941;&#24573;&#30053;&#65292;&#36825;&#21487;&#20197;&#20026;&#28145;&#24230;&#20272;&#35745;&#25552;&#20379;&#20813;&#36153;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35774;&#22791;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#12290;&#21033;&#29992;&#25668;&#20687;&#26426;&#33258;&#36523;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#25668;&#20687;&#26426;&#27169;&#22411;&#30340;&#20449;&#24687;&#26469;&#25913;&#36827;&#28145;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#65292;&#26080;&#39035;&#39069;&#22806;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#22330;&#26223;&#19979;&#25668;&#20687;&#26426;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#23454;&#38469;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#30340;&#23398;&#20064;&#21644;&#25913;&#36827;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01565v1 Announce Type: new  Abstract: Depth estimationn is a critical topic for robotics and vision-related tasks. In monocular depth estimation, in comparison with supervised learning that requires expensive ground truth labeling, self-supervised methods possess great potential due to no labeling cost. However, self-supervised learning still has a large gap with supervised learning in depth estimation performance. Meanwhile, scaling is also a major issue for monocular unsupervised depth estimation, which commonly still needs ground truth scale from GPS, LiDAR, or existing maps to correct. In deep learning era, while existing methods mainly rely on the exploration of image relationships to train the unsupervised neural networks, fundamental information provided by the camera itself has been generally ignored, which can provide extensive supervision information for free, without the need for any extra equipment to provide supervision signals. Utilizing the camera itself's int
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#20840;&#23616;&#32622;&#20449;&#24230;&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#26377;&#25928;&#25552;&#39640;&#20102;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#26512;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#65292;&#20943;&#23569;&#20102;&#20154;&#31867;&#26631;&#27880;&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#21319;&#20102;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01558</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#21512;&#25104;&#25968;&#25454;&#21644;&#20840;&#23616;&#32622;&#20449;&#24230;&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#36895;&#39046;&#22495;&#29305;&#23450;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Accelerating Domain-Aware Electron Microscopy Analysis Using Deep Learning Models with Synthetic Data and Image-Wide Confidence Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01558
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#20840;&#23616;&#32622;&#20449;&#24230;&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#26377;&#25928;&#25552;&#39640;&#20102;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#26512;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#65292;&#20943;&#23569;&#20102;&#20154;&#31867;&#26631;&#27880;&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#21319;&#20102;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01558v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#21457;&#24067; &#25688;&#35201;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#24341;&#20837;&#25913;&#21892;&#20102;&#26174;&#24494;&#25104;&#20687;&#29305;&#24449;&#26816;&#27979;&#30340;&#25928;&#29575;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#21457;&#23637;&#21644;&#36866;&#29992;&#24615;&#21463;&#38480;&#20110;&#23545;&#31232;&#32570;&#19988;&#24120;&#24120;&#26377;&#32570;&#38519;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#20197;&#21450;&#23545;&#39046;&#22495;&#24847;&#35782;&#30340;&#32570;&#20047;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#36798;&#21040;&#20102;&#19982;&#22312;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#20284;&#30340;&#31934;&#30830;&#24230;&#65288;0.86&#65289;&#12289;&#21484;&#22238;&#29575;&#65288;0.63&#65289;&#12289;F1&#20998;&#25968;&#65288;0.71&#65289;&#20197;&#21450;&#24037;&#31243;&#23646;&#24615;&#39044;&#27979;&#65288;R2=0.82&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#39044;&#27979;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#22686;&#24378;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20840;&#23616;&#32622;&#20449;&#24230;&#25351;&#26631;&#65292;&#33021;&#20801;&#35768;&#31616;&#21333;&#30340;&#38408;&#20540;&#35774;&#32622;&#26469;&#28040;&#38500;&#19981;&#26126;&#30830;&#21644;&#19981;&#23646;&#20110;&#39046;&#22495;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#22312;&#19981;&#20002;&#24323;25%&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#36798;5&#33267;30%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#20013;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#39046;&#22495;&#29305;&#23450;&#30005;&#23376;&#26174;&#24494;&#38236;&#30340;&#22270;&#20687;&#20998;&#26512;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01558v1 Announce Type: new  Abstract: The integration of machine learning (ML) models enhances the efficiency, affordability, and reliability of feature detection in microscopy, yet their development and applicability are hindered by the dependency on scarce and often flawed manually labeled datasets and a lack of domain awareness. We addressed these challenges by creating a physics-based synthetic image and data generator, resulting in a machine learning model that achieves comparable precision (0.86), recall (0.63), F1 scores (0.71), and engineering property predictions (R2=0.82) to a model trained on human-labeled data. We enhanced both models by using feature prediction confidence scores to derive an image-wide confidence metric, enabling simple thresholding to eliminate ambiguous and out-of-domain images resulting in performance boosts of 5-30% with a filtering-out rate of 25%. Our study demonstrates that synthetic data can eliminate human reliance in ML and provides a 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#21464;&#24418;&#25216;&#26415;&#33258;&#21160;&#19988;&#31934;&#30830;&#37325;&#24314;&#26415;&#21069;&#33181;&#20851;&#33410;&#26893;&#20837;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#26041;&#20415;&#20102;&#25163;&#26415;&#35268;&#21010;&#21644;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2408.01557</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;&#33181;&#20851;&#33410;&#36816;&#21160;&#23398;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#21464;&#24418;&#31639;&#27861;&#20026;3D&#26893;&#20837;&#27169;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Enhanced Knee Kinematics: Leveraging Deep Learning and Morphing Algorithms for 3D Implant Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01557
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#21464;&#24418;&#25216;&#26415;&#33258;&#21160;&#19988;&#31934;&#30830;&#37325;&#24314;&#26415;&#21069;&#33181;&#20851;&#33410;&#26893;&#20837;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#26041;&#20415;&#20102;&#25163;&#26415;&#35268;&#21010;&#21644;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01557v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#31934;&#30830;&#37325;&#24314;&#26893;&#20837;&#33181;&#27169;&#22411;&#22312;&#39592;&#31185;&#25163;&#26415;&#21644;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#22686;&#24378;&#20102;&#26415;&#21069;&#35268;&#21010;&#65292;&#20248;&#21270;&#20102;&#26893;&#20837;&#35774;&#35745;&#65292;&#24182;&#25913;&#21892;&#20102;&#25163;&#26415;&#32467;&#26524;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#36153;&#26102;&#19988;&#26131;&#20986;&#38169;&#30340;&#25163;&#21160;&#20998;&#21106;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#21644;&#21464;&#24418;&#25216;&#26415;&#26469;&#36827;&#34892;&#31934;&#30830;&#30340;3D&#26893;&#20837;&#33181;&#27169;&#22411;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01557v1 Announce Type: cross  Abstract: Accurate reconstruction of implanted knee models is crucial in orthopedic surgery and biomedical engineering, enhancing preoperative planning, optimizing implant design, and improving surgical outcomes. Traditional methods rely on labor-intensive and error-prone manual segmentation. This study proposes a novel approach using machine learning (ML) algorithms and morphing techniques for precise 3D reconstruction of implanted knee models.   The methodology begins with acquiring preoperative imaging data, such as fluoroscopy or X-ray images of the patient's knee joint. A convolutional neural network (CNN) is then trained to automatically segment the femur contour of the implanted components, significantly reducing manual effort and ensuring high accuracy.   Following segmentation, a morphing algorithm generates a personalized 3D model of the implanted knee joint, using the segmented data and biomechanical principles. This algorithm conside
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUE&#30340;SAR&#22270;&#20687;&#22788;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;GAN&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#26041;&#21521;&#22312;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31163;&#21644;&#26631;&#35760;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01553</link><description>&lt;p&gt;
&#36890;&#36807;GAN&#22522;&#20110;&#30340;&#26080;&#30417;&#30563;&#25805;&#20316;&#30340;&#22810;&#20219;&#21153;SAR&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-task SAR Image Processing via GAN-based Unsupervised Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUE&#30340;SAR&#22270;&#20687;&#22788;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;GAN&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#26041;&#21521;&#22312;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31163;&#21644;&#26631;&#35760;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01553v1 &#20844;&#21578;&#31867;&#22411;: &#26032; Abstract: &#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#22312;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#27169;&#24335;&#21512;&#25104;&#22823;&#37327;&#36924;&#30495;&#30340;SAR&#22270;&#20687;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#19968;&#20123;GAN&#33021;&#22815;&#22312;&#26410;&#24341;&#20837;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22270;&#20687;&#32534;&#36753;&#65292;&#23637;&#31034;&#20102;&#23545;SAR&#22270;&#20687;&#22788;&#29702;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#19982;&#20256;&#32479;SAR&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;GAN latent&#31354;&#38388;&#30340;&#25511;&#21046;&#26159;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#65292;&#20801;&#35768;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAN-based Unsupervised Editing (GUE)&#30340;SAR&#22270;&#20687;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;(1) &#22312;GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20998;&#31163;&#20986;&#35821;&#20041;&#26041;&#21521;&#24182;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#26041;&#21521;&#65307;(2) &#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#30340;SAR&#22270;&#20687;&#22788;&#29702;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#22810;&#20010;&#22270;&#20687;&#22788;&#29702;&#21151;&#33021;&#12290;&#22312;&#23454;&#29616;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01553v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) have shown tremendous potential in synthesizing a large number of realistic SAR images by learning patterns in the data distribution. Some GANs can achieve image editing by introducing latent codes, demonstrating significant promise in SAR image processing. Compared to traditional SAR image processing methods, editing based on GAN latent space control is entirely unsupervised, allowing image processing to be conducted without any labeled data. Additionally, the information extracted from the data is more interpretable. This paper proposes a novel SAR image processing framework called GAN-based Unsupervised Editing (GUE), aiming to address the following two issues: (1) disentangling semantic directions in the GAN latent space and finding meaningful directions; (2) establishing a comprehensive SAR image processing framework while achieving multiple image processing functions. In the implementation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#20010;&#33258;&#35757;&#32451;&#35299;&#30721;&#22120;&#27169;&#22359;&#26088;&#22312;&#25913;&#21892;&#28857;&#20113;&#20998;&#21106;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#20302;&#23494;&#24230;&#28857;&#20113;&#26102;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#24615;&#28857;&#20540;&#36716;&#25442;&#65292;&#39044;&#27979;&#22240;&#25237;&#24433;&#32780;&#20002;&#22833;&#30340;&#28857;&#31867;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01548</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#28857;&#20540;&#35299;&#30721;&#22120;&#27169;&#22359;&#29992;&#20110;&#28857;&#20113;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Trainable Pointwise Decoder Module for Point Cloud Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#20010;&#33258;&#35757;&#32451;&#35299;&#30721;&#22120;&#27169;&#22359;&#26088;&#22312;&#25913;&#21892;&#28857;&#20113;&#20998;&#21106;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#20302;&#23494;&#24230;&#28857;&#20113;&#26102;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#24615;&#28857;&#20540;&#36716;&#25442;&#65292;&#39044;&#27979;&#22240;&#25237;&#24433;&#32780;&#20002;&#22833;&#30340;&#28857;&#31867;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01548v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#28857;&#20113;&#20998;&#21106;&#65288;PCS&#65289;&#26088;&#22312;&#36827;&#34892;&#28857;&#23545;&#28857;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#29702;&#35299;&#20854;&#29615;&#22659;&#12290;&#33539;&#22260;&#22270;&#20687;&#26159;&#22823;&#33539;&#22260;&#23460;&#22806;&#28857;&#20113;&#30340;&#23494;&#38598;&#34920;&#31034;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#21106;&#27169;&#22411;&#36816;&#34892;&#25928;&#29575;&#36890;&#24120;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#28857;&#20113;&#25237;&#23556;&#21040;&#33539;&#22260;&#22270;&#20687;&#19978;&#26102;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20002;&#22833;&#28857;&#22240;&#20026;&#65292;&#22312;&#27599;&#20010;&#22270;&#20687;&#22352;&#26631;&#19978;&#65292;&#23613;&#31649;&#22810;&#20010;&#28857;&#34987;&#25237;&#23556;&#21040;&#21516;&#19968;&#20010;&#20301;&#32622;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#28857;&#34987;&#20445;&#30041;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24456;&#38590;&#20026;&#25481;&#33853;&#30340;&#28857;&#20998;&#37197;&#27491;&#30830;&#30340;&#39044;&#27979;&#65292;&#36825;&#20123;&#28857;&#23646;&#20110;&#19982;&#20445;&#30041;&#28857;&#31867;&#19981;&#21516;&#30340;&#31867;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#25628;&#32034;&#21644;&#26680;&#28857;&#21367;&#31215;&#65288;KPConv&#65289;&#65292;&#26080;&#27861;&#19982;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#25110;&#32773;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#23494;&#24230;&#19981;&#21516;&#30340;&#23460;&#22806;&#28857;&#20113;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#38590;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#33258;&#35757;&#32451;&#35299;&#30721;&#22120;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#23494;&#24230;&#30340;&#28857;&#20113;&#65292;&#24182;&#22312;&#20445;&#30041;&#28857;&#21608;&#22260;&#23398;&#20064;&#33258;&#36866;&#24212;&#28857;&#20540;&#20043;&#38388;&#30340;&#36716;&#25442;&#20197;&#39044;&#27979;&#25481;&#33853;&#30340;&#28857;&#31867;&#21035;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#28857;&#20113;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01548v1 Announce Type: new  Abstract: Point cloud segmentation (PCS) aims to make per-point predictions and enables robots and autonomous driving cars to understand the environment. The range image is a dense representation of a large-scale outdoor point cloud, and segmentation models built upon the image commonly execute efficiently. However, the projection of the point cloud onto the range image inevitably leads to dropping points because, at each image coordinate, only one point is kept despite multiple points being projected onto the same location. More importantly, it is challenging to assign correct predictions to the dropped points that belong to the classes different from the kept point class. Besides, existing post-processing methods, such as K-nearest neighbor (KNN) search and kernel point convolution (KPConv), cannot be trained with the models in an end-to-end manner or cannot process varying-density outdoor point clouds well, thereby enabling the models to achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;25&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#23545;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#21333;&#20010;&#38450;&#24481;&#25514;&#26045;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2408.01541</link><description>&lt;p&gt;
&#23432;&#21355;&#22270;&#20687;&#36136;&#37327;&#65306;&#23545;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#38450;&#24481;&#25514;&#26045;&#30340;&#23545;&#25239;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;25&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#23545;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#21333;&#20010;&#38450;&#24481;&#25514;&#26045;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;(IQA)&#39046;&#22495;&#65292;&#25351;&#26631;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;IQA&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;25&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;&#25239;&#32431;&#20928;&#24230;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#35748;&#35777; robustness &#26041;&#27861;&#12290;&#22312;&#38750;&#36866;&#24212;&#24615;&#21644;&#36866;&#24212;&#24615;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;14&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38450;&#24481;&#25514;&#26045;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#23427;&#20204;&#22312;IQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#24212;&#35813;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#24471;&#20998;&#21644;&#22270;&#20687;&#36136;&#37327;&#12290;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#26088;&#22312;&#25351;&#23548;&#26410;&#26469;&#30340;&#21457;&#23637;&#65292;&#25509;&#21463;&#26032;&#26041;&#27861;&#30340;&#25552;&#20132;&#65292;&#26368;&#26032;&#30340;&#32467;&#26524;&#21487;&#36890;&#36807;&#22312;&#32447;:https://videoprocessing.ai/benchmarks/iqa-defenses.html&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01541v1 Announce Type: new  Abstract: In the field of Image Quality Assessment (IQA), the adversarial robustness of the metrics poses a critical concern. This paper presents a comprehensive benchmarking study of various defense mechanisms in response to the rise in adversarial attacks on IQA. We systematically evaluate 25 defense strategies, including adversarial purification, adversarial training, and certified robustness methods. We applied 14 adversarial attack algorithms of various types in both non-adaptive and adaptive settings and tested these defenses against them. We analyze the differences between defenses and their applicability to IQA tasks, considering that they should preserve IQA scores and image quality. The proposed benchmark aims to guide future developments and accepts submissions of new methods, with the latest results available online: https://videoprocessing.ai/benchmarks/iqa-defenses.html.
&lt;/p&gt;</description></item><item><title>SceneMotion&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#23558;&#23616;&#37096;agent-centric&#23884;&#20837;&#36716;&#25442;&#20026;&#20840;&#26223;&#32423;&#39044;&#27979;&#65292;&#22312;Waymo Open Interaction Prediction Challenge&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2408.01537</link><description>&lt;p&gt;
SceneMotion: &#20174;Agent-Centric&#23884;&#20837;&#21040;&#22330;&#26223;&#32423;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01537
&lt;/p&gt;
&lt;p&gt;
SceneMotion&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#23558;&#23616;&#37096;agent-centric&#23884;&#20837;&#36716;&#25442;&#20026;&#20840;&#26223;&#32423;&#39044;&#27979;&#65292;&#22312;Waymo Open Interaction Prediction Challenge&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01537v1 &#20844;&#21578;&#31867;&#22411;: cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01537v1 Announce Type: cross  Abstract: Self-driving vehicles rely on multimodal motion forecasts to effectively interact with their environment and plan safe maneuvers. We introduce SceneMotion, an attention-based model for forecasting scene-wide motion modes of multiple traffic agents. Our model transforms local agent-centric embeddings into scene-wide forecasts using a novel latent context module. This module learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling. The competitive performance in the Waymo Open Interaction Prediction Challenge demonstrates the effectiveness of our approach. Moreover, we cluster future waypoints in time and space to quantify the interaction between agents. We merge all modes and analyze each mode independently to determine which clusters are resolved through interaction or result in conflict. Our implementation is available at: https://github.com/kit-mrt/future-motion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#24182;&#23450;&#20301;&#20266;&#36896;&#22270;&#20687;&#30340;&#19981;&#33258;&#28982;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2408.01532</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#24182;&#23450;&#20301;&#20266;&#36896;&#22270;&#20687;&#30340;&#19981;&#33258;&#28982;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#28145;&#24230;&#20266;&#36896;&#21644;&#21512;&#25104;&#23186;&#20307;&#30340;&#20986;&#29616;&#23545;&#31038;&#20250;&#25919;&#27835;&#30340;&#23436;&#25972;&#24615;&#21644;&#35802;&#20449;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#22522;&#20110;&#22810;&#27169;&#24577;&#25805;&#32437;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#22914;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#65292;&#26356;&#21152;&#36924;&#30495;&#65292;&#23041;&#32961;&#26356;&#22823;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36890;&#24120;&#22522;&#20110;&#36328;&#27169;&#24577;&#25968;&#25454;&#30340;&#24322;&#26500;&#27969;&#34701;&#21512;&#65292;&#22914;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#22312;&#26377;&#25928;&#34701;&#21512;&#21644;&#22240;&#27492;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#21019;&#36896;&#20102;&#20998;&#24067;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#30340;&#26816;&#27979;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#27880;&#24847;&#21147;&#21040;&#22810;&#27169;&#24577;&#22810;&#24207;&#21015;&#34920;&#31034;&#19978;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#36129;&#29486;&#24615;&#29305;&#24449;&#65292;&#29992;&#20110;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#20301;&#32622;&#35782;&#21035;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#23545;&#20266;&#36896;&#22270;&#20687;&#20013;&#30340;&#19981;&#33258;&#28982;&#21306;&#22495;&#36827;&#34892;&#23450;&#20301;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#21046;&#22270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#26631;&#25968;&#25454;&#21644;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22312;&#32447;&#39640;&#28165;&#22320;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2408.01471</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22686;&#24378;&#22312;&#32447;&#36947;&#36335;&#32593;&#32476;&#24863;&#30693;&#19982;&#25512;&#29702;&#30340;&#26631;&#20934;&#23450;&#20041;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01471
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#21046;&#22270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#26631;&#25968;&#25454;&#21644;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22312;&#32447;&#39640;&#28165;&#22320;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#33258;&#21160;&#39550;&#39542;&#23545;&#20110;&#22478;&#24066;&#21644;&#39640;&#36895;&#20844;&#36335;&#39550;&#39542;&#36890;&#24120;&#38656;&#35201;&#39640;&#28165;&#22320;&#22270;&#65288;HD&#65289;&#26469;&#29983;&#25104;&#23548;&#33322;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#22312;&#25104;&#35268;&#27169;&#29983;&#25104;&#21644;&#32500;&#25252;HD&#22320;&#22270;&#26102;&#65292;&#23384;&#22312;&#21508;&#31181;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#32447;&#21046;&#22270;&#26041;&#27861;&#24050;&#32463;&#24320;&#22987;&#20986;&#29616;&#65292;&#20294;&#23427;&#20204;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#24847;&#20041;&#19978;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#20102;&#37325;&#24433;&#36974;&#25377;&#30340;&#38480;&#21046;&#12290;&#22522;&#20110;&#36825;&#20123;&#32771;&#34385;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#22312;&#24320;&#21457;&#22312;&#32447;&#30690;&#37327;&#21270;HD&#22320;&#22270;&#34920;&#31034;&#26102;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#21487;&#25193;&#23637;&#30340;&#20808;&#39564;&#8212;&#8212;&#26631;&#20934;&#23450;&#20041;&#65288;SD&#65289;&#22320;&#22270;&#12290;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#23558;&#36718;&#24275;&#21270;&#30340;SD&#22320;&#22270;&#34920;&#31034;&#38598;&#25104;&#21040;&#21508;&#31181;&#22312;&#32447;&#21046;&#22270;&#26550;&#26500;&#20013;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35782;&#21035;&#36731;&#37327;&#32423;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23558;OpenLane-V2&#25968;&#25454;&#38598;&#19982;OpenStreetMaps&#25193;&#23637;&#65292;&#24182;&#35780;&#20272;&#22270;&#24418;&#21270;SD&#22320;&#22270;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;&#35774;&#35745;SD&#22320;&#22270;&#38598;&#25104;&#32452;&#20214;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;SD&#22320;&#22270;&#32534;&#30721;&#33021;&#22815;&#25552;&#39640;&#22312;&#32447;&#36947;&#36335;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#25928;&#33021;&#12290;&#30001;&#20110;&#30456;&#20851;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#21046;&#22270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#26631;&#25968;&#25454;&#21644;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22312;&#32447;&#39640;&#28165;&#22320;&#22270;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#22312;&#24320;&#25918;&#36947;&#36335;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#20102;&#20854;&#23545;&#20110;&#39640;&#28165;&#22320;&#22270;&#30340;&#23454;&#26102;&#22312;&#32447;&#26356;&#26032;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#36827;&#19968;&#27493;&#39564;&#35777;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#25506;&#32034;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#21040;&#26356;&#24191;&#27867;&#30340;&#36947;&#36335;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01471v1 Announce Type: cross  Abstract: Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definition (SD) maps-in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25351;&#25968;PreIndex&#65292;&#29992;&#26469;&#20272;&#35745;&#22312;&#27169;&#22411;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#30340;&#20877;&#22521;&#35757;&#36807;&#31243;&#20013;&#65292;&#20174;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#20013;&#65292;&#25152;&#28041;&#21450;&#30340;&#29615;&#22659;&#25104;&#26412;&#65288;&#22914;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#28040;&#32791;&#65289;&#12290;</title><link>https://arxiv.org/abs/2408.01446</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#25104;&#26412;&#20272;&#35745;&#27169;&#22411;&#21450;&#20854;&#22312;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Estimating Environmental Cost Throughout Model's Adaptive Life Cycle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25351;&#25968;PreIndex&#65292;&#29992;&#26469;&#20272;&#35745;&#22312;&#27169;&#22411;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#30340;&#20877;&#22521;&#35757;&#36807;&#31243;&#20013;&#65292;&#20174;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#20013;&#65292;&#25152;&#28041;&#21450;&#30340;&#29615;&#22659;&#25104;&#26412;&#65288;&#22914;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#28040;&#32791;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01446v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#22312;&#24403;&#21069;&#26102;&#20195;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#33021;&#28304;&#26469;&#35757;&#32451;&#21644;&#20351;&#29992;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20276;&#38543;&#30528;&#30899;&#25490;&#25918;&#21040;&#29615;&#22659;&#20013;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#24037;&#26234;&#33021;/&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#30340;&#30899;&#36275;&#36857;&#20197;&#21450;&#30456;&#20851;&#30340;&#33021;&#28304;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20877;&#22521;&#35757;&#30340;&#39044;&#27979;&#25351;&#25968;&#65292;PreIndex&#65292;&#20197;&#20272;&#35745;&#19982;&#27169;&#22411;&#20877;&#22521;&#35757;&#30456;&#20851;&#30340;&#29615;&#22659;&#25104;&#26412;&#65292;&#22914;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;&#27169;&#22411;&#20877;&#22521;&#35757;&#26159;&#20026;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25110;&#22312;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#21464;&#21270;/&#24046;&#24322;&#12290;PreIndex&#25351;&#25968;&#21487;&#20197;&#29992;&#26469;&#20272;&#31639;&#20174;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#26032;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#23427;&#20063;&#19982;Model&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20026;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#30340;&#35774;&#35745;&#25552;&#20379;&#25903;&#25345;&#65292;&#20174;&#32780;&#24110;&#21161;&#38477;&#20302;&#20877;&#22521;&#35757;&#30340;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01446v1 Announce Type: cross  Abstract: With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20004;&#27493;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#35889;&#32858;&#31867;&#12289;&#23616;&#37096;&#28508;&#22312;&#22330;&#26041;&#27861;&#21644;&#36229;&#36951;&#20256;&#31639;&#27861;&#26469;&#20026;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.01435</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#26032;&#24314;&#31569;&#26816;&#26597;&#26080;&#20154;&#26426;&#35270;&#35282;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Clustering-based View Planning Method for Building Inspection with Drone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20004;&#27493;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#35889;&#32858;&#31867;&#12289;&#23616;&#37096;&#28508;&#22312;&#22330;&#26041;&#27861;&#21644;&#36229;&#36951;&#20256;&#31639;&#27861;&#26469;&#20026;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01435v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#19978;&#32447;&#30340;&#25688;&#35201;: &#38543;&#30528;&#26080;&#20154;&#26426;&#25216;&#26415;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#37197;&#22791;&#35270;&#35273;&#20256;&#24863;&#22120;&#30340;&#26080;&#20154;&#26426;&#22312;&#24314;&#31569;&#26816;&#26597;&#21644;&#30417;&#25511;&#26041;&#38754;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#35768;&#22810;&#20851;&#27880;&#12290;&#35270;&#35282;&#35268;&#21010;&#26088;&#22312;&#20026;&#22522;&#20110;&#35270;&#32447;&#30340;&#20219;&#21153;&#25214;&#21040;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#35270;&#37326;&#35206;&#30422;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#35889;&#32858;&#31867;&#12289;&#23616;&#37096;&#28508;&#22312;&#22330;&#26041;&#27861;&#21644;&#36229;&#36951;&#20256;&#31639;&#27861;&#26469;&#25214;&#21040;&#35206;&#30422;&#30446;&#26631;&#24314;&#31569;&#34920;&#38754;&#30340;&#36817;&#20284;&#26368;&#20248;&#35270;&#35282;&#12290;&#22312;&#31532;&#19968;&#27493;&#65292;&#35813;&#25552;&#35758;&#30340;&#26041;&#27861;&#22522;&#20110;&#35889;&#32858;&#31867;&#29983;&#25104;&#20505;&#36873;&#35270;&#35282;&#65292;&#24182;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#22411;&#23616;&#37096;&#28508;&#22312;&#22330;&#26041;&#27861;&#32416;&#27491;&#20505;&#36873;&#35270;&#35282;&#30340;&#20301;&#32622;&#12290;&#22312;&#31532;&#20108;&#37096;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#25442;&#20026;Set Covering Problem (SCP)&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#36229;&#36951;&#20256;&#31639;&#27861;&#26469;&#35299;&#20915;&#26368;&#20339;&#35270;&#35282;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20943;&#23569;&#35268;&#21010;&#27493;&#39588;&#30340;&#25968;&#37327;&#24182;&#25552;&#39640;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#24314;&#31569;&#26816;&#26597;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01435v1 Announce Type: cross  Abstract: With the rapid development of drone technology, the application of drones equipped with visual sensors for building inspection and surveillance has attracted much attention. View planning aims to find a set of near-optimal viewpoints for vision-related tasks to achieve the vision coverage goal. This paper proposes a new clustering-based two-step computational method using spectral clustering, local potential field method, and hyper-heuristic algorithm to find near-optimal views to cover the target building surface. In the first step, the proposed method generates candidate viewpoints based on spectral clustering and corrects the positions of candidate viewpoints based on our newly proposed local potential field method. In the second step, the optimization problem is converted into a Set Covering Problem (SCP), and the optimal viewpoint subset is solved using our proposed hyper-heuristic algorithm. Experimental results show that the pro
&lt;/p&gt;</description></item><item><title>VLG-CBM&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35821;&#35328;&#25351;&#23548;&#26426;&#21046;&#65292;&#25552;&#39640;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#35299;&#20915;&#27010;&#24565;&#39044;&#27979;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01432</link><description>&lt;p&gt;
VLG-CBM&#65306;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#25351;&#23548;&#30340;&#35757;&#32451;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01432
&lt;/p&gt;
&lt;p&gt;
VLG-CBM&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35821;&#35328;&#25351;&#23548;&#26426;&#21046;&#65292;&#25552;&#39640;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#35299;&#20915;&#27010;&#24565;&#39044;&#27979;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01432v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20013;&#38388;&#30340;&#27010;&#24565;&#29942;&#39048;&#23618;&#65288;CBL&#65289;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#65292;&#35813;&#23618;&#32534;&#30721;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#33258;&#21160;&#35757;&#32451;CBMs&#65292;&#36825;&#20351;&#24471;&#23427;&#26356;&#21152;&#21487;&#25193;&#23637;&#21644;&#33258;&#21160;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#65306;&#39318;&#20808;&#65292;CBL&#39044;&#27979;&#30340;&#27010;&#24565;&#32463;&#24120;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#31526;&#65292;&#36825;&#35753;&#20154;&#23545;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#34920;&#31034;&#24576;&#30097;&#12290;&#20854;&#27425;&#65292;&#24050;&#32463;&#34920;&#26126;&#27010;&#24565;&#20540;&#32534;&#30721;&#20102;&#19981;&#24517;&#35201;&#30340;&#20449;&#24687;&#65306;&#21363;&#20351;&#26159;&#38543;&#26426;&#27010;&#24565;&#38598;&#20063;&#33021;&#33719;&#24471;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;CBMs&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#35270;&#35273;&#35821;&#35328;&#25351;&#23548;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#8221;&#65288;VLG-CBM&#65289;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#22679;&#24378;&#35299;&#37322;&#24615;&#30340;&#35757;&#32451;&#65292;&#21516;&#26102;&#20139;&#21463;&#33258;&#21160;&#21270;&#30340;&#25910;&#30410;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#23548;&#26426;&#21046;&#65292;VLG-CBM&#33021;&#22815;&#30830;&#20445;&#27010;&#24565;&#29942;&#39048;&#23618;&#30340;&#27010;&#24565;&#19982;&#22270;&#20687;&#26356;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#27010;&#24565;&#20540;&#20013;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#39044;&#27979;&#20915;&#31574;&#12290;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;VLG-CBM&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20445;&#35777;&#39044;&#27979;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;VLG-CBM&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#27010;&#24565;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01432v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models (LLMs) and pre-trained Vision-Language Models (VLMs) to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#33021;&#22312;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#22918;&#23481;&#20449;&#24687;&#31561;&#19968;&#31181;&#29305;&#24449;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2408.01428</link><description>&lt;p&gt;
&#36801;&#31227;&#24335;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#23545;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Transferable Adversarial Facial Images for Privacy Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#33021;&#22312;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#22918;&#23481;&#20449;&#24687;&#31561;&#19968;&#31181;&#29305;&#24449;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01428v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#30001;&#20110;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#65288;FR&#65289;&#31995;&#32479;&#30340;&#25104;&#21151;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#36861;&#36394;&#29992;&#25143;&#30340;&#33021;&#21147;&#32473;&#20104;&#20102;&#39640;&#24230;&#37325;&#35270;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#38754;&#37096;&#22270;&#20687;&#20013;&#24341;&#20837;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#26469;&#35823;&#23548;&#36825;&#20123;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#29992;&#25143;&#36873;&#25321;&#30340;&#21442;&#32771;&#26469;&#25351;&#23548;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#29983;&#25104;&#65292;&#24182;&#19988;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#26080;&#27861;&#21516;&#26102;&#21019;&#24314;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#36716;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#35758;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#21033;&#29992;&#31867;&#20284;&#20110;&#22918;&#23481;&#20449;&#24687;&#30340;&#19968;&#31181;&#38754;&#37096;&#29305;&#24449;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#20840;&#23616;&#23545;&#25239;&#24615;&#28508;&#21464;&#37327;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01428v1 Announce Type: new  Abstract: The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent sear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Siamese Transformer&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;Euclidean distance measure&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01427</link><description>&lt;p&gt;
Siamese Transformer &#32593;&#32476;&#23545;&#20110;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Siamese Transformer Networks for Few-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Siamese Transformer&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;Euclidean distance measure&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01427v1 &#23459;&#24067;&#31867;&#22411;: &#26032; &#25688;&#35201;: &#20154;&#31867;&#22312;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#25928;&#29575;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21644;&#20998;&#31867;&#26032;&#22270;&#20687;&#65292;&#38656;&#35201;&#30340;&#31034;&#20363;&#24456;&#23569;&#12290;&#36825;&#31181;&#33021;&#21147;&#24402;&#21151;&#20110;&#20182;&#20204;&#33021;&#22815;&#19987;&#27880;&#20110;&#32454;&#33410;&#24182;&#35782;&#21035;&#20043;&#21069;&#30475;&#21040;&#30340;&#21644;&#26032;&#22270;&#20687;&#20043;&#38388;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#24448;&#24448;&#20391;&#37325;&#20110;&#20840;&#23616;&#29305;&#24449;&#25110;&#23616;&#37096;&#29305;&#24449;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#36825;&#20004;&#31181;&#29305;&#24449;&#30340;&#25972;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Siamese Transformer &#32593;&#32476;(STN)&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#32593;&#32476;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer(ViT)&#26550;&#26500;&#26469;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#20351;&#29992;ViT-Small&#32593;&#32476;&#26550;&#26500;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#23545;&#20998;&#25903;&#32593;&#32476;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26469;&#23545;&#20840;&#23616;&#29305;&#24449;&#21644;&#23616;&#37096;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#23569;&#25968;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#24212;&#23569;&#26679;&#26412;&#21644;&#21333;&#19968;&#26679;&#26412;&#26465;&#20214;&#19979;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01427v1 Announce Type: new  Abstract: Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global featur
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.01355</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01355
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01355v1 Announce Type: new  Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable performance on various visual-language understanding and generation tasks. However, MLLMs occasionally generate content inconsistent with the given images, which is known as "hallucination". Prior works primarily center on evaluating hallucination using standard, unperturbed benchmarks, which overlook the prevalent occurrence of perturbed inputs in real-world scenarios-such as image cropping or blurring-that are critical for a comprehensive assessment of MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI, the first benchmark designed to evaluate Hallucination in MLLMs within Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios, containing 1,260 perturbed images from 11 object types. Each image is accompanied by detailed annotations, which include fine-grained hallucination types, such as existence, attribute, and rel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#21457;&#29616;&#21363;&#20351;&#22312;&#26368;&#22522;&#26412;&#30340;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20063;&#33021;&#36798;&#21040;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#21363;&#20351;&#22312;&#20551;&#24819;&#24773;&#22659;&#19979;&#20063;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2408.00677</link><description>&lt;p&gt;
&#36870;&#21521;&#25193;&#23637;&#65306;&#26368;&#23567;&#21512;&#25104;&#39044;&#35757;&#32451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Scaling Backwards: Minimal Synthetic Pre-training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00677
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#21457;&#29616;&#21363;&#20351;&#22312;&#26368;&#22522;&#26412;&#30340;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20063;&#33021;&#36798;&#21040;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#21363;&#20351;&#22312;&#20551;&#24819;&#24773;&#22659;&#19979;&#20063;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00677v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#36890;&#24120;&#22312;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#20294;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#30495;&#30340;&#26159;&#24517;&#35201;&#30340;&#21527;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#23547;&#25214;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30340;&#12289;&#32431;&#31929;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#25105;&#20204;&#33719;&#24471;&#19982;ImageNet-1k&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#21333;&#19968;&#30340;&#20960;&#20309;&#22270;&#24418;&#20013;&#26500;&#36896;&#20102;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#23427;&#36827;&#34892;&#20102;&#25200;&#21160;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;(i) &#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#22312;&#26368;&#23567;&#30340;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23545;&#20110;&#23436;&#20840;&#30340;&#24494;&#35843;&#65292;&#24615;&#33021;&#20063;&#19982;&#20687;ImageNet-1k&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;(ii) &#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#26500;&#36896;&#20154;&#24037;&#31867;&#21035;&#25152;&#38656;&#30340;&#21807;&#19968;&#21442;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24418;&#29366;&#24046;&#24322;&#21487;&#33021;&#23545;&#20154;&#31867;&#26469;&#35828;&#38590;&#20197;&#21306;&#20998;&#65292;&#20294;&#23545;&#20110;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;(iii) &#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#21512;&#25104;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;Zero-shot Learning&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00677v2 Announce Type: replace  Abstract: Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we inve
&lt;/p&gt;</description></item><item><title>MSA&#178;Net &#26159;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35774;&#35745;&#24555;&#25463;&#30340;&#36339;&#36716;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#23545;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#34701;&#21512;&#30340;&#38382;&#39064;&#65292;&#20197;&#22788;&#29702;&#32467;&#26500;&#21464;&#21270;&#65292;&#20419;&#36827;&#29305;&#24449;&#30340;&#21160;&#24577;&#21152;&#26435;&#21644;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2407.21640</link><description>&lt;p&gt;
MSA&#178;Net: &#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#23548;&#21521;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21640
&lt;/p&gt;
&lt;p&gt;
MSA&#178;Net &#26159;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35774;&#35745;&#24555;&#25463;&#30340;&#36339;&#36716;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#23545;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#34701;&#21512;&#30340;&#38382;&#39064;&#65292;&#20197;&#22788;&#29702;&#32467;&#26500;&#21464;&#21270;&#65292;&#20419;&#36827;&#29305;&#24449;&#30340;&#21160;&#24577;&#21152;&#26435;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21640v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#28041;&#21450;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31163;&#23545;&#35937;&#23454;&#20363;&#65292;&#20197;&#32472;&#21046;&#21508;&#31181;&#32452;&#32455;&#21644;&#32467;&#26500;&#65292;&#36825;&#39033;&#20219;&#21153;&#22240;&#36825;&#20123;&#29305;&#24449;&#22312;&#22823;&#23567;&#12289;&#24418;&#29366;&#21644;&#23494;&#24230;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#34987;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#33719;&#38271;&#36317;&#31163;&#30456;&#20851;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21464;&#25442;&#22120;&#65292;&#37197;&#22791;&#26377;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#21512;&#24182;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26159;&#26377;&#30410;&#30340;&#65292;&#20197;&#26377;&#25928;&#22320;&#22312;&#21508;&#31181;&#23610;&#24230;&#19978;&#25972;&#21512;&#29305;&#24449;&#26144;&#23556;&#65292;&#25429;&#25417;&#32467;&#26500;&#21464;&#21270;&#30340;&#35814;&#32454;&#29305;&#24449;&#21644;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#35201;&#32032;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSA&#178;Net&#65292;&#19968;&#20010;&#26032;&#30340;&#28145;&#23618;&#20998;&#21106;&#26694;&#26550;&#65292;&#20855;&#26377;&#24555;&#25463;&#30340;&#35774;&#35745;&#30340;&#36339;&#36716;&#36830;&#25509;&#12290;&#36825;&#20123;&#36830;&#25509;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#21644;&#32452;&#21512;&#31895;&#32454;&#29305;&#24449;&#65292;&#20419;&#36827;&#20102;&#29305;&#24449;&#34701;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#32467;&#26500;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21640v2 Announce Type: replace-cross  Abstract: Medical image segmentation involves identifying and separating object instances in a medical image to delineate various tissues and structures, a task complicated by the significant variations in size, shape, and density of these features. Convolutional neural networks (CNNs) have traditionally been used for this task but have limitations in capturing long-range dependencies. Transformers, equipped with self-attention mechanisms, aim to address this problem. However, in medical image segmentation it is beneficial to merge both local and global features to effectively integrate feature maps across various scales, capturing both detailed features and broader semantic elements for dealing with variations in structures. In this paper, we introduce MSA$^2$Net, a new deep segmentation framework featuring an expedient design of skip-connections. These connections facilitate feature fusion by dynamically weighting and combining coarse-
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ControlMM&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#32479;&#19968;&#30340;&#27169;&#22411;&#26469;&#21516;&#26102;&#25511;&#21046;&#25972;&#20010;&#36523;&#20307;&#30340;&#36816;&#21160;&#29983;&#25104;&#65292;&#24182;&#22788;&#29702;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#24335;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35821;&#38899;&#21644;&#38899;&#20048;&#12290;ControlMM&#35299;&#20915;&#20102;&#36816;&#21160;&#20998;&#24067;&#22312;&#19981;&#21516;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20559;&#31227;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2407.21136</link><description>&lt;p&gt;
&#22312;&#25972;&#20010;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#20013;&#28155;&#21152;&#22810;&#27169;&#24577;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Adding Multimodal Controls to Whole-body Human Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ControlMM&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#32479;&#19968;&#30340;&#27169;&#22411;&#26469;&#21516;&#26102;&#25511;&#21046;&#25972;&#20010;&#36523;&#20307;&#30340;&#36816;&#21160;&#29983;&#25104;&#65292;&#24182;&#22788;&#29702;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#24335;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35821;&#38899;&#21644;&#38899;&#20048;&#12290;ControlMM&#35299;&#20915;&#20102;&#36816;&#21160;&#20998;&#24067;&#22312;&#19981;&#21516;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20559;&#31227;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21136v2 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#25972;&#20010;&#20154;&#20307;&#22810;&#27169;&#24577;&#36816;&#21160;&#29983;&#25104;&#65292;&#36890;&#36807;&#25991;&#26412;&#65292;&#35821;&#38899;&#25110;&#38899;&#20048;&#25511;&#21046;&#65292;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#35270;&#39057;&#29983;&#25104;&#21644;&#35282;&#33394;&#21160;&#30011;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#26469;&#23454;&#29616;&#19981;&#21516;&#26465;&#20214;&#27169;&#24335;&#30340;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#26377;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22312;&#19981;&#21516;&#29983;&#25104;&#22330;&#26223;&#20043;&#38388;&#36816;&#21160;&#20998;&#24067;&#20250;&#21457;&#29983;&#20559;&#31227;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#30340;&#28151;&#21512;&#26465;&#20214;&#36827;&#34892;&#20248;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#19981;&#19968;&#33268;&#30340;&#36816;&#21160;&#26684;&#24335;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#26377;&#25928;&#22810;&#27169;&#24577;&#36816;&#21160;&#29983;&#25104;&#12290;&#22312;&#27492;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;ControlMM&#65292;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;&#36816;&#21160;&#29983;&#25104;&#30340;&#21487;&#25554;&#25300;&#26041;&#24335;&#20013;&#25511;&#21046;&#25972;&#20010;&#20154;&#20307;&#36816;&#21160;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20998;&#24067;&#20043;&#38388;&#36716;&#31227;&#36816;&#21160;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ControlMM-Attn&#65292;&#29992;&#20110;&#24182;&#34892;&#24314;&#27169;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#20154;&#31867;&#25299;&#25169;&#22270;&#12290;&#20026;&#20102;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#30340;&#26465;&#20214;&#65292;ControlMM employs a coarse-grained
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-
&lt;/p&gt;</description></item><item><title>EAR&#32593;&#32476;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#25913;&#36827;&#65292;&#21152;&#24378;&#20102;&#36793;&#32536;&#20449;&#24687;&#30340;&#24863;&#30693;&#24182;&#25552;&#39640;&#20102;&#26894;&#20307;&#24418;&#29366;&#30340;&#37325;&#24314;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2407.20937</link><description>&lt;p&gt;
EAR: &#36793;&#32536;&#24863;&#30693;&#27861;&#20174;&#21452;&#24179;&#38754;X&#23556;&#32447;&#22270;&#20687;&#37325;&#24314;3D&#26894;&#20307;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from bi-planar X-ray images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20937
&lt;/p&gt;
&lt;p&gt;
EAR&#32593;&#32476;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#25913;&#36827;&#65292;&#21152;&#24378;&#20102;&#36793;&#32536;&#20449;&#24687;&#30340;&#24863;&#30693;&#24182;&#25552;&#39640;&#20102;&#26894;&#20307;&#24418;&#29366;&#30340;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20937v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;Abstract: X&#23556;&#32447;&#25668;&#24433;&#22240;&#20854;&#24555;&#36895;&#30340;&#25104;&#20687;&#36895;&#24230;&#21644;&#36739;&#39640;&#20998;&#36776;&#29575;&#32780;&#20351;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#31243;&#21464;&#24471;&#26356;&#20026;&#31616;&#20415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;X&#23556;&#32447;&#25668;&#24433;&#30340;&#25237;&#24433;&#24615;&#36136;&#65292;&#22270;&#20687;&#31354;&#38388;&#20013;&#20002;&#22833;&#20102;&#22823;&#37327;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#20934;&#30830;&#25552;&#20379;&#33034;&#26894;&#30340;&#24418;&#24577;&#32467;&#26500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#35201;&#27714;&#20174;2D X&#23556;&#32447;&#22270;&#20687;&#37325;&#24314;&#20986;3D&#32467;&#26500;&#30340;&#33034;&#26894;&#12290;&#30001;&#20110;&#26894;&#20307;&#30340;&#19981;&#23545;&#31216;&#32467;&#26500;&#65292;&#29616;&#26377;&#30340;&#37325;&#24314;&#26041;&#27861;&#22312;&#20445;&#30041;&#36793;&#32536;&#20449;&#24687;&#21644;&#24418;&#29366;&#26041;&#38754;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#24863;&#30693;&#37325;&#24314;&#32593;&#32476;&#65288;EAR&#65289;&#65292;&#26088;&#22312;&#25552;&#21319;&#36793;&#32536;&#20449;&#24687;&#21644;&#26894;&#20307;&#24418;&#29366;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#20316;&#20026;&#26680;&#24515;&#65292;&#28155;&#21152;&#20102;&#36793;&#32536;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#39057;&#35889;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#36793;&#32536;&#37325;&#24314;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#32467;&#21512;&#20102;&#22235;&#20010;&#25439;&#22833;&#39033;&#65292;&#21253;&#25324;&#37325;&#24314;&#25439;&#22833;&#12289;&#36793;&#32536;&#25439;&#22833;&#12289;&#39057;&#29575;&#25439;&#22833;&#21644;&#33410;&#28857;&#25439;&#22833;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#36824;&#38598;&#25104;&#20102;&#23545;&#25239;&#25439;&#22833;&#26469;&#25913;&#36827;&#37325;&#24314;&#32467;&#26500;&#30340;&#32454;&#21270;&#31243;&#24230;&#65292;&#24182;&#23545;&#26894;&#20307;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#36827;&#34892;&#20102;&#20445;&#25252;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;EAR&#22312;&#36793;&#32536;&#20449;&#24687;&#21644;&#26894;&#20307;&#24418;&#29366;&#30340;&#20445;&#30041;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20937v2 Announce Type: replace-cross  Abstract: X-ray images ease the diagnosis and treatment process due to their rapid imaging speed and high resolution. However, due to the projection process of X-ray imaging, much spatial information has been lost. To accurately provide efficient spinal morphological and structural information, reconstructing the 3-D structures of the spine from the 2-D X-ray images is essential. It is challenging for current reconstruction methods to preserve the edge information and local shapes of the asymmetrical vertebrae structures. In this study, we propose a new Edge-Aware Reconstruction network (EAR) to focus on the performance improvement of the edge information and vertebrae shapes. In our network, by using the auto-encoder architecture as the backbone, the edge attention module and frequency enhancement module are proposed to strengthen the perception of the edge reconstruction. Meanwhile, we also combine four loss terms, including reconstruc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#21644;&#32416;&#27491;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24110;&#21161;&#25552;&#39640;&#22270;&#20687;&#24207;&#21015;&#20013;&#30456;&#26426;&#20301;&#32622;&#21644;&#26041;&#21521;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2407.20391</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#30340;&#31283;&#20581;&#24230;&#37327;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#21644;&#32416;&#27491;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24110;&#21161;&#25552;&#39640;&#22270;&#20687;&#24207;&#21015;&#20013;&#30456;&#26426;&#20301;&#32622;&#21644;&#26041;&#21521;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19977;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#22320;&#38754;&#30495;&#20540;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#30456;&#26426;&#23039;&#24577;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#19977;&#31181;&#24230;&#37327;&#26041;&#27861;&#26159;&#32763;&#35793;&#23545;&#40784;&#20998;&#25968;&#65288;TAS&#65289;&#12289;&#26059;&#36716;&#23545;&#40784;&#20998;&#25968;&#65288;RAS&#65289;&#21644;&#23039;&#24577;&#23545;&#40784;&#20998;&#25968;&#65288;PAS&#65289;&#12290;TAS&#29420;&#31435;&#35780;&#20272;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;RAS&#29420;&#31435;&#35780;&#20272;&#26059;&#36716;&#30340;&#20934;&#30830;&#24615;&#65292;PAS&#21017;&#26159;&#20004;&#32773;&#20998;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#29992;&#20110;&#35780;&#20272;&#32763;&#35793;&#21644;&#26059;&#36716;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;TAS&#30340;&#35745;&#31639;&#27493;&#39588;&#22914;&#19979;&#65306;&#65288;1&#65289;&#25214;&#21040;&#26368;&#36817;&#23545;&#36317;&#31163;&#30340;&#19978;&#22235;&#20998;&#20301;&#25968;&#65292;d&#12290;&#65288;2&#65289;&#20351;&#29992;&#31283;&#20581;&#27880;&#20876;&#26041;&#27861;&#23545;&#20272;&#35745;&#30340;&#36712;&#36857;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#20854;&#19982;&#22320;&#38754;&#30495;&#20540;&#23545;&#40784;&#12290;&#65288;3&#65289;&#25910;&#38598;&#25152;&#26377;&#36317;&#31163;&#35823;&#24046;&#65292;&#24182;&#33719;&#21462;&#20174;0.01d&#21040;d&#30340;&#22810;&#20010;&#38408;&#20540;&#33539;&#22260;&#20869;&#30340;&#32047;&#31215;&#39057;&#25968;&#65292;&#20998;&#36776;&#29575;&#26159;0.01d&#12290;&#65288;4&#65289;&#23558;&#36825;&#20123;&#32047;&#31215;&#39057;&#25968;&#30456;&#21152;&#24182;&#23545;&#20854;&#36827;&#34892;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#30830;&#20445;&#29702;&#35770;&#19978;&#30340;&#26368;&#22823;&#20540;&#26159;1&#12290;TAS&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#23545;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#30340;&#32467;&#26524;&#36827;&#34892;&#26356;&#23458;&#35266;&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20391v2 Announce Type: replace-cross  Abstract: We propose three novel metrics for evaluating the accuracy of a set of estimated camera poses given the ground truth: Translation Alignment Score (TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS evaluates the translation accuracy independently of the rotations, and the RAS evaluates the rotation accuracy independently of the translations. The PAS is the average of the two scores, evaluating the combined accuracy of both translations and rotations. The TAS is computed in four steps: (1) Find the upper quartile of the closest-pair-distances, $d$. (2) Align the estimated trajectory to the ground truth using a robust registration method. (3) Collect all distance errors and obtain the cumulative frequencies for multiple thresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up these cumulative frequencies and normalize them such that the theoretical maximum is 1. The TAS has practical
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#34903;&#26223;&#22270;&#20687;&#26469;&#35780;&#20272;&#22478;&#24066;&#23433;&#20840;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#22823;&#35268;&#27169;&#30340;&#23433;&#20840;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.19719</link><description>&lt;p&gt;
&#38761;&#21629;&#24615;&#22478;&#24066;&#23433;&#20840;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65306;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#34903;&#26223;&#22270;&#20687;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#34903;&#26223;&#22270;&#20687;&#26469;&#35780;&#20272;&#22478;&#24066;&#23433;&#20840;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#22823;&#35268;&#27169;&#30340;&#23433;&#20840;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35848;&#21040;&#30340;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22478;&#24066;&#23433;&#20840;&#24863;&#30693;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#34903;&#26223;&#22270;&#20687;&#12290;&#36825;&#20123;&#34903;&#26223;&#22270;&#20687;&#21487;&#20197;&#36866;&#29992;&#20110;&#23433;&#20840;&#26816;&#27979;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20381;&#38752;&#20154;&#31867;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#22823;&#35268;&#27169;&#22320;&#36827;&#34892;&#23433;&#20840;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#26377;&#26395;&#24443;&#24213;&#25913;&#21464;&#22478;&#24066;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19719v2 Announce Type: replace  Abstract: Measuring urban safety perception is an important and complex task that traditionally relies heavily on human resources. This process often involves extensive field surveys, manual data collection, and subjective assessments, which can be time-consuming, costly, and sometimes inconsistent. Street View Images (SVIs), along with deep learning methods, provide a way to realize large-scale urban safety detection. However, achieving this goal often requires extensive human annotation to train safety ranking models, and the architectural differences between cities hinder the transferability of these models. Thus, a fully automated method for conducting safety evaluations is essential. Recent advances in multimodal large language models (MLLMs) have demonstrated powerful reasoning and analytical capabilities. Cutting-edge models, e.g., GPT-4 have shown surprising performance in many tasks. We employed these models for urban safety ranking o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20559;&#28608;&#28040;&#38500;&#26694;&#26550;VersusDebias&#65292;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25163;&#26426;&#21046;&#26469;&#20943;&#23569;&#20559;&#28608;&#24819;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26080;&#20559;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2407.19524</link><description>&lt;p&gt;
VersusDebias: &#36890;&#29992;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22522;&#20110;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#23545;&#25163;&#30340;&#20559;&#28608;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19524
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20559;&#28608;&#28040;&#38500;&#26694;&#26550;VersusDebias&#65292;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25163;&#26426;&#21046;&#26469;&#20943;&#23569;&#20559;&#28608;&#24819;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26080;&#20559;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19524v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#26576;&#20123;&#31038;&#20250;&#32676;&#20307;&#30340;&#20559;&#35265;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#29305;&#23450;&#27169;&#22411;&#30340;&#22266;&#23450;&#25552;&#31034;&#65292;&#26080;&#27861;&#36866;&#24212;T2I&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#24555;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#25552;&#31034;&#22810;&#26679;&#24615;&#30340;&#36235;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#19981;&#33021;&#32771;&#34385;&#21040;&#24819;&#35937;&#30340;&#39118;&#38505;&#65292;&#23548;&#33268;&#39044;&#26399;&#32467;&#26524;&#21644;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VersusDebias&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;T2I&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#20840;&#26032;&#19988;&#36890;&#29992;&#30340;&#20559;&#28608;&#28040;&#38500;&#26694;&#26550;&#65292;&#23427;&#30001;&#19968;&#20010;&#29983;&#25104;&#23545;&#25163;&#26426;&#21046;&#65288;GAM&#65289;&#21644;&#19968;&#20010;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#28608;&#28040;&#38500;&#29983;&#25104;&#26426;&#21046;&#32452;&#25104;&#12290;&#33258;&#36866;&#24212;&#30340;GAM&#20026;&#27599;&#20010;&#25552;&#31034;&#29983;&#25104;&#19987;&#29992;&#30340;&#23646;&#24615;&#25968;&#32452;&#65292;&#20197;&#20943;&#23569;T2I&#27169;&#22411;&#20013;&#24819;&#35937;&#30340;&#24433;&#21709;&#12290;SLM&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#20026;T2I&#27169;&#22411;&#29983;&#25104;&#26080;&#20559;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19524v2 Announce Type: replace  Abstract: With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;RPD&#26041;&#27861;&#36890;&#36807;&#33976;&#39311;&#26426;&#21046;&#35753;3D&#32593;&#32476;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#35270;&#35273;&#32593;&#32476;&#20013;&#23398;&#20064;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#36328;&#22495;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.18534</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;2D&#21464;&#21387;&#22120;&#20013;&#25552;&#28860;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#26469;&#25552;&#21319;&#36328;&#22495;&#28857;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;RPD&#26041;&#27861;&#36890;&#36807;&#33976;&#39311;&#26426;&#21046;&#35753;3D&#32593;&#32476;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#35270;&#35273;&#32593;&#32476;&#20013;&#23398;&#20064;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#36328;&#22495;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18534v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#25688;&#35201;&#65306;&#23545;&#35937;&#28857;&#20113;&#30340;&#35821;&#20041;&#27169;&#24335;&#26159;&#30001;&#20854;&#23616;&#37096;&#20960;&#20309;&#37197;&#32622;&#20915;&#23450;&#30340;&#12290;&#23398;&#20064;&#21306;&#20998;&#24615;&#34920;&#31034;&#22312;&#23616;&#37096;&#21306;&#22495;&#24418;&#29366;&#22810;&#21464;&#21644;&#20840;&#23616;&#35270;&#35282;&#19979;&#34920;&#38754;&#19981;&#23436;&#22791;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#38750;&#24120;&#22256;&#38590;&#65292;&#36825;&#22312;&#26080;&#30417;&#30563;&#22495;&#35843;&#25972;&#65288;UDA&#65289;&#30340;&#32972;&#26223;&#19979;&#26356;&#26159;&#22914;&#27492;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20256;&#32479;&#30340;3D&#32593;&#32476;&#20027;&#35201;&#38598;&#20013;&#22312;&#23616;&#37096;&#20960;&#20309;&#32454;&#33410;&#19978;&#65292;&#24573;&#35270;&#20102;&#23616;&#37096;&#20960;&#20309;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#21508;&#31181;&#22270;&#20687;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24471;&#30410;&#20110;&#23427;&#24378;&#22823;&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36825;&#20123;&#33021;&#21147;&#28304;&#20110;&#22312;&#23616;&#37096;&#22270;&#26001;&#38388;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#30456;&#20851;&#24615;&#12290;&#21463;&#21040;&#27492;&#31867;&#35270;&#35273;&#21464;&#21387;&#22120;&#25104;&#21151;&#32463;&#39564;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#25552;&#28860;&#8221;&#65288;RPD&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#21462;&#28857;&#20113;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#30340;2D&#35270;&#35273;&#32593;&#32476;&#23545;&#20851;&#31995;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#32534;&#30721;&#30340;&#20449;&#24687;&#36890;&#36807;&#33976;&#39311;&#26426;&#21046;&#20256;&#36882;&#32473;3D&#32593;&#32476;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;3D&#32593;&#32476;&#21487;&#20197;&#20174;2D&#32593;&#32476;&#20013;&#8220;&#23398;&#20064;&#8221;&#21040;&#20851;&#31995;&#20808;&#39564;&#65292;&#20174;&#32780;&#25552;&#21319;&#20854;&#23545;&#19981;&#21516;&#22495;&#25968;&#25454;&#38598;&#30340;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18534v2 Announce Type: replace  Abstract: Semantic pattern of an object point cloud is determined by its topological configuration of local geometries. Learning discriminative representations can be challenging due to large shape variations of point sets in local regions and incomplete surface in a global perspective, which can be made even more severe in the context of unsupervised domain adaptation (UDA). In specific, traditional 3D networks mainly focus on local geometric details and ignore the topological structure between local geometries, which greatly limits their cross-domain generalization. Recently, the transformer-based models have achieved impressive performance gain in a range of image-based tasks, benefiting from its strong generalization capability and scalability stemming from capturing long range correlation across local patches. Inspired by such successes of visual transformers, we propose a novel Relational Priors Distillation (RPD) method to extract relat
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;MARINE&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#30772;&#20102;&#21160;&#29289;&#35270;&#39057;&#20013;&#32597;&#35265;&#25429;&#39135;&#32773;-&#29454;&#29289;&#30340;&#20132;&#20114;&#34892;&#20026;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21160;&#29289;&#34892;&#20026;&#20998;&#26512;&#24179;&#21488;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2407.18289</link><description>&lt;p&gt;
MARINE: &#19968;&#31181;&#29992;&#20110;&#22312;&#21160;&#29289;&#35270;&#39057;&#20013;&#26816;&#27979;&#32597;&#35265;&#25429;&#39135;&#32773;-&#29454;&#29289;&#20132;&#20114;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18289
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;MARINE&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#30772;&#20102;&#21160;&#29289;&#35270;&#39057;&#20013;&#32597;&#35265;&#25429;&#39135;&#32773;-&#29454;&#29289;&#30340;&#20132;&#20114;&#34892;&#20026;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21160;&#29289;&#34892;&#20026;&#20998;&#26512;&#24179;&#21488;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18289v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18289v2 Announce Type: replace  Abstract: Encounters between predator and prey play an essential role in ecosystems, but their rarity makes them difficult to detect in video recordings. Although advances in action recognition (AR) and temporal action detection (AD), especially transformer-based models and vision foundation models, have achieved high performance on human action datasets, animal videos remain relatively under-researched. This thesis addresses this gap by proposing the model MARINE, which utilizes motion-based frame selection designed for fast animal actions and DINOv2 feature extraction with a trainable classification head for action recognition. MARINE outperforms VideoMAE in identifying predator attacks in videos of fish, both on a small and specific coral reef dataset (81.53\% against 52.64\% accuracy), and on a subset of the more extensive Animal Kingdom dataset (94.86\% against 83.14\% accuracy). In a multi-label setting on a representative sample of Anim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21033;&#29992;&#20102;DINOv2&#22522;&#30784;&#27169;&#22411;&#65292;&#23558;&#20854;&#29305;&#24449;&#33976;&#39311;&#21040;FairMOT&#20013;&#65292;&#34429;&#28982;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#26469;&#39564;&#35777;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.18288</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#21019;&#26032;&#65306;&#23558;DINOv2&#29305;&#24449;&#33976;&#39311;&#21040;FairMOT
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21033;&#29992;&#20102;DINOv2&#22522;&#30784;&#27169;&#22411;&#65292;&#23558;&#20854;&#29305;&#24449;&#33976;&#39311;&#21040;FairMOT&#20013;&#65292;&#34429;&#28982;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#26469;&#39564;&#35777;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18288v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#26159;&#19968;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;MOT&#30340;&#19968;&#20123;&#24120;&#35265;&#23616;&#38480;&#24615;&#21253;&#25324;&#23545;&#35937;&#22806;&#35266;&#30340;&#21464;&#21270;&#12289;&#36974;&#25377;&#25110;&#25317;&#25380;&#30340;&#22330;&#26223;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#22823;&#37327;&#37319;&#29992;&#65292;&#24182;&#21033;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#12289;&#22797;&#26434;&#30340;&#27169;&#22411;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#30001;&#20110;&#23454;&#38469;&#38480;&#21046;&#65292;&#19981;&#19968;&#23450;&#24635;&#33021;&#33719;&#24471;&#19978;&#36848;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#30693;&#21517;&#20154;&#24037;&#26234;&#33021;&#20844;&#21496;&#26368;&#36817;&#21457;&#24067;&#30340;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23545;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#36825;&#39033;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#21033;&#29992;&#36825;&#26679;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#21363;DINOv2&#12290;&#25552;&#20986;&#30340;&#31574;&#30053;&#21033;&#29992;&#20102;&#19968;&#31181;&#32769;&#24072;-&#23398;&#29983;&#26550;&#26500;&#65292;&#20854;&#20013;DINOv2&#20316;&#20026;&#32769;&#24072;&#65292;FairMOT&#30340;&#21518;&#31471;HRNetv2 W18&#20316;&#20026;&#23398;&#29983;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25552;&#20986;&#30340;&#31574;&#30053;&#26174;&#31034;&#20986;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#39564;&#35777;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18288v2 Announce Type: replace  Abstract: Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGMN&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35270;&#39057;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#24182;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#20135;&#21697;&#29305;&#24449;&#65292;&#20026;&#26410;&#26469;&#30340;&#23454;&#26102;&#20135;&#21697;&#35782;&#21035;&#19982;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2407.16248</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#26102;&#38388;&#22270;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30452;&#25773;&#20135;&#21697;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Graph Guided Multi-modal Network for Livestreaming Product Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGMN&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35270;&#39057;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#24182;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#20135;&#21697;&#29305;&#24449;&#65292;&#20026;&#26410;&#26469;&#30340;&#23454;&#26102;&#20135;&#21697;&#35782;&#21035;&#19982;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16248v3 &#36890;&#21578;&#31867;&#22411;: &#26367;&#25442; Abstract: &#22312;&#30005;&#23376;&#21830;&#21153;&#24555;&#36895;&#25193;&#24352;&#30340;&#32972;&#26223;&#19979;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28040;&#36153;&#32773;&#20064;&#24815;&#20110;&#36890;&#36807;&#30452;&#25773;&#36827;&#34892;&#36141;&#29289;&#12290;&#20934;&#30830;&#35782;&#21035;&#38144;&#21806;&#20154;&#21592;&#27491;&#22312;&#38144;&#21806;&#30340;&#20135;&#21697;&#65292;&#21363;&#30452;&#25773;&#20135;&#21697;&#26816;&#32034;&#65288;LPR&#65289;&#65292;&#26159;&#19968;&#39033;&#22522;&#30784;&#19988;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;LPR&#20219;&#21153;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#38590;&#39064;&#65306;1&#65289;&#20174;&#32972;&#26223;&#20013;&#30340;&#24178;&#25200;&#20135;&#21697;&#20013;&#35782;&#21035;&#20986;&#24847;&#22270;&#20135;&#21697;&#65307;2&#65289;&#30452;&#25773;&#20013;&#23637;&#31034;&#30340;&#20135;&#21697;&#22806;&#35266;&#24120;&#24120;&#19982;&#21830;&#24215;&#20013;&#30340;&#26631;&#20934;&#21270;&#20135;&#21697;&#22270;&#29255;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65307;3&#65289;&#21830;&#24215;&#20013;&#26377;&#35768;&#22810;&#35270;&#35273;&#32454;&#24494;&#24046;&#21035;&#30340;&#21442;&#32771;&#20135;&#21697;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31354;&#38388;&#26102;&#38388;&#22270;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65288;SGMN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25991;&#26412;&#25351;&#23548;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21033;&#29992;&#38144;&#21806;&#20154;&#21592;&#30340;&#21475;&#22836;&#20869;&#23481;&#26469;&#25351;&#23548;&#27169;&#22411;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#24847;&#22270;&#20135;&#21697;&#19978;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#38745;&#24577;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#21644;&#24418;&#24577;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#35270;&#39057;&#27969;&#20869;&#30340;&#20135;&#21697;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#20063;&#34987;&#25552;&#21462;&#21644;&#21033;&#29992;&#65292;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#26681;&#25454;&#20135;&#21697;&#22312;&#35270;&#39057;&#27969;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#25429;&#25417;&#20135;&#21697;&#22238;&#25918;&#30340;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#35270;&#39057;&#27969;&#20013;&#20135;&#21697;&#30340;&#22806;&#35266;&#29305;&#24449;&#12289;&#20301;&#32622;&#20449;&#24687;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#30452;&#25773;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;SGMN&#26088;&#22312;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#20135;&#21697;&#29305;&#24449;&#65292;&#20026;&#26410;&#26469;&#30340;&#23454;&#26102;&#20135;&#21697;&#35782;&#21035;&#19982;&#25512;&#33616;&#25552;&#20379;&#20851;&#38190;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16248v3 Announce Type: replace  Abstract: With the rapid expansion of e-commerce, more consumers have become accustomed to making purchases via livestreaming. Accurately identifying the products being sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a fundamental and daunting challenge. The LPR task encompasses three primary dilemmas in real-world scenarios: 1) the recognition of intended products from distractor products present in the background; 2) the video-image heterogeneity that the appearance of products showcased in live streams often deviates substantially from standardized product images in stores; 3) there are numerous confusing products with subtle visual nuances in the shop. To tackle these challenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN). First, we employ a text-guided attention mechanism that leverages the spoken content of salespeople to guide the model to focus toward intended products, emphasizing their s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;PointFormer&#65288;APF&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;2D&#35270;&#35273;&#27169;&#22411;&#30452;&#25509;&#22788;&#29702;3D&#28857;&#20113;&#65292;&#20174;&#32780;&#26080;&#38656;&#23558;&#28857;&#20113;&#26144;&#23556;&#21040;&#22270;&#20687;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#34920;&#31034;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.13200</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;PointFormer: &#36890;&#36807;&#36866;&#24212;2D&#35270;&#35273;&#21464;&#25442;&#22120;&#20998;&#26512;3D&#28857;&#20113;
&lt;/p&gt;
&lt;p&gt;
Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.13200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;PointFormer&#65288;APF&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;2D&#35270;&#35273;&#27169;&#22411;&#30452;&#25509;&#22788;&#29702;3D&#28857;&#20113;&#65292;&#20174;&#32780;&#26080;&#38656;&#23558;&#28857;&#20113;&#26144;&#23556;&#21040;&#22270;&#20687;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#34920;&#31034;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.13200v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;2D&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;3D&#28857;&#20113;&#26102;&#65292;&#19982;&#22270;&#20687;&#30340;&#22823;&#37327;&#25968;&#25454;&#24211;&#30456;&#27604;&#65292;&#25968;&#25454;&#35775;&#38382;&#30340;&#23616;&#38480;&#24615;&#20026;&#24320;&#21457;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35797;&#22270;&#30452;&#25509;&#21033;&#29992;&#20855;&#26377;2D&#20808;&#39564;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;3D&#28857;&#20113;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PointFormer&#65288;APF&#65289;&#65292;&#23427;&#20165;&#20351;&#29992;&#23569;&#37327;&#21442;&#25968;&#23545;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20415;&#30452;&#25509;&#22788;&#29702;&#28857;&#20113;&#65292;&#26080;&#38656;&#26144;&#23556;&#21040;&#22270;&#20687;&#19978;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#28857;&#20113;&#36716;&#25442;&#20026;&#28857;&#23884;&#20837;&#65292;&#20197;&#19982;&#22270;&#20687;&#31526;&#21495;&#30340;&#32500;&#24230;&#23545;&#40784;&#12290;&#32771;&#34385;&#21040;&#28857;&#20113;&#30340;&#20869;&#22312;&#26080;&#24207;&#24615;&#19982;&#22270;&#20687;&#30340;&#32467;&#26500;&#24615;&#36136;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#25509;&#30528;&#23545;&#28857;&#23884;&#20837;&#36827;&#34892;&#24207;&#21015;&#21270;&#65292;&#20197;&#20248;&#21270;2D&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#32593;&#32476;&#26469;&#36741;&#21161;&#29983;&#25104;&#32454;&#21270;&#21518;&#30340;&#28857;&#20113;&#65292;&#24182;&#21033;&#29992;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#20943;&#23569;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#21508;&#31867;&#28857;&#20113;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;APF&#19981;&#20165;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#34920;&#31034;&#65292;&#21516;&#26102;&#36824;&#33021;&#26174;&#33879;&#25552;&#39640;FD&#178;-Net&#31561;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.13200v2 Announce Type: replace  Abstract: Pre-trained large-scale models have exhibited remarkable efficacy in computer vision, particularly for 2D image analysis. However, when it comes to 3D point clouds, the constrained accessibility of data, in contrast to the vast repositories of images, poses a challenge for the development of 3D pre-trained models. This paper therefore attempts to directly leverage pre-trained models with 2D prior knowledge to accomplish the tasks for 3D point cloud analysis. Accordingly, we propose the Adaptive PointFormer (APF), which fine-tunes pre-trained 2D models with only a modest number of parameters to directly process point clouds, obviating the need for mapping to images. Specifically, we convert raw point clouds into point embeddings for aligning dimensions with image tokens. Given the inherent disorder in point clouds, in contrast to the structured nature of images, we then sequence the point embeddings to optimize the utilization of 2D a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#30340;&#20998;&#23618;&#28176;&#36827;&#24335;&#20307;&#31215;&#35270;&#39057;&#32534;&#30721;&#26694;&#26550;HPC&#65292;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#27604;&#29305;&#29575;&#35843;&#25972;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#32593;&#32476;&#21644;&#35774;&#22791;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.09026</link><description>&lt;p&gt;
HPC: &#20998;&#23618;&#28176;&#36827;&#24335;&#32534;&#30721;&#26694;&#26550;&#29992;&#20110;&#20307;&#31215;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
HPC: Hierarchical Progressive Coding Framework for Volumetric Video
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#30340;&#20998;&#23618;&#28176;&#36827;&#24335;&#20307;&#31215;&#35270;&#39057;&#32534;&#30721;&#26694;&#26550;HPC&#65292;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#27604;&#29305;&#29575;&#35843;&#25972;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#32593;&#32476;&#21644;&#35774;&#22791;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.09026v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#20307;&#31215;&#35270;&#39057;&#20855;&#26377;&#22810;&#31181;3D&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#23545;&#20110;&#21387;&#32553;&#21644;&#20256;&#36755;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;NeRF&#21387;&#32553;&#30446;&#21069;&#32570;&#20047;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#23545;&#35270;&#39057;&#36136;&#37327;&#21644;&#27604;&#29305;&#29575;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#30340;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#32593;&#32476;&#21644;&#35774;&#22791;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#28176;&#36827;&#24335;&#20307;&#31215;&#35270;&#39057;&#32534;&#30721;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#23454;&#29616;&#21487;&#21464;&#27604;&#29305;&#29575;&#21387;&#32553;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HPC&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#20998;&#36776;&#29575;&#27531;&#24046;&#36752;&#23556;&#22330;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#20197;&#20943;&#23569;&#38271;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20887;&#20313;&#65292;&#21516;&#26102;&#20135;&#29983;&#21508;&#31181;&#32454;&#33410;&#32423;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#22810;&#36895;&#29575;&#22833;&#30495;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21516;&#26102;&#20248;&#21270;&#23618;&#27425;&#32467;&#26500;&#21644;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;HPC&#21482;&#38656;&#35757;&#32451;&#19968;&#27425;&#65292;&#23601;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#27604;&#29305;&#29575;&#30340;&#32534;&#30721;&#25928;&#29575;&#65292;&#23545;&#20110;&#20302;&#24102;&#23485;&#21644;&#20302;&#24310;&#36831;&#30340;&#35201;&#27714;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.09026v2 Announce Type: replace  Abstract: Volumetric video based on Neural Radiance Field (NeRF) holds vast potential for various 3D applications, but its substantial data volume poses significant challenges for compression and transmission. Current NeRF compression lacks the flexibility to adjust video quality and bitrate within a single model for various network and device capacities. To address these issues, we propose HPC, a novel hierarchical progressive volumetric video coding framework achieving variable bitrate using a single model. Specifically, HPC introduces a hierarchical representation with a multi-resolution residual radiance field to reduce temporal redundancy in long-duration sequences while simultaneously generating various levels of detail. Then, we propose an end-to-end progressive learning approach with a multi-rate-distortion loss function to jointly optimize both hierarchical representation and compression. Our HPC trained only once can realize multiple
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#21327;&#21516;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#34920;&#26126;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#36136;&#37327;&#25552;&#21319;&#20013;&#36215;&#21040;&#30456;&#20114;&#20419;&#36827;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2407.08583</link><description>&lt;p&gt;
&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#28436;&#36827;&#30340;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.08583
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#21327;&#21516;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#34920;&#26126;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#36136;&#37327;&#25552;&#21319;&#20013;&#36215;&#21040;&#30456;&#20114;&#20419;&#36827;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08583v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08583v2 Announce Type: replace-cross  Abstract: The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs; on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approache
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SvANet&#30340;&#26032;&#32593;&#32476;&#65292;&#26088;&#22312;&#21033;&#29992;&#21487;&#21464;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#22312;&#23567;&#21307;&#30103;&#22270;&#29255;&#20013;&#35782;&#21035;&#23567;&#21306;&#22495;&#30340;&#30446;&#26631;&#25928;&#29575;&#12290;&#27492;&#32593;&#32476;&#33021;&#26356;&#31934;&#30830;&#22320;&#20998;&#21106;&#22270;&#29255;&#65292;&#24182;&#19988;&#33021;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#35786;&#26029;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#30142;&#30149;&#30340;&#26089;&#26399;&#35782;&#21035;&#29575;&#21644;&#27835;&#30103;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2407.07720</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#21464;&#23610;&#24230;&#27880;&#24847;&#21147;&#23545;&#23567;&#21307;&#30103;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting Scale-Variant Attention for Segmenting Small Medical Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.07720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SvANet&#30340;&#26032;&#32593;&#32476;&#65292;&#26088;&#22312;&#21033;&#29992;&#21487;&#21464;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#22312;&#23567;&#21307;&#30103;&#22270;&#29255;&#20013;&#35782;&#21035;&#23567;&#21306;&#22495;&#30340;&#30446;&#26631;&#25928;&#29575;&#12290;&#27492;&#32593;&#32476;&#33021;&#26356;&#31934;&#30830;&#22320;&#20998;&#21106;&#22270;&#29255;&#65292;&#24182;&#19988;&#33021;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#35786;&#26029;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#30142;&#30149;&#30340;&#26089;&#26399;&#35782;&#21035;&#29575;&#21644;&#27835;&#30103;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.07720v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#26089;&#26399;&#26816;&#27979;&#21644;&#20934;&#30830;&#35786;&#26029;&#21487;&#20197;&#39044;&#27979;&#24694;&#24615;&#30142;&#30149;&#36716;&#21270;&#39118;&#38505;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#27835;&#30103;&#30340;&#21487;&#33021;&#24615;&#12290;&#35782;&#21035;&#36731;&#24494;&#32508;&#21512;&#24449;&#30340;&#23567;&#30149;&#29702;&#21306;&#22495;&#26159;&#30142;&#30149;&#26089;&#26399;&#35786;&#26029;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#65292;&#22312;&#20998;&#21106;&#21307;&#23398;&#23545;&#35937;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#23567;&#21306;&#22495;&#30340;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#31181;&#22256;&#38590;&#28304;&#20110;&#21367;&#31215;&#21644;&#27744;&#21270;&#36816;&#31639;&#22312;CNN&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#21644;&#21387;&#32553;&#32570;&#38519;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#21152;&#28145;&#26102;&#65292;&#36825;&#23545;&#23567;&#21307;&#30103;&#23545;&#35937;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21464;&#23610;&#24230;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;SvANet&#65289;&#65292;&#29992;&#20110;&#20934;&#30830;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#23567;&#23610;&#24230;&#23545;&#35937;&#12290;SvANet&#30001;&#21487;&#21464;&#23610;&#24230;&#27880;&#24847;&#21147;&#12289;&#36328;&#23610;&#24230;&#24341;&#23548;&#21644;&#33945;&#29305;&#21345;&#27931;&#20248;&#21270;&#32452;&#25104;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#21464;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32454;&#21270;&#27169;&#22359;&#65292;SvANet&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#23610;&#24230;&#19978;&#30340;&#23545;&#35937;&#29305;&#24501;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#25351;&#23548;&#30340;&#23567;&#23610;&#24230;&#29305;&#24449;&#65292;&#25552;&#39640;&#36793;&#32536;&#31934;&#24230;&#21644;&#20687;&#32032;&#32423;&#20998;&#21106;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#26102;&#22810;&#23610;&#24230;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36827;&#34892;&#23454;&#26102;&#30340;&#22270;&#20687;&#20998;&#21106;&#25805;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SvANet&#22312;&#19987;&#19994;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#25552;&#39640;&#20102;&#23545;&#23567;&#32959;&#30244;&#21644;&#23567;&#30149;&#21464;&#30340;&#31934;&#30830;&#26816;&#20986;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#23545;&#27604;&#24230;&#21644;&#39640;&#22122;&#22768;&#30340;&#21307;&#23398;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#30446;&#21069;&#23567;&#35268;&#27169;&#21307;&#23398;&#23545;&#35937;&#20998;&#21106;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23545;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.07720v4 Announce Type: replace-cross  Abstract: Early detection and accurate diagnosis can predict the risk of malignant disease transformation, thereby increasing the probability of effective treatment. Identifying mild syndrome with small pathological regions serves as an ominous warning and is fundamental in the early diagnosis of diseases. While deep learning algorithms, particularly convolutional neural networks (CNNs), have shown promise in segmenting medical objects, analyzing small areas in medical images remains challenging. This difficulty arises due to information losses and compression defects from convolution and pooling operations in CNNs, which become more pronounced as the network deepens, especially for small medical objects. To address these challenges, we propose a novel scale-variant attention-based network (SvANet) for accurately segmenting small-scale objects in medical images. The SvANet consists of scale-variant attention, cross-scale guidance, Monte 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25991;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#35206;&#30422;&#22810;&#20010;&#22269;&#23478;&#12289;&#22810;&#20010;&#25991;&#21270;&#30340;&#25991;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#25991;&#21270;&#24847;&#35782;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2407.06863</link><description>&lt;p&gt;
&#36229;&#36234;&#32654;&#23398;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Aesthetics: Cultural Competence in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.06863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25991;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#35206;&#30422;&#22810;&#20010;&#22269;&#23478;&#12289;&#22810;&#20010;&#25991;&#21270;&#30340;&#25991;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#25991;&#21270;&#24847;&#35782;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.06863v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#27491;&#22312;&#20840;&#29699;&#21508;&#22320;&#19981;&#26029;&#34987;&#37319;&#29992;&#65292;&#23427;&#20204;&#21019;&#36896;&#20986;&#21508;&#33258;&#29420;&#29305;&#25991;&#21270;&#30340;&#35270;&#35273;&#34920;&#29616;&#12290;&#30446;&#21069;T2I&#27169;&#22411;&#30340;&#35780;&#20215;&#20027;&#35201;&#38598;&#20013;&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#24544;&#23454;&#24615;&#65292;&#32654;&#24863;&#21644;&#29616;&#23454;&#24863;&#19978;&#65292;&#32780;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#32500;&#24230;&#65306;&#25991;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;T2I&#27169;&#22411;&#30340;&#25991;&#21270;&#33021;&#21147;&#65292;&#35813;&#26694;&#26550;&#28041;&#21450;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#23545;&#25991;&#21270;&#30340;&#24847;&#35782;&#21644;&#23545;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#23562;&#37325;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#25991;&#21270; artefacts&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#35780;&#20215;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24314;&#31435;CUBE&#65288;&#25991;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#27979;&#35797;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#34920;&#29616;&#65289;&#65292;&#36825;&#26159;&#19968;&#39033;&#39318;&#27425;&#36890;&#36807;&#19981;&#21516;&#22320;&#29702;&#21644;&#25991;&#21270;&#21306;&#22495;&#21644;&#30456;&#20851;&#27010;&#24565;&#26469;&#35780;&#20272;&#25991;&#21270;&#33021;&#21147;&#30340;&#31532;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#12290;CUBE&#35206;&#30422;&#20102;&#19982;8&#20010;&#19981;&#21516;&#22269;&#23478;&#30456;&#20851;&#32852;&#30340;&#25991;&#21270; artefacts&#65292;&#24182;&#27839;&#30528;3&#20010;&#19981;&#21516;&#30340;&#27010;&#24565;&#36827;&#34892;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.06863v4 Announce Type: replace  Abstract: Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concept
&lt;/p&gt;</description></item><item><title>ViG-Bias&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#33258;&#21160;&#21457;&#29616;&#21644;&#32531;&#35299;&#28508;&#34255;&#30340;&#20559;&#24046;&#65292;&#36890;&#36807;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#36328;&#27169;&#24577;&#23884;&#20837;&#65292;&#21363;&#20351;&#22312;&#23646;&#24615;&#26410;&#30693;&#30340;&#22330;&#26223;&#20013;&#20063;&#33021;&#35782;&#21035;&#20986;&#31995;&#32479;&#22833;&#36133;&#30340;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2407.01996</link><description>&lt;p&gt;
ViG-Bias&#65306;&#22522;&#20110;&#35270;&#35273;&#30340;&#20559;&#24046;&#21457;&#29616;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
ViG-Bias: Visually Grounded Bias Discovery and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.01996
&lt;/p&gt;
&lt;p&gt;
ViG-Bias&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#33258;&#21160;&#21457;&#29616;&#21644;&#32531;&#35299;&#28508;&#34255;&#30340;&#20559;&#24046;&#65292;&#36890;&#36807;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#36328;&#27169;&#24577;&#23884;&#20837;&#65292;&#21363;&#20351;&#22312;&#23646;&#24615;&#26410;&#30693;&#30340;&#22330;&#26223;&#20013;&#20063;&#33021;&#35782;&#21035;&#20986;&#31995;&#32479;&#22833;&#36133;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.01996v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#22312;&#20851;&#38190;&#20915;&#31574;&#36807;&#31243;&#20013;&#24191;&#27867;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20984;&#26174;&#20102;&#21457;&#29616;&#21644;&#32531;&#35299;&#20559;&#24046;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#12290;&#30001;&#20110;&#24456;&#22810;&#26102;&#20505;&#36825;&#20123;&#20559;&#24046;&#19982;&#38590;&#20197;&#23519;&#35273;&#30340;&#38544;&#34255;&#30340;&#20266;&#30456;&#20851;&#24615;&#26377;&#20851;&#65292;&#22240;&#27492;&#30830;&#23450;&#31995;&#32479;&#34920;&#29616;&#20986;&#20559;&#24046;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#19981;&#30452;&#25509;&#12290;&#26631;&#20934;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#23545;&#27169;&#22411;&#22312;&#25968;&#25454;&#26679;&#26412;&#39044;&#23450;&#20041;&#23376;&#32676;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20998;&#26512;&#26469;&#25191;&#34892;&#20559;&#24046;&#23457;&#26680;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#21040;&#20154;&#31867;&#26102;&#65292;&#36890;&#24120;&#20250;&#22522;&#20110;&#24615;&#21035;&#25110;&#31181;&#26063;&#31561;&#24120;&#35265;&#23646;&#24615;&#65292;&#25110;&#32773;&#26159;&#20026;&#20102;&#22270;&#20687;&#31561;&#20854;&#20182;&#29305;&#23450;&#23646;&#24615;&#23450;&#20041;&#30340;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#32452;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#24635;&#26159;&#26377;&#21487;&#33021;&#20107;&#20808;&#30693;&#36947;&#23450;&#20041;&#35270;&#35273;&#35782;&#21035;&#31995;&#32479;&#22833;&#36133;&#27169;&#24335;&#30340;&#35814;&#32454;&#23646;&#24615;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#21457;&#29616;&#36825;&#20123;&#23567;&#32452;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25552;&#21462;&#36328;&#27169;&#24577;&#23884;&#20837;&#24182;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#26410;&#39044;&#20808;&#23450;&#20041;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#23548;&#33268;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#30340;&#23646;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.01996v3 Announce Type: replace  Abstract: The proliferation of machine learning models in critical decision making processes has underscored the need for bias discovery and mitigation strategies. Identifying the reasons behind a biased system is not straightforward, since in many occasions they are associated with hidden spurious correlations which are not easy to spot. Standard approaches rely on bias audits performed by analyzing model performance in pre-defined subgroups of data samples, usually characterized by common attributes like gender or ethnicity when it comes to people, or other specific attributes defining semantically coherent groups of images. However, it is not always possible to know a-priori the specific attributes defining the failure modes of visual recognition systems. Recent approaches propose to discover these groups by leveraging large vision language models, which enable the extraction of cross-modal embeddings and the generation of textual descripti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#22312;&#22823;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#27169;&#24335;&#26102;&#20986;&#29616;&#30340;&#39033;&#30446;&#26631;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#23398;&#20064;&#39033;&#21644;&#21021;&#27493;&#30340;&#37096;&#32626;&#26631;&#31614;&#26469;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2406.19905</link><description>&lt;p&gt;
&#35299;&#20915;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;&#22823;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39033;&#30446;&#26631;&#26799;&#24230;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.19905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#22312;&#22823;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#27169;&#24335;&#26102;&#20986;&#29616;&#30340;&#39033;&#30446;&#26631;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#23398;&#20064;&#39033;&#21644;&#21021;&#27493;&#30340;&#37096;&#32626;&#26631;&#31614;&#26469;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.19905v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#30740;&#31350;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23427;&#20351;&#29992;&#31232;&#30095;&#27169;&#22411;&#26469;&#26367;&#25442;&#31264;&#23494;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#28608;&#27963;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#21644;&#21442;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;&#22312;LVLMs&#20013;&#30340;&#29616;&#26377;MoE&#26041;&#27861;&#40723;&#21169;&#19981;&#21516;&#30340;&#19987;&#23478;&#22788;&#29702;&#19981;&#21516;&#30340;&#39033;&#30446;&#26631;&#65292;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#19968;&#20010;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#27599;&#20010;&#39033;&#30446;&#26631;&#30340;&#27966;&#36963;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#27979;&#20165;&#22522;&#20110;&#31034;&#20363;&#29305;&#24449;&#65292;&#24182;&#19981;&#33021;&#30495;&#27491;&#25581;&#31034;&#34987;&#20998;&#37197;&#21040;&#19987;&#23478;&#20013;&#30340;&#39033;&#30446;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#36825;&#21487;&#33021;&#22312;&#20998;&#37197;&#32473;&#21516;&#19968;&#20010;&#19987;&#23478;&#30340;&#19981;&#21516;&#39033;&#30446;&#20013;&#24341;&#36215;&#20005;&#37325;&#30340;&#20248;&#21270;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#26631;&#32423;&#26799;&#24230;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#35299;&#20915;&#39033;&#30446;&#26631;&#26799;&#24230;&#20914;&#31361;&#65288;STGC&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39033;&#30446;&#26631;&#32423;&#26799;&#24230;&#26469;&#35782;&#21035;&#19987;&#23478;&#20013;&#30340;&#20914;&#31361;&#39033;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20914;&#31361;&#39033;&#30446;&#26631;&#28155;&#21152;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#23398;&#20064;&#39033;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21021;&#27493;&#30340;&#37096;&#32626;&#26631;&#31614;&#65292;&#23427;&#20204;&#30340;&#26356;&#26032;&#23558;&#25104;&#20026;&#20027;&#30417;&#30563;&#30340;&#34917;&#20805;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;SimP&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;Transformer-XL&#21644;T5&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23427;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#35821;&#20041;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#36880;&#27493;&#20174;&#20302;&#23618;&#32423;&#21040;&#39640;&#23618;&#32423;&#20256;&#36882;&#39044;&#27979;&#26631;&#31614;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo
&lt;/p&gt;</description></item><item><title>InterCLIP-MEP &#26694;&#26550;&#20351;&#29992;&#20132;&#20114;&#24335; CLIP &#21644;&#22686;&#24378;&#30340;&#35760;&#24518;&#39044;&#27979;&#22120;&#25913;&#36827;&#20102;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#22810;&#27169;&#24577; sarcasm &#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2406.16464</link><description>&lt;p&gt;
InterCLIP-MEP&#65306;&#20132;&#20114;&#24335;CLIP&#21644;&#22686;&#24378;&#35760;&#24518;&#39044;&#27979;&#22120;&#22312;&#22810;&#27169;&#24577; sarcasm &#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.16464
&lt;/p&gt;
&lt;p&gt;
InterCLIP-MEP &#26694;&#26550;&#20351;&#29992;&#20132;&#20114;&#24335; CLIP &#21644;&#22686;&#24378;&#30340;&#35760;&#24518;&#39044;&#27979;&#22120;&#25913;&#36827;&#20102;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#22810;&#27169;&#24577; sarcasm &#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16464v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978; sarcasm &#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#36890;&#36807;&#25991;&#23383;&#21644;&#22270;&#20687;&#30340;&#32452;&#21512;&#34920;&#36798;&#65292;&#20026;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#22270;&#25366;&#25496;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577; sarcasm &#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#23384;&#22312;&#39640;&#20272;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#25991;&#23383;&#21644;&#22270;&#20687;&#20043;&#38388;&#20114;&#21160;&#20135;&#29983;&#30340;&#31934;&#32454; sarcastic &#32447;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; InterCLIP-MEP&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577; sarcasm &#26816;&#27979;&#30340;&#20840;&#26032;&#26694;&#26550;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20132;&#20114;&#24335; CLIP&#65288;InterCLIP&#65289;&#20316;&#20026;&#39592;&#24178;&#65292;&#25552;&#21462;&#25991;&#23383;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#32534;&#30721;&#22120;&#20013;&#30452;&#25509;&#23884;&#20837;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#34920;&#31034;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#25991;&#23383;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22521;&#35757;&#31574;&#30053;&#65292;&#20026;&#20102;&#36866;&#24212;&#25105;&#20204;&#25552;&#20986;&#30340;&#22686;&#24378;&#35760;&#24518;&#39044;&#27979;&#22120;&#65288;MEP&#65289;&#35843;&#25972;InterCLIP&#12290;MEP&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#30340;&#12289;&#22266;&#23450;&#38271;&#24230;&#30340;&#21452;&#36890;&#36947;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#23545; sarcasm &#30340;&#26816;&#27979;&#65292;&#24182;&#19988;&#20934;&#30830;&#29575;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16464v3 Announce Type: replace-cross  Abstract: The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Existing multi-modal sarcasm detection methods have been proven to overestimate performance, as they struggle to effectively capture the intricate sarcastic cues that arise from the interaction between an image and text. To address these issues, we propose InterCLIP-MEP, a novel framework for multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP (InterCLIP) as the backbone to extract text-image representations, enhancing them by embedding cross-modality information directly within each encoder, thereby improving the representations to capture text-image interactions better. Furthermore, an efficient training strategy is designed to adapt InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic, fixed-length dual-channel mem
&lt;/p&gt;</description></item><item><title>CM2-Net&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#19981;&#26029;&#23398;&#20064;&#26032;&#27169;&#24577;&#29305;&#24449;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#26144;&#23556;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#22810;&#31181;&#27169;&#24577;&#25968;&#25454;&#19978;&#25552;&#39640;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2406.11340</link><description>&lt;p&gt;
CM2-Net&#65306;&#19981;&#26029;&#30340;&#22810;&#27169;&#24577;&#26144;&#23556;&#32593;&#32476;&#29992;&#20110;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.11340
&lt;/p&gt;
&lt;p&gt;
CM2-Net&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#19981;&#26029;&#23398;&#20064;&#26032;&#27169;&#24577;&#29305;&#24449;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#26144;&#23556;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#22810;&#31181;&#27169;&#24577;&#25968;&#25454;&#19978;&#25552;&#39640;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.11340v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#32763;&#35793;&#65306;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#22312;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22914;&#32418;&#22806;&#21644;&#28145;&#24230;&#65289;&#26469;&#22686;&#24378;&#39550;&#39542;&#21592;&#19982;&#36710;&#36742;&#20114;&#21160;&#24182;&#30830;&#20445;&#34892;&#36710;&#23433;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19982;&#20165;&#20351;&#29992;RGB&#27169;&#24577;&#30456;&#27604;&#65292;&#22312;&#36710;&#33329;&#29615;&#22659;&#20013;&#25910;&#38598;&#25152;&#26377;&#31867;&#22411;&#38750;RGB&#27169;&#24577;&#30340;&#25968;&#25454;&#24635;&#26159;&#26082;&#36153;&#26102;&#21448;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292; previous works &#24314;&#35758;&#29420;&#31435;&#22320;&#36890;&#36807;&#22312;RGB&#35270;&#39057;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#24494;&#35843;&#27599;&#20010;&#38750;RGB&#27169;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#26032;&#30340;&#27169;&#24577;&#26102;&#25552;&#21462;&#26377;&#20449;&#24687;&#29305;&#24449;&#26102;&#19981;&#22826;&#26377;&#25928;&#65292;&#22240;&#20026;&#23384;&#22312;&#36739;&#22823;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#26029;&#30340;&#22810;&#27169;&#24577;&#26144;&#23556;&#32593;&#32476;&#65288;CM2-Net&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#27599;&#31181;&#26032;&#27169;&#24577;&#19978;&#25552;&#20379;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#27169;&#24577;&#30340;&#25351;&#23548;&#24615;&#25552;&#31034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32047;&#31215;&#30340;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#31034;&#65288;ACMP&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#36776;&#21035;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#20174;&#19981;&#21516;&#27169;&#24577;&#26144;&#23556;&#20986;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#20010;&#27169;&#24577;&#30340;&#36328;&#30028;&#23398;&#20064;&#38590;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#32423;&#21644;&#36890;&#36947;&#32423;&#30340;&#22810;&#32423;&#26144;&#23556;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#25429;&#33719;&#21644;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#21644;&#36890;&#36947;&#22810;&#23610;&#24230;&#12289;&#22810;&#35282;&#24230;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;CM2-Net&#19981;&#20165;&#33021;&#22815;&#19981;&#26029;&#23398;&#20064;&#26032;&#27169;&#24577;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#22312;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20986;&#19968;&#20010;&#27867;&#21270;&#24615;&#24378;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#35782;&#21035;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ANNEAL&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#19982;&#22810;&#26679;&#24615;&#30340;&#35780;&#20215;&#26631;&#20934;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#23567;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#38598;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2406.10107</link><description>&lt;p&gt;
&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval
&lt;/p&gt;
&lt;p&gt;
Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ANNEAL&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36965;&#24863;&#22270;&#20687;&#26816;&#32034;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#19982;&#22810;&#26679;&#24615;&#30340;&#35780;&#20215;&#26631;&#20934;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#23567;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#38598;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#26367;&#25442;&#20844;&#21578;&#31867;&#22411;&#65306;replace&#65292;&#25688;&#35201;&#65306;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#22270;&#20687;&#26816;&#32034;&#65288;CBIR&#65289;&#20013;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;CBIR&#30340;DML&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#27880;&#22270;&#30340;&#25968;&#20197;&#20934;&#30830;&#23398;&#20064;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;RS&#20013;DML&#39537;&#21160;&#30340;CBIR&#30340;&#26631;&#27880;&#25104;&#26412;&#39640;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65288;ANNEAL&#65289;&#12290;ANNEAL&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#30001;&#31867;&#20284;&#21644;&#19981;&#21516;&#22270;&#20687;&#23545;&#32452;&#25104;&#30340;&#36739;&#23567;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#38598;&#65292;&#20197;&#20415;&#20934;&#30830;&#22320;&#23398;&#20064;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#12290;&#22270;&#20687;&#23545;&#30340;&#20449;&#24687;&#37327;&#36890;&#36807;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#26631;&#20934;&#26469;&#35780;&#20272;&#12290;&#20026;&#20102;&#35780;&#20272;&#22270;&#20687;&#23545;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#39033;&#31639;&#27861;&#65306;1&#65289;&#22522;&#20110;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;MGUE&#65289;&#65307;&#21644;2&#65289;&#22522;&#20110;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;BCGUE&#65289;&#12290;MGUE&#31639;&#27861;&#33258;&#21160;&#20272;&#35745;&#19968;&#20010;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;BCGUE&#31639;&#27861;&#21033;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#31934;&#30830;&#25351;&#23450;&#38656;&#35201;&#39069;&#22806;&#26631;&#27880;&#30340;&#22270;&#20687;&#23545;&#12290;&#36890;&#36807;&#19968;&#20010;&#36965;&#24863;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#23545;&#25968;&#25439;&#22833;&#21644;IoU&#20934;&#21017;&#30456;&#27604;&#65292;ANNEAL&#22312;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#19978;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;ANNEAL&#19982;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21644;&#24230;&#37327;&#26631;&#20934;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#25968;&#30340;&#22238;&#28335;&#31574;&#30053;&#12290;&#36825;&#20123;&#23454;&#39564;&#39564;&#35777;&#20102;ANNEAL&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#36965;&#24863;CBIR&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.10107v2 Announce Type: replace  Abstract: Deep metric learning (DML) has shown to be effective for content-based image retrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a high number of annotated images to accurately learn model parameters of deep neural networks (DNNs). However, gathering such data is time-consuming and costly. To address this, we propose an annotation cost-efficient active learning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to create a small but informative training set made up of similar and dissimilar image pairs to be utilized for accurately learning a metric space. The informativeness of image pairs is evaluated by combining uncertainty and diversity criteria. To assess the uncertainty of image pairs, we introduce two algorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary classifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically estimates a threshold value that acts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#26080;&#30028;&#25551;&#25721;&#26694;&#26550;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#23450;&#21046;&#32467;&#26524;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20419;&#36827;&#20102;&#20013;&#24515;&#21644;&#21608;&#36793;&#22270;&#20687;&#21306;&#22495;&#30340;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2406.01059</link><description>&lt;p&gt;
VIP: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#26080;&#30028;&#22270;&#20687;&#25551;&#25721;
&lt;/p&gt;
&lt;p&gt;
VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.01059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#26080;&#30028;&#25551;&#25721;&#26694;&#26550;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#23450;&#21046;&#32467;&#26524;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20419;&#36827;&#20102;&#20013;&#24515;&#21644;&#21608;&#36793;&#22270;&#20687;&#21306;&#22495;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.01059v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#26412;&#25991;&#38024;&#23545;&#22270;&#20687;&#26080;&#30028;&#25551;&#25721;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#32473;&#23450;&#20013;&#24515;&#22270;&#20687;&#20869;&#23481;&#26102;&#65292;&#24310;&#20280;&#21608;&#36793;&#22270;&#20687;&#21306;&#22495;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#36866;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#33021;&#21147;&#38459;&#30861;&#20102;&#20854;&#22312;&#26356;&#24191;&#27867;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26080;&#30028;&#25551;&#25721;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#23450;&#21046;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#24182;&#32452;&#32455;&#32473;&#23450;&#22270;&#20687;&#30340;&#36974;&#32617;&#37096;&#20998;&#21644;&#38750;&#36974;&#32617;&#37096;&#20998;&#30340;&#30456;&#24212;&#25991;&#26412;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#25152;&#33719;&#24471;&#30340;&#25991;&#26412;&#25552;&#31034;&#34987;&#20171;&#32461;&#32473;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20197;&#36171;&#20104;&#20854;&#23450;&#21046;&#26080;&#30028;&#25551;&#25721;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#27530;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#8220;&#20013;&#24515;-&#24635;&#37327;-&#21608;&#36793;&#8221;&#65288;CTS&#65289;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.01059v2 Announce Type: replace  Abstract: In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirement of users. First of all, we take advantage of a Multimodal Large Language Model (MLLM) that automatically extracts and organizes the corresponding textual descriptions of the masked and unmasked part of a given image. Accordingly, the obtained text prompts are introduced to endow our model with the capacity to customize the outpainting results. In addition, a special Cross-Attention module, namely Center-Total-Surrounding (CTS), is elaborately designed to enhance further the the interact
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;SE3D&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19977;&#32500;&#25104;&#20687;&#20013;&#30340;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;3D CNN&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20215;&#25351;&#26631;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2405.14584</link><description>&lt;p&gt;
SE3D: &#19977;&#32500;&#25104;&#20687;&#20013;&#27880;&#24847;&#21147;&#26041;&#27861;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SE3D: A Framework For Saliency Method Evaluation In 3D Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SE3D&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19977;&#32500;&#25104;&#20687;&#20013;&#30340;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;3D CNN&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20215;&#25351;&#26631;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#22810;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20108;&#32500;&#25104;&#20687;&#20219;&#21153;&#20013;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#12290;&#29616;&#22312;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#27491;&#22312;&#25193;&#23637;&#21040;&#19977;&#32500;&#25104;&#20687;&#39046;&#22495;&#65292;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D CNNs&#65289;&#33021;&#22815;&#22788;&#29702;LiDAR&#12289;MRI&#21644;CT&#25195;&#25551;&#65292;&#36825;&#23545;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#25104;&#20687;&#31561;&#39046;&#22495;&#26377;&#30528;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#20123;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#21017;&#26497;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#21487;&#35299;&#37322;&#30340;&#26234;&#33021;AI&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#33268;&#21147;&#20110;&#35299;&#37322;3D CNN&#27169;&#22411;&#65292;&#32780;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#19981;&#20805;&#20998;&#30340;&#20108;&#32500;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;&#25193;&#23637;&#26469;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14584v2 Announce Type: replace  Abstract: For more than a decade, deep learning models have been dominating in various 2D imaging tasks. Their application is now extending to 3D imaging, with 3D Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and CT scans, with significant implications for fields such as autonomous driving and medical imaging. In these critical settings, explaining the model's decisions is fundamental. Despite recent advances in Explainable Artificial Intelligence, however, little effort has been devoted to explaining 3D CNNs, and many works explain these models via inadequate extensions of 2D saliency methods.   A fundamental limitation to the development of 3D saliency methods is the lack of a benchmark to quantitatively assess these on 3D data. To address this issue, we propose SE3D: a framework for Saliency method Evaluation in 3D imaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and evaluation metrics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20943;&#23569;&#23384;&#20648;&#30452;&#25509;&#24352;&#37327;&#29615;&#20998;&#35299;&#30340;&#26032;&#22411;CNNs&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#22823;&#24133;&#38477;&#20302;&#65292;&#24182;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2405.10802</link><description>&lt;p&gt;
&#20351;&#29992;&#20943;&#23569;&#23384;&#20648;&#30340;&#30452;&#25509;&#24352;&#37327;&#29615;&#20998;&#35299;&#25216;&#26415;&#21387;&#32553;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Reduced storage direct tensor ring decomposition for convolutional neural networks compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20943;&#23569;&#23384;&#20648;&#30452;&#25509;&#24352;&#37327;&#29615;&#20998;&#35299;&#30340;&#26032;&#22411;CNNs&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#22823;&#24133;&#38477;&#20302;&#65292;&#24182;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10802v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10802v2 Announce Type: replace  Abstract: Convolutional neural networks (CNNs) are among the most widely used machine learning models for computer vision tasks, such as image classification. To improve the efficiency of CNNs, many CNNs compressing approaches have been developed. Low-rank methods approximate the original convolutional kernel with a sequence of smaller convolutional kernels, which leads to reduced storage and time complexities. In this study, we propose a novel low-rank CNNs compression method that is based on reduced storage direct tensor ring decomposition (RSDTR). The proposed method offers a higher circular mode permutation flexibility, and it is characterized by large parameter and FLOPS compression rates, while preserving a good classification accuracy of the compressed network. The experiments, performed on the CIFAR-10 and ImageNet datasets, clearly demonstrate the efficiency of RSDTR in comparison to other state-of-the-art CNNs compression approaches.
&lt;/p&gt;</description></item><item><title>CoReX&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2405.01661</link><description>&lt;p&gt;
CoReX&#65306;&#22522;&#20110;&#20851;&#31995;&#30340;&#27010;&#24565;&#35299;&#37322;&#22120;&#22312;&#25506;&#32034;&#21644;&#35780;&#20215;&#20998;&#31867;&#22120;&#20915;&#23450;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01661
&lt;/p&gt;
&lt;p&gt;
CoReX&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01661v2&#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#22522;&#20110;&#36755;&#20837;&#20687;&#32032;&#30456;&#20851;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35299;&#37322;&#21487;&#33021;&#36807;&#20110;&#19981;&#20855;&#20307;&#65292;&#26080;&#27861;&#35780;&#20272;&#21738;&#20123;&#21644;&#22914;&#20309;&#36755;&#20837;&#29305;&#24449;&#24433;&#21709;&#27169;&#22411;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#31561;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#65292;&#29305;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#21450;&#20854;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#20250;&#21306;&#20998;&#20110;&#19981;&#21516;&#30340;&#31867;&#12290;&#20687;&#32032;&#30456;&#20851;&#24615;&#19981;&#36275;&#20197;&#20256;&#36798;&#27492;&#31867;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#35780;&#20215;&#21463;&#21040;&#38480;&#21046;&#65292;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#30456;&#20851;&#26041;&#38754;&#20197;&#21450;&#24433;&#21709;&#27169;&#22411;&#20915;&#31574;&#30340;&#22240;&#32032;&#21487;&#33021;&#20250;&#34987;&#24573;&#30053;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#35780;&#20215;CNN&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#35299;&#37322;&#22120;&#65288;CoReX&#65289;&#12290;&#23427;&#36890;&#36807;&#20174;&#20915;&#31574;&#36807;&#31243;&#20013;&#23631;&#34109;&#65288;&#19981;&#65289;&#30456;&#20851;&#30340;&#27010;&#24565;&#20197;&#21450;&#36890;&#36807;&#38480;&#21046;&#23398;&#20064;&#21040;&#30340;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#20851;&#31995;&#65292;&#23545;&#19968;&#32452;&#22270;&#20687;&#30340;&#39044;&#27979;&#34892;&#20026;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19988;&#35777;&#26126;CoReX&#33021;&#22815;&#20026;&#20915;&#31574;&#25552;&#20379;&#26356;&#26126;&#30830;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;CoReX&#33021;&#22815;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#21033;&#29992;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#35299;&#37322;&#26469;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#22522;&#20110;DNN&#30340;IQA&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35821;&#20041;&#29305;&#24449;&#26102;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2404.17762</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#36741;&#21161;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large Multi-modality Model Assisted AI-Generated Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.17762
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#22522;&#20110;DNN&#30340;IQA&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35821;&#20041;&#29305;&#24449;&#26102;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17762v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#25688;&#35201;&#65306;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;IQA&#65289;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;Transformer&#26469;&#23398;&#20064;&#36136;&#37327;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#22330;&#26223;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65288;AGI&#65289;&#26102;&#65292;&#36825;&#20123;&#22522;&#20110;DNN&#30340;IQA&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#24773;&#20917;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#26576;&#20123;AGI&#22266;&#26377;&#30340;&#35821;&#20041;&#19981;&#20934;&#30830;&#65292;&#36825;&#30001;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#25152;&#24341;&#36215;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#35780;&#20272;AGI&#30340;&#36136;&#37327;&#65292;&#33021;&#22815;&#20998;&#36776;&#20986;&#35821;&#20041;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#21442;&#25968;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#65292;&#20256;&#32479;&#22522;&#20110;DNN&#30340;IQA&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#31934;&#32454;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25226;&#25569;&#25972;&#20010;&#22270;&#20687;&#30340;&#35821;&#20041;&#20869;&#23481;&#30340;&#23384;&#22312;&#21644;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;IQA&#27169;&#22411;&#22312;&#35821;&#20041;&#20869;&#23481;&#24863;&#30693;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#22686;&#24378;AI&#22270;&#20687;&#36136;&#37327;&#30340;&#35780;&#20272;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17762v2 Announce Type: replace  Abstract: Traditional deep neural network (DNN)-based image quality assessment (IQA) models leverage convolutional neural networks (CNN) or Transformer to learn the quality-aware feature representation, achieving commendable performance on natural scene images. However, when applied to AI-Generated images (AGIs), these DNN-based IQA models exhibit subpar performance. This situation is largely due to the semantic inaccuracies inherent in certain AGIs caused by uncontrollable nature of the generation process. Thus, the capability to discern semantic content becomes crucial for assessing the quality of AGIs. Traditional DNN-based IQA models, constrained by limited parameter complexity and training data, struggle to capture complex fine-grained semantic features, making it challenging to grasp the existence and coherence of semantic content of the entire image. To address the shortfall in semantic content perception of current IQA models, we intro
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35201;&#28857;: Fact &#33539;&#24335;&#36890;&#36807;&#29983;&#25104;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#24544;&#23454;&#12289;&#31616;&#27905;&#12289;&#21487;&#36716;&#31227;&#35770;&#25454;&#65292;&#35299;&#38145;&#20102;&#20182;&#20204;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28145;&#23618;&#29702;&#35299;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#21162;&#21147;&#35299;&#20915;&#24403;&#21069;&#30340;MLLMs&#22312;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.11129</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: Fact: &#20351;&#29992;&#24544;&#23454;&#12289;&#31616;&#27905;&#19988;&#21487;&#36716;&#31227;&#30340;&#35770;&#25454;&#25945; LLM &#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.11129
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35201;&#28857;: Fact &#33539;&#24335;&#36890;&#36807;&#29983;&#25104;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#24544;&#23454;&#12289;&#31616;&#27905;&#12289;&#21487;&#36716;&#31227;&#35770;&#25454;&#65292;&#35299;&#38145;&#20102;&#20182;&#20204;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28145;&#23618;&#29702;&#35299;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#21162;&#21147;&#35299;&#20915;&#24403;&#21069;&#30340;MLLMs&#22312;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;: arXiv:2404.11129v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#35770;&#25991;&#25688;&#35201;: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (MLLMs) &#30340;&#21331;&#36234;&#24615;&#33021;&#26080;&#21487;&#20105;&#35758;&#22320;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#25512;&#29702;&#36807;&#31243;&#20173;&#28982;&#26159;&#26410;&#35299;&#20043;&#35868;&#65292;&#20351;&#23427;&#20204;&#19981;&#21487;&#35299;&#37322;&#65292;&#24182;&#19988;&#22312;&#24187;&#35273;&#26041;&#38754;&#25379;&#25166;&#12290;&#36825;&#20123;&#27169;&#22411;&#25191;&#34892;&#22797;&#26434;&#30340;&#32452;&#25104;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#20063;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#26368;&#32456;&#23548;&#33268;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#20572;&#28382;&#19981;&#21069;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Fact&#30340;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#20026;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#12289;&#31616;&#27905;&#19988;&#21487;&#36716;&#31227;&#30340;&#35770;&#25454;&#12290;&#36825;&#31181;&#33539;&#24335;&#20351;&#29992;&#20102;&#21487;&#39564;&#35777;&#30340;&#35270;&#35273;&#32534;&#31243;&#26469;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#65292;&#20445;&#35777;&#20102;&#24544;&#23454;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#65292;&#21253;&#25324;&#20462;&#21098;&#12289;&#21512;&#24182;&#21644;&#26725;&#25509;&#65292;&#29702;&#30001;&#22686;&#24378;&#20102;&#20854;&#31616;&#27905;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36807;&#28388;&#20102;&#26234;&#33021;&#20307;&#30340;&#35770;&#25454;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#25317;&#26377;&#26356;&#21152;&#20016;&#23500;&#30340;&#30693;&#35782;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#38598;&#25104;&#20102;MLLMs+Fact&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20855;&#20307;&#20248;&#21183;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25506;&#35752;&#12290;&#20107;&#23454;&#36825;&#19968;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#27492;&#26469;&#35299;&#38145;&#22823;&#22411;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28145;&#23618;&#29702;&#35299;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11129v2 Announce Type: replace  Abstract: The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks. Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision. Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21367;&#31215;&#32593;&#32476;&#30340;&#22825;&#21306;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;GNSS&#20449;&#21495;&#30340;NLOS&#26816;&#27979;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Sky-GVIO&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#21512;GNSS&#12289;INS&#21644;&#35270;&#35273;&#29305;&#24449;&#23454;&#29616;&#20102;&#22312;&#22478;&#24066;&#23777;&#35895;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#21644;&#31934;&#30830;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2404.11070</link><description>&lt;p&gt;
&#22825;&#31354;&#20013;&#22686;&#24378;&#22411;GNSS/INS/&#35270;&#35273;&#23548;&#33322;&#31995;&#32479;&#65306;&#22478;&#24066;&#23777;&#35895;&#20013;&#39118;&#36710;&#32593;&#32476;&#32467;&#26500;&#22522;&#20110;&#22825;&#21306;&#20998;&#21106;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based sky-segmentation in urban canyon
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21367;&#31215;&#32593;&#32476;&#30340;&#22825;&#21306;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;GNSS&#20449;&#21495;&#30340;NLOS&#26816;&#27979;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Sky-GVIO&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#21512;GNSS&#12289;INS&#21644;&#35270;&#35273;&#29305;&#24449;&#23454;&#29616;&#20102;&#22312;&#22478;&#24066;&#23777;&#35895;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#21644;&#31934;&#30830;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11070v2&#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#25688;&#35201;&#65306;&#31934;&#30830;&#12289;&#36830;&#32493;&#21644;&#21487;&#38752;&#30340;&#23450;&#20301;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24314;&#31569;&#29289;&#12289;&#26641;&#26408;&#21644; elevated structures &#31561;&#21453;&#24120;&#35270;&#36335;&#65288;NLOS&#65289;&#30340;&#22797;&#26434;&#22478;&#24066;&#23777;&#35895;&#29615;&#22659;&#20013;&#65292;&#21333;&#20010;&#20256;&#24863;&#22120;&#30340;&#33030;&#24369;&#24615;&#21644;&#35270;&#37326;&#22806;&#65288;NLOS&#65289;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#20102;&#23450;&#20301;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21367;&#31215;&#32593;&#32476;&#65288;FCN&#65289;&#30340;&#22825;&#21306;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;GNSS NLOS&#26816;&#27979;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#19968;&#31181;&#21517;&#20026;S-NDM&#30340;NLOS&#26816;&#27979;&#21644;&#32531;&#35299;&#31639;&#27861;&#65292;&#23558;&#20854;&#19982;&#32039;&#23494;&#32806;&#21512;&#30340;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#35270;&#35273;&#29305;&#24449;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;Sky-GVIO&#65292;&#26088;&#22312;&#23454;&#29616;&#22478;&#24066;&#23777;&#35895;&#29615;&#22659;&#20013;&#36830;&#32493;&#21644;&#31934;&#30830;&#30340;&#23450;&#20301;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#23558;&#21333;&#28857;&#23450;&#20301;&#65288;SPP&#65289;&#19982;&#23454;&#26102;&#21160;&#24577;&#65288;RTK&#65289;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#20854;&#36816;&#34892;&#30340;&#21487;&#38752;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11070v2 Announce Type: replace  Abstract: Accurate, continuous, and reliable positioning is a critical component of achieving autonomous driving. However, in complex urban canyon environments, the vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused by high buildings, trees, and elevated structures seriously affect positioning results. To address these challenges, a sky-view images segmentation algorithm based on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection. Building upon this, a novel NLOS detection and mitigation algorithm (named S-NDM) is extended to the tightly coupled Global Navigation Satellite Systems (GNSS), Inertial Measurement Units (IMU), and visual feature system which is called Sky-GVIO, with the aim of achieving continuous and accurate positioning in urban canyon environments. Furthermore, the system harmonizes Single Point Positioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its operational ver
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;PET/CT&#21327;&#21516;&#25104;&#20687;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21058;&#37327;&#25104;&#20687;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2404.08748</link><description>&lt;p&gt;
&#22810;&#20998;&#25903;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#36890;&#36947;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;PET/CT&#21327;&#21516;&#37325;&#24314;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Synergistic Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.08748
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;PET/CT&#21327;&#21516;&#25104;&#20687;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21058;&#37327;&#25104;&#20687;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22810;&#20998;&#25903;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#21307;&#23398;&#22270;&#20687;&#21327;&#21516;&#37325;&#24314;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#21516;&#19968;&#26102;&#38388;&#22788;&#29702;&#25104;&#23545;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#21435;&#22122;&#21644;&#37325;&#24314;&#12290;&#21327;&#21516;&#22270;&#20687;&#37325;&#24314;&#26159;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#32435;&#20837;&#19968;&#20010;&#35780;&#20215;&#22270;&#20687;&#19982;&#27169;&#22411;&#36317;&#31163;&#30340;&#27491;&#35268;&#21270;&#22120;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#22312;MNIST&#21644;Positron Emission Tomography&#65288;PET&#65289;/Computed Tomography&#65288;CT&#65289;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#20302;&#21058;&#37327;&#25104;&#20687;&#26041;&#38754;&#30340;&#22270;&#20687;&#36136;&#37327;&#25552;&#21319;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22914;&#22359;&#20998;&#35299;&#21644;&#27169;&#22411;&#38480;&#21046;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#21307;&#23398;&#25104;&#20687;&#37325;&#24314;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.08748v2 Announce Type: replace-cross  Abstract: This paper presents a novel approach for learned synergistic reconstruction of medical images using multi-branch generative models. Leveraging variational autoencoders (VAEs), our model learns from pairs of images simultaneously, enabling effective denoising and reconstruction. Synergistic image reconstruction is achieved by incorporating the trained models in a regularizer that evaluates the distance between the images and the model. We demonstrate the efficacy of our approach on both Modified National Institute of Standards and Technology (MNIST) and positron emission tomography (PET)/computed tomography (CT) datasets, showcasing improved image quality for low-dose imaging. Despite challenges such as patch decomposition and model limitations, our results underscore the potential of generative models for enhancing medical imaging reconstruction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;OpenBias&#65292;&#19968;&#20010;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#24320;&#25918;&#38598;&#20559;&#35265;&#30340;&#31995;&#32479;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#24046;&#38598;&#21512;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#24320;&#25918;&#38598;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.07990</link><description>&lt;p&gt;
&#24320;&#25918;&#20559;&#35265;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#38598;&#20559;&#24046;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OpenBias: Open-set Bias Detection in Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;OpenBias&#65292;&#19968;&#20010;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#24320;&#25918;&#38598;&#20559;&#35265;&#30340;&#31995;&#32479;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#24046;&#38598;&#21512;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#24320;&#25918;&#38598;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07990v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#19988;&#23545;&#26222;&#36890;&#22823;&#20247;&#36234;&#26469;&#36234;&#21487;&#35775;&#38382;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#28145;&#20837;&#25506;&#31350;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36991;&#20813;&#20256;&#25773;&#21644;&#24310;&#32493;&#20219;&#20309;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26816;&#27979;&#39044;&#20808;&#23450;&#20041;&#30340;&#26377;&#38480;&#38598;&#20559;&#24046;&#65292;&#38480;&#21046;&#30740;&#31350;&#23616;&#38480;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#38598;&#20559;&#24046;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;OpenBias&#65292;&#19968;&#20010;&#26032;&#30340;&#31649;&#36947;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#20219;&#20309;&#39044;&#32534;&#35793;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21644;&#37327;&#21270;&#20559;&#35265;&#30340;&#20005;&#37325;&#24615;&#12290;OpenBias&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22522;&#20110;&#19968;&#32452;&#25552;&#31034;&#25552;&#20986;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#30446;&#26631;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#30456;&#21516;&#30340;&#25552;&#31034;&#38598;&#29983;&#25104;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#19968;&#20010;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#35782;&#21035;&#20808;&#21069;&#25552;&#35758;&#20559;&#24046;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#23545;11&#20010;&#31867;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#21253;&#25324;&#8220;&#20154;&#31181;&#8221;&#12289;&#8220;&#24615;&#21035;&#8221;&#21644;&#8220;&#32463;&#27982;&#29366;&#24577;&#8221;&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;OpenBias&#33021;&#22815;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#24320;&#25918;&#38598;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#26080;&#27861;&#22788;&#29702;&#30340;&#24320;&#25918;&#38598;&#38382;&#39064;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#20013;&#23454;&#38469;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#23450;&#37327;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07990v2 Announce Type: replace  Abstract: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previou
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#21644;&#27867;&#21270;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#24418;&#29366;&#20989;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#21435;&#38500;&#20102;&#8220;&#40657;&#31665;&#8221;&#25928;&#24212;&#65292;&#20351;&#24471;&#26041;&#27861;&#26356;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#21644;&#27867;&#21270;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#24418;&#29366;&#20989;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#21435;&#38500;&#20102;&#8220;&#40657;&#31665;&#8221;&#25928;&#24212;&#65292;&#20351;&#24471;&#26041;&#27861;&#26356;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#20248;&#31168;&#30340;&#25968;&#25454;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#29615;&#22659;&#34920;&#31034;&#30340;&#36136;&#37327;&#30452;&#25509;&#24433;&#21709;&#20102;&#23398;&#20064;&#20219;&#21153;&#30340;&#25104;&#21151;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#26174;&#24335;&#25110;&#38544;&#24335;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#29615;&#22659;&#65292;&#22914;&#22270;&#20687;&#12289;&#28857;&#12289;&#20307;&#32032;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#26041;&#27861;&#23384;&#22312;&#33509;&#24178;&#32570;&#28857;&#12290;&#23427;&#20204;&#35201;&#20040;&#26080;&#27861;&#25551;&#36848;&#22797;&#26434;&#30340;&#23616;&#37096;&#20960;&#20309;&#65292;&#35201;&#20040;&#22312;&#20174;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#25110;&#32773;&#38656;&#35201;&#31934;&#30830;&#30340;&#32972;&#26223;&#36974;&#32617;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#23601;&#20687;&#26159;&#19968;&#20010;&#8220;&#40657;&#31665;&#8221;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;3D&#39640;&#26031;&#28857;&#31215;(3DGS)&#20316;&#20026;&#19968;&#31181;&#26174;&#24335;&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#21487;&#24494;&#30340;&#28210;&#26579;&#26041;&#27861;&#65292;&#34987;&#35748;&#20026;&#26159;&#23545;&#37325;&#25490;&#21644;&#34920;&#31034;&#26041;&#27861;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#22330;&#26223;&#25551;&#36848;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#24418;&#29366;&#20989;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#21644;&#21435;&#8220;&#40657;&#31665;&#8221;&#65292;&#20174;&#32780;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>VRSO&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#21442;&#32771;&#28145;&#24230;&#65292;&#22312;3D&#31354;&#38388;&#20013;&#20934;&#30830;&#24674;&#22797;&#38745;&#24577;&#23545;&#35937;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38590;&#20197;&#25429;&#25417;&#30340;&#32454;&#33410;&#65292;&#22312;Waymo Open Dataset&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.15026</link><description>&lt;p&gt;
VRSO&#65306;&#22522;&#20110;&#35270;&#35273;&#30340;&#38745;&#24577;&#23545;&#35937;&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VRSO: Visual-Centric Reconstruction for Static Object Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15026
&lt;/p&gt;
&lt;p&gt;
VRSO&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#21442;&#32771;&#28145;&#24230;&#65292;&#22312;3D&#31354;&#38388;&#20013;&#20934;&#30830;&#24674;&#22797;&#38745;&#24577;&#23545;&#35937;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38590;&#20197;&#25429;&#25417;&#30340;&#32454;&#33410;&#65292;&#22312;Waymo Open Dataset&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15026v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#20316;&#20026;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#32467;&#26524;&#30340;&#19968;&#37096;&#20998;&#65292;&#31354;&#38388;&#20013;&#38745;&#24577;&#23545;&#35937;&#30340;&#26816;&#27979;&#65288;SOD&#65289;&#20026;&#39550;&#39542;&#29615;&#22659;&#29702;&#35299;&#25552;&#20379;&#20102;&#20851;&#38190;&#25552;&#31034;&#12290;&#38543;&#30528;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;SOD&#20219;&#21153;&#19978;&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#23545;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#38656;&#27714;&#27491;&#22312;&#39129;&#21319;&#12290;&#20256;&#32479;&#19988;&#21487;&#38752;&#30340;&#26041;&#27861;&#26159;&#23545;&#23494;&#38598;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#21644;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#25163;&#21160;&#26631;&#35760;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#20844;&#20849;&#39550;&#39542;&#25968;&#25454;&#38598;&#37117;&#37319;&#29992;&#36825;&#31181;&#26041;&#24335;&#26469;&#25552;&#20379;SOD&#30340; ground truth (GT)&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20173;&#28982;&#38750;&#24120;&#26114;&#36149;&#21644;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;VRSO&#65292;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#38745;&#24577;&#23545;&#35937;&#26631;&#27880;&#26041;&#27861;&#12290;&#22312;Waymo Open Dataset&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;VRSO&#26631;&#27880;&#30340;&#37325;&#25237;&#24433;&#35823;&#24046;&#21482;&#26377;2.6&#20010;&#20687;&#32032;&#65292;&#22823;&#32422;&#26159;Waymo Open Dataset&#26631;&#31614;&#65288;10.6&#20687;&#32032;&#65289;&#30340;&#22235;&#20493;&#12290;VRSO&#22312;&#25104;&#26412;&#20302;&#12289;&#25928;&#29575;&#39640;&#21644;&#36136;&#37327;&#39640;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65306;&#65288;1&#65289;&#23427;&#21482;&#38656;&#35201;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#21442;&#32771;&#28145;&#24230;&#65292;&#23601;&#33021;&#22312;3D&#31354;&#38388;&#20013;&#24674;&#22797;&#38745;&#24577;&#23545;&#35937;&#65307;&#65288;2&#65289;&#19982;&#20854;&#20182;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;VRSO&#33021;&#22815;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#65307;&#65288;3&#65289;VRSO&#36824;&#33021;&#22815;&#22788;&#29702;&#38590;&#20197;&#25429;&#25417;&#30340;&#32454;&#33410;&#65292;&#22914;&#34892;&#20154;&#21644;&#38750;&#22266;&#23450;&#30340;&#20132;&#36890;&#26631;&#24535;&#12290;&#27492;&#22806;&#65292;VRSO&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#36807;&#31243;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#26159;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24182;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#36890;&#36807;&#30495;&#23454;&#36710;&#36742;&#36827;&#34892;&#27979;&#35797;&#12290;&#30446;&#21069;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;VRSO&#65292;&#20320;&#21487;&#20197;&#22312;arXiv&#19978;&#33719;&#21462;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#24182;&#22312;&#26410;&#26469;&#30340;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15026v2 Announce Type: replace  Abstract: As a part of the perception results of intelligent driving systems, static object detection (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labelling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive and time-consuming in practice. This paper introduces VRSO, a visual-centric approach for static object annotation. Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static objects in 3D space with only camer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SSAP&#65288;Shape-Sensitive Adversarial Patch&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20013;&#20840;&#38754;&#30772;&#22351;&#21333;&#30446;Depth&#20272;&#35745;&#65288;MDE&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#24418;&#29366;&#24863;&#30693;&#29305;&#24615;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#25925;&#24847;&#35823;&#23548; MDE &#31995;&#32479;&#65292;&#20351;&#20854;&#22312;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#20013;&#20173;&#28982;&#26377;&#25928;&#12290;&#23454;&#39564;&#34920;&#26126; SSAP &#33021;&#22815;&#35825;&#23548;&#28145;&#24230;&#20272;&#35745;&#38169;&#35823;&#21644;&#34892;&#20026;&#20915;&#31574;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.11515</link><description>&lt;p&gt;
SSAP&#65306;&#38754;&#21521;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#30340;&#24418;&#29366;&#25935;&#24863;&#23545;&#25239;&#24615;&#34917;&#19969;&#20840;&#38754;&#30772;&#22351;&#21333;&#30446; Depth &#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SSAP&#65288;Shape-Sensitive Adversarial Patch&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20013;&#20840;&#38754;&#30772;&#22351;&#21333;&#30446;Depth&#20272;&#35745;&#65288;MDE&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#24418;&#29366;&#24863;&#30693;&#29305;&#24615;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#25925;&#24847;&#35823;&#23548; MDE &#31995;&#32479;&#65292;&#20351;&#20854;&#22312;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#20013;&#20173;&#28982;&#26377;&#25928;&#12290;&#23454;&#39564;&#34920;&#26126; SSAP &#33021;&#22815;&#35825;&#23548;&#28145;&#24230;&#20272;&#35745;&#38169;&#35823;&#21644;&#34892;&#20026;&#20915;&#31574;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11515v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#24050;&#32463;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#25972;&#21512;&#20197;&#21450;&#26368;&#36817;&#65292;Transformer &#30340;&#20351;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#30340;&#25285;&#24551;&#24050;&#32463;&#20986;&#29616;&#12290;&#35780;&#20272;&#22522;&#20110; CNN &#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#35825;&#23548;&#23545;&#35270;&#35273;&#31995;&#32479;&#30340;&#20840;&#38754;&#30772;&#22351;&#26041;&#38754;&#24050;&#32463;&#19981;&#36275;&#65292;&#24448;&#24448;&#38480;&#20110;&#29305;&#23450;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; SSAP&#65288;Shape-Sensitive Adversarial Patch&#65289;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20013;&#20840;&#38754;&#30772;&#22351;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#12290;&#25105;&#20204;&#30340;&#34917;&#19969;&#34987;&#31934;&#24515;&#35774;&#35745;&#25104;&#20197;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#30772;&#22351; MDE&#65306;&#36890;&#36807;&#25197;&#26354;&#20272;&#35745;&#30340;&#36317;&#31163;&#25110;&#36890;&#36807;&#22312;&#31995;&#32479;&#35270;&#35282;&#20013;&#21019;&#24314;&#29289;&#20307;&#28040;&#22833;&#30340;&#38169;&#35273;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#34917;&#19969;&#20855;&#26377;&#24418;&#29366;&#24863;&#30693;&#29305;&#24322;&#24615;&#65292;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#24433;&#21709; MDE&#65292;&#21363;&#20351;&#22312;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#20013;&#20063;&#20445;&#25345;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSAP &#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#22810;&#20010;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#35825;&#23548;&#28145;&#24230;&#20272;&#35745;&#30340;&#38169;&#35823;&#21644;&#34892;&#20026;&#20915;&#31574;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#29702;&#35299;&#28145;&#24230;&#20272;&#35745;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#28145;&#21051;&#30340;&#27934;&#35265;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#31574;&#30053;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11515v2 Announce Type: replace-cross  Abstract: Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motion Mamba&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#36816;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;U-Net&#26550;&#26500;&#20013;&#20351;&#29992;&#20102;&#23618;&#27425;&#26102;&#24207;Mamba&#65288;HTM&#65289;&#22359;&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#37319;&#29992;&#31227;&#21160;&#24179;&#22343;&#39044;&#27979;&#20197;&#20445;&#25345;&#36816;&#21160;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07487</link><description>&lt;p&gt;
Motion Mamba&#65306;&#39640;&#25928;&#19988;&#38271;&#24207;&#21015;&#30340;&#36816;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Motion Mamba: Efficient and Long Sequence Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07487
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motion Mamba&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#36816;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;U-Net&#26550;&#26500;&#20013;&#20351;&#29992;&#20102;&#23618;&#27425;&#26102;&#24207;Mamba&#65288;HTM&#65289;&#22359;&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#37319;&#29992;&#31227;&#21160;&#24179;&#22343;&#39044;&#27979;&#20197;&#20445;&#25345;&#36816;&#21160;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07487v4 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#29983;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#20154;&#31867;&#36816;&#21160;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36861;&#27714;&#65292;&#32780;&#23454;&#29616;&#38271;&#26399;&#24207;&#21015;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38271;&#26399;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;Mamba&#65292;&#24050;&#32463;&#22312;&#30828;&#20214;&#21451;&#22909;&#30340;&#35774;&#35745;&#19978;&#23637;&#31034;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#65292;&#36825;&#20284;&#20046;&#26159;&#24314;&#31435;&#22312;&#36816;&#21160;&#29983;&#25104;&#27169;&#22411;&#20043;&#19978;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23558;SSM&#24212;&#29992;&#20110;&#36816;&#21160;&#29983;&#25104;&#38754;&#20020;&#38556;&#30861;&#65292;&#22240;&#20026;&#32570;&#20047;&#19968;&#31181;&#19987;&#38376;&#30340;&#35774;&#35745;&#26550;&#26500;&#26469;&#24314;&#27169;&#36816;&#21160;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Motion Mamba&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23637;&#29616;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#36816;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;SSM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23618;&#27425;&#26102;&#24207;Mamba&#65288;HTM&#65289;&#22359;&#65292;&#29992;&#20110;&#36890;&#36807;U-Net&#26550;&#26500;&#20013;&#38598;&#25104;&#30340;&#21508;&#31181;&#38548;&#31163;SSM&#27169;&#22359;&#26469;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36825;&#20123;&#27169;&#22359;&#30340;&#22823;&#23567;&#22312;&#32479;&#35745;&#23545;&#31216;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#26088;&#22312;&#20445;&#30041;&#24103;&#20043;&#38388;&#30340;&#36816;&#21160;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32500;&#25252;&#36816;&#21160;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#31227;&#21160;&#24179;&#22343;&#39044;&#27979;&#20197;&#20943;&#23569;&#38271;&#26102;&#38388;&#38388;&#38548;&#20013;&#30340;&#36816;&#21160;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#36816;&#21160;&#26102;&#20351;&#29992;&#20102;&#20195;&#29702;&#32534;&#30721;&#22120;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#36816;&#21160;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;Motion Mamba&#22312;&#38271;&#26399;&#36816;&#21160;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#36816;&#21160;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07487v4 Announce Type: replace  Abstract: Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between fr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#21069;&#26223;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#24341;&#23548;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#20102;&#21512;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
&#12298;PrimeComposer: &#20351;&#29992;&#27880;&#24847;&#21147;&#24341;&#23548;&#21152;&#36895;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#12299;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#21069;&#26223;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#24341;&#23548;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#20102;&#21512;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#24403;&#21069;&#30340;&#35757;&#32451;&#20813;&#36153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#25552;&#21462;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#25351;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26159;&#28304;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#19968;&#33268;&#24615;&#30340;&#22256;&#24785;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#23427;&#20204;&#36807;&#20998;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#26102;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#21363;&#20351;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#24517;&#35201;&#30340;&#20219;&#21153;&#12290;&#36825;&#19981;&#20165;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#23454;&#26045;&#65292;&#32780;&#19988;&#20063;&#24433;&#21709;&#20102;&#21069;&#26223;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#33402;&#26415;&#39068;&#26009;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#35270;&#20026;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#30340;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27425;&#32534;&#36753;&#20013;&#65292;&#25152;&#32534;&#36753;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#32500;&#25345;&#22330;&#26223;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#20313;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24555;&#30340;&#21069;&#26223;&#20026;&#20027;&#25552;&#21319;&#27880;&#24847;&#21147;&#24341;&#23548;&#21152;&#36895;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v2 Announce Type: replace  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. Current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only impedes their swift implementation but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; NiNformer&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#32593;&#32476;&#22312;&#32593;&#32476; (Network in Network) &#21644; Transformer &#26550;&#26500;&#30340;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992; Token &#28151;&#21512;&#20316;&#20026;&#38376;&#25511;&#20989;&#25968;&#29983;&#25104;&#22120;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20943;&#23569;&#25968;&#25454;&#38598;&#22823;&#23567;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20351;&#29992; Token &#28151;&#21512;&#20316;&#20026;&#38376;&#25511;&#20989;&#25968;&#29983;&#25104;&#22120;&#30340;&#32593;&#32476;&#22312;&#32593;&#32476; transformer
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; NiNformer&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#32593;&#32476;&#22312;&#32593;&#32476; (Network in Network) &#21644; Transformer &#26550;&#26500;&#30340;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992; Token &#28151;&#21512;&#20316;&#20026;&#38376;&#25511;&#20989;&#25968;&#29983;&#25104;&#22120;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20943;&#23569;&#25968;&#25454;&#38598;&#22823;&#23567;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v5 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#27880;&#24847;&#26426;&#21046;&#19968;&#30452;&#26159; transformer &#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#24182;&#24341;&#39046;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#21644;&#22810;&#31181;&#20219;&#21153;&#12290;&#27880;&#24847;&#26426;&#21046;&#34987;&#29992;&#20316;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340; Vision Transformer ViT&#65292;&#20854;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;&#22810;&#20010;&#35270;&#35273;&#39046;&#22495;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#36825;&#20010;&#26426;&#21046;&#34429;&#28982;&#34920;&#36798;&#33021;&#21147;&#38750;&#24120;&#24378;&#65292;&#20294;&#23427;&#20063;&#26377;&#19968;&#20123;&#19981;&#36275;&#65292;&#22914;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#38656;&#35201;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#32531;&#35299;&#23545;&#25968;&#25454;&#22823;&#23567;&#30340;&#35201;&#27714;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#19968;&#20123;&#36825;&#26041;&#38754;&#30340;&#23581;&#35797;&#21253;&#25324; MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO &#31561;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20316;&#20026;&#38376;&#25511;&#20989;&#25968;&#29983;&#25104;&#22120;&#30340;&#26032;&#30340;&#35745;&#31639;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#39044;&#22788;&#29702;&#38450;&#24481;&#32593;&#32476;AADN&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#20687;&#19978;&#36827;&#34892;&#38450;&#24481;&#24615;&#36716;&#25442;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#30446;&#26631;&#36319;&#36394;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17976</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#23545;&#25239;&#38450;&#24481;&#32593;&#32476;&#22686;&#24378;&#36319;&#36394;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#39044;&#22788;&#29702;&#38450;&#24481;&#32593;&#32476;AADN&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#20687;&#19978;&#36827;&#34892;&#38450;&#24481;&#24615;&#36716;&#25442;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#30446;&#26631;&#36319;&#36394;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17976v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#35270;&#35273;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#24341;&#20837;&#32905;&#30524;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24050;&#32463;&#20005;&#37325;&#38477;&#20302;&#20102;&#39640;&#32423;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#30446;&#26631;&#36319;&#36394;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36741;&#21161;&#39044;&#22788;&#29702;&#38450;&#24481;&#32593;&#32476;AADN&#65292;&#23427;&#22312;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#38450;&#24481;&#24615;&#36716;&#25442;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#33267;&#36319;&#36394;&#22120;&#20043;&#21069;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#20854;&#20182;&#35270;&#35273;&#36319;&#36394;&#22120;&#20013;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#32780;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#35757;&#32451;AADN&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;DuAL-Loss&#29983;&#25104;&#21516;&#26102;&#25915;&#20987;&#36319;&#36394;&#22120;&#20998;&#31867;&#21644;&#22238;&#24402;&#20998;&#25903;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;OTB100&#12289;LaSOT&#21644;VOT2018&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;AADN&#22312;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#30340;&#38450;&#24481;&#40065;&#26834;&#24615;&#32500;&#25345;&#24471;&#24456;&#22909;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36319;&#36394;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17976v3 Announce Type: replace  Abstract: Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. However, there is still a lack of research on designing adversarial defense methods for object tracking. To address these issues, we propose an effective auxiliary pre-processing defense network, AADN, which performs defensive transformations on the input images before feeding them into the tracker. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without parameter adjustments. We train AADN using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that AADN maintains excellent defense robustness against adversarial att
&lt;/p&gt;</description></item><item><title>AVS-Net&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20307;&#32032;&#22823;&#23567;&#28857;&#37319;&#26679;&#26041;&#27861;&#65292;&#23427;&#25552;&#39640;&#20102;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#39640;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17521</link><description>&lt;p&gt;
AVS-Net&#65306;&#33258;&#36866;&#24212;&#20307;&#32032;&#22823;&#23567;&#28857;&#37319;&#26679;&#32593;&#32476;&#29992;&#20110;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17521
&lt;/p&gt;
&lt;p&gt;
AVS-Net&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20307;&#32032;&#22823;&#23567;&#28857;&#37319;&#26679;&#26041;&#27861;&#65292;&#23427;&#25552;&#39640;&#20102;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#39640;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17521v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#28857;&#20113;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#26234;&#33021;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#22312;3D&#29615;&#22659;&#29702;&#35299;&#26041;&#38754;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22823;&#35268;&#27169;3D&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#27492;&#65292;&#39640;&#25928;&#30340;&#28857;&#20113;&#38477;&#37319;&#26679;&#26041;&#27861;&#26159;&#28857;&#20113;&#23398;&#20064;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#29616;&#26377;&#30340;&#38477;&#37319;&#26679;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#35201;&#20040;&#29306;&#29298;&#20102;&#31934;&#32454;&#30340;&#31435;&#20307;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32423;&#37319;&#26679;&#22120;&#65292;&#23427;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#25928;&#29575;&#30340;&#32467;&#21512;&#12290;&#25552;&#20986;&#30340;&#35813;&#26041;&#27861;&#20197;&#20307;&#32032;&#20013;&#24515;&#28857;&#37319;&#26679;&#20026;&#22522;&#30784;&#65292;&#21364;&#33021;&#26377;&#25928;&#22320;&#20811;&#26381;&#20307;&#32032;&#22823;&#23567;&#30830;&#23450;&#21644;&#20851;&#38190;&#20960;&#20309;&#29305;&#24449;&#20445;&#23384;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20307;&#32032;&#36866;&#24212;&#27169;&#22359;&#65292;&#23427;&#26681;&#25454;&#28857;&#20113;&#22522;&#30340;&#38477;&#37319;&#26679;&#27604;&#20363;&#36866;&#24403;&#22320;&#35843;&#25972;&#20307;&#32032;&#22823;&#23567;&#65292;&#30830;&#20445;&#20102;&#37319;&#26679;&#32467;&#26524;&#22312;&#29702;&#35299;&#21464;&#37327;&#22330;&#26223;&#26041;&#38754;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17521v3 Announce Type: replace  Abstract: The recent advancements in point cloud learning have enabled intelligent vehicles and robots to comprehend 3D environments better. However, processing large-scale 3D scenes remains a challenging problem, such that efficient downsampling methods play a crucial role in point cloud learning. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. For such purpose, this paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel centroid sampling as a foundation but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures that the sampling results exhibit a favorable distribution for comprehending var
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#30340;&#19977;&#28857;&#37319;&#26679;&#21644;&#25490;&#24207;&#26426;&#21046;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#27604;&#20363;&#19979;&#23454;&#29616;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16598</link><description>&lt;p&gt;
PCR-99: &#19968;&#31181;&#36866;&#29992;&#20110;99%&#24322;&#24120;&#20540;&#28857;&#20113;&#27880;&#20876;&#30340;&#23454;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCR-99: A Practical Method for Point Cloud Registration with 99 Percent Outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#30340;&#19977;&#28857;&#37319;&#26679;&#21644;&#25490;&#24207;&#26426;&#21046;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#27604;&#20363;&#19979;&#23454;&#29616;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#28857;&#20113;&#27880;&#20876;&#38382;&#39064;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#30693;&#27604;&#20363;&#23610;&#21644;&#26497;&#39640;&#30340;&#24322;&#24120;&#20540;&#27604;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PCR-99&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#19977;&#28857;&#37319;&#26679;&#26041;&#27861;&#30340;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#20004;&#32773;&#30340;&#21019;&#26032;&#26426;&#21046;&#65306;&#65288;1&#65289;&#22522;&#20110;&#25104;&#23545;&#27604;&#20363;&#23610;&#19968;&#33268;&#24615;&#30340;&#26679;&#26412;&#25490;&#24207;&#65292;&#20248;&#20808;&#32771;&#34385;&#26356;&#26377;&#21487;&#33021;&#25104;&#20026;&#20869;&#28857;&#30340;&#28857;&#23545;&#24212;&#20851;&#31995;&#65307;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#19977;&#20803;&#32452;&#27604;&#20363;&#23610;&#19968;&#33268;&#24615;&#30340;&#39640;&#25928;&#24322;&#24120;&#20540;&#25298;&#32477;&#26041;&#26696;&#65292;&#39044;&#20808;&#31579;&#36873;&#19981;&#33391;&#26679;&#26412;&#65292;&#20943;&#23569;&#38656;&#35201;&#34987;&#27979;&#35797;&#30340;&#20551;&#35774;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#22312;98%&#24322;&#24120;&#20540;&#27604;&#20363;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#22312;99%&#24322;&#24120;&#20540;&#27604;&#20363;&#26102;&#65292;&#23427;&#22312;&#24050;&#30693;&#27604;&#20363;&#23610;&#21644;&#26410;&#30693;&#27604;&#20363;&#23610;&#38382;&#39064;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#23588;&#20854;&#26159;&#22312;&#21518;&#32773;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#40065;&#26834;&#24615;&#21644;&#36895;&#24230;&#19978;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16598v5 Announce Type: replace-cross  Abstract: We propose a robust method for point cloud registration that can handle both unknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a deterministic 3-point sampling approach with two novel mechanisms that significantly boost the speed: (1) an improved ordering of the samples based on pairwise scale consistency, prioritizing the point correspondences that are more likely to be inliers, and (2) an efficient outlier rejection scheme based on triplet scale consistency, prescreening bad samples and reducing the number of hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio, the proposed method achieves comparable performance to the state of the art. At 99% outlier ratio, however, it outperforms the state of the art for both known-scale and unknown-scale problems. Especially for the latter, we observe a clear superiority in terms of robustness and speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#20167;&#24680;meme&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#24403;&#21069;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#26410;&#26469;&#30740;&#31350;&#26377;&#28508;&#21147;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#25193;&#22823;&#20854;&#22312;&#32593;&#32476;&#31354;&#38388;&#20167;&#24680;&#20869;&#23481;&#26816;&#27979;&#21644;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;VLM&#22312;&#20167;&#24680;meme&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#25105;&#20204;&#36824;&#19981;&#36275;&#20197;&#24212;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#20167;&#24680;meme&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#24403;&#21069;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#26410;&#26469;&#30740;&#31350;&#26377;&#28508;&#21147;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#25193;&#22823;&#20854;&#22312;&#32593;&#32476;&#31354;&#38388;&#20167;&#24680;&#20869;&#23481;&#26816;&#27979;&#21644;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v2 &#26356;&#25913;&#28040;&#24687;&#31867;&#22411;&#65306;&#26367;&#25442;&#35777;&#20070;  &#25688;&#35201;&#26356;&#25913;&#36890;&#30693;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;meme&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#24418;&#24335;&#36234;&#26469;&#36234;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#19981;&#33391;&#29992;&#25143;&#21033;&#29992;meme&#26469;&#25915;&#20987;&#20010;&#20154;&#25110;&#33030;&#24369;&#30340;&#31038;&#21306;&#65292;&#36825;&#20351;&#24471;&#35782;&#21035;&#21644;&#35299;&#20915;&#36825;&#31867;&#20167;&#24680;meme&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#22810;&#31181;&#20167;&#24680;meme&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#30528;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#31614;&#25968;&#25454;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;meme&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#65292;&#19987;&#27880;&#20110;&#20167;&#24680;/&#26377;&#23475;meme&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26576;&#20123;&#38646;&#26679;&#26412;&#35774;&#32622;&#26174;&#31034;&#20986;&#22788;&#29702;&#22797;&#26434;&#30690;&#37327;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35201;&#25351;&#20986;&#65292;&#30446;&#21069;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38543;&#30528;&#30740;&#31350;&#30340;&#19981;&#26029;&#28145;&#20837;&#65292;&#25105;&#20204;&#26377;&#29702;&#30001;&#30456;&#20449;&#65292;&#36825;&#20123;&#30001;&#26368;&#26032;&#25216;&#26415;&#39537;&#21160;&#30340;&#27169;&#22411;&#23558;&#20026;&#35782;&#21035;&#21644;&#32531;&#35299;&#32593;&#19978;&#20167;&#24680;meme&#30340;&#20256;&#25773;&#20570;&#20986;&#37325;&#22823;&#36129;&#29486;&#12290;  &#25105;&#20204;&#22312;&#20351;&#29992;&#22823;&#37327;&#31034;&#20363;&#30701;&#35821;&#30340;&#22522;&#30784;&#19978;&#36827;&#20102;&#19968;&#27493;&#65292;&#24212;&#29992;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#31867;&#20167;&#24680;meme&#30340;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#25552;&#31034;&#35789;&#65288;prompts&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32500;&#22522;&#30334;&#31185;&#30340;&#35789;&#26465;&#21644;&#25551;&#36848;&#24615;&#30701;&#35821;&#21487;&#29992;&#20110;&#25351;&#23450;&#29305;&#23450;&#31867;&#22411;&#30340;&#20167;&#24680;&#20869;&#23481;&#65292;&#21363;&#20351;&#22312;&#19981;&#38656;&#35201;&#39044;&#20808;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20063;&#33021;&#20419;&#36827;&#27169;&#22411;&#30340;&#26377;&#25928;&#21028;&#21035;&#24335;&#23398;&#20064;&#12290;&#22312;&#21518;&#32493;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;Prompt&#25913;&#20889;&#65288;rephrasing&#65289;&#31574;&#30053;&#24212;&#29992;&#20110;&#26497;&#31471;&#35328;&#35770;&#12289;&#25935;&#24863;&#20027;&#39064;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#21069;&#35757;&#32451;&#65288;prompt rephrasing for multi-modal learning pretraining&#65289;&#65292;&#24182;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#25552;&#39640;&#38646;&#26679;&#26412;&#21040;&#23454;&#39564;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#38754;&#20020;&#25361;&#25112;&#65292;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#25552;&#31034;&#35789;&#65288;prompts&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#20854;&#28508;&#21147;&#24050;&#34987;&#35777;&#26126;&#26159;&#24191;&#22823;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#23558;&#26469;&#30340;&#24037;&#20316;&#24212;&#33268;&#21147;&#20110;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#20197;&#35299;&#20915;&#27492;&#31867;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25193;&#23637;&#23427;&#20204;&#22312;&#32593;&#32476;&#31354;&#38388;&#20167;&#24680;&#20869;&#23481;&#26816;&#27979;&#21644;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v2 Announce Type: replace-cross  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our anal
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#34701;&#21512;&#22768;&#23398;&#21644;&#20809;&#23398;&#25968;&#25454;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#21463;&#38480;&#20110;&#23567;&#22522;&#32447;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#65292;&#20063;&#33021;&#20934;&#30830;&#37325;&#24314;&#27700;&#19979;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.03309</link><description>&lt;p&gt;
AONeuS: &#19968;&#20010;&#32467;&#21512;&#22768;&#23398;-&#20809;&#23398;&#20256;&#24863;&#34701;&#21512;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#34701;&#21512;&#22768;&#23398;&#21644;&#20809;&#23398;&#25968;&#25454;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#21463;&#38480;&#20110;&#23567;&#22522;&#32447;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#65292;&#20063;&#33021;&#20934;&#30830;&#37325;&#24314;&#27700;&#19979;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.03309v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#27700;&#19979;&#24863;&#30693;&#21644;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#30340;&#38590;&#39064;&#65292;&#36825;&#20123;&#24212;&#29992;&#21253;&#25324;&#24314;&#31569;&#12289;&#23433;&#20840;&#12289;&#28023;&#27915;&#32771;&#21476;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#12290;&#30001;&#20110;&#24694;&#21155;&#30340;&#25805;&#20316;&#26465;&#20214;&#12289;&#33030;&#24369;&#30340;&#29615;&#22659;&#20197;&#21450;&#26377;&#38480;&#30340;&#23548;&#33322;&#25511;&#21046;&#65292;&#28508;&#27700;&#22120;&#32463;&#24120;&#34987;&#38480;&#21046;&#22312;&#20854;&#21487;&#25429;&#33719;&#27979;&#37327;&#20540;&#30340;&#22522;&#32447;&#33539;&#22260;&#20869;&#12290;&#22312;&#19977;&#32500;&#22330;&#26223;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#37117;&#30693;&#36947;&#65292;&#36739;&#23567;&#30340;&#22522;&#32447;&#20250;&#20351;&#37325;&#24314;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#23398;-&#20809;&#23398;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65288;AONeuS&#65289;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#20540;&#19982;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#20998;&#36776;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#20540;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#22312;&#21463;&#38480;&#21046;&#30340;&#22522;&#32447;&#33539;&#22260;&#20869;&#25429;&#33719;&#30340;&#27979;&#37327;&#20540;&#20013;&#37325;&#24314;&#20986;&#20934;&#30830;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#34920;&#26126;&#65292;AONeuS&#33021;&#22815;&#22788;&#29702;&#21644;&#25972;&#21512;&#22768;&#23398;&#21644;&#20809;&#23398;&#25968;&#25454;&#65292;&#21363;&#20351;&#22312;&#27700;&#28145;&#21644;&#25104;&#20687;&#27169;&#31946;&#30340;&#26465;&#20214;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#26159;&#22312;&#21463;&#38480;&#22522;&#32447;&#26465;&#20214;&#19979;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;3D&#34920;&#38754;&#37325;&#24314;&#30340;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.03309v3 Announce Type: replace  Abstract: Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;3D&#24418;&#29366;&#35782;&#21035;&#20013;&#35270;&#22270;&#32423;&#21035;&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;GMViT&#30340;&#39640;&#24615;&#33021;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#21517;&#20026;DeCoV&#30340;&#31574;&#30053;&#19979;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16477</link><description>&lt;p&gt;
&#32452;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31354;&#38388;&#32534;&#30721;&#30340;3D&#24418;&#29366;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;3D&#24418;&#29366;&#35782;&#21035;&#20013;&#35270;&#22270;&#32423;&#21035;&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;GMViT&#30340;&#39640;&#24615;&#33021;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#21517;&#20026;DeCoV&#30340;&#31574;&#30053;&#19979;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16477v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#35270;&#22270;&#30340;3D&#24418;&#29366;&#35782;&#21035;&#26041;&#27861;&#30340;&#32467;&#26524;&#24050;&#32463;&#39281;&#21644;&#65292;&#24182;&#19988;&#30001;&#20110;&#21442;&#25968;&#23610;&#23544;&#24040;&#22823;&#65292;&#24615;&#33021;&#20248;&#24322;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#39046;&#22495;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#23613;&#21487;&#33021;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#25552;&#39640;&#23567;&#22411;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#31216;&#20026;&#32452;&#22810;&#35270;&#22270;&#35270;&#35273;Transformer&#65288;GMViT&#65289;&#12290;&#22312;GMViT&#20013;&#65292;&#35270;&#22270;&#32423;&#21035;&#30340;ViT&#39318;&#20808;&#24314;&#31435;&#20102;&#35270;&#22270;&#32423;&#21035;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25429;&#33719;&#26356;&#28145;&#23618;&#27425;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23545;&#35270;&#22270;&#32423;&#21035;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#32452;&#27169;&#22359;&#30340;&#22788;&#29702;&#65292;&#20351;&#20854;&#25552;&#21319;&#21040;&#20102;&#32452;&#32423;&#21035;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#32452;&#32423;&#21035;ViT&#23558;&#32452;&#32423;&#21035;&#29305;&#24449;&#25972;&#21512;&#25104;&#23436;&#25972;&#30340;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;3D&#24418;&#29366;&#25551;&#36848;&#31526;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#20004;&#20010;ViT&#20013;&#65292;&#25105;&#20204;&#37117;&#24341;&#20837;&#20102;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#21387;&#32553;&#31574;&#30053;&#21483;&#20570;DeCoV,&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#27604;&#24403;&#21069;&#26368;&#20339;&#21387;&#32553;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#22914;&#20309;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23454;&#29616;&#36825;&#19968;&#24895;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;DeCoV&#22312;&#21478;&#19968;&#26041;&#38754;&#30340;&#24433;&#21709;&#21487;&#33021;&#32473;&#25105;&#20204;&#24102;&#26469;&#30340;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#8220;De-fine&#8221;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#20998;&#35299;&#22797;&#26434;&#20219;&#21153;&#20026;&#31616;&#21333;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21453;&#39304;&#26469;&#25913;&#36827;&#31243;&#24207;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#12290;en_tdlr:Our study introduces a training-free framework "De-fine" that automatically decomposes complex tasks into simpler subtasks and refines programs with auto-feedback, significantly improving logical reasoning performance.</title><link>https://arxiv.org/abs/2311.12890</link><description>&lt;p&gt;
&#20998;&#35299;&#19982;&#32454;&#21270;&#35270;&#35273;&#31243;&#24207;&#30340;&#33258;&#21160;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
De-fine: Decomposing and Refining Visual Programs with Auto-Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#8220;De-fine&#8221;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#20998;&#35299;&#22797;&#26434;&#20219;&#21153;&#20026;&#31616;&#21333;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21453;&#39304;&#26469;&#25913;&#36827;&#31243;&#24207;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#12290;en_tdlr:Our study introduces a training-free framework "De-fine" that automatically decomposes complex tasks into simpler subtasks and refines programs with auto-feedback, significantly improving logical reasoning performance.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12890v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#35270;&#35273;&#31243;&#24207;&#35774;&#35745;&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#23427;&#23558;&#19981;&#21516;&#30340;&#27169;&#22359;&#21644;Python&#36816;&#31639;&#31526;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#12290;&#19982;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#36827;&#27493;&#65292;&#33021;&#22815;&#36827;&#34892;&#35270;&#35273;&#22788;&#29702;&#21644;&#25512;&#29702;&#12290;&#30446;&#21069;&#65292;&#35270;&#35273;&#31243;&#24207;&#35774;&#35745;&#30340;&#26041;&#27861;&#20026;&#27599;&#20010;&#20219;&#21153;&#29983;&#25104;&#31243;&#24207;&#65292;&#19968;&#27425;&#24615;&#36890;&#36807;&#65292;&#36951;&#25022;&#30340;&#26159;&#65292;&#32570;&#20047;&#26681;&#25454;&#21453;&#39304;&#35780;&#20272;&#21644;&#20248;&#21270;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#36825;&#26368;&#32456;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#22810;&#27493;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#21040;Benders&#35299;&#32858;&#28789;&#24863;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Define&#8221;&#30340;&#35757;&#32451;&#20813;&#36153;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#35299;&#32858;&#22797;&#26434;&#20219;&#21153;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21453;&#39304;&#26469;&#32454;&#21270;&#31243;&#24207;&#12290;&#36825;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#26469;&#25913;&#36827;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Define&#21019;&#24314;&#20102;&#26356;&#22810;&#20855;&#26377;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#30340;&#31243;&#24207;&#29255;&#27573;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38169;&#35823;&#24674;&#22797;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#21407;&#25991;&#65306;Visual programming, a modular and generalizable paradigm, integrates different modules and Python operators to solve various vision-language tasks. Unlike end-to-end models that need task-specific data, it advances in performing visual processing and reasoning in an unsupervised manner. Current visual programming methods generate programs in a single pass for each task where the ability to evaluate and optimize based on feedback, unfortunately, is lacking, which consequentially limits their effectiveness for complex, multi-step problems. Drawing inspiration from benders decomposition, we introduce De-fine, a training-free framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback. This model-agnostic approach can improve logical reasoning performance by integrating the strengths of multiple models. Our experiments across various visual tasks show that De-fine creates more robust program fragments with better error recovery and decision-making processes.
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12890v3 Announce Type: replace  Abstract: Visual programming, a modular and generalizable paradigm, integrates different modules and Python operators to solve various vision-language tasks. Unlike end-to-end models that need task-specific data, it advances in performing visual processing and reasoning in an unsupervised manner. Current visual programming methods generate programs in a single pass for each task where the ability to evaluate and optimize based on feedback, unfortunately, is lacking, which consequentially limits their effectiveness for complex, multi-step problems. Drawing inspiration from benders decomposition, we introduce De-fine, a training-free framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback. This model-agnostic approach can improve logical reasoning performance by integrating the strengths of multiple models. Our experiments across various visual tasks show that De-fine creates more ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#22495;&#30340;&#20840;&#23616;&#21160;&#24577;&#39057;&#29575;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#25913;&#36827;&#26333;&#20809;&#26657;&#27491;&#20219;&#21153;&#65292;&#21253;&#25324;&#20302;&#20809;&#29031;&#22686;&#24378;&#12289;&#26333;&#20809;&#26657;&#27491;&#21644;&#22810;&#26333;&#20809;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2309.01183</link><description>&lt;p&gt;
&#20840;&#23616;&#21160;&#24577;&#39057;&#29575;&#21464;&#21387;&#22120;&#29992;&#20110;&#22270;&#20687;&#34701;&#21512;&#21644;&#26333;&#20809;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Holistic Dynamic Frequency Transformer for Image Fusion and Exposure Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#22495;&#30340;&#20840;&#23616;&#21160;&#24577;&#39057;&#29575;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#25913;&#36827;&#26333;&#20809;&#26657;&#27491;&#20219;&#21153;&#65292;&#21253;&#25324;&#20302;&#20809;&#29031;&#22686;&#24378;&#12289;&#26333;&#20809;&#26657;&#27491;&#21644;&#22810;&#26333;&#20809;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01183v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#30340;&#36807;&#31243;&#20013;&#65292;&#26333;&#20809;&#30456;&#20851;&#38382;&#39064;&#30340;&#26657;&#27491;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23545;&#20110;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26377;&#37325;&#22823;&#30340;&#24847;&#20041;&#12290;&#21382;&#21490;&#19978;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#22312;&#31354;&#38388;&#22495;&#20869;&#36827;&#34892;&#24674;&#22797;&#65292;&#23545;&#39057;&#29575;&#22495;&#30340;&#21487;&#33021;&#24615;&#30340;&#32771;&#34385;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#20809;&#29031;&#22686;&#24378;&#12289;&#26333;&#20809;&#26657;&#27491;&#21644;&#22810;&#26333;&#20809;&#34701;&#21512;&#26041;&#38754;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#36825;&#20351;&#24471;&#22270;&#20687;&#22788;&#29702;&#20248;&#21270;&#21464;&#24471;&#22797;&#26434;&#21644;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#29575;&#22495;&#26469;&#25913;&#36827;&#21644;&#32479;&#19968;&#26333;&#20809;&#26657;&#27491;&#20219;&#21153;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#20840;&#23616;&#39057;&#29575;&#27880;&#24847;&#21147;&#65288;Holistic Frequency Attention&#65289;&#21644;&#21160;&#24577;&#39057;&#29575;&#21453;&#39304;&#21069;&#21521;&#32593;&#32476;&#65288;Dynamic Frequency Feed-Forward Network&#65289;&#65292;&#23427;&#20204;&#21462;&#20195;&#20102;&#31354;&#38388;&#22495;&#20013;&#24120;&#29992;&#30340;&#30456;&#20851;&#36816;&#31639;&#12290;&#23427;&#20204;&#26500;&#25104;&#20102;&#19968;&#20010;&#22522;&#30784;&#26500;&#24314;&#22359;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;U&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01183v2 Announce Type: replace  Abstract: The correction of exposure-related issues is a pivotal component in enhancing the quality of images, offering substantial implications for various computer vision tasks. Historically, most methodologies have predominantly utilized spatial domain recovery, offering limited consideration to the potentialities of the frequency domain. Additionally, there has been a lack of a unified perspective towards low-light enhancement, exposure correction, and multi-exposure fusion, complicating and impeding the optimization of image processing. In response to these challenges, this paper proposes a novel methodology that leverages the frequency domain to improve and unify the handling of exposure correction tasks. Our method introduces Holistic Frequency Attention and Dynamic Frequency Feed-Forward Network, which replace conventional correlation computation in the spatial-domain. They form a foundational building block that facilitates a U-shaped
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#37325;&#24314;&#38544;&#34255;&#22330;&#26223;&#30340;&#38750;&#35270;&#32447;&#25104;&#20687;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#24517;&#35201;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#34920;&#38754;&#27861;&#32447;&#20934;&#30830;&#39640;&#25928;&#22320;&#24314;&#27169;&#35270;&#21521;&#21453;&#23556;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#37325;&#26500;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2308.10269</link><description>&lt;p&gt;
&#38750;&#35270;&#32447;&#25104;&#20687;&#20013;&#30340;&#22495;&#20943;&#23569;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Domain Reduction Strategy for Non Line of Sight Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#37325;&#24314;&#38544;&#34255;&#22330;&#26223;&#30340;&#38750;&#35270;&#32447;&#25104;&#20687;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#24517;&#35201;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#34920;&#38754;&#27861;&#32447;&#20934;&#30830;&#39640;&#25928;&#22320;&#24314;&#27169;&#35270;&#21521;&#21453;&#23556;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#37325;&#26500;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#38750;&#35270;&#32447;&#65288;NLOS&#65289;&#25104;&#20687;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#36890;&#29992;&#37197;&#32622;&#19979;&#22823;&#22823;&#20943;&#23569;&#37325;&#26500;&#26102;&#38388;&#30340;&#21516;&#26102;&#20934;&#30830;&#37325;&#24314;&#38544;&#34255;&#22330;&#26223;&#12290;&#22312;&#38750;&#35270;&#32447;&#25104;&#20687;&#20013;&#65292;&#30446;&#26631;&#23545;&#35937;&#30340;&#21487;&#35270;&#34920;&#38754;&#26174;&#33879;&#31232;&#30095;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#36825;&#20123;&#35745;&#31639;&#28304;&#33258;&#31354;&#27934;&#21306;&#22495;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35813;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#38544;&#34255;&#31354;&#38388;&#20013;&#36830;&#32493;&#37319;&#26679;&#30340;&#19968;&#32452;&#28857;&#36827;&#34892;&#37096;&#20998;&#20256;&#25773;&#65292;&#20174;&#32780;&#28210;&#26579;&#30636;&#24577;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#21033;&#29992;&#34920;&#38754;&#27861;&#32447;&#24314;&#27169;&#35270;&#21521;&#21453;&#23556;&#29575;&#65292;&#20174;&#32780;&#20351;&#24471;&#33021;&#22815;&#33719;&#24471;&#34920;&#38754;&#20960;&#20309;&#24418;&#29366;&#20197;&#21450;&#21453;&#29031;&#29575;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#20943;&#23569;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#31895;&#21040;&#32454;&#30340;&#27493;&#39588;&#20013;&#23450;&#26399;&#20174;&#37319;&#26679;&#22495;&#20013;&#21076;&#38500;&#31354;&#27934;&#21306;&#22495;&#65292;&#23548;&#33268;&#35745;&#31639;&#20943;&#23569;&#12290;&#36825;&#20419;&#36827;&#20102;&#25972;&#20307;&#20248;&#21270;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#34255;&#22330;&#26223;&#30340;&#37325;&#26500;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10269v2 Announce Type: replace  Abstract: This paper presents a novel optimization-based method for non-line-of-sight (NLOS) imaging that aims to reconstruct hidden scenes under general setups with significantly reduced reconstruction time. In NLOS imaging, the visible surfaces of the target objects are notably sparse. To mitigate unnecessary computations arising from empty regions, we design our method to render the transients through partial propagations from a continuously sampled set of points from the hidden space. Our method is capable of accurately and efficiently modeling the view-dependent reflectance using surface normals, which enables us to obtain surface geometry as well as albedo. In this pipeline, we propose a novel domain reduction strategy to eliminate superfluous computations in empty regions. During the optimization process, our domain reduction procedure periodically prunes the empty regions from our sampling domain in a coarse-to-fine manner, leading to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#32570;&#20047;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#27169;&#22411;&#22686;&#24378;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#36825;&#19968;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2308.04177</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Generalizable are Deepfake Image Detectors? An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.04177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#32570;&#20047;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#27169;&#22411;&#22686;&#24378;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#36825;&#19968;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.04177v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#25688;&#35201;&#65306;&#38543;&#30528;&#28145;&#24230;&#20266;&#36896;&#21464;&#24471;&#36234;&#26469;&#36234;&#36924;&#30495;&#65292;&#20854;&#28508;&#22312;&#30340;&#27450;&#35784;&#34892;&#20026;&#25110;&#32469;&#36807;&#35775;&#38382;&#25511;&#21046;&#31995;&#32479;&#30340;&#39118;&#38505;&#26085;&#30410;&#22686;&#22823;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#21306;&#20998;&#30495;&#23454;&#19982;&#21512;&#25104;&#35270;&#39057;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#26816;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#28145;&#24230;&#20266;&#36896;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#20294;&#23578;&#26410;&#26377;&#20154;&#23545;&#36825;&#31181;&#23616;&#38480;&#24615;&#36827;&#34892;&#30740;&#31350;&#25110;&#25506;&#35752;&#22914;&#20309;&#35299;&#20915;&#12290;&#23588;&#20854;&#26159;&#65292;&#21333;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#25581;&#31034;&#30340;&#20266;&#36896;&#35777;&#25454;&#24456;&#23569;&#65292;&#36825;&#30456;&#23545;&#20110;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#26159;&#26816;&#27979;&#22120;&#24517;&#39035;&#23454;&#29616;&#30340;&#30446;&#26631;&#65292;&#20197;&#30830;&#20445;&#22987;&#32456;&#39046;&#20808;&#20110;&#25915;&#20987;&#32773;&#19968;&#27493;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#20845;&#20010;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#12289;&#20116;&#31181;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;&#20197;&#21450;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26631;&#20934;&#21644;&#27169;&#22411;&#33258;&#36523;&#23646;&#24615;&#23545;&#26816;&#27979;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#21462;&#27169;&#22411;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.04177v2 Announce Type: replace  Abstract: Deepfakes are becoming increasingly credible, posing a significant threat given their potential to facilitate fraud or bypass access control systems. This has motivated the development of deepfake detection methods, in which deep learning models are trained to distinguish between real and synthesized footage. Unfortunately, existing detectors struggle to generalize to deepfakes from datasets they were not trained on, but little work has been done to examine why or how this limitation can be addressed. Especially, those single-modality deepfake images reveal little available forgery evidence, posing greater challenges than detecting deepfake videos. In this work, we present the first empirical study on the generalizability of deepfake detectors, an essential goal for detectors to stay one step ahead of attackers. Our study utilizes six deepfake datasets, five deepfake image detection methods, and two model augmentation approaches, con
&lt;/p&gt;</description></item><item><title>FreeDrag&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29305;&#24449;&#26356;&#26032;&#21644;&#22238;&#22836;&#25628;&#32034;&#25216;&#26415;&#25913;&#21892;&#20102;&#28857;&#24335;&#25302;&#25341;&#32534;&#36753;&#30340;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag&#65306;&#22522;&#20110;&#29305;&#24449;&#25302;&#25341;&#30340;&#21487;&#38752;&#28857;&#24335;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FreeDrag: Feature Dragging for Reliable Point-based Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
FreeDrag&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29305;&#24449;&#26356;&#26032;&#21644;&#22238;&#22836;&#25628;&#32034;&#25216;&#26415;&#25913;&#21892;&#20102;&#28857;&#24335;&#25302;&#25341;&#32534;&#36753;&#30340;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2307.04684v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#20026;&#20102;&#28385;&#36275;&#22270;&#20687;&#32534;&#36753;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#38656;&#27714;&#65292;&#22312;&#22270;&#20687;&#20869;&#23481;&#19978;&#36827;&#34892;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;&#32534;&#36753;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25302;&#25341;&#30340;&#32534;&#36753;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#25302;&#25341;&#19978;&#65292;&#23548;&#33268;&#20004;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#32570;&#28857;&#65292;&#21363;&#8220;&#38169;&#36807;&#36861;&#36394;&#8221;&#65292;&#22312;&#23545;&#39044;&#20808;&#30830;&#23450;&#30340;&#25163;&#26564;&#28857;&#36827;&#34892;&#31934;&#30830;&#36861;&#36394;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#20197;&#21450;&#8220;&#27169;&#31946;&#36861;&#36394;&#8221;&#65292;&#36861;&#36394;&#21040;&#30340;&#28857;&#26377;&#21487;&#33021;&#34987;&#38169;&#35823;&#22320;&#23450;&#20301;&#22312;&#31867;&#20284;&#20110;&#25163;&#26564;&#28857;&#30340;&#21306;&#22495;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeDrag&#65292;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#25302;&#25341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20813;&#38500;&#28857;&#36861;&#36394;&#30340;&#36127;&#25285;&#12290;FreeDrag&#21253;&#21547;&#20102;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#21363;&#36890;&#36807;&#33258;&#36866;&#24212;&#26356;&#26032;&#23454;&#29616;&#30340;&#27169;&#26495;&#29305;&#24449;&#21644;&#24102;&#26377;&#22238;&#22836;&#25628;&#32034;&#30340;&#32447;&#25628;&#32034;&#65292;&#21069;&#32773;&#36890;&#36807;&#31934;&#24515;&#25511;&#21046;&#27599;&#27425;&#25302;&#25341;&#21518;&#29305;&#24449;&#26356;&#26032;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#23545;&#20869;&#23481;&#21095;&#28872;&#21464;&#21270;&#26102;&#30340;&#31283;&#23450;&#24615;&#65307;&#32780;&#21518;&#32773;&#21017;&#20943;&#36731;&#20102;&#23545;&#40736;&#26631;&#21644;&#25163;&#26564;&#28857;&#20043;&#38388;&#23545;&#20934;&#31934;&#24230;&#30340;&#39640;&#35201;&#27714;&#12290;FreeDrag&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#29305;&#24449;&#26356;&#26032;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#25302;&#25341;&#32534;&#36753;&#25805;&#20316;&#30340;&#36895;&#24230;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FreeDrag&#22312;&#25552;&#39640;&#25302;&#25341;&#32534;&#36753;&#31934;&#24230;&#21644;&#20943;&#23569;&#29992;&#25143;&#25805;&#20316;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.04684v4 Announce Type: replace  Abstract: To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely "miss tracking", where difficulties arise in accurately tracking the predetermined handle points, and "ambiguous tracking", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter allev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#20998;&#21106;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20854;&#22312;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2304.09854</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#20998;&#21106;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based Visual Segmentation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.09854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#20998;&#21106;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20854;&#22312;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2304.09854v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#35270;&#35273;&#20998;&#21106;&#26088;&#22312;&#23558;&#22270;&#20687;&#12289;&#35270;&#39057;&#24103;&#25110;&#28857;&#20113;&#20998;&#21106;&#25104;&#22810;&#20010;&#21306;&#27573;&#25110;&#32452;&#12290;&#36825;&#39033;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#22270;&#20687;&#32534;&#36753;&#12289;&#26426;&#22120;&#35270;&#35273;&#21644;&#21307;&#30103;&#20998;&#26512;&#31561;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#36825;&#19968;&#28857;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#35774;&#35745;&#20026; self-attention &#26426;&#21046;&#30340;Transformer&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35270;&#35273;&#22788;&#29702;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#21367;&#31215;&#25110;&#24490;&#29615;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#35270;&#35273;Transformer&#20026;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#20063;&#20445;&#25345;&#20102;&#40065;&#26834;&#24615;&#12289;&#32479;&#19968;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#20998;&#21106;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#32972;&#26223;&#65292;&#21253;&#25324;&#38382;&#39064;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#21644;&#20808;&#21069;&#30340;&#21367;&#31215;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;Transformer&#22312;&#35270;&#35273;&#20998;&#21106;&#26041;&#38754;&#30340;&#19968;&#20123;&#26680;&#24515;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#35270;&#35273;Transformer&#30340;&#22522;&#26412;&#32593;&#32476;&#32467;&#26500;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#22312;&#20998;&#21106;&#20219;&#21153;&#20013;&#22914;&#20309;&#26377;&#25928;&#24212;&#29992;&#35813;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;Transformer&#30340;&#21464;&#20307;&#65292;&#22914;&#33258;&#29983;&#25104;Transformer&#65288;AutoFormer&#65289;&#12289;Scaled Dot-Product Attention&#65288;&#23448;&#32593;&#21152;&#31895;&#65289;&#21644;Multi-Head attention modules&#65292;&#20197;&#21450;&#22312;&#20998;&#21106;&#20219;&#21153;&#20013;&#23427;&#20204;&#30340;&#23454;&#29616;&#21644;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#30446;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.09854v4 Announce Type: replace  Abstract: Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;&#27604;&#24615;&#29305;&#24449;&#37325;&#26500;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2304.08386</link><description>&lt;p&gt;
&#28176;&#36827;&#24335;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#19982;&#23545;&#27604;&#24615;&#29305;&#24449;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Progressive Visual Prompt Learning with Contrastive Feature Re-formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;&#27604;&#24615;&#29305;&#24449;&#37325;&#26500;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08386v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#25552;&#31034;&#23398;&#20064;&#34987;&#35774;&#35745;&#20026;&#36866;&#24212;&#35270;&#35273;&#35821;&#35328;&#65288;V-L&#65289;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#21069;&#20154;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#25552;&#31034;&#19978;&#65292;&#32780;&#35270;&#35273;&#25552;&#31034;&#30340;&#24037;&#20316;&#23545;&#20110;V-L&#27169;&#22411;&#26469;&#35828;&#26159;&#38750;&#24120;&#32570;&#20047;&#30340;&#12290;&#29616;&#26377;&#30340;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#35201;&#20040;&#24615;&#33021;&#24179;&#24179;&#65292;&#35201;&#20040;&#35757;&#32451;&#36807;&#31243;&#19981;&#31283;&#23450;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#25552;&#31034;&#23398;&#20064;&#20855;&#26377;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28176;&#36827;&#24335;&#35270;&#35273;&#25552;&#31034;&#65288;ProVP&#65289;&#32467;&#26500;&#26469;&#21152;&#24378;&#19981;&#21516;&#23618;&#32423;&#25552;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ProVP&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#23884;&#20837;&#20256;&#25773;&#21040;&#28145;&#23618;&#23618;&#20013;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34920;&#29616;&#20986;&#19982;&#23454;&#20363;&#36866;&#24212;&#24615;&#25552;&#31034;&#26041;&#27861;&#30340;&#31867;&#20284;&#34892;&#20026;&#12290;&#20026;&#20102;&#32531;&#35299;&#27867;&#21270;&#24694;&#21270;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#23545;&#27604;&#24615;&#29305;&#24449;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#38450;&#27490;&#32463;&#36807;&#25552;&#31034;&#30340;&#35270;&#35273;&#29305;&#24449;&#20005;&#37325;&#20559;&#31163;&#22266;&#23450;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#20998;&#24067;&#12290;&#32467;&#21512;&#20004;&#32773;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;ProVP-Ref&#65289;&#33021;&#22815;&#22312;&#22810;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08386v3 Announce Type: replace  Abstract: Prompt learning has been designed as an alternative to fine-tuning for adapting Vision-language (V-L) models to the downstream tasks. Previous works mainly focus on text prompt while visual prompt works are limited for V-L models. The existing visual prompt methods endure either mediocre performance or unstable training process, indicating the difficulty of visual prompt learning. In this paper, we propose a new Progressive Visual Prompt (ProVP) structure to strengthen the interactions among prompts of different layers. More importantly, our ProVP could effectively propagate the image embeddings to deep layers and behave partially similar to an instance adaptive prompt method. To alleviate generalization deterioration, we further propose a new contrastive feature re-formation, which prevents the serious deviation of the prompted visual feature from the fixed CLIP visual feature distribution. Combining both, our method (ProVP-Ref) is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#21451;&#22909;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP4MC&#65292;&#23427;&#22312;Minecraft&#29615;&#22659;&#20013;&#36890;&#36807;&#32467;&#21512;&#20219;&#21153;&#23436;&#25104;&#24230;&#21644;&#35821;&#35328;&#25551;&#36848;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#24320;&#25918;&#24335;&#20219;&#21153;&#30340;&#26377;&#25928;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2303.10571</link><description>&lt;p&gt;
&#38754;&#21521;&#24378;&#21270;&#23398;&#20064;&#21451;&#22909;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#12298;&#25105;&#30340;&#19990;&#30028;&#12299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Friendly Vision-Language Model for Minecraft
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.10571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#21451;&#22909;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP4MC&#65292;&#23427;&#22312;Minecraft&#29615;&#22659;&#20013;&#36890;&#36807;&#32467;&#21512;&#20219;&#21153;&#23436;&#25104;&#24230;&#21644;&#35821;&#35328;&#25551;&#36848;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#24320;&#25918;&#24335;&#20219;&#21153;&#30340;&#26377;&#25928;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2303.10571v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#31038;&#21306;&#20013;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#22312;&#39640;&#27700;&#24179;&#20219;&#21153;&#19978;&#23454;&#29616;&#24191;&#27867;&#20219;&#21153;&#33258;&#20027;&#23884;&#20837;&#24335;&#20195;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#25152;&#26377;&#24320;&#25918;&#24335;&#20219;&#21153;&#33719;&#21462;&#25110;&#25163;&#21160;&#35774;&#35745;&#22870;&#21169;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26550;&#26500;&#65292;CLIP4MC&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#23427;&#20316;&#20026;&#24320;&#25918;&#24335;&#20219;&#21153;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#30001;&#20110;&#26631;&#20934;VLMs&#21487;&#33021;&#21482;&#22312;&#22823;&#33539;&#22260;&#20869;&#25429;&#33719;&#30456;&#20284;&#24615;&#65292;&#22240;&#27492;&#20165;&#21033;&#29992;&#35270;&#39057;&#24555;&#29031;&#21644;&#35821;&#35328;&#25552;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24182;&#19981;&#26159;RL&#21451;&#22909;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;RL&#21451;&#22909;&#24615;&#65292;&#25105;&#20204;&#23558;&#22312;VLM&#35757;&#32451;&#30446;&#26631;&#20013;&#34701;&#20837;&#20219;&#21153;&#23436;&#25104;&#31243;&#24230;&#30340;&#20449;&#24687;&#65292;&#22240;&#20026;&#36825;&#31181;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#20195;&#29702;&#21306;&#20998;&#19981;&#21516;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24178;&#20928;&#30340;YouTube&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#12298;&#25105;&#30340;&#19990;&#30028;&#12299;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35757;&#32451;VLM&#65292;&#20197;&#25552;&#21319;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#30340;RL&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;Minecraft&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#23545;CLIP4MC&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21306;&#20998;&#26368;&#37325;&#35201;&#30340;&#22870;&#21169;&#65292;&#24182;&#20419;&#36827;&#20102;&#20219;&#21153;&#25191;&#34892;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.10571v2 Announce Type: replace-cross  Abstract: One of the essential missions in the AI research community is to build an autonomous embodied agent that can achieve high-level performance across a wide spectrum of tasks. However, acquiring or manually designing rewards for all open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-ended tasks. Simply utilizing the similarity between the video snippet and the language prompt is not RL-friendly since standard VLMs may only capture the similarity at a coarse level. To achieve RL-friendliness, we incorporate the task completion degree into the VLM training objective, as this information can assist agents in distinguishing the importance between different states. Moreover, we provide neat YouTube datasets based on the l
&lt;/p&gt;</description></item><item><title>&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#31616;&#21333;&#26356;&#26032;&#31574;&#30053;&#21487;&#20197;&#21462;&#24471;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2303.07338</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#39044;&#35757;&#32451;&#27169;&#22411;&#19979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65306;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.07338
&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#31616;&#21333;&#26356;&#26032;&#31574;&#30053;&#21487;&#20197;&#21462;&#24471;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2303.07338v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#19981;&#26029;&#33719;&#24471;&#30693;&#35782;&#30340;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#23398;&#36807;&#30340;&#26087;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;CIL&#27169;&#22411;&#26159;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#65292;&#20197;&#20415;&#38543;&#30528;&#25968;&#25454;&#30340;&#21457;&#23637;&#19981;&#26029;&#33719;&#24471;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#21487;&#29992;&#20110;CIL&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#21453;&#65292;PTMs&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#20110;&#36716;&#31227;&#30340;&#36890;&#29992;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#21487;&#20197;&#20026;CIL&#25152;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20197;PTMs&#20026;&#22522;&#30784;&#30340;CIL&#36827;&#34892;&#20102;&#37325;&#26032;&#23457;&#35270;&#65292;&#24182;&#35748;&#20026;CIL&#30340;&#26680;&#24515;&#22240;&#32032;&#22312;&#20110;&#27169;&#22411;&#30340;&#26356;&#26032;&#33021;&#21147;&#21644;&#30693;&#35782;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;1&#65289;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20923;&#32467;&#30340;PTM&#33021;&#22815;&#20026;CIL&#25552;&#20379;&#36890;&#29992;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#65288;SimpleCIL&#65289;&#65292;&#23427;&#19981;&#26029;&#22320;&#23558;PTM&#30340;&#31867;&#22120;&#35774;&#32622;&#20026;&#21407;&#22411;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;2&#65289;&#30001;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#65292;PTM&#30340;&#36890;&#29992;&#23884;&#20837;&#21487;&#33021;&#19981;&#36866;&#21512;&#31435;&#21363;&#29992;&#20110;CIL&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#22871;&#36866;&#24212;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#30340;&#26377;&#25928;&#36866;&#24212;&#24615;&#21644;&#30693;&#35782;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM 
&lt;/p&gt;</description></item><item><title>&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;MUG&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#32593;&#32476;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#25913;&#36827;&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#19982;&#32593;&#32476;&#22270;&#20687;&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;MUG&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#32593;&#32476;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#25913;&#36827;&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07088v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306; &#35768;&#22810;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#32452;&#31934;&#24515;&#32534;&#25490;&#30340;&#22270;&#20687;Net-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#30001;&#32593;&#32476;&#26469;&#28304;&#30340;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#31867;&#20284;&#37197;&#32622;&#30340;&#35774;&#32622;&#20013;&#65292;&#23545;&#20195;&#34920;&#24615;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#22823;&#22411;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#25513;&#30721;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#23545;&#27604;&#24615;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#35770;&#35266;&#28857;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#35813;&#35266;&#28857;&#20026;&#35774;&#35745;&#19968;&#31181;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#21463;&#27492;&#27934;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;MUlti-modal Generator&#65288;MUG&#65289;&#65292;&#23427;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from
&lt;/p&gt;</description></item></channel></rss>