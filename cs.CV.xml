<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.CV.xml</link><description>This is arxiv RSS feed for cs.CV</description><item><title>该文章揭示了现有的大型语言模型在图像细节上的感知能力存在局限性，尤其是在预测图像像素值方面。通过研究并结合像素值预测任务，作者发现仅对连接模块和语言模型进行微调不足以大幅提升预测精度，这表明传统的无差别图像感知方法可能不再适用于当下流行的视觉语言模型。此外，通过将像素值预测集成到视觉语言模型的预训练任务中，并结合视觉编码器的适应性训练，可以使视觉语言模型在细节感知方面得到显著提升。</title><link>https://arxiv.org/abs/2408.03940</link><description>&lt;p&gt;
How Well Can Vision Language Models See Image Details?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03940
&lt;/p&gt;
&lt;p&gt;
该文章揭示了现有的大型语言模型在图像细节上的感知能力存在局限性，尤其是在预测图像像素值方面。通过研究并结合像素值预测任务，作者发现仅对连接模块和语言模型进行微调不足以大幅提升预测精度，这表明传统的无差别图像感知方法可能不再适用于当下流行的视觉语言模型。此外，通过将像素值预测集成到视觉语言模型的预训练任务中，并结合视觉编码器的适应性训练，可以使视觉语言模型在细节感知方面得到显著提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03940v1 Announce Type: new  Abstract: Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level remains unclear. In our study, we introduce a pixel value prediction task (PVP) to explore "How Well Can Vision Language Models See Image Details?" and to assist VLMs in perceiving more details. Typically, these models comprise a frozen CLIP visual encoder, a large language model, and a connecting module. After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to predict precise pixel values by only fine-tuning the connection module and LLM; and 2) prediction precision is significantly improved when the vision encoder is also adapted. Additionally, our research reveals that incorporating pixel value prediction as one of the VLM pre-training tasks and vision encoder adaptation markedly boosts VLM
&lt;/p&gt;</description></item><item><title>该文章提出了一种高效的方法，用于将动画图像分解为基本的元素或层，称为“精灵”。该方法利用优化后的精灵参数与矢量视频匹配，并通过假设精灵纹理静态来缩小搜索空间，同时通过纹理先验模型防止过时现象。为加速优化过程，该文还介绍了一种利用预训练视频对象分割模型初始化精灵参数的方法，并设计了一个从用户单帧标注中获取初始值的系统。通过构建自一个在线设计服务的“Crello动画”数据集以及定义定量评价标准来评估提取精灵的质量。实验结果表明，与类似分解任务的标准方法相比，该方法在质量和效率之间取得了显著的优势。</title><link>https://arxiv.org/abs/2408.03923</link><description>&lt;p&gt;
Fast Sprite Decomposition from Animated Graphics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03923
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种高效的方法，用于将动画图像分解为基本的元素或层，称为“精灵”。该方法利用优化后的精灵参数与矢量视频匹配，并通过假设精灵纹理静态来缩小搜索空间，同时通过纹理先验模型防止过时现象。为加速优化过程，该文还介绍了一种利用预训练视频对象分割模型初始化精灵参数的方法，并设计了一个从用户单帧标注中获取初始值的系统。通过构建自一个在线设计服务的“Crello动画”数据集以及定义定量评价标准来评估提取精灵的质量。实验结果表明，与类似分解任务的标准方法相比，该方法在质量和效率之间取得了显著的优势。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03923v1 Announce Type: new  Abstract: This paper presents an approach to decomposing animated graphics into sprites, a set of basic elements or layers. Our approach builds on the optimization of sprite parameters to fit the raster video. For efficiency, we assume static textures for sprites to reduce the search space while preventing artifacts using a texture prior model. To further speed up the optimization, we introduce the initialization of the sprite parameters utilizing a pre-trained video object segmentation model and user input of single frame annotations. For our study, we construct the Crello Animation dataset from an online design service and define quantitative metrics to measure the quality of the extracted sprites. Experiments show that our method significantly outperforms baselines for similar decomposition tasks in terms of the quality/efficiency tradeoff.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为FMiFood的全新多模态对比学习框架，该框架通过结合额外的文本描述等环境信息，有效提高了食物图像分类的准确率。</title><link>https://arxiv.org/abs/2408.03922</link><description>&lt;p&gt;
FMiFood: Multi-modal Contrastive Learning for Food Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03922
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为FMiFood的全新多模态对比学习框架，该框架通过结合额外的文本描述等环境信息，有效提高了食物图像分类的准确率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03922v1 Announce Type: new  Abstract: Food image classification is the fundamental step in image-based dietary assessment, which aims to estimate participants' nutrient intake from eating occasion images. A common challenge of food images is the intra-class diversity and inter-class similarity, which can significantly hinder classification performance. To address this issue, we introduce a novel multi-modal contrastive learning framework called FMiFood, which learns more discriminative features by integrating additional contextual information, such as food category text descriptions, to enhance classification accuracy. Specifically, we propose a flexible matching technique that improves the similarity matching between text and image embeddings to focus on multiple key information. Furthermore, we incorporate the classification objectives into the framework and explore the use of GPT-4 to enrich the text descriptions and provide more detailed context. Our method demonstrates 
&lt;/p&gt;</description></item><item><title>该文章提出的AdapMTL框架通过为共享骨干和特定于任务的头部分配可学习的软阈值，实现了对多任务学习模型的自适应剪枝，能够针对不同组件的剪枝敏感性进行精细调节，同时优化软阈值和多任务模型权重，自动确定最优剪枝策略。</title><link>https://arxiv.org/abs/2408.03913</link><description>&lt;p&gt;
AdapMTL: Adaptive Pruning Framework for Multitask Learning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03913
&lt;/p&gt;
&lt;p&gt;
该文章提出的AdapMTL框架通过为共享骨干和特定于任务的头部分配可学习的软阈值，实现了对多任务学习模型的自适应剪枝，能够针对不同组件的剪枝敏感性进行精细调节，同时优化软阈值和多任务模型权重，自动确定最优剪枝策略。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03913v1 Announce Type: new  Abstract: In the domain of multimedia and multimodal processing, the efficient handling of diverse data streams such as images, video, and sensor data is paramount. Model compression and multitask learning (MTL) are crucial in this field, offering the potential to address the resource-intensive demands of processing and interpreting multiple forms of media simultaneously. However, effectively compressing a multitask model presents significant challenges due to the complexities of balancing sparsity allocation and accuracy performance across multiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive pruning framework for MTL models. AdapMTL leverages multiple learnable soft thresholds independently assigned to the shared backbone and the task-specific heads to capture the nuances in different components' sensitivity to pruning. During training, it co-optimizes the soft thresholds and MTL model weights to automatically determine the
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用经典贝叶斯核的瘦视频去噪方法，通过小型辅助网络改进原始去噪器的性能，同时保持快速的去噪速度。这项研究采用了一种混合维纳滤波器，它使用小型辅助网络来改进维纳核估计、优化窗函数并估计未知噪声模式。通过这些方法，论文中提出的去噪器平均在0.2 dB左右优于流行的VRT变换器。这种方法比变换器方法快了超过10倍，同时还具有较低的参数数量和内存需求，提供了一种在速度和去噪效果之间取得平衡的有效视频去噪解决方案。</title><link>https://arxiv.org/abs/2408.03904</link><description>&lt;p&gt;
Lightweight Video Denoising Using a Classic Bayesian Backbone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03904
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用经典贝叶斯核的瘦视频去噪方法，通过小型辅助网络改进原始去噪器的性能，同时保持快速的去噪速度。这项研究采用了一种混合维纳滤波器，它使用小型辅助网络来改进维纳核估计、优化窗函数并估计未知噪声模式。通过这些方法，论文中提出的去噪器平均在0.2 dB左右优于流行的VRT变换器。这种方法比变换器方法快了超过10倍，同时还具有较低的参数数量和内存需求，提供了一种在速度和去噪效果之间取得平衡的有效视频去噪解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03904v1 Announce Type: cross  Abstract: In recent years, state-of-the-art image and video denoising networks have become increasingly large, requiring millions of trainable parameters to achieve best-in-class performance. Improved denoising quality has come at the cost of denoising speed, where modern transformer networks are far slower to run than smaller denoising networks such as FastDVDnet and classic Bayesian denoisers such as the Wiener filter.   In this paper, we implement a hybrid Wiener filter which leverages small ancillary networks to increase the original denoiser performance, while retaining fast denoising speeds. These networks are used to refine the Wiener coring estimate, optimise windowing functions and estimate the unknown noise profile. Using these methods, we outperform several popular denoisers and remain within 0.2 dB, on average, of the popular VRT transformer. Our method was found to be over x10 faster than the transformer method, with a far lower par
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为“Dual-Modeling Decouple Distillation”（DMDD）的技术，用于改进无监督的异常检测方法。通过使用差异化的学生和教师模型，DMDD旨在解决现有方法中可能出现的过度泛化问题，提高学生模型在异常中心或边缘上的异常检测能力，并防止因网络结构或内容差异化而导致的过拟合风险。</title><link>https://arxiv.org/abs/2408.03888</link><description>&lt;p&gt;
Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03888
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为“Dual-Modeling Decouple Distillation”（DMDD）的技术，用于改进无监督的异常检测方法。通过使用差异化的学生和教师模型，DMDD旨在解决现有方法中可能出现的过度泛化问题，提高学生模型在异常中心或边缘上的异常检测能力，并防止因网络结构或内容差异化而导致的过拟合风险。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03888v1 Announce Type: new  Abstract: Knowledge distillation based on student-teacher network is one of the mainstream solution paradigms for the challenging unsupervised Anomaly Detection task, utilizing the difference in representation capabilities of the teacher and student networks to implement anomaly localization. However, over-generalization of the student network to the teacher network may lead to negligible differences in representation capabilities of anomaly, thus affecting the detection effectiveness. Existing methods address the possible over-generalization by using differentiated students and teachers from the structural perspective or explicitly expanding distilled information from the content perspective, which inevitably result in an increased likelihood of underfitting of the student network and poor anomaly detection capabilities in anomaly center or edge. In this paper, we propose Dual-Modeling Decouple Distillation (DMDD) for the unsupervised anomaly det
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为GlInSNet的盲图像质量评估网络，它结合了全局和局部特征提取器，以同时捕捉图像的粗粒度和细粒度信息。通过将基于视觉变换器的全局特征提取器与基于卷积神经网络的局部特征提取器相结合，GlInSNet有效地解决了视觉变换器在处理图像细节和从有限数据集中学习方面的局限性。此外，文章还介绍了一种迭代特征融合机制，该机制能够将粗粒度和细粒度特征进行有效融合，从而提高了评估的准确性。总体而言，GlInSNet通过整合全局和局部的信息，提供了一种更强大的图像质量评估方法。</title><link>https://arxiv.org/abs/2408.03885</link><description>&lt;p&gt;
Global-Local Progressive Integration Network for Blind Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03885
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为GlInSNet的盲图像质量评估网络，它结合了全局和局部特征提取器，以同时捕捉图像的粗粒度和细粒度信息。通过将基于视觉变换器的全局特征提取器与基于卷积神经网络的局部特征提取器相结合，GlInSNet有效地解决了视觉变换器在处理图像细节和从有限数据集中学习方面的局限性。此外，文章还介绍了一种迭代特征融合机制，该机制能够将粗粒度和细粒度特征进行有效融合，从而提高了评估的准确性。总体而言，GlInSNet通过整合全局和局部的信息，提供了一种更强大的图像质量评估方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03885v1 Announce Type: new  Abstract: Vision transformers (ViTs) excel in computer vision for modeling long-term dependencies, yet face two key challenges for image quality assessment (IQA): discarding fine details during patch embedding, and requiring extensive training data due to lack of inductive biases. In this study, we propose a Global-Local progressive INTegration network for IQA, called GlintIQA, to address these issues through three key components: 1) Hybrid feature extraction combines ViT-based global feature extractor (VGFE) and convolutional neural networks (CNNs)-based local feature extractor (CLFE) to capture global coarse-grained features and local fine-grained features, respectively. The incorporation of CNNs mitigates the patch-level information loss and inductive bias constraints inherent to ViT architectures. 2) Progressive feature integration leverages diverse kernel sizes in embedding to spatially align coarse- and fine-grained features, and progressive
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Surgformer的手术阶段识别Transformer模型，通过利用分层的时间注意力机制有效地建模了手术视频中的空间和时间依赖性，并减少了解析输入视频时的时间和空间冗余，从而提高了手术阶段的识别精度。</title><link>https://arxiv.org/abs/2408.03867</link><description>&lt;p&gt;
Surgformer: Surgical Transformer with Hierarchical Temporal Attention for Surgical Phase Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03867
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Surgformer的手术阶段识别Transformer模型，通过利用分层的时间注意力机制有效地建模了手术视频中的空间和时间依赖性，并减少了解析输入视频时的时间和空间冗余，从而提高了手术阶段的识别精度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03867v1 Announce Type: new  Abstract: Existing state-of-the-art methods for surgical phase recognition either rely on the extraction of spatial-temporal features at a short-range temporal resolution or adopt the sequential extraction of the spatial and temporal features across the entire temporal resolution. However, these methods have limitations in modeling spatial-temporal dependency and addressing spatial-temporal redundancy: 1) These methods fail to effectively model spatial-temporal dependency, due to the lack of long-range information or joint spatial-temporal modeling. 2) These methods utilize dense spatial features across the entire temporal resolution, resulting in significant spatial-temporal redundancy. In this paper, we propose the Surgical Transformer (Surgformer) to address the issues of spatial-temporal modeling and redundancy in an end-to-end manner, which employs divided spatial-temporal attention and takes a limited set of sparse frames as input. Moreover,
&lt;/p&gt;</description></item><item><title>该文章提出了一种新型的双层级时空与通道感知变压器网络，用于学习的图像压缩，它通过引入一种新型的时空-通道注意力变压器块（HSCATB）和通道感知自注意力（CaSA）模块，能够在处理图像时更加敏感地关注频率信息，同时在通道间进行信息整合，显著提高了图像压缩的性能。</title><link>https://arxiv.org/abs/2408.03842</link><description>&lt;p&gt;
Bi-Level Spatial and Channel-aware Transformer for Learned Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03842
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新型的双层级时空与通道感知变压器网络，用于学习的图像压缩，它通过引入一种新型的时空-通道注意力变压器块（HSCATB）和通道感知自注意力（CaSA）模块，能够在处理图像时更加敏感地关注频率信息，同时在通道间进行信息整合，显著提高了图像压缩的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03842v1 Announce Type: new  Abstract: Recent advancements in learned image compression (LIC) methods have demonstrated superior performance over traditional hand-crafted codecs. These learning-based methods often employ convolutional neural networks (CNNs) or Transformer-based architectures. However, these nonlinear approaches frequently overlook the frequency characteristics of images, which limits their compression efficiency. To address this issue, we propose a novel Transformer-based image compression method that enhances the transformation stage by considering frequency components within the feature map. Our method integrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB), where a spatial-based branch independently handles high and low frequencies at the attention layer, and a Channel-aware Self-Attention (CaSA) module captures information across channels, significantly improving compression performance. Additionally, we introduce a Mixed Local-Glob
&lt;/p&gt;</description></item><item><title>该文章的创新贡献在于开发了一种使用小型光学时间飞行传感器进行平面表面几何偏差检测的方法，通过分析完整的时间飞行数据信息，识别并克服了表面几何与反射特性之间的关键差异。通过在小型数据集中训练高斯混合模型，该模型能够捕捉平面表面的预期几何和反射特性的分布，从而能够区分出可能包含偏差的测量值。文章还详细测试了该方法在不同表面和偏差情况下的有效性。</title><link>https://arxiv.org/abs/2408.03838</link><description>&lt;p&gt;
Using a Distance Sensor to Detect Deviations in a Planar Surface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03838
&lt;/p&gt;
&lt;p&gt;
该文章的创新贡献在于开发了一种使用小型光学时间飞行传感器进行平面表面几何偏差检测的方法，通过分析完整的时间飞行数据信息，识别并克服了表面几何与反射特性之间的关键差异。通过在小型数据集中训练高斯混合模型，该模型能够捕捉平面表面的预期几何和反射特性的分布，从而能够区分出可能包含偏差的测量值。文章还详细测试了该方法在不同表面和偏差情况下的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03838v1 Announce Type: new  Abstract: We investigate methods for determining if a planar surface contains geometric deviations (e.g., protrusions, objects, divots, or cliffs) using only an instantaneous measurement from a miniature optical time-of-flight sensor. The key to our method is to utilize the entirety of information encoded in raw time-of-flight data captured by off-the-shelf distance sensors. We provide an analysis of the problem in which we identify the key ambiguity between geometry and surface photometrics. To overcome this challenging ambiguity, we fit a Gaussian mixture model to a small dataset of planar surface measurements. This model implicitly captures the expected geometry and distribution of photometrics of the planar surface and is used to identify measurements that are likely to contain deviations. We characterize our method on a variety of surfaces and planar deviations across a range of scenarios. We find that our method utilizing raw time-of-flight 
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了针对信息提取的视觉语言模型的目标提示方法，以期解决传统通用提示在特定场合下的不足，通过针对性地设计提示，提高了视觉语言模型在文档生成和问答系统中的准确性。</title><link>https://arxiv.org/abs/2408.03834</link><description>&lt;p&gt;
Target Prompting for Information Extraction with Vision Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03834
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了针对信息提取的视觉语言模型的目标提示方法，以期解决传统通用提示在特定场合下的不足，通过针对性地设计提示，提高了视觉语言模型在文档生成和问答系统中的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03834v1 Announce Type: new  Abstract: The recent trend in the Large Vision and Language model has brought a new change in how information extraction systems are built. VLMs have set a new benchmark with their State-of-the-art techniques in understanding documents and building question-answering systems across various industries. They are significantly better at generating text from document images and providing accurate answers to questions. However, there are still some challenges in effectively utilizing these models to build a precise conversational system. General prompting techniques used with large language models are often not suitable for these specially designed vision language models. The output generated by such generic input prompts is ordinary and may contain information gaps when compared with the actual content of the document. To obtain more accurate and specific answers, a well-targeted prompt is required by the vision language model, along with the document
&lt;/p&gt;</description></item><item><title>该文章提出了一种创新的实时3D Gaussian Splatting（3DGS）技术，通过将3DGS与直接稀疏位姿估计算法相结合，显著优化了对单个摄像头捕捉视频的高质量容积重建过程。通过使用Direct Sparse Odometry输出的点云数据，相比于使用传统的结构光方法，可以大大减少达到高质图像渲染所需的训练时间。该研究预示了将3DGS与SLAM系统实时集成，以便在移动设备上运行的可能性。这一创新有望在增强现实和视频处理领域引发重要变革。</title><link>https://arxiv.org/abs/2408.03825</link><description>&lt;p&gt;
Towards Real-Time Gaussian Splatting: Accelerating 3DGS through Photometric SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03825
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种创新的实时3D Gaussian Splatting（3DGS）技术，通过将3DGS与直接稀疏位姿估计算法相结合，显著优化了对单个摄像头捕捉视频的高质量容积重建过程。通过使用Direct Sparse Odometry输出的点云数据，相比于使用传统的结构光方法，可以大大减少达到高质图像渲染所需的训练时间。该研究预示了将3DGS与SLAM系统实时集成，以便在移动设备上运行的可能性。这一创新有望在增强现实和视频处理领域引发重要变革。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03825v1 Announce Type: new  Abstract: Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous Localization and Mapping (VSLAM) demonstrate the generation of high-quality volumetric reconstructions from monocular video streams. However, despite these promising advancements, current 3DGS integrations have reduced tracking performance and lower operating speeds compared to traditional VSLAM. To address these issues, we propose integrating 3DGS with Direct Sparse Odometry, a monocular photometric SLAM system. We have done preliminary experiments showing that using Direct Sparse Odometry point cloud outputs, as opposed to standard structure-from-motion methods, significantly shortens the training time needed to achieve high-quality renders. Reducing 3DGS training time enables the development of 3DGS-integrated SLAM systems that operate in real-time on mobile hardware. These promising initial findings suggest further exploration is warranted in combining tradit
&lt;/p&gt;</description></item><item><title>该文章提出了一种紧凑的3D高斯拼贴法用于静态和动态的光场，通过引入近似的体渲染和减少高斯点数量，降低了所需内存和存储量，同时保持了较高的图像质量。</title><link>https://arxiv.org/abs/2408.03822</link><description>&lt;p&gt;
Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03822
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种紧凑的3D高斯拼贴法用于静态和动态的光场，通过引入近似的体渲染和减少高斯点数量，降低了所需内存和存储量，同时保持了较高的图像质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03822v1 Announce Type: new  Abstract: 3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D Gaussian-based representation and introduces an approximated volumetric rendering, achieving very fast rendering speed and promising image quality. Furthermore, subsequent studies have successfully extended 3DGS to dynamic 3D scenes, demonstrating its wide range of applications. However, a significant drawback arises as 3DGS and its following methods entail a substantial number of Gaussians to maintain the high fidelity of the rendered images, which requires a large amount of memory and storage. To address this critical issue, we place a specific emphasis on two key objectives: reducing the number of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as view-dependent color and covariance. To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians while
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于LiDAR点云的CLIP知识指导的3D物体检测方法，通过视觉和语言的关联，实现了无需标记的数据下的3D物体检测，特别是对于静止物体的检测具有较好的泛化能力，降低了手动标注的依赖。</title><link>https://arxiv.org/abs/2408.03790</link><description>&lt;p&gt;
Vision-Language Guidance for LiDAR-based Unsupervised 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03790
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于LiDAR点云的CLIP知识指导的3D物体检测方法，通过视觉和语言的关联，实现了无需标记的数据下的3D物体检测，特别是对于静止物体的检测具有较好的泛化能力，降低了手动标注的依赖。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03790v1 Announce Type: new  Abstract: Accurate 3D object detection in LiDAR point clouds is crucial for autonomous driving systems. To achieve state-of-the-art performance, the supervised training of detectors requires large amounts of human-annotated data, which is expensive to obtain and restricted to predefined object categories. To mitigate manual labeling efforts, recent unsupervised object detection approaches generate class-agnostic pseudo-labels for moving objects, subsequently serving as supervision signal to bootstrap a detector. Despite promising results, these approaches do not provide class labels or generalize well to static objects. Furthermore, they are mostly restricted to data containing multiple drives from the same scene or images from a precisely calibrated and synchronized camera setup. To overcome these limitations, we propose a vision-language-guided unsupervised 3D detection approach that operates exclusively on LiDAR point clouds. We transfer CLIP k
&lt;/p&gt;</description></item><item><title>该文章提供了一种基于不确定性的可解释自动化机制，用于在多中心CT图像研究中检测和分割肾囊肿。通过结合VAE-GAN来学习图像的潜在表示，并利用梯度来生成不同的假想图像以解释分类不准确的原因，从而提高算法的可解释性。通过这种方式，文章中的创新贡献在于提出了一个可以精确检测肾囊肿同时提供解释性的自动化检测系统，这对于早期识别潜在的恶性囊肿非常重要。</title><link>https://arxiv.org/abs/2408.03789</link><description>&lt;p&gt;
Counterfactuals and Uncertainty-Based Explainable Paradigm for the Automated Detection and Segmentation of Renal Cysts in Computed Tomography Images: A Multi-Center Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03789
&lt;/p&gt;
&lt;p&gt;
该文章提供了一种基于不确定性的可解释自动化机制，用于在多中心CT图像研究中检测和分割肾囊肿。通过结合VAE-GAN来学习图像的潜在表示，并利用梯度来生成不同的假想图像以解释分类不准确的原因，从而提高算法的可解释性。通过这种方式，文章中的创新贡献在于提出了一个可以精确检测肾囊肿同时提供解释性的自动化检测系统，这对于早期识别潜在的恶性囊肿非常重要。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03789v1 Announce Type: cross  Abstract: Routine computed tomography (CT) scans often detect a wide range of renal cysts, some of which may be malignant. Early and precise localization of these cysts can significantly aid quantitative image analysis. Current segmentation methods, however, do not offer sufficient interpretability at the feature and pixel levels, emphasizing the necessity for an explainable framework that can detect and rectify model inaccuracies. We developed an interpretable segmentation framework and validated it on a multi-centric dataset. A Variational Autoencoder Generative Adversarial Network (VAE-GAN) was employed to learn the latent representation of 3D input patches and reconstruct input images. Modifications in the latent representation using the gradient of the segmentation model generated counterfactual explanations for varying dice similarity coefficients (DSC). Radiomics features extracted from these counterfactual images, using a ground truth cy
&lt;/p&gt;</description></item><item><title>该文章开发了一种基于 variational autoencoder-multilayer perceptron (VAE-MLP) 的模型，用于预测肝细胞癌（HCC）患者手术后的肝脏衰竭（PHLF），并在决策支持系统中有效集成 counterfactual 解释和 layerwise relevance propagation 方法以提高模型的透明度。同时，该文章还提出了一套方法论框架以评价人工智能系统的解释能力，包括对解释与公认生物标志物的相关性评估、用户体验评估以及模拟临床试验，证明了解释性 AI 系统在 HCC 患者 PHLF 预测中的潜在价值。</title><link>https://arxiv.org/abs/2408.03771</link><description>&lt;p&gt;
Methodological Explainability Evaluation of an Interpretable Deep Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating Counterfactual Explanations and Layerwise Relevance Propagation: A Prospective In Silico Trial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03771
&lt;/p&gt;
&lt;p&gt;
该文章开发了一种基于 variational autoencoder-multilayer perceptron (VAE-MLP) 的模型，用于预测肝细胞癌（HCC）患者手术后的肝脏衰竭（PHLF），并在决策支持系统中有效集成 counterfactual 解释和 layerwise relevance propagation 方法以提高模型的透明度。同时，该文章还提出了一套方法论框架以评价人工智能系统的解释能力，包括对解释与公认生物标志物的相关性评估、用户体验评估以及模拟临床试验，证明了解释性 AI 系统在 HCC 患者 PHLF 预测中的潜在价值。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03771v1 Announce Type: new  Abstract: Artificial intelligence (AI)-based decision support systems have demonstrated value in predicting post-hepatectomy liver failure (PHLF) in hepatocellular carcinoma (HCC). However, they often lack transparency, and the impact of model explanations on clinicians' decisions has not been thoroughly evaluated. Building on prior research, we developed a variational autoencoder-multilayer perceptron (VAE-MLP) model for preoperative PHLF prediction. This model integrated counterfactuals and layerwise relevance propagation (LRP) to provide insights into its decision-making mechanism. Additionally, we proposed a methodological framework for evaluating the explainability of AI systems. This framework includes qualitative and quantitative assessments of explanations against recognized biomarkers, usability evaluations, and an in silico clinical trial. Our evaluations demonstrated that the model's explanation correlated with established biomarkers an
&lt;/p&gt;</description></item><item><title>该文章创新地提出了一种名为MMSummary的自动多模态摘要生成系统，用于胎儿超声视频分析，它模仿了人类超声医生进行检查的过程。系统中包含了三个阶段：关键帧检测、关键帧自动描述文本生成和关键帧中的解剖结构分割与测量。通过调整大型语言模型来为胎儿超声关键帧生成描述性文本，并对关键帧中的生物尺寸指标进行估计。这项技术为胎儿超声影像提供了全面的摘要和关键信息的精确提取，对医疗影像分析和医学教育具有重要意义。</title><link>https://arxiv.org/abs/2408.03761</link><description>&lt;p&gt;
MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03761
&lt;/p&gt;
&lt;p&gt;
该文章创新地提出了一种名为MMSummary的自动多模态摘要生成系统，用于胎儿超声视频分析，它模仿了人类超声医生进行检查的过程。系统中包含了三个阶段：关键帧检测、关键帧自动描述文本生成和关键帧中的解剖结构分割与测量。通过调整大型语言模型来为胎儿超声关键帧生成描述性文本，并对关键帧中的生物尺寸指标进行估计。这项技术为胎儿超声影像提供了全面的摘要和关键信息的精确提取，对医疗影像分析和医学教育具有重要意义。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03761v1 Announce Type: new  Abstract: We present the first automated multimodal summary generation system, MMSummary, for medical imaging video, particularly with a focus on fetal ultrasound analysis. Imitating the examination process performed by a human sonographer, MMSummary is designed as a three-stage pipeline, progressing from keyframe detection to keyframe captioning and finally anatomy segmentation and measurement. In the keyframe detection stage, an innovative automated workflow is proposed to progressively select a concise set of keyframes, preserving sufficient video information without redundancy. Subsequently, we adapt a large language model to generate meaningful captions for fetal ultrasound keyframes in the keyframe captioning stage. If a keyframe is captioned as fetal biometry, the segmentation and measurement stage estimates biometric parameters by segmenting the region of interest according to the textual prior. The MMSummary system provides comprehensive 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为3iGS的方法，通过将光照分解为局部照明场和BRDF特性，改进了3D Gaussians在渲染中的效果，有效增强了视图依赖性。</title><link>https://arxiv.org/abs/2408.03753</link><description>&lt;p&gt;
3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03753
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为3iGS的方法，通过将光照分解为局部照明场和BRDF特性，改进了3D Gaussians在渲染中的效果，有效增强了视图依赖性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03753v1 Announce Type: new  Abstract: The use of 3D Gaussians as representation of radiance fields has enabled high quality novel view synthesis at real-time rendering speed. However, the choice of optimising the outgoing radiance of each Gaussian independently as spherical harmonics results in unsatisfactory view dependent effects. In response to these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering quality. Instead of optimising a single outgoing radiance parameter, 3iGS enhances 3DGS view-dependent effects by expressing the outgoing radiance as a function of a local illumination field and Bidirectional Reflectance Distribution Function (BRDF) features. We optimise a continuous incident illumination field through a Tensorial Factorisation representation, while separately fine-tuning the BRDF features of each 3D Gaussian relative to this illumination field. Our methodology sign
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于边缘指导的对抗条件扩散模型生成数据方案，能够从可见光图像中提取边缘信息，精确控制生成图像的像素级别对齐，特别是对物体边缘的描绘。此外，通过两种模式下的对抗训练，模型能够更有效地抑制那些在热成像图像中不应出现的边缘信息，从而大幅提高了生成热模态图像的质量和实用性。</title><link>https://arxiv.org/abs/2408.03748</link><description>&lt;p&gt;
Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial Conditional Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03748
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于边缘指导的对抗条件扩散模型生成数据方案，能够从可见光图像中提取边缘信息，精确控制生成图像的像素级别对齐，特别是对物体边缘的描绘。此外，通过两种模式下的对抗训练，模型能够更有效地抑制那些在热成像图像中不应出现的边缘信息，从而大幅提高了生成热模态图像的质量和实用性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03748v1 Announce Type: new  Abstract: In challenging low light and adverse weather conditions,thermal vision algorithms,especially object detection,have exhibited remarkable potential,contrasting with the frequent struggles encountered by visible vision algorithms. Nevertheless,the efficacy of thermal vision algorithms driven by deep learning models remains constrained by the paucity of available training data samples. To this end,this paper introduces a novel approach termed the edge guided conditional diffusion model. This framework aims to produce meticulously aligned pseudo thermal images at the pixel level,leveraging edge information extracted from visible images. By utilizing edges as contextual cues from the visible domain,the diffusion model achieves meticulous control over the delineation of objects within the generated images. To alleviate the impacts of those visible-specific edge information that should not appear in the thermal domain,a two-stage modality advers
&lt;/p&gt;</description></item><item><title>该文章提出了一个基于直觉模糊认知地图（iFCM）的图像分类框架，该框架能够为深度学习模型提供可解释性。这种方法使得用户能够理解模型决策背后的逻辑，从而在使用时更加放心。通过将直觉模糊认知地图应用于图像分类任务，该研究可能在模糊系统在图像处理领域的应用方面实现了重要创新，提升了机器学习模型在复杂决策场景中的实际应用价值。</title><link>https://arxiv.org/abs/2408.03745</link><description>&lt;p&gt;
Intuitionistic Fuzzy Cognitive Maps for Interpretable Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03745
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个基于直觉模糊认知地图（iFCM）的图像分类框架，该框架能够为深度学习模型提供可解释性。这种方法使得用户能够理解模型决策背后的逻辑，从而在使用时更加放心。通过将直觉模糊认知地图应用于图像分类任务，该研究可能在模糊系统在图像处理领域的应用方面实现了重要创新，提升了机器学习模型在复杂决策场景中的实际应用价值。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03745v1 Announce Type: new  Abstract: The interpretability of machine learning models is critical, as users may be reluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been proposed as an extension of FCMs offering a natural mechanism to assess the quality of their output through the estimation of hesitancy, a concept resembling to human hesitation in decision making. To address the challenge of interpretable image classification, this paper introduces a novel framework, named Interpretable Intuitionistic FCM (I2FCM) which is domain-independent, simple to implement, and can be applied on Convolutional Neural Network (CNN) models, rendering them interpretable. To the best of our knowledge this is the first time iFCMs are applied for image classification. Further novel contributions include: a feature extraction process focusing on the most informative image regions; a learning algorithm for data-driven determination of the intuitionistic fuzzy interconnecti
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了基于组间尺度学习的方法，结合量化自适应和多模态优化，改进了多模态大型语言模型在视觉语言指导训练中的资源消耗。通过学习量化后的模型权重组间尺度因子，缓解了因激活异常值导致的量化错误，并有效提升了视觉语言指导训练的性能。此外，通过渐进式的多模态训练样本集成，该方法防止了模型对多模态数据的过拟合并保证了多模态大型语言模型在下游视觉语言任务中的稳定适应性。实验结果表明，采用该方法进行量化的模型性能有了显著提升。</title><link>https://arxiv.org/abs/2408.03735</link><description>&lt;p&gt;
Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03735
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了基于组间尺度学习的方法，结合量化自适应和多模态优化，改进了多模态大型语言模型在视觉语言指导训练中的资源消耗。通过学习量化后的模型权重组间尺度因子，缓解了因激活异常值导致的量化错误，并有效提升了视觉语言指导训练的性能。此外，通过渐进式的多模态训练样本集成，该方法防止了模型对多模态数据的过拟合并保证了多模态大型语言模型在下游视觉语言任务中的稳定适应性。实验结果表明，采用该方法进行量化的模型性能有了显著提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03735v1 Announce Type: new  Abstract: This paper presents the first study to explore the potential of parameter quantization for multimodal large language models to alleviate the significant resource constraint encountered during vision-language instruction tuning. We introduce a Quantization-aware Scale LeArning method based on multimodal Warmup, termed QSLAW. This method is grounded in two key innovations: (1) The learning of group-wise scale factors for quantized LLM weights to mitigate the quantization error arising from activation outliers and achieve more effective vision-language instruction tuning; (2) The implementation of a multimodal warmup that progressively integrates linguistic and multimodal training samples, thereby preventing overfitting of the quantized model to multimodal data while ensuring stable adaptation of multimodal large language models to downstream vision-language tasks. Extensive experiments demonstrate that models quantized by QSLAW perform on 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Soft-Hard Attention U-Net的深度学习模型，旨在增强多尺度图像中去阴影的效果，并建立了一个包含手动阴影标注和配对无阴影图像的基准数据集，以应对现有去阴影方法的限制，如模型假设的限制性、复杂阴影模式的忽视、以及仅包含简单场景的现有数据集问题。通过这个模型和数据集，研究者能够更有效地处理多样化的阴影场景，这对于提高图像质量和提升计算机视觉和数字摄影等领域的应用效率至关重要。</title><link>https://arxiv.org/abs/2408.03734</link><description>&lt;p&gt;
Soft-Hard Attention U-Net Model and Benchmark Dataset for Multiscale Image Shadow Removal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03734
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Soft-Hard Attention U-Net的深度学习模型，旨在增强多尺度图像中去阴影的效果，并建立了一个包含手动阴影标注和配对无阴影图像的基准数据集，以应对现有去阴影方法的限制，如模型假设的限制性、复杂阴影模式的忽视、以及仅包含简单场景的现有数据集问题。通过这个模型和数据集，研究者能够更有效地处理多样化的阴影场景，这对于提高图像质量和提升计算机视觉和数字摄影等领域的应用效率至关重要。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03734v1 Announce Type: new  Abstract: Effective shadow removal is pivotal in enhancing the visual quality of images in various applications, ranging from computer vision to digital photography. During the last decades physics and machine learning -based methodologies have been proposed; however, most of them have limited capacity in capturing complex shadow patterns due to restrictive model assumptions, neglecting the fact that shadows usually appear at different scales. Also, current datasets used for benchmarking shadow removal are composed of a limited number of images with simple scenes containing mainly uniform shadows cast by single objects, whereas only a few of them include both manual shadow annotations and paired shadow-free images. Aiming to address all these limitations in the context of natural scene imaging, including urban environments with complex scenes, the contribution of this study is twofold: a) it proposes a novel deep learning architecture, named Soft-
&lt;/p&gt;</description></item><item><title>该文章提出的SeRankDet通过采用“Pick of the Bunch”原则，利用Selective Rank-Aware Attention模块和非线性Top-K选择过程，有效提升了红外小目标检测的准确性，实现了超越传统hit-miss trade-off的性能。通过Large Selective Feature Fusion模块替换了U-Net结构中的静态连接，增强了网络对真实目标与虚假警报的鉴别能力。</title><link>https://arxiv.org/abs/2408.03717</link><description>&lt;p&gt;
Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss Trade-Offs via Selective Rank-Aware Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03717
&lt;/p&gt;
&lt;p&gt;
该文章提出的SeRankDet通过采用“Pick of the Bunch”原则，利用Selective Rank-Aware Attention模块和非线性Top-K选择过程，有效提升了红外小目标检测的准确性，实现了超越传统hit-miss trade-off的性能。通过Large Selective Feature Fusion模块替换了U-Net结构中的静态连接，增强了网络对真实目标与虚假警报的鉴别能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03717v1 Announce Type: new  Abstract: Infrared small target detection faces the inherent challenge of precisely localizing dim targets amidst complex background clutter. Traditional approaches struggle to balance detection precision and false alarm rates. To break this dilemma, we propose SeRankDet, a deep network that achieves high accuracy beyond the conventional hit-miss trade-off, by following the ``Pick of the Bunch'' principle. At its core lies our Selective Rank-Aware Attention (SeRank) module, employing a non-linear Top-K selection process that preserves the most salient responses, preventing target signal dilution while maintaining constant complexity. Furthermore, we replace the static concatenation typical in U-Net structures with our Large Selective Feature Fusion (LSFF) module, a dynamic fusion strategy that empowers SeRankDet with adaptive feature integration, enhancing its ability to discriminate true targets from false alarms. The network's discernment is fur
&lt;/p&gt;</description></item><item><title>该文章提出CAS-ViT: Convolutional Additive Self-attention Vision Transformers，通过构建高效的信息交互机制，改进了ViT在全球背景信息获取方面的效率，适用于资源受限的移动应用场景。</title><link>https://arxiv.org/abs/2408.03703</link><description>&lt;p&gt;
CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03703
&lt;/p&gt;
&lt;p&gt;
该文章提出CAS-ViT: Convolutional Additive Self-attention Vision Transformers，通过构建高效的信息交互机制，改进了ViT在全球背景信息获取方面的效率，适用于资源受限的移动应用场景。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03703v1 Announce Type: new  Abstract: Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer's powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we construct a novel additive similarity function following this paradigm and present an efficient implementation named Convolutional Additive Token Mixer (CATM). This simplification leads to a significant reduc
&lt;/p&gt;</description></item><item><title>该文章的主要贡献在于开发了一项大型数据集Openstory++以及一套新的训练方法，该方法通过引入实例级标注和更清晰的训练准则来改进对开放域视觉故事叙说中多实例统一性的维护。</title><link>https://arxiv.org/abs/2408.03695</link><description>&lt;p&gt;
Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03695
&lt;/p&gt;
&lt;p&gt;
该文章的主要贡献在于开发了一项大型数据集Openstory++以及一套新的训练方法，该方法通过引入实例级标注和更清晰的训练准则来改进对开放域视觉故事叙说中多实例统一性的维护。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03695v1 Announce Type: new  Abstract: Recent image generation models excel at creating high-quality images from brief captions. However, they fail to maintain consistency of multiple instances across images when encountering lengthy contexts. This inconsistency is largely due to in existing training datasets the absence of granular instance feature labeling in existing training datasets. To tackle these issues, we introduce Openstory++, a large-scale dataset combining additional instance-level annotations with both images and text. Furthermore, we develop a training methodology that emphasizes entity-centric image-text generation, ensuring that the models learn to effectively interweave visual and textual information. Specifically, Openstory++ streamlines the process of keyframe extraction from open-domain videos, employing vision-language models to generate captions that are then polished by a large language model for narrative continuity. It surpasses previous datasets by 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为L4DR的系统，该系统通过融合激光雷达（LiDAR）和4D雷达技术，实现了在恶劣天气条件下的3D对象检测。L4DR使用多模态编码（MME）和前景感知去噪（FAD）技术来解决传感器间的差异，并首次探索了激光雷达和4D雷达早期融合的互补性。此外，文章还设计了一种并行特征提取方法（{IM}2），加强了特征提取的效率和准确性。通过这些创新方法，L4DR在提高3D检测性能的同时，显著提升了系统的环境适应性。</title><link>https://arxiv.org/abs/2408.03677</link><description>&lt;p&gt;
L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03677
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为L4DR的系统，该系统通过融合激光雷达（LiDAR）和4D雷达技术，实现了在恶劣天气条件下的3D对象检测。L4DR使用多模态编码（MME）和前景感知去噪（FAD）技术来解决传感器间的差异，并首次探索了激光雷达和4D雷达早期融合的互补性。此外，文章还设计了一种并行特征提取方法（{IM}2），加强了特征提取的效率和准确性。通过这些创新方法，L4DR在提高3D检测性能的同时，显著提升了系统的环境适应性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03677v1 Announce Type: new  Abstract: LiDAR-based vision systems are integral for 3D object detection, which is crucial for autonomous navigation. However, they suffer from performance degradation in adverse weather conditions due to the quality deterioration of LiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is expected to solve this problem. However, the fusion of LiDAR and 4D radar is challenging because they differ significantly in terms of data quality and the degree of degradation in adverse weather. To address these issues, we introduce L4DR, a weather-robust 3D object detection method that effectively achieves LiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and Foreground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is the first exploration of the complementarity of early fusion between LiDAR and 4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 ) parallel feature extraction bac
&lt;/p&gt;</description></item><item><title>该文章提出了一种极低内存消耗的CNN模型，该模型使得资源有限的低端嵌入式和物联网设备能够在极其有限的63KB内存下执行包括图像分类和物体检测在内的本地视觉任务。通过结合MobileNet的瓶颈块设计原理，作者提出了三个主要的设计原则，这些原则大大减少了对CNN峰值内存的消耗，使得模型能够适应设备上有限的KB内存。文章通过将输入图像分割为一系列小块并构建独立的小块传输路径，从而有效降低了内存需求，并为每块构建了多个瓶颈块组成的隧道路径，实现了在保持轻量级内存消耗的同时，从输入块到模型最后一层的穿透性连接。</title><link>https://arxiv.org/abs/2408.03663</link><description>&lt;p&gt;
Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03663
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种极低内存消耗的CNN模型，该模型使得资源有限的低端嵌入式和物联网设备能够在极其有限的63KB内存下执行包括图像分类和物体检测在内的本地视觉任务。通过结合MobileNet的瓶颈块设计原理，作者提出了三个主要的设计原则，这些原则大大减少了对CNN峰值内存的消耗，使得模型能够适应设备上有限的KB内存。文章通过将输入图像分割为一系列小块并构建独立的小块传输路径，从而有效降低了内存需求，并为每块构建了多个瓶颈块组成的隧道路径，实现了在保持轻量级内存消耗的同时，从输入块到模型最后一层的穿透性连接。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03663v1 Announce Type: new  Abstract: In this paper, we introduce a memory-efficient CNN (convolutional neural network), which enables resource-constrained low-end embedded and IoT devices to perform on-device vision tasks, such as image classification and object detection, using extremely low memory, i.e., only 63 KB on ImageNet classification. Based on the bottleneck block of MobileNet, we propose three design principles that significantly curtail the peak memory usage of a CNN so that it can fit the limited KB memory of the low-end device. First, 'input segmentation' divides an input image into a set of patches, including the central patch overlapped with the others, reducing the size (and memory requirement) of a large input image. Second, 'patch tunneling' builds independent tunnel-like paths consisting of multiple bottleneck blocks per patch, penetrating through the entire model from an input patch to the last layer of the network, maintaining lightweight memory usage 
&lt;/p&gt;</description></item><item><title>该文章提出了PHOCUS技术，这是一种基于物理学的超声波分辨率增强技术，能够通过建模点扩散函数（PSF）直接在B-模式图像上进行去卷积处理，从而恢复出高分辨率的反射器分布，有效地改善了超声图像的清晰度和细节表现。</title><link>https://arxiv.org/abs/2408.03657</link><description>&lt;p&gt;
PHOCUS: Physics-Based Deconvolution for Ultrasound Resolution Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03657
&lt;/p&gt;
&lt;p&gt;
该文章提出了PHOCUS技术，这是一种基于物理学的超声波分辨率增强技术，能够通过建模点扩散函数（PSF）直接在B-模式图像上进行去卷积处理，从而恢复出高分辨率的反射器分布，有效地改善了超声图像的清晰度和细节表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03657v1 Announce Type: new  Abstract: Ultrasound is widely used in medical diagnostics allowing for accessible and powerful imaging but suffers from resolution limitations due to diffraction and the finite aperture of the imaging system, which restricts diagnostic use. The impulse function of an ultrasound imaging system is called the point spread function (PSF), which is convolved with the spatial distribution of reflectors in the image formation process. Recovering high-resolution reflector distributions by removing image distortions induced by the convolution process improves image clarity and detail. Conventionally, deconvolution techniques attempt to rectify the imaging system's dependent PSF, working directly on the radio-frequency (RF) data. However, RF data is often not readily accessible. Therefore, we introduce a physics-based deconvolution process using a modeled PSF, working directly on the more commonly available B-mode images. By leveraging Implicit Neural Repr
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用去噪扩散模型进行胎儿大脑异常的无监督检测方法，通过仅使用正常胎儿大脑超声图像进行训练，解决了异常数据稀缺的问题，并在实际临床数据集上取得了良好效果。</title><link>https://arxiv.org/abs/2408.03654</link><description>&lt;p&gt;
Unsupervised Detection of Fetal Brain Anomalies using Denoising Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03654
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用去噪扩散模型进行胎儿大脑异常的无监督检测方法，通过仅使用正常胎儿大脑超声图像进行训练，解决了异常数据稀缺的问题，并在实际临床数据集上取得了良好效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03654v1 Announce Type: cross  Abstract: Congenital malformations of the brain are among the most common fetal abnormalities that impact fetal development. Previous anomaly detection methods on ultrasound images are based on supervised learning, rely on manual annotations, and risk missing underrepresented categories. In this work, we frame fetal brain anomaly detection as an unsupervised task using diffusion models. To this end, we employ an inpainting-based Noise Agnostic Anomaly Detection approach that identifies the abnormality using diffusion-reconstructed fetal brain images from multiple noise levels. Our approach only requires normal fetal brain ultrasound images for training, addressing the limited availability of abnormal data. Our experiments on a real-world clinical dataset show the potential of using unsupervised methods for fetal brain anomaly detection. Additionally, we comprehensively evaluate how different noise types affect diffusion models in the fetal anoma
&lt;/p&gt;</description></item><item><title>该文章提出了SAM2-PATH模型，它通过集成可训练的Kolmogorov-Arnold Networks分类模块和迄今为止最大的预训练视觉编码器UNI，增强了SAM2在数字病理学中的语义分割能力，从而提供了更优的实例分割模型。</title><link>https://arxiv.org/abs/2408.03651</link><description>&lt;p&gt;
SAM2-PATH: A better segment anything model for semantic segmentation in digital pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03651
&lt;/p&gt;
&lt;p&gt;
该文章提出了SAM2-PATH模型，它通过集成可训练的Kolmogorov-Arnold Networks分类模块和迄今为止最大的预训练视觉编码器UNI，增强了SAM2在数字病理学中的语义分割能力，从而提供了更优的实例分割模型。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03651v1 Announce Type: cross  Abstract: The semantic segmentation task in pathology plays an indispensable role in assisting physicians in determining the condition of tissue lesions. Foundation models, such as the SAM (Segment Anything Model) and SAM2, exhibit exceptional performance in instance segmentation within everyday natural scenes. SAM-PATH has also achieved impressive results in semantic segmentation within the field of pathology. However, in computational pathology, the models mentioned above still have the following limitations. The pre-trained encoder models suffer from a scarcity of pathology image data; SAM and SAM2 are not suitable for semantic segmentation. In this paper, we have designed a trainable Kolmogorov-Arnold Networks(KAN) classification module within the SAM2 workflow, and we have introduced the largest pretrained vision encoder for histopathology (UNI) to date. Our proposed framework, SAM2-PATH, augments SAM2's capability to perform semantic segme
&lt;/p&gt;</description></item><item><title>该文章提出TALE框架，这是一种无需训练的新方法，通过自适应的潜在操纵和能量引导的优化，利用文本到图像扩散模型的生成能力，解决了跨域图像合成问题。该方法能够将用户指定的对象无缝地融入指定场景中，即使这些对象和场景属于不同的领域。它无需训练额外的网络或对扩散模型进行再训练，从而能够保持预训练扩散模型在文本和视觉方面的鲁棒性。此外，该方法通过操纵能量场来改善合成结果，确保了合成过程中视觉效果的连贯性和合理性，展现了在无需训练的情况下进行跨域图像合成的潜力。</title><link>https://arxiv.org/abs/2408.03637</link><description>&lt;p&gt;
TALE: Training-free Cross-domain Image Composition via Adaptive Latent Manipulation and Energy-guided Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03637
&lt;/p&gt;
&lt;p&gt;
该文章提出TALE框架，这是一种无需训练的新方法，通过自适应的潜在操纵和能量引导的优化，利用文本到图像扩散模型的生成能力，解决了跨域图像合成问题。该方法能够将用户指定的对象无缝地融入指定场景中，即使这些对象和场景属于不同的领域。它无需训练额外的网络或对扩散模型进行再训练，从而能够保持预训练扩散模型在文本和视觉方面的鲁棒性。此外，该方法通过操纵能量场来改善合成结果，确保了合成过程中视觉效果的连贯性和合理性，展现了在无需训练的情况下进行跨域图像合成的潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03637v1 Announce Type: new  Abstract: We present TALE, a novel training-free framework harnessing the generative capabilities of text-to-image diffusion models to address the cross-domain image composition task that focuses on flawlessly incorporating user-specified objects into a designated visual contexts regardless of domain disparity. Previous methods often involve either training auxiliary networks or finetuning diffusion models on customized datasets, which are expensive and may undermine the robust textual and visual priors of pre-trained diffusion models. Some recent works attempt to break the barrier by proposing training-free workarounds that rely on manipulating attention maps to tame the denoising process implicitly. However, composing via attention maps does not necessarily yield desired compositional outcomes. These approaches could only retain some semantic information and usually fall short in preserving identity characteristics of input objects or exhibit li
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为 Concept Conductor 的训练免费框架，能够确保在多概念定制下实现视觉真实感和正确布局，并通过自我注意为基础的空间指导纠正布局错误，通过特征融合和使用形状感知遮罩以指定每个概念的生成区域，从而在注意力层中注入个人化概念的结构和外观。</title><link>https://arxiv.org/abs/2408.03632</link><description>&lt;p&gt;
Concept Conductor: Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03632
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为 Concept Conductor 的训练免费框架，能够确保在多概念定制下实现视觉真实感和正确布局，并通过自我注意为基础的空间指导纠正布局错误，通过特征融合和使用形状感知遮罩以指定每个概念的生成区域，从而在注意力层中注入个人化概念的结构和外观。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03632v1 Announce Type: new  Abstract: The customization of text-to-image models has seen significant advancements, yet generating multiple personalized concepts remains a challenging task. Current methods struggle with attribute leakage and layout confusion when handling multiple concepts, leading to reduced concept fidelity and semantic consistency. In this work, we introduce a novel training-free framework, Concept Conductor, designed to ensure visual fidelity and correct layout in multi-concept customization. Concept Conductor isolates the sampling processes of multiple custom models to prevent attribute leakage between different concepts and corrects erroneous layouts through self-attention-based spatial guidance. Additionally, we present a concept injection technique that employs shape-aware masks to specify the generation area for each concept. This technique injects the structure and appearance of personalized concepts through feature fusion in the attention layers, e
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Batch Instance Discrimination and Feature Clustering（BIDFC）的框架，用于SAR自动目标识别。该框架对传统对比学习的目标进行了调整，旨在解决标注数据不足的问题。通过将数据划分为批次并赋予实例标签，文章提出了“弱对比学习”的概念，同时调整了嵌入距离，以应对SAR图像中样本的高相似性。此外，文中还提到了随机增强和特征聚类技术，以提高小样本学习下的识别效率。</title><link>https://arxiv.org/abs/2408.03627</link><description>&lt;p&gt;
Weakly Contrastive Learning via Batch Instance Discrimination and Feature Clustering for Small Sample SAR ATR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03627
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Batch Instance Discrimination and Feature Clustering（BIDFC）的框架，用于SAR自动目标识别。该框架对传统对比学习的目标进行了调整，旨在解决标注数据不足的问题。通过将数据划分为批次并赋予实例标签，文章提出了“弱对比学习”的概念，同时调整了嵌入距离，以应对SAR图像中样本的高相似性。此外，文中还提到了随机增强和特征聚类技术，以提高小样本学习下的识别效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03627v1 Announce Type: new  Abstract: In recent years, impressive performance of deep learning technology has been recognized in Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR). Since a large amount of annotated data is required in this technique, it poses a trenchant challenge to the issue of obtaining a high recognition rate through less labeled data. To overcome this problem, inspired by the contrastive learning, we proposed a novel framework named Batch Instance Discrimination and Feature Clustering (BIDFC). In this framework, different from that of the objective of general contrastive learning methods, embedding distance between samples should be moderate because of the high similarity between samples in the SAR images. Consequently, our flexible framework is equipped with adjustable distance between embedding, which we term as weakly contrastive learning. Technically, instance labels are assigned to the unlabeled data in per batch and random augmentat
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为AgentsCoMerge的协作决策框架，该框架结合了大型语言模型（LLMs），旨在提高连接和自主车辆在多车道合并区域的安全性和效率，以及减少交通拥堵、事故发生率和相关碳排放。通过设计场景观察和理解模块，框架能够让单个车辆作为决策者，理解其周围环境，并通过层次规划模块做出决策并计划路径。此外，该框架通过引入通信模块，允许车辆之间交换必要信息并协同行为，从而实现在合并点上的协作。整个系统通过强化学习和模拟测试来优化，展示了其在提高交通系统性能方面的创新贡献。</title><link>https://arxiv.org/abs/2408.03624</link><description>&lt;p&gt;
AgentsCoMerge: Large Language Model Empowered Collaborative Decision Making for Ramp Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03624
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为AgentsCoMerge的协作决策框架，该框架结合了大型语言模型（LLMs），旨在提高连接和自主车辆在多车道合并区域的安全性和效率，以及减少交通拥堵、事故发生率和相关碳排放。通过设计场景观察和理解模块，框架能够让单个车辆作为决策者，理解其周围环境，并通过层次规划模块做出决策并计划路径。此外，该框架通过引入通信模块，允许车辆之间交换必要信息并协同行为，从而实现在合并点上的协作。整个系统通过强化学习和模拟测试来优化，展示了其在提高交通系统性能方面的创新贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03624v1 Announce Type: new  Abstract: Ramp merging is one of the bottlenecks in traffic systems, which commonly cause traffic congestion, accidents, and severe carbon emissions. In order to address this essential issue and enhance the safety and efficiency of connected and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel collaborative decision-making framework, named AgentsCoMerge, to leverage large language models (LLMs). Specifically, we first design a scene observation and understanding module to allow an agent to capture the traffic environment. Then we propose a hierarchical planning module to enable the agent to make decisions and plan trajectories based on the observation and the agent's own state. In addition, in order to facilitate collaboration among multiple agents, we introduce a communication module to enable the surrounding agents to exchange necessary information and coordinate their actions. Finally, we develop a reinforcement reflec
&lt;/p&gt;</description></item><item><title>该文章提出的创新框架通过知识蒸馏结合图像重建技术，为单一标记的医疗图像进行了一键式分割，有效缓解了传统方法因注册错误和低质量合成图像引起的性能问题，增强了模型在实际中的广泛泛化能力。</title><link>https://arxiv.org/abs/2408.03616</link><description>&lt;p&gt;
Distillation Learning Guided by Image Reconstruction for One-Shot Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03616
&lt;/p&gt;
&lt;p&gt;
该文章提出的创新框架通过知识蒸馏结合图像重建技术，为单一标记的医疗图像进行了一键式分割，有效缓解了传统方法因注册错误和低质量合成图像引起的性能问题，增强了模型在实际中的广泛泛化能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03616v1 Announce Type: cross  Abstract: Traditional one-shot medical image segmentation (MIS) methods use registration networks to propagate labels from a reference atlas or rely on comprehensive sampling strategies to generate synthetic labeled data for training. However, these methods often struggle with registration errors and low-quality synthetic images, leading to poor performance and generalization. To overcome this, we introduce a novel one-shot MIS framework based on knowledge distillation, which allows the network to directly 'see' real images through a distillation process guided by image reconstruction. It focuses on anatomical structures in a single labeled image and a few unlabeled ones. A registration-based data augmentation network creates realistic, labeled samples, while a feature distillation module helps the student network learn segmentation from these samples, guided by the teacher network. During inference, the streamlined student network accurately se
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为JARViS的统一演员-场景上下文关系建模方法，通过利用Transformer注意力机制在全球性的时空维度上有效整合了跨模态动作语义，提高了视频动作检测的性能。</title><link>https://arxiv.org/abs/2408.03612</link><description>&lt;p&gt;
JARViS: Detecting Actions in Video Using Unified Actor-Scene Context Relation Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03612
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为JARViS的统一演员-场景上下文关系建模方法，通过利用Transformer注意力机制在全球性的时空维度上有效整合了跨模态动作语义，提高了视频动作检测的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03612v1 Announce Type: new  Abstract: Video action detection (VAD) is a formidable vision task that involves the localization and classification of actions within the spatial and temporal dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD methods utilize a pre-trained person detector to extract the region of interest features, subsequently employing these features for action detection. However, the performance of two-stage VAD methods has been limited as they depend solely on localized actor features to infer action semantics. In this study, we propose a new two-stage VAD framework called Joint Actor-scene context Relation modeling based on Visual Semantics (JARViS), which effectively consolidates cross-modal action semantics distributed globally across spatial and temporal dimensions using Transformer attention. JARViS employs a person detector to produce densely sampled actor features from a keyframe. Concurrently, it uses a video backbone to cre
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为InPer的框架，通过引入因果干预和测试时的因果扰动，旨在通过将因果理论应用于神经网络训练，以提升模型在未知环境中的表现和对外部变量的抵抗能力，从而解决深度学习模型在不同数据源之间推广问题。</title><link>https://arxiv.org/abs/2408.03608</link><description>&lt;p&gt;
InPer: Whole-Process Domain Generalization via Causal Intervention and Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03608
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为InPer的框架，通过引入因果干预和测试时的因果扰动，旨在通过将因果理论应用于神经网络训练，以提升模型在未知环境中的表现和对外部变量的抵抗能力，从而解决深度学习模型在不同数据源之间推广问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03608v1 Announce Type: cross  Abstract: Despite the considerable advancements achieved by deep neural networks, their performance tends to degenerate when the test environment diverges from the training ones. Domain generalization (DG) solves this issue by learning representations independent of domain-related information, thus facilitating extrapolation to unseen environments. Existing approaches typically focus on formulating tailored training objectives to extract shared features from the source data. However, the disjointed training and testing procedures may compromise robustness, particularly in the face of unforeseen variations during deployment. In this paper, we propose a novel and holistic framework based on causality, named InPer, designed to enhance model generalization by incorporating causal intervention during training and causal perturbation during testing. Specifically, during the training phase, we employ entropy-based causal intervention (EnIn) to refine t
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的方法PRISM（PRogressive dependency maxImization for Scale-invariant image Matching），该方法通过渐进式依赖最大化来解决尺度不变图像匹配中的问题，提高了匹配的准确性和效率。</title><link>https://arxiv.org/abs/2408.03598</link><description>&lt;p&gt;
PRISM: PRogressive dependency maxImization for Scale-invariant image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03598
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的方法PRISM（PRogressive dependency maxImization for Scale-invariant image Matching），该方法通过渐进式依赖最大化来解决尺度不变图像匹配中的问题，提高了匹配的准确性和效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03598v1 Announce Type: new  Abstract: Image matching aims at identifying corresponding points between a pair of images. Currently, detector-free methods have shown impressive performance in challenging scenarios, thanks to their capability of generating dense matches and global receptive field. However, performing feature interaction and proposing matches across the entire image is unnecessary, because not all image regions contribute to the matching process. Interacting and matching in unmatchable areas can introduce errors, reducing matching accuracy and efficiency. Meanwhile, the scale discrepancy issue still troubles existing methods. To address above issues, we propose PRogressive dependency maxImization for Scale-invariant image Matching (PRISM), which jointly prunes irrelevant patch features and tackles the scale discrepancy. To do this, we firstly present a Multi-scale Pruning Module (MPM) to adaptively prune irrelevant features by maximizing the dependency between t
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为Hierarchical Quantum Control Gates (HQCG)的量子方法，用于提高对功能性磁共振成像（fMRI）数据的理解。该方法包括两个创新的模块：Local Quantum Control Gate (LQCG)和Global Quantum Control Gate (GQCG)，旨在提取局部和全局的fMRI信号特征。通过在量子计算机上进行端到端的训练，该方法能够有效分析高维度的fMRI信号，显著优于传统方法。这项工作展示了量子计算技术在处理复杂数据问题方面的潜力。</title><link>https://arxiv.org/abs/2408.03596</link><description>&lt;p&gt;
Hierarchical Quantum Control Gates for Functional MRI Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03596
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为Hierarchical Quantum Control Gates (HQCG)的量子方法，用于提高对功能性磁共振成像（fMRI）数据的理解。该方法包括两个创新的模块：Local Quantum Control Gate (LQCG)和Global Quantum Control Gate (GQCG)，旨在提取局部和全局的fMRI信号特征。通过在量子计算机上进行端到端的训练，该方法能够有效分析高维度的fMRI信号，显著优于传统方法。这项工作展示了量子计算技术在处理复杂数据问题方面的潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03596v1 Announce Type: cross  Abstract: Quantum computing has emerged as a powerful tool for solving complex problems intractable for classical computers, particularly in popular fields such as cryptography, optimization, and neurocomputing. In this paper, we present a new quantum-based approach named the Hierarchical Quantum Control Gates (HQCG) method for efficient understanding of Functional Magnetic Resonance Imaging (fMRI) data. This approach includes two novel modules: the Local Quantum Control Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are designed to extract local and global features of fMRI signals, respectively. Our method operates end-to-end on a quantum machine, leveraging quantum mechanics to learn patterns within extremely high-dimensional fMRI signals, such as 30,000 samples which is a challenge for classical computers. Empirical results demonstrate that our approach significantly outperforms classical methods. Additionally, we found that th
&lt;/p&gt;</description></item><item><title>该文章开发了一种名为HistoSPACE的模型，该模型通过整合来自多样化的组织图像和ST数据，使用深度学习方法提取分子层面的信息，从而推动了组织图像分析在分子研究中的应用。</title><link>https://arxiv.org/abs/2408.03592</link><description>&lt;p&gt;
HistoSPACE: Histology-Inspired Spatial Transcriptome Prediction And Characterization Engine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03592
&lt;/p&gt;
&lt;p&gt;
该文章开发了一种名为HistoSPACE的模型，该模型通过整合来自多样化的组织图像和ST数据，使用深度学习方法提取分子层面的信息，从而推动了组织图像分析在分子研究中的应用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03592v1 Announce Type: cross  Abstract: Spatial transcriptomics (ST) enables the visualization of gene expression within the context of tissue morphology. This emerging discipline has the potential to serve as a foundation for developing tools to design precision medicines. However, due to the higher costs and expertise required for such experiments, its translation into a regular clinical practice might be challenging. Despite the implementation of modern deep learning to enhance information obtained from histological images using AI, efforts have been constrained by limitations in the diversity of information. In this paper, we developed a model, HistoSPACE that explore the diversity of histological images available with ST data to extract molecular insights from tissue image. Our proposed study built an image encoder derived from universal image autoencoder. This image encoder was connected to convolution blocks to built the final model. It was further fine tuned with the
&lt;/p&gt;</description></item><item><title>该文章提出了一种无需用户特定校准的眼球追踪系统，通过深度学习模型分析眼球运动特征，实现了厘米级别的焦点距离估算精度，为提高个性化视觉技术和增强现实应用的集成水平做出了贡献。</title><link>https://arxiv.org/abs/2408.03591</link><description>&lt;p&gt;
Focal Depth Estimation: A Calibration-Free, Subject- and Daytime Invariant Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03591
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种无需用户特定校准的眼球追踪系统，通过深度学习模型分析眼球运动特征，实现了厘米级别的焦点距离估算精度，为提高个性化视觉技术和增强现实应用的集成水平做出了贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03591v1 Announce Type: new  Abstract: In an era where personalized technology is increasingly intertwined with daily life, traditional eye-tracking systems and autofocal glasses face a significant challenge: the need for frequent, user-specific calibration, which impedes their practicality. This study introduces a groundbreaking calibration-free method for estimating focal depth, leveraging machine learning techniques to analyze eye movement features within short sequences. Our approach, distinguished by its innovative use of LSTM networks and domain-specific feature engineering, achieves a mean absolute error (MAE) of less than 10 cm, setting a new focal depth estimation accuracy standard. This advancement promises to enhance the usability of autofocal glasses and pave the way for their seamless integration into extended reality environments, marking a significant leap forward in personalized visual technology.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为NumCLIP的方法，通过将数字感知分解为粗略的分类和精确预测阶段，改进了VLM在视觉方面对数字理解的局限性，实现了在数字排序任务中的创新突破。</title><link>https://arxiv.org/abs/2408.03574</link><description>&lt;p&gt;
Teach CLIP to Develop a Number Sense for Ordinal Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03574
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为NumCLIP的方法，通过将数字感知分解为粗略的分类和精确预测阶段，改进了VLM在视觉方面对数字理解的局限性，实现了在数字排序任务中的创新突破。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03574v1 Announce Type: new  Abstract: Ordinal regression is a fundamental problem within the field of computer vision, with customised well-trained models on specific tasks. While pre-trained vision-language models (VLMs) have exhibited impressive performance on various vision tasks, their potential for ordinal regression has received less exploration. In this study, we first investigate CLIP's potential for ordinal regression, from which we expect the model could generalise to different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP fails on this task, since current VLMs have a well-documented limitation of encapsulating compositional concepts such as number sense. We propose a simple yet effective method called NumCLIP to improve the quantitative understanding of VLMs. We disassemble the exact image to number-specific text matching problem into coarse classification and fine prediction stages. We discretize and phrase each numerical bin with common lan
&lt;/p&gt;</description></item><item><title>该文章通过将深度学习与生成对抗网络（GAN）结合的创新方法，对比了传统图像识别方法在图像识别领域的应用，展示了GAN技术在图像生成和识别方面独特的优势，并通过对多个公共图像数据集的实验验证了GAN在图像识别中的有效性。</title><link>https://arxiv.org/abs/2408.03568</link><description>&lt;p&gt;
A comparative study of generative adversarial networks for image recognition algorithms based on deep learning and traditional methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03568
&lt;/p&gt;
&lt;p&gt;
该文章通过将深度学习与生成对抗网络（GAN）结合的创新方法，对比了传统图像识别方法在图像识别领域的应用，展示了GAN技术在图像生成和识别方面独特的优势，并通过对多个公共图像数据集的实验验证了GAN在图像识别中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03568v1 Announce Type: new  Abstract: In this paper, an image recognition algorithm based on the combination of deep learning and generative adversarial network (GAN) is studied, and compared with traditional image recognition methods. The purpose of this study is to evaluate the advantages and application prospects of deep learning technology, especially GAN, in the field of image recognition. Firstly, this paper reviews the basic principles and techniques of traditional image recognition methods, including the classical algorithms based on feature extraction such as SIFT, HOG and their combination with support vector machine (SVM), random forest, and other classifiers. Then, the working principle, network structure, and unique advantages of GAN in image generation and recognition are introduced. In order to verify the effectiveness of GAN in image recognition, a series of experiments are designed and carried out using multiple public image data sets for training and testin
&lt;/p&gt;</description></item><item><title>该文章提出EMBED方法，通过独特的视频-语言数据变换框架实现对以第三人称视角为主的视频内容进行优化，使其更适合于第一人称视角视频的模型学习，尤其是在处理人手动作图像方面。通过这种创新的转换技术，文章为第一人称视频学习提供了更多的数据支持，促进了手部动作识别的研究进展，并且增强了模型的泛化能力。</title><link>https://arxiv.org/abs/2408.03567</link><description>&lt;p&gt;
Unlocking Exocentric Video-Language Data for Egocentric Video Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03567
&lt;/p&gt;
&lt;p&gt;
该文章提出EMBED方法，通过独特的视频-语言数据变换框架实现对以第三人称视角为主的视频内容进行优化，使其更适合于第一人称视角视频的模型学习，尤其是在处理人手动作图像方面。通过这种创新的转换技术，文章为第一人称视频学习提供了更多的数据支持，促进了手部动作识别的研究进展，并且增强了模型的泛化能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03567v1 Announce Type: new  Abstract: We present EMBED (Egocentric Models Built with Exocentric Data), a method designed to transform exocentric video-language data for egocentric video representation learning. Large-scale exocentric data covers diverse activities with significant potential for egocentric learning, but inherent disparities between egocentric and exocentric data pose challenges in utilizing one view for the other seamlessly. Egocentric videos predominantly feature close-up hand-object interactions, whereas exocentric videos offer a broader perspective on human activities. Additionally, narratives in egocentric datasets are typically more action-centric and closely linked with the visual content, in contrast to the narrative styles found in exocentric datasets. To address these challenges, we employ a data transformation framework to adapt exocentric data for egocentric training, focusing on identifying specific video clips that emphasize hand-object interacti
&lt;/p&gt;</description></item><item><title>该文章提出了使用消费级无人机搭载快速扫描仪（Aerial-Aquatic Speedy Scanner, AASS）和深度学习超分辨率重建与检测网络对水下垃圾进行监测的创新方法。通过提高数据采集效率，AASS能够捕获高质量图像，结合超分辨率重建（SRR）技术，尤其是在重建后的图像上使用RCAN模型，提高了检测任务的准确率。这样的系统有望提高水下垃圾监测的效率和准确性，有助于改善水体生态环境。</title><link>https://arxiv.org/abs/2408.03564</link><description>&lt;p&gt;
Underwater litter monitoring using consumer-grade aerial-aquatic speedy scanner (AASS) and deep learning based super-resolution reconstruction and detection network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03564
&lt;/p&gt;
&lt;p&gt;
该文章提出了使用消费级无人机搭载快速扫描仪（Aerial-Aquatic Speedy Scanner, AASS）和深度学习超分辨率重建与检测网络对水下垃圾进行监测的创新方法。通过提高数据采集效率，AASS能够捕获高质量图像，结合超分辨率重建（SRR）技术，尤其是在重建后的图像上使用RCAN模型，提高了检测任务的准确率。这样的系统有望提高水下垃圾监测的效率和准确性，有助于改善水体生态环境。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03564v1 Announce Type: new  Abstract: Underwater litter is widely spread across aquatic environments such as lakes, rivers, and oceans, significantly impacting natural ecosystems. Current monitoring technologies for detecting underwater litter face limitations in survey efficiency, cost, and environmental conditions, highlighting the need for efficient, consumer-grade technologies for automatic detection. This research introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with Super-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network. AASS enhances data acquisition efficiency over traditional methods, capturing high-quality images that accurately identify underwater waste. SRR improves image-resolution by mitigating motion blur and insufficient resolution, thereby enhancing detection tasks. Specifically, the RCAN model achieved the highest mean average precision (mAP) of 78.6% for detection accuracy on reconstructed images among the tested SRR mod
&lt;/p&gt;</description></item><item><title>该文章提出了一种结合无人机遥感、超分辨率重建和改进后的YOLOv8s网络的创新方法，用于监测海蚌，显著提升了检测的准确性和效率。</title><link>https://arxiv.org/abs/2408.03559</link><description>&lt;p&gt;
Monitoring of Hermit Crabs Using drone-captured imagery and Deep Learning based Super-Resolution Reconstruction and Improved YOLOv8
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03559
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种结合无人机遥感、超分辨率重建和改进后的YOLOv8s网络的创新方法，用于监测海蚌，显著提升了检测的准确性和效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03559v1 Announce Type: new  Abstract: Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds, cleaning up debris, and disturbing soil. They serve as vital indicators of marine environmental health, responding to climate change and pollution. Traditional survey methods, like quadrat sampling, are labor-intensive, time-consuming, and environmentally dependent. This study presents an innovative approach combining UAV-based remote sensing with Super-Resolution Reconstruction (SRR) and the CRAB-YOLO detection network, a modification of YOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing issues such as motion blur and insufficient resolution, significantly improving detection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO network integrates three improvements for detection accuracy, hermit crab characteristics, and computational efficiency, achieving state-of-the-art (SOTA) performance compared to other mainstream d
&lt;/p&gt;</description></item><item><title>该文章提出了一个称为D²Styler的框架，该框架使用VQ-GANs的离散表示能力和离散差分的优势，如稳定的训练和无模式塌陷，结合了自适应实例归一化（AdaIN），以在内容和风格图像之间平滑地转移特征，显著提高了风格转移图像的视觉质量。</title><link>https://arxiv.org/abs/2408.03558</link><description>&lt;p&gt;
D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03558
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个称为D²Styler的框架，该框架使用VQ-GANs的离散表示能力和离散差分的优势，如稳定的训练和无模式塌陷，结合了自适应实例归一化（AdaIN），以在内容和风格图像之间平滑地转移特征，显著提高了风格转移图像的视觉质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03558v1 Announce Type: new  Abstract: In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We t
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为VPOcc的框架，该框架利用vanishing point（VP）对单摄像头3D语义占据预测进行了改进。其框架包括三个新的模块：VPZoomer用于实现视角几何信息平衡的特征提取，VPCA模块用于进行视角几何敏感的特征聚合，以及一个将原始和放大后的voxel特征融合的模块，以创建信息平衡的特征体积。通过这些创新模块，VPOcc能够更准确地预测场景的3D语义占据，从而提高了机器人视觉中单摄像头系统的性能。</title><link>https://arxiv.org/abs/2408.03551</link><description>&lt;p&gt;
VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03551
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为VPOcc的框架，该框架利用vanishing point（VP）对单摄像头3D语义占据预测进行了改进。其框架包括三个新的模块：VPZoomer用于实现视角几何信息平衡的特征提取，VPCA模块用于进行视角几何敏感的特征聚合，以及一个将原始和放大后的voxel特征融合的模块，以创建信息平衡的特征体积。通过这些创新模块，VPOcc能够更准确地预测场景的3D语义占据，从而提高了机器人视觉中单摄像头系统的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03551v1 Announce Type: cross  Abstract: Monocular 3D semantic occupancy prediction is becoming important in robot vision due to the compactness of using a single RGB camera. However, existing methods often do not adequately account for camera perspective geometry, resulting in information imbalance along the depth range of the image. To address this issue, we propose a vanishing point (VP) guided monocular 3D semantic occupancy prediction framework named VPOcc. Our framework consists of three novel modules utilizing VP. First, in the VPZoomer module, we initially utilize VP in feature extraction to achieve information balanced feature extraction across the scene by generating a zoom-in image based on VP. Second, we perform perspective geometry-aware feature aggregation by sampling points towards VP using a VP-guided cross-attention (VPCA) module. Finally, we create an information-balanced feature volume by effectively fusing original and zoom-in voxel feature volumes with a 
&lt;/p&gt;</description></item><item><title>该文章提出了一种预训练的点云到图像转换网络（PPCITNet），旨在解决CLIP-based点云分类中存在的局限性。这种方法通过从点云中提取多视图深度图并将其通过CLIP视觉编码器，以及使用一个小型的网络适配器在CLIP视觉编码器上进行微调，来转移点云的3D知识。然而，PPCITNet实现了一种全新的点云到图像转换的机制，将点云转换成一幅多视角的全局图像，从而利用丰富的图像信息进行分类任务。此外，PPCITNet还增加了一个局部注意力模块，可以捕捉点云的局部特征。总的来说，PPCITNet通过结合全局和局部特征，提高CLIP在点云分类中的性能。</title><link>https://arxiv.org/abs/2408.03545</link><description>&lt;p&gt;
CLIP-based Point Cloud Classification via Point Cloud to Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03545
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种预训练的点云到图像转换网络（PPCITNet），旨在解决CLIP-based点云分类中存在的局限性。这种方法通过从点云中提取多视图深度图并将其通过CLIP视觉编码器，以及使用一个小型的网络适配器在CLIP视觉编码器上进行微调，来转移点云的3D知识。然而，PPCITNet实现了一种全新的点云到图像转换的机制，将点云转换成一幅多视角的全局图像，从而利用丰富的图像信息进行分类任务。此外，PPCITNet还增加了一个局部注意力模块，可以捕捉点云的局部特征。总的来说，PPCITNet通过结合全局和局部特征，提高CLIP在点云分类中的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03545v1 Announce Type: new  Abstract: Point cloud understanding is an inherently challenging problem because of the sparse and unordered structure of the point cloud in the 3D space. Recently, Contrastive Vision-Language Pre-training (CLIP) based point cloud classification model i.e. PointCLIP has added a new direction in the point cloud classification research domain. In this method, at first multi-view depth maps are extracted from the point cloud and passed through the CLIP visual encoder. To transfer the 3D knowledge to the network, a small network called an adapter is fine-tuned on top of the CLIP visual encoder. PointCLIP has two limitations. Firstly, the point cloud depth maps lack image information which is essential for tasks like classification and recognition. Secondly, the adapter only relies on the global representation of the multi-view features. Motivated by this observation, we propose a Pretrained Point Cloud to Image Translation Network (PPCITNet) that prod
&lt;/p&gt;</description></item><item><title>该文章提出了通过自动估计覆盖面积（SAC）来最大化西班牙Extremadura地区橡树林下放养的伊比利亚猪的数量的方法。使用遥感技术，作者旨在通过自动检测技术来确定橡树的冠层覆盖面积，以及估算在特定土地上可以放养的伊比利亚猪的数量。</title><link>https://arxiv.org/abs/2408.03542</link><description>&lt;p&gt;
Automatic identification of the area covered by acorn trees in the dehesa (pastureland) Extremadura of Spain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03542
&lt;/p&gt;
&lt;p&gt;
该文章提出了通过自动估计覆盖面积（SAC）来最大化西班牙Extremadura地区橡树林下放养的伊比利亚猪的数量的方法。使用遥感技术，作者旨在通过自动检测技术来确定橡树的冠层覆盖面积，以及估算在特定土地上可以放养的伊比利亚猪的数量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03542v1 Announce Type: new  Abstract: The acorn is the fruit of the oak and is an important crop in the Spanish dehesa extreme\~na, especially for the value it provides in the Iberian pig food to obtain the "acorn" certification. For this reason, we want to maximise the production of Iberian pigs with the appropriate weight. Hence the need to know the area covered by the crowns of the acorn trees, to determine the covered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC) and thereby estimate the number of Iberian pigs that can be released per hectare, as indicated by the royal decree 4/2014. In this work, we propose the automatic estimation of the CWA, through aerial digital images (orthophotos) of the pastureland of Extremadura, and with this, to offer the possibility of determining the number of Iberian pigs to be released in a specific plot of land. Among the main issues for automatic detection are, first, the correct identification of acorn trees, seco
&lt;/p&gt;</description></item><item><title>该文章提出了PoseMamba，一个基于线性复杂度的纯状态空间模型（SSM）方法，用于单目视频中的3D人类姿势估计。该方法通过一个双向全局-局部空间-时间SSM块来全面建模单个帧内的关节关系以及帧之间的时序关联，从而克服了现有基于自注意力机制的3D人类姿势估计方法的局限性，如空间-时间建模的单一方向性和空间-时间关联学习的不足。</title><link>https://arxiv.org/abs/2408.03540</link><description>&lt;p&gt;
PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional Global-Local Spatio-Temporal State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03540
&lt;/p&gt;
&lt;p&gt;
该文章提出了PoseMamba，一个基于线性复杂度的纯状态空间模型（SSM）方法，用于单目视频中的3D人类姿势估计。该方法通过一个双向全局-局部空间-时间SSM块来全面建模单个帧内的关节关系以及帧之间的时序关联，从而克服了现有基于自注意力机制的3D人类姿势估计方法的局限性，如空间-时间建模的单一方向性和空间-时间关联学习的不足。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03540v1 Announce Type: new  Abstract: Transformers have significantly advanced the field of 3D human pose estimation (HPE). However, existing transformer-based methods primarily use self-attention mechanisms for spatio-temporal modeling, leading to a quadratic complexity, unidirectional modeling of spatio-temporal relationships, and insufficient learning of spatial-temporal correlations. Recently, the Mamba architecture, utilizing the state space model (SSM), has exhibited superior long-range modeling capabilities in a variety of vision tasks with linear complexity. In this paper, we propose PoseMamba, a novel purely SSM-based approach with linear complexity for 3D human pose estimation in monocular video. Specifically, we propose a bidirectional global-local spatio-temporal SSM block that comprehensively models human joint relations within individual frames as well as temporal correlations across frames. Within this bidirectional global-local spatio-temporal SSM block, we i
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为PRTGS（Precomputed Radiance Transfer of Gaussian Splats）的实时高质量环境光重映射方法。该方法通过预先计算3D Gaussian splats的辐射传输，可以捕捉柔和阴影和互反射效果，适用于低频照明环境。通过使用高效的3DGS（3D Gauss Splatting），相较于神经场，该文章解决了现有动态照明场景中观察到的不真实渲染问题。通过预先计算高成本的光传输模拟，并在训练和渲染阶段采用专门的预计算方法，该方法在实时动态照明下实现了高质量的阴影和间接光照计算，显著提高了渲染场景的真实感。</title><link>https://arxiv.org/abs/2408.03538</link><description>&lt;p&gt;
PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03538
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为PRTGS（Precomputed Radiance Transfer of Gaussian Splats）的实时高质量环境光重映射方法。该方法通过预先计算3D Gaussian splats的辐射传输，可以捕捉柔和阴影和互反射效果，适用于低频照明环境。通过使用高效的3DGS（3D Gauss Splatting），相较于神经场，该文章解决了现有动态照明场景中观察到的不真实渲染问题。通过预先计算高成本的光传输模拟，并在训练和渲染阶段采用专门的预计算方法，该方法在实时动态照明下实现了高质量的阴影和间接光照计算，显著提高了渲染场景的真实感。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03538v1 Announce Type: new  Abstract: We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a real-time high-quality relighting method for Gaussian splats in low-frequency lighting environments that captures soft shadows and interreflections by precomputing 3D Gaussian splats' radiance transfer. Existing studies have demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields' efficiency for dynamic lighting scenarios. However, the current relighting method based on 3DGS still struggles to compute high-quality shadow and indirect illumination in real time for dynamic light, leading to unrealistic rendering results. We solve this problem by precomputing the expensive transport simulations required for complex transfer functions like shadowing, the resulting transfer functions are represented as dense sets of vectors or matrices for every Gaussian splat. We introduce distinct precomputing methods tailored for training and rendering stages, along with
&lt;/p&gt;</description></item><item><title>该文章提出SwinShadow，一个基于transformer的架构，通过采用强化的移位窗口机制，首先在单个窗口内进行局部自注意力操作，然后移位注意力窗口以促进跨窗口注意力的捕捉，以提高对相邻阴影的检测精度。</title><link>https://arxiv.org/abs/2408.03521</link><description>&lt;p&gt;
SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03521
&lt;/p&gt;
&lt;p&gt;
该文章提出SwinShadow，一个基于transformer的架构，通过采用强化的移位窗口机制，首先在单个窗口内进行局部自注意力操作，然后移位注意力窗口以促进跨窗口注意力的捕捉，以提高对相邻阴影的检测精度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03521v1 Announce Type: new  Abstract: Shadow detection is a fundamental and challenging task in many computer vision applications. Intuitively, most shadows come from the occlusion of light by the object itself, resulting in the object and its shadow being contiguous (referred to as the adjacent shadow in this paper). In this case, when the color of the object is similar to that of the shadow, existing methods struggle to achieve accurate detection. To address this problem, we present SwinShadow, a transformer-based architecture that fully utilizes the powerful shifted window mechanism for detecting adjacent shadows. The mechanism operates in two steps. Initially, it applies local self-attention within a single window, enabling the network to focus on local details. Subsequently, it shifts the attention windows to facilitate inter-window attention, enabling the capture of a broader range of adjacent information. These combined steps significantly improve the network's capaci
&lt;/p&gt;</description></item><item><title>该文章提出了一种利用大型语言模型（LLMs）结合3D场景理解和自动驾驶的开放词汇方法，通过生成语境相关的标准短语来改进推理、分割和场景解释。这种方法通过将语言特征编码到3D高斯分布中，有效提高了在陌生环境中对感兴趣物体的检测精度，同时在处理未知词汇时表现出色，对于自动驾驶环境下的开放词汇物体检测和分割具有显著的提升效果。</title><link>https://arxiv.org/abs/2408.03516</link><description>&lt;p&gt;
Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03516
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种利用大型语言模型（LLMs）结合3D场景理解和自动驾驶的开放词汇方法，通过生成语境相关的标准短语来改进推理、分割和场景解释。这种方法通过将语言特征编码到3D高斯分布中，有效提高了在陌生环境中对感兴趣物体的检测精度，同时在处理未知词汇时表现出色，对于自动驾驶环境下的开放词汇物体检测和分割具有显著的提升效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03516v1 Announce Type: cross  Abstract: This paper introduces a novel method for open-vocabulary 3D scene understanding in autonomous driving by combining Language Embedded 3D Gaussians with Large Language Models (LLMs) for enhanced inference. We propose utilizing LLMs to generate contextually relevant canonical phrases for segmentation and scene interpretation. Our method leverages the contextual and semantic capabilities of LLMs to produce a set of canonical phrases, which are then compared with the language features embedded in the 3D Gaussians. This LLM-guided approach significantly improves zero-shot scene understanding and detection of objects of interest, even in the most challenging or unfamiliar environments. Experimental results on the WayveScenes101 dataset demonstrate that our approach surpasses state-of-the-art methods in terms of accuracy and flexibility for open-vocabulary object detection and segmentation. This work represents a significant advancement toward
&lt;/p&gt;</description></item><item><title>该文章介绍了MoExtend框架，该框架通过集成新的专家能力以扩展预训练的MoE模型（混合专家模型），从而不必重训练知识基础，实现了高效且成本低的任务扩展。</title><link>https://arxiv.org/abs/2408.03511</link><description>&lt;p&gt;
MoExtend: Tuning New Experts for Modality and Task Extension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03511
&lt;/p&gt;
&lt;p&gt;
该文章介绍了MoExtend框架，该框架通过集成新的专家能力以扩展预训练的MoE模型（混合专家模型），从而不必重训练知识基础，实现了高效且成本低的任务扩展。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03511v1 Announce Type: new  Abstract: Large language models (LLMs) excel in various tasks but are primarily trained on text data, limiting their application scope. Expanding LLM capabilities to include vision-language understanding is vital, yet training them on multimodal data from scratch is challenging and costly. Existing instruction tuning methods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs via fully fine-tuning LLMs to bridge the modality gap. However, full fine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous knowledge, and high training costs particularly in the era of increasing tasks and modalities. To solve this issue, we introduce MoExtend, an effective framework designed to streamline the modality adaptation and extension of Mixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts into pre-trained MoE models, endowing them with novel knowledge without the need to tune pretrained models such as 
&lt;/p&gt;</description></item><item><title>该文章介绍了使用最先进的YOLO深度学习模型进行用户界面元素检测的研究，强调了这些模型在处理大量紧密排列且类数量有限的GUI元素检测中的有效性。</title><link>https://arxiv.org/abs/2408.03507</link><description>&lt;p&gt;
GUI Element Detection Using SOTA YOLO Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03507
&lt;/p&gt;
&lt;p&gt;
该文章介绍了使用最先进的YOLO深度学习模型进行用户界面元素检测的研究，强调了这些模型在处理大量紧密排列且类数量有限的GUI元素检测中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03507v1 Announce Type: new  Abstract: Detection of Graphical User Interface (GUI) elements is a crucial task for automatic code generation from images and sketches, GUI testing, and GUI search. Recent studies have leveraged both old-fashioned and modern computer vision (CV) techniques. Oldfashioned methods utilize classic image processing algorithms (e.g. edge detection and contour detection) and modern methods use mature deep learning solutions for general object detection tasks. GUI element detection, however, is a domain-specific case of object detection, in which objects overlap more often, and are located very close to each other, plus the number of object classes is considerably lower, yet there are more objects in the images compared to natural images. Hence, the studies that have been carried out on comparing various object detection models, might not apply to GUI element detection. In this study, we evaluate the performance of the four most recent successful YOLO mo
&lt;/p&gt;</description></item><item><title>该文章介绍了VECTOR，一个能够改进立体重建优化（BA）过程的错误分析工具。VECTOR能够帮助分析师更深入地理解立体重建过程中的误差来源，从而提高了误差检测和分析的效率和精确度。</title><link>https://arxiv.org/abs/2408.03503</link><description>&lt;p&gt;
Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03503
&lt;/p&gt;
&lt;p&gt;
该文章介绍了VECTOR，一个能够改进立体重建优化（BA）过程的错误分析工具。VECTOR能够帮助分析师更深入地理解立体重建过程中的误差来源，从而提高了误差检测和分析的效率和精确度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03503v1 Announce Type: cross  Abstract: Reconstruction of 3D scenes from 2D images is a technical challenge that impacts domains from Earth and planetary sciences and space exploration to augmented and virtual reality. Typically, reconstruction algorithms first identify common features across images and then minimize reconstruction errors after estimating the shape of the terrain. This bundle adjustment (BA) step optimizes around a single, simplifying scalar value that obfuscates many possible causes of reconstruction errors (e.g., initial estimate of the position and orientation of the camera, lighting conditions, ease of feature detection in the terrain). Reconstruction errors can lead to inaccurate scientific inferences or endanger a spacecraft exploring a remote environment. To address this challenge, we present VECTOR, a visual analysis tool that improves error inspection for stereo reconstruction BA. VECTOR provides analysts with previously unavailable visibility into 
&lt;/p&gt;</description></item><item><title>该文章描述了一种名为e-Health CSIRO的团队在Radiology Report Generation 24 (RRG24)共享任务中取得多个第一名的方法，该方法通过在self-critical sequence training中添加entropy regularization来增强模型的表现，以保持 token 分布的更高熵。这种创新方法有助于模型探索报告多样性的广泛词汇，从而有效应对RRG24数据集中报告的多样性。</title><link>https://arxiv.org/abs/2408.03500</link><description>&lt;p&gt;
e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence Training for Radiology Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03500
&lt;/p&gt;
&lt;p&gt;
该文章描述了一种名为e-Health CSIRO的团队在Radiology Report Generation 24 (RRG24)共享任务中取得多个第一名的方法，该方法通过在self-critical sequence training中添加entropy regularization来增强模型的表现，以保持 token 分布的更高熵。这种创新方法有助于模型探索报告多样性的广泛词汇，从而有效应对RRG24数据集中报告的多样性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03500v1 Announce Type: new  Abstract: The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to expedite the development of assistive systems for interpreting and reporting on chest X-ray (CXR) images. This task challenges participants to develop models that generate the findings and impression sections of radiology reports from CXRs from a patient's study, using five different datasets. This paper outlines the e-Health CSIRO team's approach, which achieved multiple first-place finishes in RRG24. The core novelty of our approach lies in the addition of entropy regularisation to self-critical sequence training, to maintain a higher entropy in the token distribution. This prevents overfitting to common phrases and ensures a broader exploration of the vocabulary during training, essential for handling the diversity of the radiology reports in the RRG24 datasets. Our model is available on Hugging Face https://huggingface.co/aehrc/cxrmate-rrg24.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为FacialPulse的深度学习框架，该框架通过利用面部表情的时间相关性，能够高效地使用时序面部特征点检测抑郁，从而实现高准确性和快速识别。</title><link>https://arxiv.org/abs/2408.03499</link><description>&lt;p&gt;
FacialPulse: An Efficient RNN-based Depression Detection via Temporal Facial Landmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03499
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为FacialPulse的深度学习框架，该框架通过利用面部表情的时间相关性，能够高效地使用时序面部特征点检测抑郁，从而实现高准确性和快速识别。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03499v1 Announce Type: new  Abstract: Depression is a prevalent mental health disorder that significantly impacts individuals' lives and well-being. Early detection and intervention are crucial for effective treatment and management of depression. Recently, there are many end-to-end deep learning methods leveraging the facial expression features for automatic depression detection. However, most current methods overlook the temporal dynamics of facial expressions. Although very recent 3DCNN methods remedy this gap, they introduce more computational cost due to the selection of CNN-based backbones and redundant facial features.   To address the above limitations, by considering the timing correlation of facial expressions, we propose a novel framework called FacialPulse, which recognizes depression with high accuracy and speed. By harnessing the bidirectional nature and proficiently addressing long-term dependencies, the Facial Motion Modeling Module (FMMM) is designed in Faci
&lt;/p&gt;</description></item><item><title>该文章创新性地推出了MultiHateClip, 一个包含2000个视频的跨文化中文和英文多模态仇恨内容检测数据集。它不仅区分了视频是否具有仇恨性，还细化了视频中的中性内容和冒犯性内容，为理解不同文化背景下的性别歧视仇恨言论提供了一定帮助。</title><link>https://arxiv.org/abs/2408.03468</link><description>&lt;p&gt;
MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03468
&lt;/p&gt;
&lt;p&gt;
该文章创新性地推出了MultiHateClip, 一个包含2000个视频的跨文化中文和英文多模态仇恨内容检测数据集。它不仅区分了视频是否具有仇恨性，还细化了视频中的中性内容和冒犯性内容，为理解不同文化背景下的性别歧视仇恨言论提供了一定帮助。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03468v1 Announce Type: cross  Abstract: Hate speech is a pressing issue in modern society, with significant effects both online and offline. Recent research in hate speech detection has primarily centered on text-based media, largely overlooking multimodal content such as videos. Existing studies on hateful video datasets have predominantly focused on English content within a Western context and have been limited to binary labels (hateful or non-hateful), lacking detailed contextual information. This study presents MultiHateClip1 , an novel multilingual dataset created through hate lexicons and human annotation. It aims to enhance the detection of hateful videos on platforms such as YouTube and Bilibili, including content in both English and Chinese languages. Comprising 2,000 videos annotated for hatefulness, offensiveness, and normalcy, this dataset provides a cross-cultural perspective on gender-based hate speech. Through a detailed examination of human annotation results
&lt;/p&gt;</description></item><item><title>该文章总结了人工智能（AI）在遥感领域中采用的基石模型的创新和贡献。这些模型采用大规模预训练方法，能够显著提升遥感数据处理和分析的准确性和效率，推动了传统依赖于手动解读和特定任务模型的遥感领域的一系列突破性进展。</title><link>https://arxiv.org/abs/2408.03464</link><description>&lt;p&gt;
AI Foundation Models in Remote Sensing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03464
&lt;/p&gt;
&lt;p&gt;
该文章总结了人工智能（AI）在遥感领域中采用的基石模型的创新和贡献。这些模型采用大规模预训练方法，能够显著提升遥感数据处理和分析的准确性和效率，推动了传统依赖于手动解读和特定任务模型的遥感领域的一系列突破性进展。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03464v1 Announce Type: new  Abstract: Artificial Intelligence (AI) technologies have profoundly transformed the field of remote sensing, revolutionizing data collection, processing, and analysis. Traditionally reliant on manual interpretation and task-specific models, remote sensing has been significantly enhanced by the advent of foundation models--large-scale, pre-trained AI models capable of performing a wide array of tasks with unprecedented accuracy and efficiency. This paper provides a comprehensive survey of foundation models in the remote sensing domain, covering models released between June 2021 and June 2024. We categorize these models based on their applications in computer vision and domain-specific tasks, offering insights into their architectures, pre-training datasets, and methodologies. Through detailed performance comparisons, we highlight emerging trends and the significant advancements achieved by these foundation models. Additionally, we discuss the techn
&lt;/p&gt;</description></item><item><title>该文章创新性地使用Deep Learning模型进行死者的眼纹分割，通过训练Semantic segmentation方法，如SegNet和DeepLabV3+，以及多种不同的网络架构如VGG19、ResNet18、ResNet50等，提出了针对死者眼纹图像的分割方法，为法医学提供了新的分析手段。</title><link>https://arxiv.org/abs/2408.03448</link><description>&lt;p&gt;
Post-Mortem Human Iris Segmentation Analysis with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03448
&lt;/p&gt;
&lt;p&gt;
该文章创新性地使用Deep Learning模型进行死者的眼纹分割，通过训练Semantic segmentation方法，如SegNet和DeepLabV3+，以及多种不同的网络架构如VGG19、ResNet18、ResNet50等，提出了针对死者眼纹图像的分割方法，为法医学提供了新的分析手段。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03448v1 Announce Type: cross  Abstract: Iris recognition is widely used in several fields such as mobile phones, financial transactions, identification cards, airport security, international border control, voter registration for living persons. However, the possibility of identifying deceased individuals based on their iris patterns has emerged recently as a supplementary or alternative method valuable in forensic analysis. Simultaneously, it poses numerous new technological challenges and one of the most challenging among them is the image segmentation stage as conventional iris recognition approaches have struggled to reliably execute it. This paper presents and compares Deep Learning (DL) models designed for segmenting iris images collected from the deceased subjects, by training SegNet and DeepLabV3+ semantic segmentation methods where using VGG19, ResNet18, ResNet50, MobileNetv2, Xception, or InceptionResNetv2 as backbones. In this study, our experiments demonstrate th
&lt;/p&gt;</description></item><item><title>文章提出了一种新的预训练方法，通过结合监督预训练和生成预训练，有效地利用标注数据少的领域进行分割模型的微调，从而提高了在相关领域的标注效率。</title><link>https://arxiv.org/abs/2408.03433</link><description>&lt;p&gt;
Hybrid diffusion models: combining supervised and generative pretraining for label-efficient fine-tuning of segmentation models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03433
&lt;/p&gt;
&lt;p&gt;
文章提出了一种新的预训练方法，通过结合监督预训练和生成预训练，有效地利用标注数据少的领域进行分割模型的微调，从而提高了在相关领域的标注效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03433v1 Announce Type: new  Abstract: We are considering in this paper the task of label-efficient fine-tuning of segmentation models: We assume that a large labeled dataset is available and allows to train an accurate segmentation model in one domain, and that we have to adapt this model on a related domain where only a few samples are available. We observe that this adaptation can be done using two distinct methods: The first method, supervised pretraining, is simply to take the model trained on the first domain using classical supervised learning, and fine-tune it on the second domain with the available labeled samples. The second method is to perform self-supervised pretraining on the first domain using a generic pretext task in order to get high-quality representations which can then be used to train a model on the second domain in a label-efficient way. We propose in this paper to fuse these two approaches by introducing a new pretext task, which is to perform simultan
&lt;/p&gt;</description></item><item><title>该文章提出Set2Seq Transformer，这是一种新的序列多实例架构，能够学习和排名感知顺序的集合表示，这为非时序重点的静态视觉多实例学习方法带来了显著改进。通过在多模态中整合视觉内容和时间信息，该架构展示了在模式识别任务中应用序列多实例学习的巨大优势。在专注于对艺术家作品进行分析的实验中，该文章展示了Set2Seq Transformer如何预测艺术成就，并为这一任务提供了一个创新的数据集——WikiArt-Seq2Rank。</title><link>https://arxiv.org/abs/2408.03404</link><description>&lt;p&gt;
Set2Seq Transformer: Learning Permutation Aware Set Representations of Artistic Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03404
&lt;/p&gt;
&lt;p&gt;
该文章提出Set2Seq Transformer，这是一种新的序列多实例架构，能够学习和排名感知顺序的集合表示，这为非时序重点的静态视觉多实例学习方法带来了显著改进。通过在多模态中整合视觉内容和时间信息，该架构展示了在模式识别任务中应用序列多实例学习的巨大优势。在专注于对艺术家作品进行分析的实验中，该文章展示了Set2Seq Transformer如何预测艺术成就，并为这一任务提供了一个创新的数据集——WikiArt-Seq2Rank。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03404v1 Announce Type: new  Abstract: We propose Set2Seq Transformer, a novel sequential multiple instance architecture, that learns to rank permutation aware set representations of sequences. First, we illustrate that learning temporal position-aware representations of discrete timesteps can greatly improve static visual multiple instance learning methods that do not regard temporality and concentrate almost exclusively on visual content analysis. We further demonstrate the significant advantages of end-to-end sequential multiple instance learning, integrating visual content and temporal information in a multimodal manner. As application we focus on fine art analysis related tasks. To that end, we show that our Set2Seq Transformer can leverage visual set and temporal position-aware representations for modelling visual artists' oeuvres for predicting artistic success. Finally, through extensive quantitative and qualitative evaluation using a novel dataset, WikiArt-Seq2Rank, 
&lt;/p&gt;</description></item><item><title>该文章通过系统性文献回顾，系统地分析了148篇关于深度学习在生物医学图像分割中应用的文章，深入探讨了深度学习模型在生物医学图像分割中的应用，并提出了未来研究的方向。</title><link>https://arxiv.org/abs/2408.03393</link><description>&lt;p&gt;
Biomedical Image Segmentation: A Systematic Literature Review of Deep Learning Based Object Detection Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03393
&lt;/p&gt;
&lt;p&gt;
该文章通过系统性文献回顾，系统地分析了148篇关于深度学习在生物医学图像分割中应用的文章，深入探讨了深度学习模型在生物医学图像分割中的应用，并提出了未来研究的方向。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03393v1 Announce Type: cross  Abstract: Biomedical image segmentation plays a vital role in diagnosis of diseases across various organs. Deep learning-based object detection methods are commonly used for such segmentation. There exists an extensive research in this topic. However, there is no standard review on this topic. Existing surveys often lack a standardized approach or focus on broader segmentation techniques. In this paper, we conducted a systematic literature review (SLR), collected and analysed 148 articles that explore deep learning object detection methods for biomedical image segmentation. We critically analyzed these methods, identified the key challenges, and discussed the future directions. From the selected articles we extracted the results including the deep learning models, targeted imaging modalities, targeted diseases, and the metrics for the analysis of the methods. The results have been presented in tabular and/or charted forms. The results are presen
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于非负变分自编码器（Non-negative VAE）的通用伽马信念网络（Generalized Gamma Belief Network），其通过扩展线性生成模型至非线性生成模型，增强了模型的表达能力。文章还介绍了一种逆向向上Gauss-Weibull生成网络用以估计潜在变量的后验分布，提高了模型的性能。</title><link>https://arxiv.org/abs/2408.03388</link><description>&lt;p&gt;
A Non-negative VAE:the Generalized Gamma Belief Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03388
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于非负变分自编码器（Non-negative VAE）的通用伽马信念网络（Generalized Gamma Belief Network），其通过扩展线性生成模型至非线性生成模型，增强了模型的表达能力。文章还介绍了一种逆向向上Gauss-Weibull生成网络用以估计潜在变量的后验分布，提高了模型的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03388v1 Announce Type: cross  Abstract: The gamma belief network (GBN), often regarded as a deep topic model, has demonstrated its potential for uncovering multi-layer interpretable latent representations in text data. Its notable capability to acquire interpretable latent factors is partially attributed to sparse and non-negative gamma-distributed latent variables. However, the existing GBN and its variations are constrained by the linear generative model, thereby limiting their expressiveness and applicability. To address this limitation, we introduce the generalized gamma belief network (Generalized GBN) in this paper, which extends the original linear generative model to a more expressive non-linear generative model. Since the parameters of the Generalized GBN no longer possess an analytic conditional posterior, we further propose an upward-downward Weibull inference network to approximate the posterior distribution of the latent variables. The parameters of both the gen
&lt;/p&gt;</description></item><item><title>该文章介绍了GMAI-MMBench：一个旨在对一般医疗人工智能进行综合的多模态评估基准。这是一个全面的多模态评估基准，旨在评估能够处理包括医学图像、文本和生理信号等多类型数据的通用医疗人工智能模型。</title><link>https://arxiv.org/abs/2408.03361</link><description>&lt;p&gt;
GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03361
&lt;/p&gt;
&lt;p&gt;
该文章介绍了GMAI-MMBench：一个旨在对一般医疗人工智能进行综合的多模态评估基准。这是一个全面的多模态评估基准，旨在评估能够处理包括医学图像、文本和生理信号等多类型数据的通用医疗人工智能模型。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03361v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 285 datasets across 39 medical image mod
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了RayGauss方法，通过使用基于体积的Gaussian-based ray casting技术实现了高保真的视图合成。这种方法通过将发出的亮度C和密度σ分解为与球面Gaussians/Harmonics相关的Gaussian函数，对于所有频率的颜色表示具有物理一致性，同时避免了传统神经辐射场NeRF方法的局限性，并能够快速适应场景，显著减少了渲染时间和视觉上可见的伪影。整体而言，RayGauss方法为小说视角合成领域提供了一种全新的高效且高保真的解决方案。</title><link>https://arxiv.org/abs/2408.03356</link><description>&lt;p&gt;
RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03356
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了RayGauss方法，通过使用基于体积的Gaussian-based ray casting技术实现了高保真的视图合成。这种方法通过将发出的亮度C和密度σ分解为与球面Gaussians/Harmonics相关的Gaussian函数，对于所有频率的颜色表示具有物理一致性，同时避免了传统神经辐射场NeRF方法的局限性，并能够快速适应场景，显著减少了渲染时间和视觉上可见的伪影。整体而言，RayGauss方法为小说视角合成领域提供了一种全新的高效且高保真的解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03356v1 Announce Type: new  Abstract: Differentiable volumetric rendering-based methods made significant progress in novel view synthesis. On one hand, innovative methods have replaced the Neural Radiance Fields (NeRF) network with locally parameterized structures, enabling high-quality renderings in a reasonable time. On the other hand, approaches have used differentiable splatting instead of NeRF's ray casting to optimize radiance fields rapidly using Gaussian kernels, allowing for fine adaptation to the scene. However, differentiable ray casting of irregularly spaced kernels has been scarcely explored, while splatting, despite enabling fast rendering times, is susceptible to clearly visible artifacts.   Our work closes this gap by providing a physically consistent formulation of the emitted radiance c and density {\sigma}, decomposed with Gaussian functions associated with Spherical Gaussians/Harmonics for all-frequency colorimetric representation. We also introduce a met
&lt;/p&gt;</description></item><item><title>该文章聚焦于文本指导的单幅图像编辑，提出FastEdit方法，通过语义感知扩散精调技术实现速度上的显著提升，将编辑时间缩短至仅17秒，用于单张图片，较常规方法大幅减少了预训练所需的时间和资源。</title><link>https://arxiv.org/abs/2408.03355</link><description>&lt;p&gt;
FastEdit: Fast Text-Guided Single-Image Editing via Semantic-Aware Diffusion Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03355
&lt;/p&gt;
&lt;p&gt;
该文章聚焦于文本指导的单幅图像编辑，提出FastEdit方法，通过语义感知扩散精调技术实现速度上的显著提升，将编辑时间缩短至仅17秒，用于单张图片，较常规方法大幅减少了预训练所需的时间和资源。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03355v1 Announce Type: new  Abstract: Conventional Text-guided single-image editing approaches require a two-step process, including fine-tuning the target text embedding for over 1K iterations and the generative model for another 1.5K iterations. Although it ensures that the resulting image closely aligns with both the input image and the target text, this process often requires 7 minutes per image, posing a challenge for practical application due to its time-intensive nature. To address this bottleneck, we introduce FastEdit, a fast text-guided single-image editing method with semantic-aware diffusion fine-tuning, dramatically accelerating the editing process to only 17 seconds. FastEdit streamlines the generative model's fine-tuning phase, reducing it from 1.5K to a mere 50 iterations. For diffusion fine-tuning, we adopt certain time step values based on the semantic discrepancy between the input image and target text. Furthermore, FastEdit circumvents the initial fine-tu
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为IVISIT的交互式可视化模拟工具，它基于Python/Numpy，用于系统模拟、参数优化、参数管理和系统动态可视化。IVISIT通过提供快速原型化工具、交互式GUI元素和SQLite数据库支持，极大地简化了系统属性可视化和管理的过程。这个工具为研究人员提供了一种便捷的方式来开发神经网络模拟、机器学习和计算机视觉系统，并通过实例展示如何快速实施交互式应用和参数设置管理。</title><link>https://arxiv.org/abs/2408.03341</link><description>&lt;p&gt;
IVISIT: An Interactive Visual Simulation Tool for system simulation, visualization, optimization, and parameter management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03341
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为IVISIT的交互式可视化模拟工具，它基于Python/Numpy，用于系统模拟、参数优化、参数管理和系统动态可视化。IVISIT通过提供快速原型化工具、交互式GUI元素和SQLite数据库支持，极大地简化了系统属性可视化和管理的过程。这个工具为研究人员提供了一种便捷的方式来开发神经网络模拟、机器学习和计算机视觉系统，并通过实例展示如何快速实施交互式应用和参数设置管理。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03341v1 Announce Type: cross  Abstract: IVISIT is a generic interactive visual simulation tool that is based on Python/Numpy and can be used for system simulation, parameter optimization, parameter management, and visualization of system dynamics as required, for example,for developing neural network simulations, machine learning applications, or computer vision systems. It provides classes for rapid prototyping of applications and visualization and manipulation of system properties using interactive GUI elements like sliders, images, textboxes, option lists, checkboxes and buttons based on Tkinter and Matplotlib. Parameters and simulation configurations can be stored and managed based on SQLite database functions. This technical report describes the main architecture and functions of IVISIT, and provides easy examples how to rapidly implement interactive applications and manage parameter settings.
&lt;/p&gt;</description></item><item><title>该文章详细比较了多种视频帧抽样方法在多模态自动生成文本（RAG）检索中的效果，探讨了在尽可能减少存储和处理需求的同时保持高召回得分的方法，从而为视频和帧检索任务提供了高效的帧抽样策略。</title><link>https://arxiv.org/abs/2408.03340</link><description>&lt;p&gt;
An Empirical Comparison of Video Frame Sampling Methods for Multi-Modal RAG Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03340
&lt;/p&gt;
&lt;p&gt;
该文章详细比较了多种视频帧抽样方法在多模态自动生成文本（RAG）检索中的效果，探讨了在尽可能减少存储和处理需求的同时保持高召回得分的方法，从而为视频和帧检索任务提供了高效的帧抽样策略。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03340v1 Announce Type: cross  Abstract: Numerous video frame sampling methodologies detailed in the literature present a significant challenge in determining the optimal video frame method for Video RAG pattern without a comparative side-by-side analysis. In this work, we investigate the trade-offs in frame sampling methods for Video &amp;amp; Frame Retrieval using natural language questions. We explore the balance between the quantity of sampled frames and the retrieval recall score, aiming to identify efficient video frame sampling strategies that maintain high retrieval efficacy with reduced storage and processing demands. Our study focuses on the storage and retrieval of image data (video frames) within a vector database required by Video RAG pattern, comparing the effectiveness of various frame sampling techniques. Our investigation indicates that the recall@k metric for both text-to-video and text-to-frame retrieval tasks using various methods covered as part of this work is c
&lt;/p&gt;</description></item><item><title>该文章引入了InLUT3D点云数据集，这是一个专门用于室内环境场景理解的高分辨率激光点云数据集，涵盖了多变的场景空间，并提供了手动标记和评价算法的有效性。</title><link>https://arxiv.org/abs/2408.03338</link><description>&lt;p&gt;
InLUT3D: Challenging real indoor dataset for point cloud analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03338
&lt;/p&gt;
&lt;p&gt;
该文章引入了InLUT3D点云数据集，这是一个专门用于室内环境场景理解的高分辨率激光点云数据集，涵盖了多变的场景空间，并提供了手动标记和评价算法的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03338v1 Announce Type: new  Abstract: In this paper, we introduce the InLUT3D point cloud dataset, a comprehensive resource designed to advance the field of scene understanding in indoor environments. The dataset covers diverse spaces within the W7 faculty buildings of Lodz University of Technology, characterised by high-resolution laser-based point clouds and manual labelling. Alongside the dataset, we propose metrics and benchmarking guidelines essential for ensuring trustworthy and reproducible results in algorithm evaluation. We anticipate that the introduction of the InLUT3D dataset and its associated benchmarks will catalyse future advancements in 3D scene understanding, facilitating methodological rigour and inspiring new approaches in the field.
&lt;/p&gt;</description></item><item><title>该文章通过使用基于UNet架构的卷积神经网络，能够从干涉图像中准确重建不规则粗糙颗粒的三维形状，有效应用于不同对称性的复杂粒子上，展示了其在材料学和微纳加工领域的潜在应用价值。</title><link>https://arxiv.org/abs/2408.03327</link><description>&lt;p&gt;
Reconstruction of the shape of irregular rough particles from their interferometric images using a convolutional neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03327
&lt;/p&gt;
&lt;p&gt;
该文章通过使用基于UNet架构的卷积神经网络，能够从干涉图像中准确重建不规则粗糙颗粒的三维形状，有效应用于不同对称性的复杂粒子上，展示了其在材料学和微纳加工领域的潜在应用价值。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03327v1 Announce Type: new  Abstract: We have developed a convolutional neural network (CNN) to reconstruct the shape of irregular rough particles from their interferometric images. The CNN is based on a UNET architecture with residual block modules. The database has been constructed using the experimental patterns generated by perfectly known pseudo-particles programmed on a Digital Micromirror Device (DMD) and under laser illumination. The CNN has been trained on a basis of 18000 experimental interferometric images using the AUSTRAL super computer (at CRIANN in Normandy). The CNN is tested in the case of centrosymmetric (stick, cross, dendrite) and non-centrosymmetric (like T, Y or L) particles. The size and the 3D orientation of the programmed particles are random. The different shapes are reconstructed by the CNN with good accuracy. Using three angles of view, the 3D reconstruction of particles from three reconstructed faces can be further done.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Pose Magic的混合Mamba-GCN网络，实现了高效且时间一致的人体姿势估计，通过结合Mamba的高质量长范围建模能力和GCN的局部增强效果，从而在保持准确性的同时提高了计算效率。</title><link>https://arxiv.org/abs/2408.02922</link><description>&lt;p&gt;
Pose Magic: Efficient and Temporally Consistent Human Pose Estimation with a Hybrid Mamba-GCN Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02922
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Pose Magic的混合Mamba-GCN网络，实现了高效且时间一致的人体姿势估计，通过结合Mamba的高质量长范围建模能力和GCN的局部增强效果，从而在保持准确性的同时提高了计算效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02922v1 Announce Type: new  Abstract: Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are primarily based on Transformers. However, existing Transformer-based 3D HPE backbones often encounter a trade-off between accuracy and computational efficiency. To resolve the above dilemma, in this work, leveraging recent advances in state space models, we utilize Mamba for high-quality and efficient long-range modeling. Nonetheless, Mamba still faces challenges in precisely exploiting the local dependencies between joints. To address these issues, we propose a new attention-free hybrid spatiotemporal architecture named Hybrid Mamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN by capturing relationships between neighboring joints, thus producing new representations to complement Mamba's outputs. By adaptively fusing representations from Mamba and GCN, Pose Magic demonstrates superior capability in learning the underlying 3D structu
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为ProCreate的方法，该方法通过在生成过程中主动推动生成图像的嵌入远离参考图像嵌入，改进了扩散模型在图像生成方面的样本多样性与创造力，并有效防止了训练数据的重复生成。此外，该研究还创建了一个名为FSCG-8的少样本创造性生成数据集，包含八个不同类别，并在该数据集上验证了ProCreate的有效性。ProCreate的成功展示了其在防止文本提示驱动的大规模训练数据复制方面的潜力，并且相关的代码和数据集已在GitHub上发布。</title><link>https://arxiv.org/abs/2408.02226</link><description>&lt;p&gt;
ProCreate, Don\'t Reproduce! Propulsive Energy Diffusion for Creative Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02226
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为ProCreate的方法，该方法通过在生成过程中主动推动生成图像的嵌入远离参考图像嵌入，改进了扩散模型在图像生成方面的样本多样性与创造力，并有效防止了训练数据的重复生成。此外，该研究还创建了一个名为FSCG-8的少样本创造性生成数据集，包含八个不同类别，并在该数据集上验证了ProCreate的有效性。ProCreate的成功展示了其在防止文本提示驱动的大规模训练数据复制方面的潜力，并且相关的代码和数据集已在GitHub上发布。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02226v1 Announce Type: new  Abstract: In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts. Code and FSCG-8 are available at https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The project page is available at https://procreate-diffusion.github.io.
&lt;/p&gt;</description></item><item><title>该文章提出了一个确保情感分析方法公平性和一致性的新协议，通过提供详细的个人资料标注和通用的框架，对情感分析进行了重新思考，并重新计算了先前研究的方法，同时为未来的情感识别研究提供了更加公平的比较。</title><link>https://arxiv.org/abs/2408.02164</link><description>&lt;p&gt;
Rethinking Affect Analysis: A Protocol for Ensuring Fairness and Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02164
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个确保情感分析方法公平性和一致性的新协议，通过提供详细的个人资料标注和通用的框架，对情感分析进行了重新思考，并重新计算了先前研究的方法，同时为未来的情感识别研究提供了更加公平的比较。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02164v1 Announce Type: new  Abstract: Evaluating affect analysis methods presents challenges due to inconsistencies in database partitioning and evaluation protocols, leading to unfair and biased results. Previous studies claim continuous performance improvements, but our findings challenge such assertions. Using these insights, we propose a unified protocol for database partitioning that ensures fairness and comparability. We provide detailed demographic annotations (in terms of race, gender and age), evaluation metrics, and a common framework for expression recognition, action unit detection and valence-arousal estimation. We also rerun the methods with the new protocol and introduce a new leaderboards to encourage future research in affect recognition with a fairer comparison. Our annotations, code, and pre-trained models are available on \hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RICA^2的深度概率模型，它能够整合评分规则并考虑到模型预测的不确定性，用于动作质量评估（AQA）。RICA^2通过在根据评分规则定义的图结构上嵌入动作步骤，有效地量化了动作执行的质量，并在多个公开基准数据集上取得了新的SOTA性能。</title><link>https://arxiv.org/abs/2408.02138</link><description>&lt;p&gt;
RICA^2: Rubric-Informed, Calibrated Assessment of Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02138
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RICA^2的深度概率模型，它能够整合评分规则并考虑到模型预测的不确定性，用于动作质量评估（AQA）。RICA^2通过在根据评分规则定义的图结构上嵌入动作步骤，有效地量化了动作执行的质量，并在多个公开基准数据集上取得了新的SOTA性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02138v1 Announce Type: new  Abstract: The ability to quantify how well an action is carried out, also known as action quality assessment (AQA), has attracted recent interest in the vision community. Unfortunately, prior methods often ignore the score rubric used by human experts and fall short of quantifying the uncertainty of the model prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model that integrates score rubric and accounts for prediction uncertainty for AQA. Central to our method lies in stochastic embeddings of action steps, defined on a graph structure that encodes the score rubric. The embeddings spread probabilistic density in the latent space and allow our method to represent model uncertainty. The graph encodes the scoring criteria, based on which the quality scores can be decoded. We demonstrate that our method establishes new state of the art on public benchmarks, including FineDiving, MTL-AQA, and JIGSAWS, with superior performance in
&lt;/p&gt;</description></item><item><title>该文章聚焦于数据评估和选择在指令微调大型语言模型中的应用，全面梳理了现有文献，为这类策略提供了一种分类清晰、层级精细的体系结构，以提高指令微调的效率和效果。</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
该文章聚焦于数据评估和选择在指令微调大型语言模型中的应用，全面梳理了现有文献，为这类策略提供了一种分类清晰、层级精细的体系结构，以提高指令微调的效率和效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EqvAfford的框架，它通过保证点级 affordance学习中的equivariance，为下游的机器人操作任务提供了一种新的设计，能够在不同的对象姿态下实现良好的性能和泛化能力。</title><link>https://arxiv.org/abs/2408.01953</link><description>&lt;p&gt;
EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01953
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EqvAfford的框架，它通过保证点级 affordance学习中的equivariance，为下游的机器人操作任务提供了一种新的设计，能够在不同的对象姿态下实现良好的性能和泛化能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01953v1 Announce Type: new  Abstract: Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.
&lt;/p&gt;</description></item><item><title>该文章提出了一个大型视频地面真实数据集，名为“SynopGround”，该数据集结合了流行的电视剧视频和详细的剧情概要，以促进对视频内容的深层次多模态理解。</title><link>https://arxiv.org/abs/2408.01669</link><description>&lt;p&gt;
SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01669
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个大型视频地面真实数据集，名为“SynopGround”，该数据集结合了流行的电视剧视频和详细的剧情概要，以促进对视频内容的深层次多模态理解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01669v1 Announce Type: new  Abstract: Video grounding is a fundamental problem in multimodal content understanding, aiming to localize specific natural language queries in an untrimmed video. However, current video grounding datasets merely focus on simple events and are either limited to shorter videos or brief sentences, which hinders the model from evolving toward stronger multimodal understanding capabilities. To address these limitations, we present a large-scale video grounding dataset named SynopGround, in which more than 2800 hours of videos are sourced from popular TV dramas and are paired with accurately localized human-written synopses. Each paragraph in the synopsis serves as a language query and is manually annotated with precise temporal boundaries in the long video. These paragraph queries are tightly correlated to each other and contain a wealth of abstract expressions summarizing video storylines and specific descriptions portraying event details, which enab
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于循环神经网络（RNNs）的跨模态注意力框架，用于音频-视频深度伪造检测。该框架能够利用上下文信息学习模态之间的贡献特征，有效克服了音频和视频信号之间的分布性模态差异，提高了检测深度伪造的准确性和鲁棒性。</title><link>https://arxiv.org/abs/2408.01532</link><description>&lt;p&gt;
Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01532
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于循环神经网络（RNNs）的跨模态注意力框架，用于音频-视频深度伪造检测。该框架能够利用上下文信息学习模态之间的贡献特征，有效克服了音频和视频信号之间的分布性模态差异，提高了检测深度伪造的准确性和鲁棒性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于Therblig的Backbone Framework（TBBF），通过将高阶机器人任务分解为基本动作元素（therbligs），并结合当前的基础模型，提高了机器人对任务的理解能力和泛化能力。这种框架通过两个阶段来实现：首先通过Meta-RGate SynerFusion（MGSF）网络在离线训练阶段进行准确的动作元素分割，然后在使用新任务演示后，通过ActionREG方法将高阶知识编码到图像中，从而在线上测试阶段实现任务的理解。</title><link>https://arxiv.org/abs/2408.01334</link><description>&lt;p&gt;
A Backbone for Long-Horizon Robot Task Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01334
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于Therblig的Backbone Framework（TBBF），通过将高阶机器人任务分解为基本动作元素（therbligs），并结合当前的基础模型，提高了机器人对任务的理解能力和泛化能力。这种框架通过两个阶段来实现：首先通过Meta-RGate SynerFusion（MGSF）网络在离线训练阶段进行准确的动作元素分割，然后在使用新任务演示后，通过ActionREG方法将高阶知识编码到图像中，从而在线上测试阶段实现任务的理解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v2 Announce Type: replace  Abstract: End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Addition
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为IG-SLAM的快速RGB-only SLAM系统，它结合了3D Gaussian Splatting和Dense-SLAM跟踪技术，以提供精确的地图更新和快速操作速度。</title><link>https://arxiv.org/abs/2408.01126</link><description>&lt;p&gt;
IG-SLAM: Instant Gaussian SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01126
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为IG-SLAM的快速RGB-only SLAM系统，它结合了3D Gaussian Splatting和Dense-SLAM跟踪技术，以提供精确的地图更新和快速操作速度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01126v2 Announce Type: replace-cross  Abstract: 3D Gaussian Splatting has recently shown promising results as an alternative scene representation in SLAM systems to neural implicit representations. However, current methods either lack dense depth maps to supervise the mapping process or detailed training designs that consider the scale of the environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only SLAM system that employs robust Dense-SLAM methods for tracking and combines them with Gaussian Splatting. A 3D map of the environment is constructed using accurate pose and dense depth provided by tracking. Additionally, we utilize depth uncertainty in map optimization to improve 3D reconstruction. Our decay strategy in map optimization enhances convergence and allows the system to run at 10 fps in a single process. We demonstrate competitive performance with state-of-the-art RGB-only SLAM systems while achieving faster operation speeds. We present our experi
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EnPrompt的框架，通过在VLMs（预训练视觉-语言模型）中集成文本外部层（EnLa），以增加模型的泛化能力。EnLa包含了预训练CLIP的有效文本嵌入和可学习的视觉嵌入，旨在为下游任务提供更好的适应性。通过这种方式，文章旨在平衡视觉和文本分支的学习能力，并通过一种独特的协同学习机制来增强它们之间的交互作用。这种设计允许模型在保持灵活性的同时，能够学习到更加有效的任务特异性文本嵌入，从而提升在各种视觉相关任务上的性能。</title><link>https://arxiv.org/abs/2407.19674</link><description>&lt;p&gt;
Advancing Prompt Learning through an External Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19674
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EnPrompt的框架，通过在VLMs（预训练视觉-语言模型）中集成文本外部层（EnLa），以增加模型的泛化能力。EnLa包含了预训练CLIP的有效文本嵌入和可学习的视觉嵌入，旨在为下游任务提供更好的适应性。通过这种方式，文章旨在平衡视觉和文本分支的学习能力，并通过一种独特的协同学习机制来增强它们之间的交互作用。这种设计允许模型在保持灵活性的同时，能够学习到更加有效的任务特异性文本嵌入，从而提升在各种视觉相关任务上的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19674v3 Announce Type: replace  Abstract: Prompt learning represents a promising method for adapting pre-trained vision-language models (VLMs) to various downstream tasks by learning a set of text embeddings. One challenge inherent to these methods is the poor generalization performance due to the invalidity of the learned text embeddings for unseen tasks. A straightforward approach to bridge this gap is to freeze the text embeddings in prompts, which results in a lack of capacity to adapt VLMs for downstream tasks. To address this dilemma, we propose a paradigm called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a textual external layer and learnable visual embeddings for adapting VLMs to downstream tasks. The learnable external layer is built upon valid embeddings of pre-trained CLIP. This design considers the balance of learning capabilities between the two branches. To align the textual and visual features, we propose a novel two-pronged approach
&lt;/p&gt;</description></item><item><title>该文章介绍了GP-VLS，这是一种融合医学和手术知识，具备视觉场景理解能力的通用工程手术语言模型。通过开发新的数据集并提出SurgiQual评估标准，本文展示了GP-VLS在模拟手术场景中展现出的强大性能，从而预示着未来的智能辅助手术系统可能拥有更广泛的知识处理和语言交互能力。</title><link>https://arxiv.org/abs/2407.19305</link><description>&lt;p&gt;
GP-VLS: A general-purpose vision language model for surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19305
&lt;/p&gt;
&lt;p&gt;
该文章介绍了GP-VLS，这是一种融合医学和手术知识，具备视觉场景理解能力的通用工程手术语言模型。通过开发新的数据集并提出SurgiQual评估标准，本文展示了GP-VLS在模拟手术场景中展现出的强大性能，从而预示着未来的智能辅助手术系统可能拥有更广泛的知识处理和语言交互能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19305v2 Announce Type: replace  Abstract: Surgery requires comprehensive medical knowledge, visual assessment skills, and procedural expertise. While recent surgical AI models have focused on solving task-specific problems, there is a need for general-purpose systems that can understand surgical scenes and interact through natural language. This paper introduces GP-VLS, a general-purpose vision language model for surgery that integrates medical and surgical knowledge with visual scene understanding. For comprehensively evaluating general-purpose surgical models, we propose SurgiQual, which evaluates across medical and surgical knowledge benchmarks as well as surgical vision-language questions. To train GP-VLS, we develop six new datasets spanning medical knowledge, surgical textbooks, and vision-language pairs for tasks like phase recognition and tool identification. We show that GP-VLS significantly outperforms existing open- and closed-source models on surgical vision-lang
&lt;/p&gt;</description></item><item><title>该文章提出了一种多阶段的方法来增强无人机图像中树类的检测，并引入了一种颜色编码系统来评估森林火灾风险。通过优化现有的森林火灾风险评估框架，结合多阶段对象检测算法，该框架能够更加精准地识别潜在的火灾风险区域。通过进行实验，文章证明了这些改进对提高森林火灾检测和评估的准确性具有显著效果。</title><link>https://arxiv.org/abs/2407.19184</link><description>&lt;p&gt;
Enhancing Tree Type Detection in Forest Fire Risk Assessment: Multi-Stage Approach and Color Encoding with Forest Fire Risk Evaluation Framework for UAV Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19184
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种多阶段的方法来增强无人机图像中树类的检测，并引入了一种颜色编码系统来评估森林火灾风险。通过优化现有的森林火灾风险评估框架，结合多阶段对象检测算法，该框架能够更加精准地识别潜在的火灾风险区域。通过进行实验，文章证明了这些改进对提高森林火灾检测和评估的准确性具有显著效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19184v2 Announce Type: replace  Abstract: Forest fires pose a significant threat to ecosystems, economies, and human health worldwide. Early detection and assessment of forest fires are crucial for effective management and conservation efforts. Unmanned Aerial Vehicles (UAVs) equipped with advanced computer vision algorithms offer a promising solution for forest fire detection and assessment. In this paper, we optimize an integrated forest fire risk assessment framework using UAVs and multi-stage object detection algorithms. We introduce improvements to our previous framework, including the adoption of Faster R-CNN, Grid R-CNN, Sparse R-CNN, Cascade R-CNN, Dynamic R-CNN, and Libra R-CNN detectors, and explore optimizations such as CBAM for attention enhancement, random erasing for preprocessing, and different color space representations. We evaluate these enhancements through extensive experimentation using aerial image footage from various regions in British Columbia, Canad
&lt;/p&gt;</description></item><item><title>该文章提出了一种新型的自监督单幅图像深度估计方法，它通过局部结构光运动在有限的帧范围内进行深度和对应关系的估计，并使用一个创新的RANSAC调整算法来优化相机姿态和深度，随后使用深度神经网元来细化深度图，为深度估计领域带来了重要的创新和贡献。</title><link>https://arxiv.org/abs/2407.19166</link><description>&lt;p&gt;
Revisit Self-supervised Depth Estimation with Local Structure-from-Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19166
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新型的自监督单幅图像深度估计方法，它通过局部结构光运动在有限的帧范围内进行深度和对应关系的估计，并使用一个创新的RANSAC调整算法来优化相机姿态和深度，随后使用深度神经网元来细化深度图，为深度估计领域带来了重要的创新和贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19166v2 Announce Type: replace  Abstract: Both self-supervised depth estimation and Structure-from-Motion (SfM) recover scene depth from RGB videos. Despite sharing a similar objective, the two approaches are disconnected. Prior works of self-supervision backpropagate losses defined within immediate neighboring frames. Instead of learning-through-loss, this work proposes an alternative scheme by performing local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and correspondence estimator to infer depthmaps and pair-wise correspondence maps. Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses and one depth adjustment for each depthmap. Finally, we fix camera poses and employ a NeRF, however, without a neural network, for dense triangulation and geometric verification. Poses, depth adjustments, and triangulated sparse depths are our outputs. For the first time, we show self-supervision within $5$ frames already benefits SoTA super
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为TRM-ML的新方法，该方法通过优化多标签提示 tuning，在多标签图像中有效地匹配文本和视觉特征，特别是通过探索类别特定区域的额外信息，而不是整个图像或像素信息，这有助于在一种一对一匹配方式中弥合文本和视觉表示之间的语义差距。</title><link>https://arxiv.org/abs/2407.18520</link><description>&lt;p&gt;
Text-Region Matching for Multi-Label Image Recognition with Missing Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18520
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为TRM-ML的新方法，该方法通过优化多标签提示 tuning，在多标签图像中有效地匹配文本和视觉特征，特别是通过探索类别特定区域的额外信息，而不是整个图像或像素信息，这有助于在一种一对一匹配方式中弥合文本和视觉表示之间的语义差距。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18520v2 Announce Type: replace  Abstract: Recently, large-scale visual language pre-trained (VLP) models have demonstrated impressive performance across various downstream tasks. Motivated by these advancements, pioneering efforts have emerged in multi-label image recognition with missing labels, leveraging VLP prompt-tuning technology. However, they usually cannot match text and vision features well, due to complicated semantics gaps and missing labels in a multi-label image. To tackle this challenge, we propose \textbf{T}ext-\textbf{R}egion \textbf{M}atching for optimizing \textbf{M}ulti-\textbf{L}abel prompt tuning, namely TRM-ML, a novel method for enhancing meaningful cross-modal matching. Compared to existing methods, we advocate exploring the information of category-aware regions rather than the entire image or pixels, which contributes to bridging the semantic gap between textual and visual representations in a one-to-one matching manner. Concurrently, we further int
&lt;/p&gt;</description></item><item><title>该文章构建了一个高质量、多样化的视觉指令微调数据集MMInstruct，该数据集包含来自24个领域的973万条指令，以四种不同的指令类型存在，旨在提高视觉语言大型模型（VLLMs）在处理多样化真实世界场景输出时的性能。</title><link>https://arxiv.org/abs/2407.15838</link><description>&lt;p&gt;
MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15838
&lt;/p&gt;
&lt;p&gt;
该文章构建了一个高质量、多样化的视觉指令微调数据集MMInstruct，该数据集包含来自24个领域的973万条指令，以四种不同的指令类型存在，旨在提高视觉语言大型模型（VLLMs）在处理多样化真实世界场景输出时的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15838v2 Announce Type: replace  Abstract: Despite the effectiveness of vision-language supervised fine-tuning in enhancing the performance of Vision Large Language Models (VLLMs). However, existing visual instruction tuning datasets include the following limitations: (1) Instruction annotation quality: despite existing VLLMs exhibiting strong performance, instructions generated by those advanced VLLMs may still suffer from inaccuracies, such as hallucinations. (2) Instructions and image diversity: the limited range of instruction types and the lack of diversity in image data may impact the model's ability to generate diversified and closer to real-world scenarios outputs. To address these challenges, we construct a high-quality, diverse visual instruction tuning dataset MMInstruct, which consists of 973K instructions from 24 domains. There are four instruction types: Judgement, Multiple-Choice, Long Visual Question Answering and Short Visual Question Answering. To construct 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为 CLIP with Generative Latent Replay 的强有力基线方法，用于解决增量学习中的遗忘问题。该方法通过利用生成式回放技术，同时在保持模型零样本能力的同时，在截然不同的任务域中适应模型。通过在多个任务域上的广泛实验，文章展示了该方法在适应新任务方面的有效性，同时显著提升了 CL 基准测试中的零样本能力。</title><link>https://arxiv.org/abs/2407.15793</link><description>&lt;p&gt;
CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15793
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为 CLIP with Generative Latent Replay 的强有力基线方法，用于解决增量学习中的遗忘问题。该方法通过利用生成式回放技术，同时在保持模型零样本能力的同时，在截然不同的任务域中适应模型。通过在多个任务域上的广泛实验，文章展示了该方法在适应新任务方面的有效性，同时显著提升了 CL 基准测试中的零样本能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15793v2 Announce Type: replace  Abstract: With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improvin
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为ESP-MedSAM的 efficient self-prompting SAM（Segment Anything Model），针对医疗图像 segmentation 的三个主要挑战：显著降低计算成本、简化数据标注过程并提升对不同医疗模态的适应性。通过 Multi-Modal Decoupled Knowledge Distillation（MMDKD）策略，文章创新性地将通用的图像知识和特定于医学领域的知识从基础模型中提炼出来，用于训练一个轻量级的图像编码器和一种称为 modality controller 的模块，以此减轻了专家标注的负担并提高了模型在不同医学模态上的性能。 Additionally, a self-patch prompting generator was introduced to further enhance the model's ability to adapt to diverse medical imaging tasks, making ESP-MedSAM a significant improvement over previous SAM models in terms of efficiency and applicability for medical image segmentation.</title><link>https://arxiv.org/abs/2407.14153</link><description>&lt;p&gt;
ESP-MedSAM: Efficient Self-Prompting SAM for Universal Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.14153
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为ESP-MedSAM的 efficient self-prompting SAM（Segment Anything Model），针对医疗图像 segmentation 的三个主要挑战：显著降低计算成本、简化数据标注过程并提升对不同医疗模态的适应性。通过 Multi-Modal Decoupled Knowledge Distillation（MMDKD）策略，文章创新性地将通用的图像知识和特定于医学领域的知识从基础模型中提炼出来，用于训练一个轻量级的图像编码器和一种称为 modality controller 的模块，以此减轻了专家标注的负担并提高了模型在不同医学模态上的性能。 Additionally, a self-patch prompting generator was introduced to further enhance the model's ability to adapt to diverse medical imaging tasks, making ESP-MedSAM a significant improvement over previous SAM models in terms of efficiency and applicability for medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.14153v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) has demonstrated outstanding adaptation to medical image segmentation but still faces three major challenges. Firstly, the huge computational costs of SAM limit its real-world applicability. Secondly, SAM depends on manual annotations (e.g., points, boxes) as prompts, which are laborious and impractical in clinical scenarios. Thirdly, SAM handles all segmentation targets equally, which is suboptimal for diverse medical modalities with inherent heterogeneity. To address these issues, we propose an Efficient Self-Prompting SAM for universal medical image segmentation, named ESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD) strategy to distil common image knowledge and domain-specific medical knowledge from the foundation model to train a lightweight image encoder and a modality controller. Further, they combine with the additionally introduced Self-Patch Prompt Generator
&lt;/p&gt;</description></item><item><title>该文章通过采用雷达和热成像 camera 结合的方法在十位参与者中进行了一项试验，研究结果表明采用热成像检测呼吸和睡眠呼吸暂停的准确性显著高于雷达检测方法，在检测睡眠中的呼吸暂停事件上准确率达到 0.99，说明该方法在非接触式睡眠监测领域具有较大的应用潜力。</title><link>https://arxiv.org/abs/2407.11936</link><description>&lt;p&gt;
Thermal Imaging and Radar for Remote Sleep Monitoring of Breathing and Apnea
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11936
&lt;/p&gt;
&lt;p&gt;
该文章通过采用雷达和热成像 camera 结合的方法在十位参与者中进行了一项试验，研究结果表明采用热成像检测呼吸和睡眠呼吸暂停的准确性显著高于雷达检测方法，在检测睡眠中的呼吸暂停事件上准确率达到 0.99，说明该方法在非接触式睡眠监测领域具有较大的应用潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11936v2 Announce Type: replace  Abstract: Polysomnography (PSG), the current gold standard method for monitoring and detecting sleep disorders, is cumbersome and costly. At-home testing solutions, known as home sleep apnea testing (HSAT), exist. However, they are contact-based, a feature which limits the ability of some patient populations to tolerate testing and discourages widespread deployment. Previous work on non-contact sleep monitoring for sleep apnea detection either estimates respiratory effort using radar or nasal airflow using a thermal camera, but has not compared the two or used them together. We conducted a study on 10 participants, ages 34 - 78, with suspected sleep disorders using a hardware setup with a synchronized radar and thermal camera. We show the first comparison of radar and thermal imaging for sleep monitoring, and find that our thermal imaging method outperforms radar significantly. Our thermal imaging method detects apneas with an accuracy of 0.99
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CCVA-FL的系统，用于在医疗影像领域中通过联邦学习解决来自不同客户端的图像数据之间的差异化问题，并介绍了一种旨在最小化跨客户端差异的方法，涉及使用Scalable Diffusion Models with Transformers（DiT）生成反映目标客户端图像数据的合成图像，这些合成图像被用于共享以帮助其他客户端的图像数据进入目标客户端的图像空间，从而在分布式医疗影像数据中实现更好的模型训练效果。</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CCVA-FL的系统，用于在医疗影像领域中通过联邦学习解决来自不同客户端的图像数据之间的差异化问题，并介绍了一种旨在最小化跨客户端差异的方法，涉及使用Scalable Diffusion Models with Transformers（DiT）生成反映目标客户端图像数据的合成图像，这些合成图像被用于共享以帮助其他客户端的图像数据进入目标客户端的图像空间，从而在分布式医疗影像数据中实现更好的模型训练效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v4 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Ref-MC2的方法，通过引入多时间蒙特卡洛采样来全面计算环境照明，同时考虑物体表面的反射光。此外，文章还提出了一种适应性采样策略，以提高采样效率并减少累积的几何误差。这些改进使得反射性重建成为可能，这对于处理多个光滑物体之间的互反射特别有效。</title><link>https://arxiv.org/abs/2407.05771</link><description>&lt;p&gt;
Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.05771
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Ref-MC2的方法，通过引入多时间蒙特卡洛采样来全面计算环境照明，同时考虑物体表面的反射光。此外，文章还提出了一种适应性采样策略，以提高采样效率并减少累积的几何误差。这些改进使得反射性重建成为可能，这对于处理多个光滑物体之间的互反射特别有效。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.05771v2 Announce Type: replace  Abstract: Inverse rendering methods have achieved remarkable performance in reconstructing high-fidelity 3D objects with disentangled geometries, materials, and environmental light. However, they still face huge challenges in reflective surface reconstruction. Although recent methods model the light trace to learn specularity, the ignorance of indirect illumination makes it hard to handle inter-reflections among multiple smooth objects. In this work, we propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which comprehensively computes the environmental illumination and meanwhile considers the reflective light from object surfaces. To address the computation challenge as the times of Monte Carlo sampling grow, we propose a specularity-adaptive sampling strategy, significantly reducing the computational complexity. Besides the computational resource, higher geometry accuracy is also required because geometric errors accumulate mu
&lt;/p&gt;</description></item><item><title>该文章提出了使用Kolmogorov-Arnold网络（KAN）代替传统神经网络中的线性与卷积层，显著提高了对hyperspectral图像的分类准确率。通过替换不同类型的神经网络架构的各个组件，包括基础的MLP、最新的1D CNN、3D CNN和Transformer架构，研究人员成功地提升了所有架构的分类精度，证明了KAN在这些应用中的有效性。</title><link>https://arxiv.org/abs/2407.05278</link><description>&lt;p&gt;
HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image Classificators Smarter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.05278
&lt;/p&gt;
&lt;p&gt;
该文章提出了使用Kolmogorov-Arnold网络（KAN）代替传统神经网络中的线性与卷积层，显著提高了对hyperspectral图像的分类准确率。通过替换不同类型的神经网络架构的各个组件，包括基础的MLP、最新的1D CNN、3D CNN和Transformer架构，研究人员成功地提升了所有架构的分类精度，证明了KAN在这些应用中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.05278v2 Announce Type: replace  Abstract: In traditional neural network architectures, a multilayer perceptron (MLP) is typically employed as a classification block following the feature extraction stage. However, the Kolmogorov-Arnold Network (KAN) presents a promising alternative to MLP, offering the potential to enhance prediction accuracy. In this paper, we propose the replacement of linear and convolutional layers of traditional networks with KAN-based counterparts. These modifications allowed us to significantly increase the per-pixel classification accuracy for hyperspectral remote-sensing images. We modified seven different neural network architectures for hyperspectral image classification and observed a substantial improvement in the classification accuracy across all the networks. The architectures considered in the paper include baseline MLP, state-of-the-art 1D (1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer (SSFTT) architectures, as
&lt;/p&gt;</description></item><item><title>该文章提出了一种针对移动图形用户界面代理开发的多模态大型语言模型——MobileFlow，该模型能够高效地理解和处理移动应用程序的用户操作，并显著提升了中文图形用户界面的理解和决策能力，同时也解决了访问系统API可能引发的用户隐私问题，并通过保持更高的图像分辨率来确保用户界面的细节不被损失。</title><link>https://arxiv.org/abs/2407.04346</link><description>&lt;p&gt;
MobileFlow: A Multimodal LLM For Mobile GUI Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.04346
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种针对移动图形用户界面代理开发的多模态大型语言模型——MobileFlow，该模型能够高效地理解和处理移动应用程序的用户操作，并显著提升了中文图形用户界面的理解和决策能力，同时也解决了访问系统API可能引发的用户隐私问题，并通过保持更高的图像分辨率来确保用户界面的细节不被损失。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.04346v2 Announce Type: replace  Abstract: Currently, the integration of mobile Graphical User Interfaces (GUIs) is ubiquitous in most people's daily lives. And the ongoing evolution of multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly bolstered the capabilities of GUI comprehension and user action analysis, showcasing the potentiality of intelligent GUI assistants. However, current GUI Agents often need to access page layout information through calling system APIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a certain low resolution might result in the loss of fine-grained image details. At the same time, the multimodal large models built for GUI Agents currently have poor understanding and decision-making abilities for Chinese GUI interfaces, making them difficult to apply to a large number of Chinese apps. This paper introduces MobileFlow, a multimodal large language model meticulously crafted for mobile GUI agents. T
&lt;/p&gt;</description></item><item><title>该文章揭示了个性化扩散模型在对抗性攻击下的脆弱性，并提出了一种保护隐私的新方法，该方法通过在图像生成过程中引入特定的过滤技术，可以在抵抗对抗性攻击的同时最小化信息损失。</title><link>https://arxiv.org/abs/2406.18944</link><description>&lt;p&gt;
Investigating and Defending Shortcut Learning in Personalized Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.18944
&lt;/p&gt;
&lt;p&gt;
该文章揭示了个性化扩散模型在对抗性攻击下的脆弱性，并提出了一种保护隐私的新方法，该方法通过在图像生成过程中引入特定的过滤技术，可以在抵抗对抗性攻击的同时最小化信息损失。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.18944v3 Announce Type: replace  Abstract: Personalized diffusion models have gained popularity for adapting pre-trained text-to-image models to generate images of specific topics with minimal training data. However, these models are vulnerable to minor adversarial perturbations, leading to degraded performance on corrupted datasets. Such vulnerabilities are further exploited to craft protective perturbations on sensitive images like portraits that prevent unauthorized generation. In response, diffusion-based purification methods have been proposed to remove these perturbations and retain generation performance. However, existing works turn to over-purifying the images, which causes information loss. In this paper, we take a closer look at the fine-tuning process of personalized diffusion models through the lens of shortcut learning. And we propose a hypothesis explaining the manipulation mechanisms of existing perturbation methods, demonstrating that perturbed images signifi
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为“命令学习”（Imperative Learning）的自监督神经符号学习框架，用于提高机器人的自主能力。该框架通过三部分组成——神经模块、推理引擎和记忆系统，实现了神经和符号推理的结合，不需要大量标记数据，通过形式化的双层优化问题，解决了数据驱动方法的标签依赖问题和基于符号推理的长处。</title><link>https://arxiv.org/abs/2406.16087</link><description>&lt;p&gt;
Imperative Learning: A Self-supervised Neural-Symbolic Learning Framework for Robot Autonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.16087
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为“命令学习”（Imperative Learning）的自监督神经符号学习框架，用于提高机器人的自主能力。该框架通过三部分组成——神经模块、推理引擎和记忆系统，实现了神经和符号推理的结合，不需要大量标记数据，通过形式化的双层优化问题，解决了数据驱动方法的标签依赖问题和基于符号推理的长处。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16087v4 Announce Type: replace  Abstract: Data-driven methods such as reinforcement and imitation learning have achieved remarkable success in robot autonomy. However, their data-centric nature still hinders them from generalizing well to ever-changing environments. Moreover, collecting large datasets for robotic tasks is often impractical and expensive. To overcome these challenges, we introduce a new self-supervised neural-symbolic (NeSy) computational framework, imperative learning (IL), for robot autonomy, leveraging the generalization abilities of symbolic reasoning. The framework of IL consists of three primary components: a neural module, a reasoning engine, and a memory system. We formulate IL as a special bilevel optimization (BLO), which enables reciprocal learning over the three modules. This overcomes the label-intensive obstacles associated with data-driven approaches and takes advantage of symbolic reasoning concerning logical reasoning, physical principles, ge
&lt;/p&gt;</description></item><item><title>该文章揭示了一种使用机器学习和代谢组学数据对干眼病患者与健康对照进行分类的创新方法。通过对三种不同代谢物数据集的九种不同机器学习模型的比较分析，研究提出通过选择最佳模型能够显著提升预测质量和后续代谢组学分析的准确性。文章强调了代谢组学在早期诊断干眼病中的潜力，并强调了机器学习在处理复杂生物数据时的关键作用。</title><link>https://arxiv.org/abs/2406.14068</link><description>&lt;p&gt;
Classifying Dry Eye Disease Patients from Healthy Controls Using Machine Learning and Metabolomics Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.14068
&lt;/p&gt;
&lt;p&gt;
该文章揭示了一种使用机器学习和代谢组学数据对干眼病患者与健康对照进行分类的创新方法。通过对三种不同代谢物数据集的九种不同机器学习模型的比较分析，研究提出通过选择最佳模型能够显著提升预测质量和后续代谢组学分析的准确性。文章强调了代谢组学在早期诊断干眼病中的潜力，并强调了机器学习在处理复杂生物数据时的关键作用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.14068v2 Announce Type: replace  Abstract: Dry eye disease is a common disorder of the ocular surface, leading patients to seek eye care. Clinical signs and symptoms are currently used to diagnose dry eye disease. Metabolomics, a method for analyzing biological systems, has been found helpful in identifying distinct metabolites in patients and in detecting metabolic profiles that may indicate dry eye disease at early stages. In this study, we explored using machine learning and metabolomics information to identify which cataract patients suffered from dry eye disease. As there is no one-size-fits-all machine learning model for metabolomics data, choosing the most suitable model can significantly affect the quality of predictions and subsequent metabolomics analyses. To address this challenge, we conducted a comparative analysis of nine machine learning models on three metabolomics data sets from cataract patients with and without dry eye disease. The models were evaluated and
&lt;/p&gt;</description></item><item><title>该文章提出了一种新颖的上身姿态跟踪方法，名为SeamPose，通过在衬衫的现有缝线中嵌入绝缘导电线，将缝线重新定义为电容传感器。这种方法能够在不破坏衬衫外观的前提下，提供类似传统衬衫的穿着体验，同时实现有效的姿态跟踪。通过使用一个12名参与者参与的实验研究，证明了该方法在无需连线的衬衫上准确估计上身3D关节位置的能力。</title><link>https://arxiv.org/abs/2406.11645</link><description>&lt;p&gt;
SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.11645
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新颖的上身姿态跟踪方法，名为SeamPose，通过在衬衫的现有缝线中嵌入绝缘导电线，将缝线重新定义为电容传感器。这种方法能够在不破坏衬衫外观的前提下，提供类似传统衬衫的穿着体验，同时实现有效的姿态跟踪。通过使用一个12名参与者参与的实验研究，证明了该方法在无需连线的衬衫上准确估计上身3D关节位置的能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.11645v2 Announce Type: replace-cross  Abstract: Seams are areas of overlapping fabric formed by stitching two or more pieces of fabric together in the cut-and-sew apparel manufacturing process. In SeamPose, we repurposed seams as capacitive sensors in a shirt for continuous upper-body pose estimation. Compared to previous all-textile motion-capturing garments that place the electrodes on the clothing surface, our solution leverages existing seams inside of a shirt by machine-sewing insulated conductive threads over the seams. The unique invisibilities and placements of the seams afford the sensing shirt to look and wear similarly as a conventional shirt while providing exciting pose-tracking capabilities. To validate this approach, we implemented a proof-of-concept untethered shirt with 8 capacitive sensing seams. With a 12-participant user study, our customized deep-learning pipeline accurately estimates the relative (to the pelvis) upper-body 3D joint positions with a mean
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的Radiance Field处理方法，能够独立于NeRF训练过程去除多视图输入图像中的ISP处理效果，并在渲染生成的新颖视角图像时重新应用用户自定义的图像增强处理，以保证不同视角之间的视觉一致性。</title><link>https://arxiv.org/abs/2406.00448</link><description>&lt;p&gt;
Bilateral Guided Radiance Field Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.00448
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的Radiance Field处理方法，能够独立于NeRF训练过程去除多视图输入图像中的ISP处理效果，并在渲染生成的新颖视角图像时重新应用用户自定义的图像增强处理，以保证不同视角之间的视觉一致性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.00448v2 Announce Type: replace  Abstract: Neural Radiance Fields (NeRF) achieves unprecedented performance in synthesizing novel view synthesis, utilizing multi-view consistency. When capturing multiple inputs, image signal processing (ISP) in modern cameras will independently enhance them, including exposure adjustment, color correction, local tone mapping, etc. While these processings greatly improve image quality, they often break the multi-view consistency assumption, leading to "floaters" in the reconstructed radiance fields. To address this concern without compromising visual aesthetics, we aim to first disentangle the enhancement by ISP at the NeRF training stage and re-apply user-desired enhancements to the reconstructed radiance fields at the finishing stage. Furthermore, to make the re-applied enhancements consistent between novel views, we need to perform imaging signal processing in 3D space (i.e. "3D ISP"). For this goal, we adopt the bilateral grid, a locally-a
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于量子Transformer框架的QClusformer模型，用于无监督视觉聚类任务。该方法通过对Transformer架构进行量子化设计，以在量子硬件上执行，并适用于处理大量未标注数据。通过整合量子化模块和无监督任务目标，QClusformer有望加速优化 unsupervised vision clustering。</title><link>https://arxiv.org/abs/2405.19722</link><description>&lt;p&gt;
QClusformer: A Quantum Transformer-based Framework for Unsupervised Visual Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.19722
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于量子Transformer框架的QClusformer模型，用于无监督视觉聚类任务。该方法通过对Transformer架构进行量子化设计，以在量子硬件上执行，并适用于处理大量未标注数据。通过整合量子化模块和无监督任务目标，QClusformer有望加速优化 unsupervised vision clustering。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19722v2 Announce Type: replace  Abstract: Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding significant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based framework leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quantum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusform
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为FourierMamba的框架，该框架利用状态空间模型结合傅里叶学习来集成不同频率间的依赖关系，以提升图像去雨的效率和效果。</title><link>https://arxiv.org/abs/2405.19450</link><description>&lt;p&gt;
FourierMamba: Fourier Learning Integration with State Space Models for Image Deraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.19450
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为FourierMamba的框架，该框架利用状态空间模型结合傅里叶学习来集成不同频率间的依赖关系，以提升图像去雨的效率和效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19450v2 Announce Type: replace  Abstract: Image deraining aims to remove rain streaks from rainy images and restore clear backgrounds. Currently, some research that employs the Fourier transform has proved to be effective for image deraining, due to it acting as an effective frequency prior for capturing rain streaks. However, despite there exists dependency of low frequency and high frequency in images, these Fourier-based methods rarely exploit the correlation of different frequencies for conjuncting their learning procedures, limiting the full utilization of frequency information for image deraining. Alternatively, the recently emerged Mamba technique depicts its effectiveness and efficiency for modeling correlation in various domains (e.g., spatial, temporal), and we argue that introducing Mamba into its unexplored Fourier spaces to correlate different frequencies would help improve image deraining. This motivates us to propose a new framework termed FourierMamba, which 
&lt;/p&gt;</description></item><item><title>该文章通过研究生成对抗网络（GANs）中的生成结构与其隐藏层激活之间的关联性，提出了一种新的方法来更好地理解和控制GANs的生成机制。研究人员通过引入可平铺特征的概念，能够在不需要训练数据中包含相关分割的情况下，基于语义分割图来生成图像。这种方法为GANs的实例化提供了一种新的途径，使得用户可以直接从语义分割图中获取逼真的图像，而不需要训练网络来识别这些分割图。</title><link>https://arxiv.org/abs/2405.15636</link><description>&lt;p&gt;
Visualize and Paint GAN Activations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.15636
&lt;/p&gt;
&lt;p&gt;
该文章通过研究生成对抗网络（GANs）中的生成结构与其隐藏层激活之间的关联性，提出了一种新的方法来更好地理解和控制GANs的生成机制。研究人员通过引入可平铺特征的概念，能够在不需要训练数据中包含相关分割的情况下，基于语义分割图来生成图像。这种方法为GANs的实例化提供了一种新的途径，使得用户可以直接从语义分割图中获取逼真的图像，而不需要训练网络来识别这些分割图。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.15636v3 Announce Type: replace  Abstract: We investigate how generated structures of GANs correlate with their activations in hidden layers, with the purpose of better understanding the inner workings of those models and being able to paint structures with unconditionally trained GANs. This gives us more control over the generated images, allowing to generate them from a semantic segmentation map while not requiring such a segmentation in the training data. To this end we introduce the concept of tileable features, allowing us to identify activations that work well for painting.
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的自监督学习架构，该架构使用胶囊网络（CapsNets）来学习具有高度不变性和自适应性的特征表示。该架构在3D旋转不变性任务上实现了最佳性能，并通过改进的自适应目标函数减少了模型参数和训练时间。</title><link>https://arxiv.org/abs/2405.14386</link><description>&lt;p&gt;
Capsule Network Projectors are Equivariant and Invariant Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14386
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的自监督学习架构，该架构使用胶囊网络（CapsNets）来学习具有高度不变性和自适应性的特征表示。该架构在3D旋转不变性任务上实现了最佳性能，并通过改进的自适应目标函数减少了模型参数和训练时间。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14386v2 Announce Type: replace  Abstract: Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets) which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach which we name CapsIE (Capsule Invariant Equivariant Network) achieves state-of-the-art performance across invariant and equivariant tasks on the 3DIEBench dataset co
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Cascade Multi-path Shortcut Diffusion Model（CMDM）的新方法，用于提高医疗图像翻译的质量并实现不确定性估计。通过结合生成对抗网络（GAN）和扩散模型的优势，CMDM克服了单独使用这些方法时面临的不稳定性和不确定性估计缺失的问题。该方法在减少迭代次数的同时保证了翻译的稳健性，并通过使用条件GAN生成的先验图像来提高翻译质量。</title><link>https://arxiv.org/abs/2405.12223</link><description>&lt;p&gt;
Cascaded Multi-path Shortcut Diffusion Model for Medical Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.12223
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Cascade Multi-path Shortcut Diffusion Model（CMDM）的新方法，用于提高医疗图像翻译的质量并实现不确定性估计。通过结合生成对抗网络（GAN）和扩散模型的优势，CMDM克服了单独使用这些方法时面临的不稳定性和不确定性估计缺失的问题。该方法在减少迭代次数的同时保证了翻译的稳健性，并通过使用条件GAN生成的先验图像来提高翻译质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.12223v2 Announce Type: replace-cross  Abstract: Image-to-image translation is a vital component in medical imaging processing, with many uses in a wide range of imaging modalities and clinical scenarios. Previous methods include Generative Adversarial Networks (GANs) and Diffusion Models (DMs), which offer realism but suffer from instability and lack uncertainty estimation. Even though both GAN and DM methods have individually exhibited their capability in medical image translation tasks, the potential of combining a GAN and DM to further improve translation performance and to enable uncertainty estimation remains largely unexplored. In this work, we address these challenges by proposing a Cascade Multi-path Shortcut Diffusion Model (CMDM) for high-quality medical image translation and uncertainty estimation. To reduce the required number of iterations and ensure robust performance, our method first obtains a conditional GAN-generated prior image that will be used for the ef
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为"FA-Depth"的自监督单目深度估计方法，旨在提高速度和精度。文章首先提出了一种名为SmallDepth的轻量级模型，并通过引入等价变换模块（ETM）增强其模型训练时的特征表示能力。同时，文章还提出了一种金字塔损失函数来提高模型对不同层次上下文信息的感知能力，以增强模型对左/右方向和光照变化的不变性。此外，文章利用功能逼近损失（APX）将高精度预训练模型HQDecv2的知识转移给SmallDepth，以克服一些区域中的网格失真问题。实验结果表明，FA-Depth方法在提高速度的同时，深度估计的准确性得到了显著提升。</title><link>https://arxiv.org/abs/2405.10885</link><description>&lt;p&gt;
FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.10885
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为"FA-Depth"的自监督单目深度估计方法，旨在提高速度和精度。文章首先提出了一种名为SmallDepth的轻量级模型，并通过引入等价变换模块（ETM）增强其模型训练时的特征表示能力。同时，文章还提出了一种金字塔损失函数来提高模型对不同层次上下文信息的感知能力，以增强模型对左/右方向和光照变化的不变性。此外，文章利用功能逼近损失（APX）将高精度预训练模型HQDecv2的知识转移给SmallDepth，以克服一些区域中的网格失真问题。实验结果表明，FA-Depth方法在提高速度的同时，深度估计的准确性得到了显著提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10885v2 Announce Type: replace  Abstract: Most existing methods often rely on complex models to predict scene depth with high accuracy, resulting in slow inference that is not conducive to deployment. To better balance precision and speed, we first designed SmallDepth based on sparsity. Second, to enhance the feature representation ability of SmallDepth during training under the condition of equal complexity during inference, we propose an equivalent transformation module(ETM). Third, to improve the ability of each layer in the case of a fixed SmallDepth to perceive different context information and improve the robustness of SmallDepth to the left-right direction and illumination changes, we propose pyramid loss. Fourth, to further improve the accuracy of SmallDepth, we utilized the proposed function approximation loss (APX) to transfer knowledge in the pretrained HQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in some regions, to SmallDepth. Ext
&lt;/p&gt;</description></item><item><title>该文章提出一种基于扩散模型的少数样例风格转移学习框架，该框架首先通过文本到动作的模型进行预训练，然后在有限样例基础上对其进行微调，以实现动作风格转移目标。</title><link>https://arxiv.org/abs/2405.06646</link><description>&lt;p&gt;
Diffusion-based Human Motion Style Transfer with Semantic Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.06646
&lt;/p&gt;
&lt;p&gt;
该文章提出一种基于扩散模型的少数样例风格转移学习框架，该框架首先通过文本到动作的模型进行预训练，然后在有限样例基础上对其进行微调，以实现动作风格转移目标。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.06646v2 Announce Type: replace-cross  Abstract: 3D Human motion style transfer is a fundamental problem in computer graphic and animation processing. Existing AdaIN- based methods necessitate datasets with balanced style distribution and content/style labels to train the clustered latent space. However, we may encounter a single unseen style example in practical scenarios, but not in sufficient quantity to constitute a style cluster for AdaIN-based methods. Therefore, in this paper, we propose a novel two-stage framework for few-shot style transfer learning based on the diffusion model. Specifically, in the first stage, we pre-train a diffusion-based text-to-motion model as a generative prior so that it can cope with various content motion inputs. In the second stage, based on the single style example, we fine-tune the pre-trained diffusion model in a few-shot manner to make it capable of style transfer. The key idea is regarding the reverse process of diffusion as a motion-
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于压缩实现的深度结构网络（CRDS），通过引入与经典压缩编码器架构中三个主要过程相匹配的三个经验性偏差，有效增强了压缩视频的质量。这种设计同时融合了经典编码器和深度神经网络的优点，并实现了对视频质量提升任务的有效解析和优化。</title><link>https://arxiv.org/abs/2405.06342</link><description>&lt;p&gt;
Compression-Realized Deep Structural Network for Video Quality Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.06342
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于压缩实现的深度结构网络（CRDS），通过引入与经典压缩编码器架构中三个主要过程相匹配的三个经验性偏差，有效增强了压缩视频的质量。这种设计同时融合了经典编码器和深度神经网络的优点，并实现了对视频质量提升任务的有效解析和优化。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.06342v2 Announce Type: replace  Abstract: This paper focuses on the task of quality enhancement for compressed videos. Although deep network-based video restorers achieve impressive progress, most of the existing methods lack a structured design to optimally leverage the priors within compression codecs. Since the quality degradation of the video is primarily induced by the compression algorithm, a new paradigm is urgently needed for a more ``conscious'' process of quality enhancement. As a result, we propose the Compression-Realized Deep Structural Network (CRDS), introducing three inductive biases aligned with the three primary processes in the classic compression codec, merging the strengths of classical encoder architecture with deep network capabilities. Inspired by the residual extraction and domain transformation process in the codec, a pre-trained Latent Degradation Residual Auto-Encoder is proposed to transform video frames into a latent feature space, and the mutua
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CrossMatch的框架，通过结合图像级别的和特征级别的数据扰动策略，以及知识蒸馏技术，显著提升了半监督学习在医学图像分割任务中的效果。这一创新方法通过多分支编码器和解码器生成多样化的数据流，并在数据流之间进行了自我知识蒸馏，从而提高了预测的一致性和可靠性，并在标准基准测试中大幅超越了现有最先进的半监督学习算法。</title><link>https://arxiv.org/abs/2405.00354</link><description>&lt;p&gt;
CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with Perturbation Strategies and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00354
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CrossMatch的框架，通过结合图像级别的和特征级别的数据扰动策略，以及知识蒸馏技术，显著提升了半监督学习在医学图像分割任务中的效果。这一创新方法通过多分支编码器和解码器生成多样化的数据流，并在数据流之间进行了自我知识蒸馏，从而提高了预测的一致性和可靠性，并在标准基准测试中大幅超越了现有最先进的半监督学习算法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00354v2 Announce Type: replace  Abstract: Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data. Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy. In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's learning from both labeled and unlabeled data. CrossMatch employs multiple encoders and decoders to generate diverse data streams, which undergo self-knowledge distillation to enhance consistency and reliability of predictions across varied perturbations. Our method significantly surpasses other state-of-the-art techniques in standard benchmarks by effectively minimizing the gap between training on labeled and unlabeled data and improving edge 
&lt;/p&gt;</description></item><item><title>该文章提出了一种通过对比学习对正负样本进行比例缩放的方法，改进了基于复合图像检索的任务，使用生成数据的方法解决正样本不足的问题，同时通过两阶段训练框架增加负样本数量，优化了模型在检索任务中的表现。</title><link>https://arxiv.org/abs/2404.11317</link><description>&lt;p&gt;
Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.11317
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种通过对比学习对正负样本进行比例缩放的方法，改进了基于复合图像检索的任务，使用生成数据的方法解决正样本不足的问题，同时通过两阶段训练框架增加负样本数量，优化了模型在检索任务中的表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11317v2 Announce Type: replace  Abstract: The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be ef
&lt;/p&gt;</description></item><item><title>该文章介绍了两项基于鸟瞰视图（BEV）的技术，RoadBEV-mono和RoadBEV-stereo，用于高效的实时道路表面重建，不受传统单目深度估计和立体匹配性能不佳的限制。这些模型能够准确地识别和重建道路的 elevation，对于提高自动驾驶车辆的驾驶性能具有重要意义。</title><link>https://arxiv.org/abs/2404.06605</link><description>&lt;p&gt;
RoadBEV: Road Surface Reconstruction in Bird's Eye View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.06605
&lt;/p&gt;
&lt;p&gt;
该文章介绍了两项基于鸟瞰视图（BEV）的技术，RoadBEV-mono和RoadBEV-stereo，用于高效的实时道路表面重建，不受传统单目深度估计和立体匹配性能不佳的限制。这些模型能够准确地识别和重建道路的 elevation，对于提高自动驾驶车辆的驾驶性能具有重要意义。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.06605v3 Announce Type: replace  Abstract: Road surface conditions, especially geometry profiles, enormously affect driving performance of autonomous vehicles. Vision-based online road reconstruction promisingly captures road information in advance. Existing solutions like monocular depth estimation and stereo matching suffer from modest performance. The recent technique of Bird's-Eye-View (BEV) perception provides immense potential to more reliable and accurate reconstruction. This paper uniformly proposes two simple yet effective models for road elevation reconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate road elevation with monocular and stereo images, respectively. The former directly fits elevation values based on voxel features queried from image view, while the latter efficiently recognizes road elevation patterns based on BEV volume representing correlation between left and right voxel features. Insightful analyses reveal their consistence and
&lt;/p&gt;</description></item><item><title>该文章利用不同的人工智能方法对COVID-19患者和未感染人群的血检参数进行了自动分类，从而提高了疾病的早期检测效率。</title><link>https://arxiv.org/abs/2404.02348</link><description>&lt;p&gt;
COVID-19 Detection Based on Blood Test Parameters using Various Artificial Intelligence Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02348
&lt;/p&gt;
&lt;p&gt;
该文章利用不同的人工智能方法对COVID-19患者和未感染人群的血检参数进行了自动分类，从而提高了疾病的早期检测效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02348v3 Announce Type: replace-cross  Abstract: In 2019, the world faced a new challenge: a COVID-19 disease caused by the novel coronavirus, SARS-CoV-2. The virus rapidly spread across the globe, leading to a high rate of mortality, which prompted health organizations to take measures to control its transmission. Early disease detection is crucial in the treatment process, and computer-based automatic detection systems have been developed to aid in this effort. These systems often rely on artificial intelligence (AI) approaches such as machine learning, neural networks, fuzzy systems, and deep learning to classify diseases. This study aimed to differentiate COVID-19 patients from others using self-categorizing classifiers and employing various AI methods. This study used two datasets: the blood test samples and radiography images. The best results for the blood test samples obtained from San Raphael Hospital, which include two classes of individuals, those with COVID-19 and
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为DPA-Net的框架，它结合了可微分渲染技术，能够从少量的RGB图像中学习到三维物体的结构化抽象表示，无需任何三维监督信息。该框架通过使用可微分体积渲染，能够在网络中实现对三维物体的结构化抽象，即通过预测一系列多面体抽象来表示目标物体。通过引入可微分的基本组成（DPA），该文图网络能够输出一个三维占据场，而不是传统的密度预测，并且通过预测的占据场在图像空间上应用抽象损失和遮罩损失，从而对三维物体的抽象表示进行优化。</title><link>https://arxiv.org/abs/2404.00875</link><description>&lt;p&gt;
DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable Primitive Assembly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00875
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为DPA-Net的框架，它结合了可微分渲染技术，能够从少量的RGB图像中学习到三维物体的结构化抽象表示，无需任何三维监督信息。该框架通过使用可微分体积渲染，能够在网络中实现对三维物体的结构化抽象，即通过预测一系列多面体抽象来表示目标物体。通过引入可微分的基本组成（DPA），该文图网络能够输出一个三维占据场，而不是传统的密度预测，并且通过预测的占据场在图像空间上应用抽象损失和遮罩损失，从而对三维物体的抽象表示进行优化。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00875v3 Announce Type: replace  Abstract: We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional samplin
&lt;/p&gt;</description></item><item><title>该文章提出了一种优化后的轻量级卷积神经网络结构（ENet-21），专门用于车道检测，通过将车道检测问题处理为一个二值分割任务并使用Affinity Fields处理可变车道数和车道变化情况，实现了对现代车辆驾驶辅助系统的有效改进。</title><link>https://arxiv.org/abs/2403.19782</link><description>&lt;p&gt;
ENet-21: An Optimized light CNN Structure for Lane Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19782
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种优化后的轻量级卷积神经网络结构（ENet-21），专门用于车道检测，通过将车道检测问题处理为一个二值分割任务并使用Affinity Fields处理可变车道数和车道变化情况，实现了对现代车辆驾驶辅助系统的有效改进。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19782v2 Announce Type: replace  Abstract: Lane detection for autonomous vehicles is an important concept, yet it is a challenging issue of driver assistance systems in modern vehicles. The emergence of deep learning leads to significant progress in self-driving cars. Conventional deep learning-based methods handle lane detection problems as a binary segmentation task and determine whether a pixel belongs to a line. These methods rely on the assumption of a fixed number of lanes, which does not always work. This study aims to develop an optimal structure for the lane detection problem, offering a promising solution for driver assistance features in modern vehicles by utilizing a machine learning method consisting of binary segmentation and Affinity Fields that can manage varying numbers of lanes and lane change scenarios. In this approach, the Convolutional Neural Network (CNN), is selected as a feature extractor, and the final output is obtained through clustering of the sem
&lt;/p&gt;</description></item><item><title>该文章介绍了基于传统DenseNets的改进方法，强调了与ResNet和Vision Transformers相比，DenseNets在理论模型和实际效果上的潜力，并在多个数据集上获得了优于这些主流架构的性能。</title><link>https://arxiv.org/abs/2403.19588</link><description>&lt;p&gt;
DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19588
&lt;/p&gt;
&lt;p&gt;
该文章介绍了基于传统DenseNets的改进方法，强调了与ResNet和Vision Transformers相比，DenseNets在理论模型和实际效果上的潜力，并在多个数据集上获得了优于这些主流架构的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19588v2 Announce Type: replace  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EgoNav的方法，通过融合身体佩戴的摄像头和传感器数据，实现了在人造环境中对人类行走轨迹的预测，并考虑到了周围静态环境的影响。通过使用一种扩散模型，该技术能够生成一系列可能的未来行走轨迹分布，并且能够有效地将用户的视觉环境记忆纳入考量。此外，文章还介绍了用于训练和验证算法的数据集，为未来的相关研究提供了支持。</title><link>https://arxiv.org/abs/2403.19026</link><description>&lt;p&gt;
EgoNav: Egocentric Scene-aware Human Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19026
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EgoNav的方法，通过融合身体佩戴的摄像头和传感器数据，实现了在人造环境中对人类行走轨迹的预测，并考虑到了周围静态环境的影响。通过使用一种扩散模型，该技术能够生成一系列可能的未来行走轨迹分布，并且能够有效地将用户的视觉环境记忆纳入考量。此外，文章还介绍了用于训练和验证算法的数据集，为未来的相关研究提供了支持。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19026v3 Announce Type: replace  Abstract: Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to constantly adapt to the surrounding scene based on egocentric vision, and predict the ego motion of the wearer. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We then present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. To that end, we introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-g
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为UniTraj的统一框架，用于车辆轨迹预测，并在分布式数据格式、地图分辨率和语义标注类型等方面进行了统一，为该领域的研究提供了新的机会。通过在多个数据集上的广泛实验，文章发现模型在转移到其他数据集时性能会显著下降，但通过增加数据量和多样性可以显著提高性能，并在nuScenes数据集上达到了最新技术水平。文章还提供了关于数据多样性对模型性能影响的见解。</title><link>https://arxiv.org/abs/2403.15098</link><description>&lt;p&gt;
UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15098
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为UniTraj的统一框架，用于车辆轨迹预测，并在分布式数据格式、地图分辨率和语义标注类型等方面进行了统一，为该领域的研究提供了新的机会。通过在多个数据集上的广泛实验，文章发现模型在转移到其他数据集时性能会显著下降，但通过增加数据量和多样性可以显著提高性能，并在nuScenes数据集上达到了最新技术水平。文章还提供了关于数据多样性对模型性能影响的见解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15098v3 Announce Type: replace  Abstract: Vehicle trajectory prediction has increasingly relied on data-driven solutions, but their ability to scale to different data domains and the impact of larger dataset sizes on their generalization remain under-explored. While these questions can be studied by employing multiple datasets, it is challenging due to several discrepancies, e.g., in data formats, map resolution, and semantic annotation types. To address these challenges, we introduce UniTraj, a comprehensive framework that unifies various datasets, models, and evaluation criteria, presenting new opportunities for the vehicle trajectory prediction field. In particular, using UniTraj, we conduct extensive experiments and find that model performance significantly drops when transferred to other datasets. However, enlarging data size and diversity can substantially improve performance, leading to a new state-of-the-art result for the nuScenes dataset. We provide insights into d
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的基于视角轨迹相邻图像三元组数据的无监督学习方法，用于学习可以提升多种视觉任务（包括语义分类和姿态估计）性能的视觉特征。借助这种数据和损失函数，研究成果强调了将视角信息整合到视觉特征学习中的重要性，并可能促进在无监督学习框架下视角感知能力的进一步提升。</title><link>https://arxiv.org/abs/2403.14973</link><description>&lt;p&gt;
Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14973
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的基于视角轨迹相邻图像三元组数据的无监督学习方法，用于学习可以提升多种视觉任务（包括语义分类和姿态估计）性能的视觉特征。借助这种数据和损失函数，研究成果强调了将视角信息整合到视觉特征学习中的重要性，并可能促进在无监督学习框架下视角感知能力的进一步提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14973v2 Announce Type: replace  Abstract: Learning visual features from unlabeled images has proven successful for semantic categorization, often by mapping different $views$ of the same object to the same feature to achieve recognition invariance. However, visual recognition involves not only identifying $what$ an object is but also understanding $how$ it is presented. For example, seeing a car from the side versus head-on is crucial for deciding whether to stay put or jump out of the way. While unsupervised feature learning for downstream viewpoint reasoning is important, it remains under-explored, partly due to the lack of a standardized evaluation method and benchmarks.   We introduce a new dataset of adjacent image triplets obtained from a viewpoint trajectory, without any semantic or pose labels. We benchmark both semantic classification and pose estimation accuracies on the same visual feature. Additionally, we propose a viewpoint trajectory regularization loss for le
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于线性混合皮肤（LBS）的全新方法，用于通过声音输入驱动具有仿生面部表情的机械机器人。这种方法不仅优化了机器人的外观设计，而且使得表情动作的生成更加准确和同步。通过这种方式，实现了实时、高质量的面部表情，对于推动机器人与人类的自然互动具有重要意义。</title><link>https://arxiv.org/abs/2403.12670</link><description>&lt;p&gt;
Driving Animatronic Robot Facial Expression From Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12670
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于线性混合皮肤（LBS）的全新方法，用于通过声音输入驱动具有仿生面部表情的机械机器人。这种方法不仅优化了机器人的外观设计，而且使得表情动作的生成更加准确和同步。通过这种方式，实现了实时、高质量的面部表情，对于推动机器人与人类的自然互动具有重要意义。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12670v3 Announce Type: replace  Abstract: Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' abili
&lt;/p&gt;</description></item><item><title>该文章构建了首个大规模文本到视频质量评估数据库T2VQA-DB，包含10,000个由9种不同文本到视频模型生成的视频，并对每个视频进行了主观评测，在此基础上提出了一个基于自监督学习的文本到视频质量评估模型。</title><link>https://arxiv.org/abs/2403.11956</link><description>&lt;p&gt;
Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11956
&lt;/p&gt;
&lt;p&gt;
该文章构建了首个大规模文本到视频质量评估数据库T2VQA-DB，包含10,000个由9种不同文本到视频模型生成的视频，并对每个视频进行了主观评测，在此基础上提出了一个基于自监督学习的文本到视频质量评估模型。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11956v5 Announce Type: replace  Abstract: With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video's corresponding mean opinion score. Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language 
&lt;/p&gt;</description></item><item><title>该文章通过提出Meta-Prompting for Visual Recognition (MPVR)方法，实现了自动化生成针对特定任务的零射识别任务视觉提示。这种方法只需输入任务的简短描述和类标签列表，就可以自动创建出覆盖广泛视觉概念和任务相关风格的多样化的提示集，从而显著提高了VLM在零射识别任务中的表现。该创新通过简化人工干预，成功将人从提示生成过程中解放出来，实现了提示自动化的目标。此外，这种方法使用大型语言模型（LLM）产生的特定类别提示，这种方法虽然可靠，但仍然存在局限性，即手动编写提示不能覆盖所有相关的视觉概念和任务特有风格。通过引入Meta-Prompting，文章作者成功自动化了生成任务特异性视觉提示的过程，这是当前技术中的重大突破。</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
该文章通过提出Meta-Prompting for Visual Recognition (MPVR)方法，实现了自动化生成针对特定任务的零射识别任务视觉提示。这种方法只需输入任务的简短描述和类标签列表，就可以自动创建出覆盖广泛视觉概念和任务相关风格的多样化的提示集，从而显著提高了VLM在零射识别任务中的表现。该创新通过简化人工干预，成功将人从提示生成过程中解放出来，实现了提示自动化的目标。此外，这种方法使用大型语言模型（LLM）产生的特定类别提示，这种方法虽然可靠，但仍然存在局限性，即手动编写提示不能覆盖所有相关的视觉概念和任务特有风格。通过引入Meta-Prompting，文章作者成功自动化了生成任务特异性视觉提示的过程，这是当前技术中的重大突破。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v3 Announce Type: replace  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of c
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为S^2Former-OR的单阶段双模态 transformer框架，用于在手术室场景图中进行综合自洽的学习。该模型通过一种视图同步融合方案促进了多视角视觉信息的交互，并通过几何视觉协同操作将2D语义特征与3D点云数据结合，实现了在手术室中以端到端的方式进行多模态数据融合和场景图生成。</title><link>https://arxiv.org/abs/2402.14461</link><description>&lt;p&gt;
S^2Former-OR: Single-Stage Bi-Modal Transformer for Scene Graph Generation in OR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14461
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为S^2Former-OR的单阶段双模态 transformer框架，用于在手术室场景图中进行综合自洽的学习。该模型通过一种视图同步融合方案促进了多视角视觉信息的交互，并通过几何视觉协同操作将2D语义特征与3D点云数据结合，实现了在手术室中以端到端的方式进行多模态数据融合和场景图生成。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14461v2 Announce Type: replace  Abstract: Scene graph generation (SGG) of surgical procedures is crucial in enhancing holistically cognitive intelligence in the operating room (OR). However, previous works have primarily relied on multi-stage learning, where the generated semantic scene graphs depend on intermediate processes with pose estimation and object detection. This pipeline may potentially compromise the flexibility of learning multimodal representations, consequently constraining the overall effectiveness. In this study, we introduce a novel single-stage bi-modal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an end-to-end manner. Concretely, our model embraces a View-Sync Transfusion scheme to encourage multi-view visual information interaction. Concurrently, a Geometry-Visual Cohesion operation is designed to integrate the synergic 2D semantic features into 3D point
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为M-CAT（Multi-modal Contrastive Anticipative Transformer）的模型，该模型是一种视频变换器架构，它结合了多模态特征和动作及物体描述的文本信息。文章通过两个阶段的训练方法，首先训练模型将视频片段与未来动作的描述进行对齐，然后对其进行微调，以提高动作预测的准确性。此外，文章还探讨了文本描述在限定潜在动作选择中的作用，证明了文本描述在动作预测中的有效性。</title><link>https://arxiv.org/abs/2401.12972</link><description>&lt;p&gt;
On the Efficacy of Text-Based Input Modalities for Action Anticipation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12972
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为M-CAT（Multi-modal Contrastive Anticipative Transformer）的模型，该模型是一种视频变换器架构，它结合了多模态特征和动作及物体描述的文本信息。文章通过两个阶段的训练方法，首先训练模型将视频片段与未来动作的描述进行对齐，然后对其进行微调，以提高动作预测的准确性。此外，文章还探讨了文本描述在限定潜在动作选择中的作用，证明了文本描述在动作预测中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12972v2 Announce Type: replace  Abstract: Anticipating future actions is a highly challenging task due to the diversity and scale of potential future actions; yet, information from different modalities help narrow down plausible action choices. Each modality can provide diverse and often complementary context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text descriptions of actions and objects can also lead to more accurate action anticipation by providing additional contextual cues, e.g., about the environment and its contents. We propose a Multi-modal Contrastive Anticipative Transformer (M-CAT), a video transformer architecture that jointly learns from multi-modal features and text descriptions of actions and objects. We train our model in two stages, where the model first learns to align video clips with descriptions of future actions, and is subsequently fine-tuned 
&lt;/p&gt;</description></item><item><title>该文章提供一种名为BiasPainter的框架，能够准确、自动且全面地触发图像生成模型中的社会偏见，弥补了以往研究在评估图像生成模型偏见时的局限性，包括准确度不足、过度依赖人工劳动及分析不全面的问题。</title><link>https://arxiv.org/abs/2401.00763</link><description>&lt;p&gt;
New Job, New Gender? Measuring the Social Bias in Image Generation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00763
&lt;/p&gt;
&lt;p&gt;
该文章提供一种名为BiasPainter的框架，能够准确、自动且全面地触发图像生成模型中的社会偏见，弥补了以往研究在评估图像生成模型偏见时的局限性，包括准确度不足、过度依赖人工劳动及分析不全面的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00763v2 Announce Type: replace-cross  Abstract: Image generation models can generate or edit images from a given text. Recent advancements in image generation technology, exemplified by DALL-E and Midjourney, have been groundbreaking. These advanced models, despite their impressive capabilities, are often trained on massive Internet datasets, making them susceptible to generating content that perpetuates social stereotypes and biases, which can lead to severe consequences. Prior research on assessing bias within image generation models suffers from several shortcomings, including limited accuracy, reliance on extensive human labor, and lack of comprehensive analysis. In this paper, we propose BiasPainter, a novel evaluation framework that can accurately, automatically and comprehensively trigger social bias in image generation models. BiasPainter uses a diverse range of seed images of individuals and prompts the image generation models to edit these images using gender, race
&lt;/p&gt;</description></item><item><title>该文章提出了SAFE-SIM，一种利用扩散模型生成的具有可控对抗行为的封闭式安全关键性仿真框架，能够模拟接近真实世界条件下的长尾安全关键性场景，并允许对规划器进行更全面和互动的评估。</title><link>https://arxiv.org/abs/2401.00391</link><description>&lt;p&gt;
SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with Diffusion-Controllable Adversaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00391
&lt;/p&gt;
&lt;p&gt;
该文章提出了SAFE-SIM，一种利用扩散模型生成的具有可控对抗行为的封闭式安全关键性仿真框架，能够模拟接近真实世界条件下的长尾安全关键性场景，并允许对规划器进行更全面和互动的评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00391v3 Announce Type: replace  Abstract: Evaluating the performance of autonomous vehicle planning algorithms necessitates simulating long-tail safety-critical traffic scenarios. However, traditional methods for generating such scenarios often fall short in terms of controllability and realism; they also neglect the dynamics of agent interactions. To address these limitations, we introduce SAFE-SIM, a novel diffusion-based controllable closed-loop safety-critical simulation framework. Our approach yields two distinct advantages: 1) generating realistic long-tail safety-critical scenarios that closely reflect real-world conditions, and 2) providing controllable adversarial behavior for more comprehensive and interactive evaluations. We develop a novel approach to simulate safety-critical scenarios through an adversarial term in the denoising process of diffusion models, which allows an adversarial agent to challenge a planner with plausible maneuvers while all agents in the 
&lt;/p&gt;</description></item><item><title>该文章研究了深度学习在加速磁共振成像中的鲁棒性，发现如果训练数据与实际应用数据差别较大，神经网络的表现会有所下降。研究中，使用了来自不同MRI扫描器和不同解剖结构的多样化数据集训练模型，发现这种训练方式能够提高模型对于未知数据的适应能力。文中还提到，使用多样化数据集不仅能够提升模型对于未知数据的鲁棒性，而且不会降低在已知数据上模型的性能。</title><link>https://arxiv.org/abs/2312.10271</link><description>&lt;p&gt;
Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10271
&lt;/p&gt;
&lt;p&gt;
该文章研究了深度学习在加速磁共振成像中的鲁棒性，发现如果训练数据与实际应用数据差别较大，神经网络的表现会有所下降。研究中，使用了来自不同MRI扫描器和不同解剖结构的多样化数据集训练模型，发现这种训练方式能够提高模型对于未知数据的适应能力。文中还提到，使用多样化数据集不仅能够提升模型对于未知数据的鲁棒性，而且不会降低在已知数据上模型的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10271v2 Announce Type: replace-cross  Abstract: Deep learning based methods for image reconstruction are state-of-the-art for a variety of imaging tasks. However, neural networks often perform worse if the training data differs significantly from the data they are applied to. For example, a model trained for accelerated magnetic resonance imaging (MRI) on one scanner performs worse on another scanner. In this work, we investigate the impact of the training data on a model's performance and robustness for accelerated MRI. We find that models trained on the combination of various data distributions, such as those obtained from different MRI scanners and anatomies, exhibit robustness equal or superior to models trained on the best single distribution for a specific target distribution. Thus training on such diverse data tends to improve robustness. Furthermore, training on such a diverse dataset does not compromise in-distribution performance, i.e., a model trained on diverse d
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于Supervised Domain Adaptation的方法，用于改善从斜角航空图像中提取建筑物的性能，解决训练数据中标签与斜角源图像之间的不匹配问题。通过使用高绩效的轻量级编码器如EfficientNet、ResNeSt和MobileViT训练Encoder-Decoder网络，该方法能够显著提高从斜角航空图像中提取建筑物的准确性和性能，相较于传统的方法如知识蒸馏（KD）和深度互相学习（DML），该方法在多个新的数据集上取得了更好的结果。</title><link>https://arxiv.org/abs/2311.03867</link><description>&lt;p&gt;
Supervised domain adaptation for building extraction from off-nadir aerial images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03867
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于Supervised Domain Adaptation的方法，用于改善从斜角航空图像中提取建筑物的性能，解决训练数据中标签与斜角源图像之间的不匹配问题。通过使用高绩效的轻量级编码器如EfficientNet、ResNeSt和MobileViT训练Encoder-Decoder网络，该方法能够显著提高从斜角航空图像中提取建筑物的准确性和性能，相较于传统的方法如知识蒸馏（KD）和深度互相学习（DML），该方法在多个新的数据集上取得了更好的结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03867v2 Announce Type: replace  Abstract: Building extraction $-$ needed for inventory management and planning of urban environment $-$ is affected by the misalignment between labels and off-nadir source imagery in training data. Teacher-Student learning of noise-tolerant convolutional neural networks (CNNs) is the existing solution, but the Student networks typically have lower accuracy and cannot surpass the Teacher's performance. This paper proposes a supervised domain adaptation (SDA) of encoder-decoder networks (EDNs) between noisy and clean datasets to tackle the problem. EDNs are configured with high-performing lightweight encoders such as EfficientNet, ResNeSt, and MobileViT. The proposed method is compared against the existing Teacher-Student learning methods like knowledge distillation (KD) and deep mutual learning (DML) with three newly developed datasets. The methods are evaluated for different urban buildings (low-rise, mid-rise, high-rise, and skyscrapers), whe
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的训练框架，能够训练出在多样化的3D和2D数据集上具有极佳泛化能力的单目3D物体检测模型，极大地提升了在仅有2D标注的未知场景中的检测性能。</title><link>https://arxiv.org/abs/2310.00920</link><description>&lt;p&gt;
Every Dataset Counts: Scaling up Monocular 3D Object Detection with Joint Datasets Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00920
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的训练框架，能够训练出在多样化的3D和2D数据集上具有极佳泛化能力的单目3D物体检测模型，极大地提升了在仅有2D标注的未知场景中的检测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00920v3 Announce Type: replace  Abstract: Monocular 3D object detection plays a crucial role in autonomous driving. However, existing monocular 3D detection algorithms depend on 3D labels derived from LiDAR measurements, which are costly to acquire for new datasets and challenging to deploy in novel environments. Specifically, this study investigates the pipeline for training a monocular 3D object detection model on a diverse collection of 3D and 2D datasets. The proposed framework comprises three components: (1) a robust monocular 3D model capable of functioning across various camera settings, (2) a selective-training strategy to accommodate datasets with differing class annotations, and (3) a pseudo 3D training approach using 2D labels to enhance detection performance in scenes containing only 2D labels. With this framework, we could train models on a joint set of various open 3D/2D datasets to obtain models with significantly stronger generalization capability and enhance
&lt;/p&gt;</description></item><item><title>该文章提出了基于神经辐射场的手术内窥镜动态视频重建方法，能够学习具有可变形场景的3D隐式表示，并对动态场景进行准确的相机姿态估计，解决了现有技术中依赖静态场景进行准确重建的问题。</title><link>https://arxiv.org/abs/2309.15329</link><description>&lt;p&gt;
BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction using Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.15329
&lt;/p&gt;
&lt;p&gt;
该文章提出了基于神经辐射场的手术内窥镜动态视频重建方法，能够学习具有可变形场景的3D隐式表示，并对动态场景进行准确的相机姿态估计，解决了现有技术中依赖静态场景进行准确重建的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.15329v2 Announce Type: replace  Abstract: Reconstruction of deformable scenes from endoscopic videos is important for many applications such as intraoperative navigation, surgical visual perception, and robotic surgery. It is a foundational requirement for realizing autonomous robotic interventions for minimally invasive surgery. However, previous approaches in this domain have been limited by their modular nature and are confined to specific camera and scene settings. Our work adopts the Neural Radiance Fields (NeRF) approach to learning 3D implicit representations of scenes that are both dynamic and deformable over time, and furthermore with unknown camera poses. We demonstrate this approach on endoscopic surgical scenes from robotic surgery. This work removes the constraints of known camera poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic scene reconstruction technique, which relies on the static part of the scene for accurate reconstruction.
&lt;/p&gt;</description></item><item><title>该文章提出了一种针对异常检测的全面增强学习框架，通过考虑不同类别的异常标准差异，选代地应用适当的增强策略，并采用分段训练策略缓解过度拟合问题，同时在无干扰的图像重构过程中保持性能，在MTVae异常检测数据集上的评估结果表明该方法表现出优越性能。</title><link>https://arxiv.org/abs/2308.15068</link><description>&lt;p&gt;
A Comprehensive Augmentation Framework for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.15068
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种针对异常检测的全面增强学习框架，通过考虑不同类别的异常标准差异，选代地应用适当的增强策略，并采用分段训练策略缓解过度拟合问题，同时在无干扰的图像重构过程中保持性能，在MTVae异常检测数据集上的评估结果表明该方法表现出优越性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.15068v4 Announce Type: replace-cross  Abstract: Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previou
&lt;/p&gt;</description></item><item><title>该文章提出LoRAPrune框架，实现了对大型语言模型在下游任务上的低秩参数高效精调过程中的结构化压缩，通过利用LoRA指导的评残准则，该方法在无需原始预训练权重梯度的情况下，实现了高精度压缩和高内存效率。</title><link>https://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
该文章提出LoRAPrune框架，实现了对大型语言模型在下游任务上的低秩参数高效精调过程中的结构化压缩，通过利用LoRA指导的评残准则，该方法在无需原始预训练权重梯度的情况下，实现了高精度压缩和高内存效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18403v5 Announce Type: replace-cross  Abstract: Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather tha
&lt;/p&gt;</description></item><item><title>该文章通过重新形式化蒸馏过程的动力学，提供了对真实数据集内在冗余的理论和实证洞察。文章提出了一种基于经验损失值的数据静态裁剪标准，并进一步根据数据对蒸馏贡献的因果效应，找到最能影响蒸馏过程的数据样本，从而有效利用训练数据集，超越了现有技术。</title><link>https://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
该文章通过重新形式化蒸馏过程的动力学，提供了对真实数据集内在冗余的理论和实证洞察。文章提出了一种基于经验损失值的数据静态裁剪标准，并进一步根据数据对蒸馏贡献的因果效应，找到最能影响蒸馏过程的数据样本，从而有效利用训练数据集，超越了现有技术。
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18381v4 Announce Type: replace-cross  Abstract: Data-efficient learning has garnered significant attention, especially given the current trend of large multi-modal models. Recently, dataset distillation has become an effective approach by synthesizing data samples that are essential for network training. However, it remains to be explored which samples are essential for the dataset distillation process itself. In this work, we study the data efficiency and selection for the dataset distillation task. By re-formulating the dynamics of distillation, we provide insight into the inherent redundancy in the real dataset, both theoretically and empirically. We propose to use the empirical loss value as a static data pruning criterion. To further compensate for the variation of the data value in training, we find the most contributing samples based on their causal effects on the distillation. The proposed selection strategy can efficiently exploit the training dataset, outperform th
&lt;/p&gt;</description></item><item><title>该文章提出了SpaCoNet，一个通过语义分割引导的空间关系和对象共现同时建模的模型，旨在解决室内场景识别中的多样性和共存对象的挑战，通过构建Semantic Spatial Relation Module（SSRM）来建模场景的空间特征，并利用语义分割帮助识别和处理场景中的空间布局和共存对象，从而提高场景识别的准确性。</title><link>https://arxiv.org/abs/2305.12661</link><description>&lt;p&gt;
Semantic-guided modeling of spatial relation and object co-occurrence for indoor scene recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12661
&lt;/p&gt;
&lt;p&gt;
该文章提出了SpaCoNet，一个通过语义分割引导的空间关系和对象共现同时建模的模型，旨在解决室内场景识别中的多样性和共存对象的挑战，通过构建Semantic Spatial Relation Module（SSRM）来建模场景的空间特征，并利用语义分割帮助识别和处理场景中的空间布局和共存对象，从而提高场景识别的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12661v4 Announce Type: replace  Abstract: Exploring the semantic context in scene images is essential for indoor scene recognition. However, due to the diverse intra-class spatial layouts and the coexisting inter-class objects, modeling contextual relationships to adapt various image characteristics is a great challenge. Existing contextual modeling methods for scene recognition exhibit two limitations: 1) They typically model only one type of spatial relationship (order or metric) among objects within scenes, with limited exploration of diverse spatial layouts. 2) They often overlook the differences in coexisting objects across different scenes, suppressing scene recognition performance. To overcome these limitations, we propose SpaCoNet, which simultaneously models Spatial relation and Co-occurrence of objects guided by semantic segmentation. Firstly, the Semantic Spatial Relation Module (SSRM) is constructed to model scene spatial features. With the help of semantic segme
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于空间频率Krawtchouk分解的鉴别性检测器，以全面捕捉深层神经网络中的对抗性扰动特征，并能有效抵御具有防御洞察的攻击。</title><link>https://arxiv.org/abs/2305.10856</link><description>&lt;p&gt;
Spatial-Frequency Discriminability for Revealing Adversarial Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.10856
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于空间频率Krawtchouk分解的鉴别性检测器，以全面捕捉深层神经网络中的对抗性扰动特征，并能有效抵御具有防御洞察的攻击。
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.10856v3 Announce Type: replace  Abstract: The vulnerability of deep neural networks to adversarial perturbations has been widely perceived in the computer vision community. From a security perspective, it poses a critical risk for modern vision systems, e.g., the popular Deep Learning as a Service (DLaaS) frameworks. For protecting deep models while not modifying them, current algorithms typically detect adversarial patterns through discriminative decomposition for natural and adversarial data. However, these decompositions are either biased towards frequency resolution or spatial resolution, thus failing to capture adversarial patterns comprehensively. Also, when the detector relies on few fixed features, it is practical for an adversary to fool the model while evading the detector (i.e., defense-aware attack). Motivated by such facts, we propose a discriminative detector relying on a spatial-frequency Krawtchouk decomposition. It expands the above works from two aspects: 1
&lt;/p&gt;</description></item><item><title>该文章利用机器学习方法分析了政治领导人面部表情的差异，并发现不同程度使用民粹主义修辞的领导人在负面情绪平均得分上存在统计显著差异。</title><link>https://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
该文章利用机器学习方法分析了政治领导人面部表情的差异，并发现不同程度使用民粹主义修辞的领导人在负面情绪平均得分上存在统计显著差异。
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.09914v4 Announce Type: replace-cross  Abstract: Populist rhetoric employed on online media is characterized as deeply impassioned and often imbued with strong emotions. The aim of this paper is to empirically investigate the differences in affective nonverbal communication of political leaders. We use a deep-learning approach to process a sample of 220 YouTube videos of political leaders from 15 different countries, analyze their facial expressions of emotion and then examine differences in average emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the YouTube video. Based on a sample of manually coded images, we find that this deep-learning approach has 53-60\% agreement with human labels. We observe statistically significant differences in the average score of negative emotions between groups of leaders with varying degrees of populist rhetoric.
&lt;/p&gt;</description></item><item><title>该文章提出的注意力相似结构再参数化技术，通过允许不同网络架构之间变换的等效参数变换，能够在不增加训练成本的情况下，为工业和实际应用中性能提高提供可能。</title><link>https://arxiv.org/abs/2304.06345</link><description>&lt;p&gt;
ASR: Attention-alike Structural Re-parameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.06345
&lt;/p&gt;
&lt;p&gt;
该文章提出的注意力相似结构再参数化技术，通过允许不同网络架构之间变换的等效参数变换，能够在不增加训练成本的情况下，为工业和实际应用中性能提高提供可能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.06345v3 Announce Type: replace  Abstract: The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, and multi-branch convolution. However, the widely used attention modules which drastically slow inference speed cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RCA（Region Conditioned Adaptation）的方法，该方法对视觉观察进行推理以做出可能性的解释。RCA是一种简单的、有效的区域条件适应方法，它为冻结的CLIP模型提供了解码器，使得模型能够根据局部视觉提示进行推理。通过在精细和粗粒度级别上分别编码“局部提示”和“全局上下文”，该模型训练了一个新的注意力适配器，该适配器直接通过训练可调用的查询和键投影来引导注意力图的焦点。最后，通过修改的对比损失，该模型同时将视觉特征回归到描述符和可信解释的特征上，从而保持了CLIP的感知和推理能力。实验在Sherlock视觉关联推理任务上验证了该方法的有效性。</title><link>https://arxiv.org/abs/2303.10428</link><description>&lt;p&gt;
RCA: Region Conditioned Adaptation for Visual Abductive Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.10428
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RCA（Region Conditioned Adaptation）的方法，该方法对视觉观察进行推理以做出可能性的解释。RCA是一种简单的、有效的区域条件适应方法，它为冻结的CLIP模型提供了解码器，使得模型能够根据局部视觉提示进行推理。通过在精细和粗粒度级别上分别编码“局部提示”和“全局上下文”，该模型训练了一个新的注意力适配器，该适配器直接通过训练可调用的查询和键投影来引导注意力图的焦点。最后，通过修改的对比损失，该模型同时将视觉特征回归到描述符和可信解释的特征上，从而保持了CLIP的感知和推理能力。实验在Sherlock视觉关联推理任务上验证了该方法的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.10428v5 Announce Type: replace  Abstract: Visual abductive reasoning aims to make likely explanations for visual observations. We propose a simple yet effective Region Conditioned Adaptation, a hybrid parameter-efficient fine-tuning method that equips the frozen CLIP with the ability to infer explanations from local visual cues. We encode ``local hints'' and ``global contexts'' into visual prompts of the CLIP model separately at fine and coarse-grained levels. Adapters are used for fine-tuning CLIP models for downstream tasks and we design a new attention adapter, that directly steers the focus of the attention map with trainable query and key projections of a frozen CLIP model. Finally, we train our new model with a modified contrastive loss to regress the visual feature simultaneously toward features of literal description and plausible explanations. The loss enables CLIP to maintain both perception and reasoning abilities. Experiments on the Sherlock visual abductive reas
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于课程学习方法的DeepLens设计方法，能够在没有任何人工干预的情况下，从随机初始化的表面学习复杂光学元件的补偿性设计和优化，从而无需好的初始设计。通过这种方式，实现了在全自动设计下，为智能手机型风格的光学元件设计出能够大幅提高透视感和图像质量的折射光学系统。</title><link>https://arxiv.org/abs/2302.01089</link><description>&lt;p&gt;
Curriculum Learning for ab initio Deep Learned Refractive Optics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01089
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于课程学习方法的DeepLens设计方法，能够在没有任何人工干预的情况下，从随机初始化的表面学习复杂光学元件的补偿性设计和优化，从而无需好的初始设计。通过这种方式，实现了在全自动设计下，为智能手机型风格的光学元件设计出能够大幅提高透视感和图像质量的折射光学系统。
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01089v4 Announce Type: replace  Abstract: Deep optical optimization has recently emerged as a new paradigm for designing computational imaging systems using only the output image as the objective. However, it has been limited to either simple optical systems consisting of a single element such as a diffractive optical element (DOE) or metalens, or the fine-tuning of compound lenses from good initial designs. Here we present a DeepLens design method based on curriculum learning, which is able to learn optical designs of compound lenses ab initio from randomly initialized surfaces without human intervention, therefore overcoming the need for a good initial design. We demonstrate the effectiveness of our approach by fully automatically designing both classical imaging lenses and a large field-of-view extended depth-of-field computational lens in a cellphone-style form factor, with highly aspheric surfaces and a short back focal length.
&lt;/p&gt;</description></item><item><title>该文章提出了一个用于保护图像生成式对抗网络（GANs）知识产权的全新指纹识别方案，有效突破了以往仅适用于分类模型、难以适应GANs的隐形性和鲁棒性瓶颈。通过构建一个由目标GAN和分类器组成的复合深度学习模型，并从中生成指纹样本，将其嵌入到分类器中以实现所有权验证。这种方案为现代图像翻译GANs的实际保护提供了一些具体的策略和方法。</title><link>https://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
Fingerprinting Image-to-Image Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个用于保护图像生成式对抗网络（GANs）知识产权的全新指纹识别方案，有效突破了以往仅适用于分类模型、难以适应GANs的隐形性和鲁棒性瓶颈。通过构建一个由目标GAN和分类器组成的复合深度学习模型，并从中生成指纹样本，将其嵌入到分类器中以实现所有权验证。这种方案为现代图像翻译GANs的实际保护提供了一些具体的策略和方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.11760v5 Announce Type: replace-cross  Abstract: Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. This paper presents a novel fingerprinting scheme for the Intellectual Property (IP) protection of image-to-image GANs based on a trusted third party. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern image-to-image translation GANs. Theoretical analys
&lt;/p&gt;</description></item></channel></rss>