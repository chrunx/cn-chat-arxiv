<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.CV.xml</link><description>This is arxiv RSS feed for cs.CV</description><item><title>该文章的重要创新贡献在于提出LLaVA-OneVision，这是一种开放的大型多模态模型，可以在单图像、多图像和视频场景中同时推动性能边界，通过单一模型展现强大的跨模态和跨场景任务迁移能力，特别是将图像理解能力成功迁移到视频场景中。</title><link>https://arxiv.org/abs/2408.03326</link><description>&lt;p&gt;
LLaVA-OneVision: Easy Visual Task Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03326
&lt;/p&gt;
&lt;p&gt;
该文章的重要创新贡献在于提出LLaVA-OneVision，这是一种开放的大型多模态模型，可以在单图像、多图像和视频场景中同时推动性能边界，通过单一模型展现强大的跨模态和跨场景任务迁移能力，特别是将图像理解能力成功迁移到视频场景中。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03326v1 Announce Type: new  Abstract: We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为Segment Anything Model 2（SAM2）的分割基础模型，其在多个医学图像和视频模态中的表现进行了全面评估，并展示了它如何通过快速微调快速适应医学领域，并且在医学图像和视频的3D分割方面具有实用性。</title><link>https://arxiv.org/abs/2408.03322</link><description>&lt;p&gt;
Segment Anything in Medical Images and Videos: Benchmark and Deployment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03322
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为Segment Anything Model 2（SAM2）的分割基础模型，其在多个医学图像和视频模态中的表现进行了全面评估，并展示了它如何通过快速微调快速适应医学领域，并且在医学图像和视频的3D分割方面具有实用性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03322v1 Announce Type: cross  Abstract: Recent advances in segmentation foundation models have enabled accurate and efficient segmentation across a wide range of natural images and videos, but their utility to medical data remains unclear. In this work, we first present a comprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11 medical image modalities and videos and point out its strengths and weaknesses by comparing it to SAM1 and MedSAM. Then, we develop a transfer learning pipeline and demonstrate SAM2 can be quickly adapted to medical domain by fine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio API for efficient 3D image and video segmentation. The code has been made publicly available at \url{https://github.com/bowang-lab/MedSAM}.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为MDT-A2G的新型Masked Diffusion Transformer模型，用于同步手势生成。该模型采用了一种特殊的Masked Diffusion Transformer架构，能够在不依赖CNNs的情况下，通过模式掩码来加强动作序列中的时间关联学习，从而加速了同步手势生成的学习过程，并通过直接实施于手势序列的退火过程来提高整个任务的质量。</title><link>https://arxiv.org/abs/2408.03312</link><description>&lt;p&gt;
MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03312
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为MDT-A2G的新型Masked Diffusion Transformer模型，用于同步手势生成。该模型采用了一种特殊的Masked Diffusion Transformer架构，能够在不依赖CNNs的情况下，通过模式掩码来加强动作序列中的时间关联学习，从而加速了同步手势生成的学习过程，并通过直接实施于手势序列的退火过程来提高整个任务的质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03312v1 Announce Type: new  Abstract: Recent advancements in the field of Diffusion Transformers have substantially improved the generation of high-quality 2D images, 3D videos, and 3D shapes. However, the effectiveness of the Transformer architecture in the domain of co-speech gesture generation remains relatively unexplored, as prior methodologies have predominantly employed the Convolutional Neural Network (CNNs) or simple a few transformer layers. In an attempt to bridge this research gap, we introduce a novel Masked Diffusion Transformer for co-speech gesture generation, referred to as MDT-A2G, which directly implements the denoising process on gesture sequences. To enhance the contextual reasoning capability of temporally aligned speech-driven gestures, we incorporate a novel Masked Diffusion Transformer. This model employs a mask modeling scheme specifically designed to strengthen temporal relation learning among sequence gestures, thereby expediting the learning proc
&lt;/p&gt;</description></item><item><title>该文章提出了一种深度神经网络，该网络能够利用人类指导来改善现有的图像分割掩码。这种方法显著减少了手动输入的需求，使得手动标注的流程更加高效，同时保持了高质量的标注结果。</title><link>https://arxiv.org/abs/2408.03304</link><description>&lt;p&gt;
Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03304
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种深度神经网络，该网络能够利用人类指导来改善现有的图像分割掩码。这种方法显著减少了手动输入的需求，使得手动标注的流程更加高效，同时保持了高质量的标注结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03304v1 Announce Type: new  Abstract: Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improveme
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为TextIM的框架，该框架利用条件扩散模型和大型语言模型，能够精确地将文本描述中的交互身体部位和交互含义对齐，生成更加精确和自然的交互动作。</title><link>https://arxiv.org/abs/2408.03302</link><description>&lt;p&gt;
TextIM: Part-aware Interactive Motion Synthesis from Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03302
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为TextIM的框架，该框架利用条件扩散模型和大型语言模型，能够精确地将文本描述中的交互身体部位和交互含义对齐，生成更加精确和自然的交互动作。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03302v1 Announce Type: new  Abstract: In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics. Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes. To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions. Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion. Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion. We design a spatial co
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为DopQ-ViT的框架，通过引入一种名为TanQ的分布友好的Tan量化器，改善了对视觉变换器模型的后训练量化，其特别关注接近1的值，从而更准确地保留了后Softmax激活的幂律分布，从而降低了计算成本和延迟，促进了视觉变换器模型的大范围应用。</title><link>https://arxiv.org/abs/2408.03291</link><description>&lt;p&gt;
DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03291
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为DopQ-ViT的框架，通过引入一种名为TanQ的分布友好的Tan量化器，改善了对视觉变换器模型的后训练量化，其特别关注接近1的值，从而更准确地保留了后Softmax激活的幂律分布，从而降低了计算成本和延迟，促进了视觉变换器模型的大范围应用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03291v1 Announce Type: new  Abstract: Vision transformers (ViTs) have garnered significant attention for their performance in vision tasks; however, the high computational cost and significant latency issues have hinder widespread adoption. Post-training quantization (PTQ), a promising method for model compression, still faces accuracy degradation challenges with ViTs. There are two reasons for this: the existing quantization paradigm does not fit the power-law distribution of post-Softmax activations well, and accuracy inevitably decreases after reparameterizing post-LayerNorm activations. We propose a Distribution-Friendly and Outlier-Aware Post-training Quantization method for Vision Transformers, named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more on values near 1, more accurately preserving the power-law distribution of post-Softmax activations, and achieves favora
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为BioSAM 2的增强型基础模型，该模型优化自SAM 2，专为生物医学应用设计，并在单帧图像和多帧视频分割中展示了超越现有方法的性能，甚至与专门模型相匹敌，显示出其在医疗领域的效能和潜力。</title><link>https://arxiv.org/abs/2408.03286</link><description>&lt;p&gt;
Biomedical SAM 2: Segment Anything in Biomedical Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03286
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为BioSAM 2的增强型基础模型，该模型优化自SAM 2，专为生物医学应用设计，并在单帧图像和多帧视频分割中展示了超越现有方法的性能，甚至与专门模型相匹敌，显示出其在医疗领域的效能和潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03286v1 Announce Type: new  Abstract: Medical image segmentation and video object segmentation are essential for diagnosing and analyzing diseases by identifying and measuring biological structures. Recent advances in natural domain have been driven by foundation models like the Segment Anything Model 2 (SAM 2). To explore the performance of SAM 2 in biomedical applications, we designed two evaluation pipelines for single-frame image segmentation and multi-frame video segmentation with varied prompt designs, revealing SAM 2's limitations in medical contexts. Consequently, we developed BioSAM 2, an enhanced foundation model optimized for biomedical data based on SAM 2. Our experiments show that BioSAM 2 not only surpasses the performance of existing state-of-the-art foundation models but also matches or even exceeds specialist models, demonstrating its efficacy and potential in the medical domain.
&lt;/p&gt;</description></item><item><title>该文章提出了ReSyncer框架，这是一个统一的框架，能够高效地将音频和视频中的面部信息同步。通过重构风格生成器的设计，并将其与3D面部动态预测相结合，文章实现了高保真的音频至视频面部同步，并且该框架支持多种有吸引力的特性。</title><link>https://arxiv.org/abs/2408.03284</link><description>&lt;p&gt;
ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03284
&lt;/p&gt;
&lt;p&gt;
该文章提出了ReSyncer框架，这是一个统一的框架，能够高效地将音频和视频中的面部信息同步。通过重构风格生成器的设计，并将其与3D面部动态预测相结合，文章实现了高保真的音频至视频面部同步，并且该框架支持多种有吸引力的特性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03284v1 Announce Type: new  Abstract: Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers. While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts. In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information. The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer. By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training. Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing prope
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为AMES的模型，该模型采用了一种新颖的异步和记忆高效的相似性估计方法，用于解决实例级别图像检索中的内存问题。通过设计一个基于transformer的架构，该模型能够在保持较低内存消耗（每个图像仅使用1KB）的同时，有效地估计图像之间的相似性。此外，AMES模型还具有一个独特的能力，即进行异步相似性估计，这是通过使用不同的本地描述符数量来实现的，从而在保持性能的同时不增加内存消耗。文章还介绍了一种可适应不同应用需求的通用模型，该模型在测试阶段能够灵活调整以适应不同的本地描述符数量。](/backtranslated)</title><link>https://arxiv.org/abs/2408.03282</link><description>&lt;p&gt;
AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03282
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为AMES的模型，该模型采用了一种新颖的异步和记忆高效的相似性估计方法，用于解决实例级别图像检索中的内存问题。通过设计一个基于transformer的架构，该模型能够在保持较低内存消耗（每个图像仅使用1KB）的同时，有效地估计图像之间的相似性。此外，AMES模型还具有一个独特的能力，即进行异步相似性估计，这是通过使用不同的本地描述符数量来实现的，从而在保持性能的同时不增加内存消耗。文章还介绍了一种可适应不同应用需求的通用模型，该模型在测试阶段能够灵活调整以适应不同的本地描述符数量。](/backtranslated)
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03282v1 Announce Type: new  Abstract: This work investigates the problem of instance-level image retrieval re-ranking with the constraint of memory efficiency, ultimately aiming to limit memory usage to 1KB per image. Departing from the prevalent focus on performance enhancements, this work prioritizes the crucial trade-off between performance and memory requirements. The proposed model uses a transformer-based architecture designed to estimate image-to-image similarity by capturing interactions within and across images based on their local descriptors. A distinctive property of the model is the capability for asymmetric similarity estimation. Database images are represented with a smaller number of descriptors compared to query images, enabling performance improvements without increasing memory consumption. To ensure adaptability across different applications, a universal model is introduced that adjusts to a varying number of local descriptors during the testing phase. Res
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为LAC-Net的线性融合注意力指导卷积网络，旨在通过融合来自RGB图像的语义特征和来自深度图像的几何信息，提高机器人对遮挡场景中目标对象的准确抓取能力。</title><link>https://arxiv.org/abs/2408.03238</link><description>&lt;p&gt;
LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03238
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为LAC-Net的线性融合注意力指导卷积网络，旨在通过融合来自RGB图像的语义特征和来自深度图像的几何信息，提高机器人对遮挡场景中目标对象的准确抓取能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03238v1 Announce Type: new  Abstract: This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-m
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于MoCo v2框架的对比学习方法（CLIC），用于无需人工标注即可学习图像复杂性的特征表示。该方法发现图像不同局部区域存在复杂性差异，并提出了随机裁剪和混合策略（RCM）来产生多尺度局部crop的对比样本，有效扩展了训练集并增强了数据多样性。实验结果证明了CLIC相比于其他无监督和监督方法的有效性。</title><link>https://arxiv.org/abs/2408.03230</link><description>&lt;p&gt;
Contrastive Learning for Image Complexity Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03230
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于MoCo v2框架的对比学习方法（CLIC），用于无需人工标注即可学习图像复杂性的特征表示。该方法发现图像不同局部区域存在复杂性差异，并提出了随机裁剪和混合策略（RCM）来产生多尺度局部crop的对比样本，有效扩展了训练集并增强了数据多样性。实验结果证明了CLIC相比于其他无监督和监督方法的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03230v1 Announce Type: new  Abstract: Quantifying and evaluating image complexity can be instrumental in enhancing the performance of various computer vision tasks. Supervised learning can effectively learn image complexity features from well-annotated datasets. However, creating such datasets requires expensive manual annotation costs. The models may learn human subjective biases from it. In this work, we introduce the MoCo v2 framework. We utilize contrastive learning to represent image complexity, named CLIC (Contrastive Learning for Image Complexity). We find that there are complexity differences between different local regions of an image, and propose Random Crop and Mix (RCM), which can produce positive samples consisting of multi-scale local crops. RCM can also expand the train set and increase data diversity without introducing additional data. We conduct extensive experiments with CLIC, comparing it with both unsupervised and supervised methods. The results demonstr
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于事件相机的6自由度物体姿态估计和跟踪方法，该方法使用线信息来估计和跟踪平面或非平面物体的姿态，能够处理高动态范围场景和高速度运动，为物体姿态估计带来了新的解决方案。</title><link>https://arxiv.org/abs/2408.03225</link><description>&lt;p&gt;
Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03225
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于事件相机的6自由度物体姿态估计和跟踪方法，该方法使用线信息来估计和跟踪平面或非平面物体的姿态，能够处理高动态范围场景和高速度运动，为物体姿态估计带来了新的解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03225v1 Announce Type: new  Abstract: Pose estimation and tracking of objects is a fundamental application in 3D vision. Event cameras possess remarkable attributes such as high dynamic range, low latency, and resilience against motion blur, which enables them to address challenging high dynamic range scenes or high-speed motion. These features make event cameras an ideal complement over standard cameras for object pose estimation. In this work, we propose a line-based robust pose estimation and tracking method for planar or non-planar objects using an event camera. Firstly, we extract object lines directly from events, then provide an initial pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line correspondences are not known in advance. Subsequently, we utilize event-line matching to establish correspondences between 2D events and 3D models. Furthermore, object poses are refined and continuously tracked by minimizing event-line distances. Events are assi
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于注意力机制的元学习方法，以避免在持续学习过程中忘记先前学到的知识，并通过使优化器元学习能够有效地更新模型参数，从而加速新任务的学</title><link>https://arxiv.org/abs/2408.03219</link><description>&lt;p&gt;
Learning to Learn without Forgetting using Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03219
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于注意力机制的元学习方法，以避免在持续学习过程中忘记先前学到的知识，并通过使优化器元学习能够有效地更新模型参数，从而加速新任务的学
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03219v1 Announce Type: cross  Abstract: Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchma
&lt;/p&gt;</description></item><item><title>该文章提出IPAdapter-Instruct，一种结合自然图像条件和“Instruct”提示的方法，能够在同一流程中灵活切换相同条件图像的不同解释，包括风格转换、对象提取等，显著提高了多任务学习效率并保持了图像生成质量。</title><link>https://arxiv.org/abs/2408.03209</link><description>&lt;p&gt;
IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03209
&lt;/p&gt;
&lt;p&gt;
该文章提出IPAdapter-Instruct，一种结合自然图像条件和“Instruct”提示的方法，能够在同一流程中灵活切换相同条件图像的不同解释，包括风格转换、对象提取等，显著提高了多任务学习效率并保持了图像生成质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03209v1 Announce Type: new  Abstract: Diffusion models continuously push the boundary of state-of-the-art image generation, but the process is hard to control with any nuance: practice proves that textual prompts are inadequate for accurately describing image style or fine structural details (such as faces). ControlNet and IPAdapter address this shortcoming by conditioning the generative process on imagery instead, but each individual instance is limited to modeling a single conditional posterior: for practical use-cases, where multiple different posteriors are desired within the same workflow, training and using multiple adapters is cumbersome. We propose IPAdapter-Instruct, which combines natural-image conditioning with ``Instruct'' prompts to swap between interpretations for the same conditioning image: style transfer, object extraction, both, or something else still? IPAdapterInstruct efficiently learns multiple tasks with minimal loss in quality compared to dedicated pe
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为PFedSIS的联邦学习方法，用于个性化手术器械分割。该方法通过引入视觉先验，结合全局个性化 disentanglement、外观调节个性化增强和形状相似性全局增强机制，解决了现有方法未能充分考虑多头自我注意力个性化的问题，并且能够捕捉手术场景中的外观多样性与器械形状相似性。通过对多头自我注意力的头权重个性化，该方法实现了对单个训练站点特征的精确适应，提高了在多个独立站点上的器械分割性能。</title><link>https://arxiv.org/abs/2408.03208</link><description>&lt;p&gt;
Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03208
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为PFedSIS的联邦学习方法，用于个性化手术器械分割。该方法通过引入视觉先验，结合全局个性化 disentanglement、外观调节个性化增强和形状相似性全局增强机制，解决了现有方法未能充分考虑多头自我注意力个性化的问题，并且能够捕捉手术场景中的外观多样性与器械形状相似性。通过对多头自我注意力的头权重个性化，该方法实现了对单个训练站点特征的精确适应，提高了在多个独立站点上的器械分割性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03208v1 Announce Type: cross  Abstract: Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site d
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于新提出的空间频率联合查询注意力机制的SGSR框架，用于多对比度MRI超级分辨率。该框架通过共享的结构查询提取、融合和精炼多对比度图像的特征，从而充分利用了多对比度MRI中的结构和对比度不变信息。通过这种方法，文章的创新点在于如何更好地结合结构信息和对比度信息，以提升MRI图像的分辨率，并且减少由于运动引入的 artifacts。</title><link>https://arxiv.org/abs/2408.03194</link><description>&lt;p&gt;
SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via Spatio-Frequency Co-Query Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03194
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于新提出的空间频率联合查询注意力机制的SGSR框架，用于多对比度MRI超级分辨率。该框架通过共享的结构查询提取、融合和精炼多对比度图像的特征，从而充分利用了多对比度MRI中的结构和对比度不变信息。通过这种方法，文章的创新点在于如何更好地结合结构信息和对比度信息，以提升MRI图像的分辨率，并且减少由于运动引入的 artifacts。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03194v1 Announce Type: cross  Abstract: Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide range of exams, where multiple contrast images are often acquired for characterizing different tissues. However, acquiring high-resolution MRI typically extends scan time, which can introduce motion artifacts. Super-resolution of MRI therefore emerges as a promising approach to mitigate these challenges. Earlier studies have investigated the use of multiple contrasts for MRI super-resolution (MCSR), whereas majority of them did not fully exploit the rich contrast-invariant structural information. To fully utilize such crucial prior knowledge of multi-contrast MRI, in this work, we propose a novel structure-guided MCSR (SGSR) framework based on a new spatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs attention on features of multiple contrasts with a shared structural query, which is particularly designed to extract, fuse, and refin
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一种名为在线难样本挖掘的策略，用于提高神经辐射场（NeRF）模型的训练效率。通过观察到在训练过程中，大量计算资源被用于处理已经学会的样本，这意味着这部分样本不再对模型更新产生重要影响，作者识别出随机样本的逆传播是优化过程中的计算瓶颈。因此，文章提出首先使用第一遍前向推理来筛选出"硬样本"，只对这些样本构建计算图并更新网络参数。研究展示了该策略能够有效节省训练NeRF模型所需的时间和资源。</title><link>https://arxiv.org/abs/2408.03193</link><description>&lt;p&gt;
Efficient NeRF Optimization -- Not All Samples Remain Equally Hard
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03193
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一种名为在线难样本挖掘的策略，用于提高神经辐射场（NeRF）模型的训练效率。通过观察到在训练过程中，大量计算资源被用于处理已经学会的样本，这意味着这部分样本不再对模型更新产生重要影响，作者识别出随机样本的逆传播是优化过程中的计算瓶颈。因此，文章提出首先使用第一遍前向推理来筛选出"硬样本"，只对这些样本构建计算图并更新网络参数。研究展示了该策略能够有效节省训练NeRF模型所需的时间和资源。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03193v1 Announce Type: new  Abstract: We propose an application of online hard sample mining for efficient training of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality for many 3D reconstruction and rendering tasks but require substantial computational resources. The encoding of the scene information within the NeRF network parameters necessitates stochastic sampling. We observe that during the training, a major part of the compute time and memory usage is spent on processing already learnt samples, which no longer affect the model update significantly. We identify the backward pass on the stochastic samples as the computational bottleneck during the optimization. We thus perform the first forward pass in inference mode as a relatively low-cost search for hard samples. This is followed by building the computational graph and updating the NeRF network parameters using only the hard samples. To demonstrate the effectiveness of the proposed approach, 
&lt;/p&gt;</description></item><item><title>该文章提出了一种将3D模型简化为64x64像素图像的新方法，以生成带有UV映射的逼真3D形状。通过这种方式，复杂的三维形状被转换成更易处理的二维结构，从而解决了网格形状的几何和语义非一致性问题。该方法允许使用图像生成模型直接生成3D形状，并在ABO数据集上获得了与最近3D生成模型相似的点云FID性能。</title><link>https://arxiv.org/abs/2408.03178</link><description>&lt;p&gt;
An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03178
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种将3D模型简化为64x64像素图像的新方法，以生成带有UV映射的逼真3D形状。通过这种方式，复杂的三维形状被转换成更易处理的二维结构，从而解决了网格形状的几何和语义非一致性问题。该方法允许使用图像生成模型直接生成3D形状，并在ABO数据集上获得了与最近3D生成模型相似的点云FID性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03178v1 Announce Type: new  Abstract: We introduce a new approach for generating realistic 3D models with UV maps through a representation termed "Object Images." This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.
&lt;/p&gt;</description></item><item><title>该文章展示了使用可学习间隔的空间膨胀卷积（DCLS）的方法，这种方法不仅在计算机视觉基准测试中超越了标准和膨胀卷积，还在提升模型的可解释性方面取得了显著进步。可解释性通过与人类视觉策略的对应关系来衡量，即通过比较基于模型的GradCAM热图与反映人类视觉注意度的ClickMe数据集热图之间的Spearman相关系数来评估。该方法在多个基准模型中得到了应用，并提高了模型的可解释性评分，表明DCLS增加了模型与人类视觉策略的一致性。</title><link>https://arxiv.org/abs/2408.03164</link><description>&lt;p&gt;
Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03164
&lt;/p&gt;
&lt;p&gt;
该文章展示了使用可学习间隔的空间膨胀卷积（DCLS）的方法，这种方法不仅在计算机视觉基准测试中超越了标准和膨胀卷积，还在提升模型的可解释性方面取得了显著进步。可解释性通过与人类视觉策略的对应关系来衡量，即通过比较基于模型的GradCAM热图与反映人类视觉注意度的ClickMe数据集热图之间的Spearman相关系数来评估。该方法在多个基准模型中得到了应用，并提高了模型的可解释性评分，表明DCLS增加了模型与人类视觉策略的一致性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03164v1 Announce Type: new  Abstract: Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random he
&lt;/p&gt;</description></item><item><title>该文章研究了由大型语言模型（LLMs）驱动的现代多模态推理模型在辅助视觉辅助设备完成多步骤日常活动方面的能力。我们通过在线视频数据集中的动作预测任务，对两种不同类型的多模态LLM方法——Socratic模型和视觉条件语言模型（VCLM）进行了基准测试，以评估其将视觉历史编码化和在中长期预测动作的能力。然而，这些在线视频数据集仅仅允许我们评估辅助设备的前两个能力，并不能评估在用户参与下重新规划的能力。因此，我们设计了一个用户参与式的评估框架，为VCLM提供了实时用户交互的补充评估方式，从而评估了VCLM在中长期预测和动态行动规划方面的实际表现。</title><link>https://arxiv.org/abs/2408.03160</link><description>&lt;p&gt;
User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03160
&lt;/p&gt;
&lt;p&gt;
该文章研究了由大型语言模型（LLMs）驱动的现代多模态推理模型在辅助视觉辅助设备完成多步骤日常活动方面的能力。我们通过在线视频数据集中的动作预测任务，对两种不同类型的多模态LLM方法——Socratic模型和视觉条件语言模型（VCLM）进行了基准测试，以评估其将视觉历史编码化和在中长期预测动作的能力。然而，这些在线视频数据集仅仅允许我们评估辅助设备的前两个能力，并不能评估在用户参与下重新规划的能力。因此，我们设计了一个用户参与式的评估框架，为VCLM提供了实时用户交互的补充评估方式，从而评估了VCLM在中长期预测和动态行动规划方面的实际表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03160v1 Announce Type: new  Abstract: Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches -- Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we
&lt;/p&gt;</description></item><item><title>该文章提出了一种新的CT重建方法，该方法通过将退化扩散概率模型的潜变量与迭代CT重建的保真度损失结合起来进行优化，以改进图像质量。</title><link>https://arxiv.org/abs/2408.03156</link><description>&lt;p&gt;
Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03156
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新的CT重建方法，该方法通过将退化扩散概率模型的潜变量与迭代CT重建的保真度损失结合起来进行优化，以改进图像质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03156v1 Announce Type: new  Abstract: Image generative AI has garnered significant attention in recent years. In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity. In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference. We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data. Despite the simplicity of the implementation, the proposed method 
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为EGMS的模型，通过集成细粒度实体知识，该模型可以更精细地整合视觉信息，从而增强文本摘要的生成，并在选取图像方面进行精炼处理。在BART的基础上，EGMS模型采用了双模态编码器，并利用共享权重来处理文本-图像和实体-图像信息，有效地结合了视觉数据以生成更高质量的文本摘要。</title><link>https://arxiv.org/abs/2408.03149</link><description>&lt;p&gt;
Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03149
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为EGMS的模型，通过集成细粒度实体知识，该模型可以更精细地整合视觉信息，从而增强文本摘要的生成，并在选取图像方面进行精炼处理。在BART的基础上，EGMS模型采用了双模态编码器，并利用共享权重来处理文本-图像和实体-图像信息，有效地结合了视觉数据以生成更高质量的文本摘要。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03149v1 Announce Type: new  Abstract: The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge 
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为SuperSimpleNet的创新模型，该模型根据SimpleNet模型演变而来，主要用于快速且可靠地识别物体表面的缺陷。SuperSimpleNet能够在仅使用正常训练图像的条件下进行自监督学习，并在有异常标签图像的情况下进一步提升性能。该模型在监督和无监督两种环境下均取得了领先的结果，展示了其在工业应用中具备的高性能、快速操作以及数据使用效率。</title><link>https://arxiv.org/abs/2408.03143</link><description>&lt;p&gt;
SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast and Reliable Surface Defect Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03143
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为SuperSimpleNet的创新模型，该模型根据SimpleNet模型演变而来，主要用于快速且可靠地识别物体表面的缺陷。SuperSimpleNet能够在仅使用正常训练图像的条件下进行自监督学习，并在有异常标签图像的情况下进一步提升性能。该模型在监督和无监督两种环境下均取得了领先的结果，展示了其在工业应用中具备的高性能、快速操作以及数据使用效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03143v1 Announce Type: new  Abstract: The aim of surface defect detection is to identify and localise abnormal regions on the surfaces of captured objects, a task that's increasingly demanded across various industries. Current approaches frequently fail to fulfil the extensive demands of these industries, which encompass high performance, consistency, and fast operation, along with the capacity to leverage the entirety of the available training data. Addressing these gaps, we introduce SuperSimpleNet, an innovative discriminative model that evolved from SimpleNet. This advanced model significantly enhances its predecessor's training consistency, inference time, as well as detection performance. SuperSimpleNet operates in an unsupervised manner using only normal training images but also benefits from labelled abnormal training images when they are available. SuperSimpleNet achieves state-of-the-art results in both the supervised and the unsupervised settings, as demonstrated 
&lt;/p&gt;</description></item><item><title>该文章提出的综述对现有植物疾病分类模型在实验室条件下表现出色，但野外图像识别上表现不佳的现象进行了深入剖析，并针对植物疾病的相似外观和不同疾病外观的矛盾提出了解决方案。文章还介绍了最大的植物疾病文本描述数据集，旨在通过提供丰富的文本信息来解决野外识别中存在的不同疾病相似和同一种疾病外观差异大的问题。通过这些努力，文章旨在提高植物疾病野外图识别中模型在处理小类间差异和大类内差异问题时的性能。</title><link>https://arxiv.org/abs/2408.03120</link><description>&lt;p&gt;
Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03120
&lt;/p&gt;
&lt;p&gt;
该文章提出的综述对现有植物疾病分类模型在实验室条件下表现出色，但野外图像识别上表现不佳的现象进行了深入剖析，并针对植物疾病的相似外观和不同疾病外观的矛盾提出了解决方案。文章还介绍了最大的植物疾病文本描述数据集，旨在通过提供丰富的文本信息来解决野外识别中存在的不同疾病相似和同一种疾病外观差异大的问题。通过这些努力，文章旨在提高植物疾病野外图识别中模型在处理小类间差异和大类内差异问题时的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03120v1 Announce Type: new  Abstract: Existing plant disease classification models have achieved remarkable performance in recognizing in-laboratory diseased images. However, their performance often significantly degrades in classifying in-the-wild images. Furthermore, we observed that in-the-wild plant images may exhibit similar appearances across various diseases (i.e., small inter-class discrepancy) while the same diseases may look quite different (i.e., large intra-class variance). Motivated by this observation, we propose an in-the-wild multimodal plant disease recognition dataset that contains the largest number of disease classes but also text-based descriptions for each disease. Particularly, the newly provided text descriptions are introduced to provide rich information in textual modality and facilitate in-the-wild disease classification with small inter-class discrepancy and large intra-class variance issues. Therefore, our proposed dataset can be regarded as an i
&lt;/p&gt;</description></item><item><title>该文章描述了一种创新的微手势分类方法，采用了交叉模态融合模块和原型精炼模块，显著提高了微手势特征的区分度，从而在IJCAI 2024的MiGA挑战赛中取得了第一名，相较于去年最佳成绩大幅提升了6.13%的Top-1准确率。</title><link>https://arxiv.org/abs/2408.03097</link><description>&lt;p&gt;
Prototype Learning for Micro-gesture Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03097
&lt;/p&gt;
&lt;p&gt;
该文章描述了一种创新的微手势分类方法，采用了交叉模态融合模块和原型精炼模块，显著提高了微手势特征的区分度，从而在IJCAI 2024的MiGA挑战赛中取得了第一名，相较于去年最佳成绩大幅提升了6.13%的Top-1准确率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03097v1 Announce Type: new  Abstract: In this paper, we briefly introduce the solution developed by our team, HFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge at IJCAI 2024. The task of micro-gesture classification task involves recognizing the category of a given video clip, which focuses on more fine-grained and subtle body movements compared to typical action recognition tasks. Given the inherent complexity of micro-gesture recognition, which includes large intra-class variability and minimal inter-class differences, we utilize two innovative modules, i.e., the cross-modal fusion module and prototypical refinement module, to improve the discriminative ability of MG features, thereby improving the classification accuracy. Our solution achieved significant success, ranking 1st in the track of Micro-gesture Classification. We surpassed the performance of last year's leading team by a substantial margin, improving Top-1 accuracy by 6.13%.
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一种面向手术应用的开放域单目视觉SLAM框架BodySLAM，该框架能够充分利用单目摄像头的输入，无需任何传统的传感器输入，有效提高了手术操作中的深度感知和操纵精准度。</title><link>https://arxiv.org/abs/2408.03078</link><description>&lt;p&gt;
BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03078
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一种面向手术应用的开放域单目视觉SLAM框架BodySLAM，该框架能够充分利用单目摄像头的输入，无需任何传统的传感器输入，有效提高了手术操作中的深度感知和操纵精准度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03078v1 Announce Type: cross  Abstract: Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM) has emerged as a promising solution to address these limitations, its implementation in endoscopic procedures presents significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents a robust deep learning-based SLAM approach that combines state-of-the-art and newly developed models. It consists of three main parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method based on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the novel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous models to create a coherent surgical map. The performance of the procedure was rigorously eva
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SCOPE的合成多模态数据集，用于集体感知研究。该数据集包括多种环境和天气条件下的传感器模拟，特别是物理准确的风雨模拟，为开发和测试新型集体感知技术提供了基础。SCOPE数据集通过模拟现实的传感器数据和复杂的交通场景，促进了自动驾驶和车辆感知领域的技术创新。</title><link>https://arxiv.org/abs/2408.03065</link><description>&lt;p&gt;
SCOPE: A Synthetic Multi-Modal Dataset for Collective Perception Including Physical-Correct Weather Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03065
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SCOPE的合成多模态数据集，用于集体感知研究。该数据集包括多种环境和天气条件下的传感器模拟，特别是物理准确的风雨模拟，为开发和测试新型集体感知技术提供了基础。SCOPE数据集通过模拟现实的传感器数据和复杂的交通场景，促进了自动驾驶和车辆感知领域的技术创新。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03065v1 Announce Type: new  Abstract: Collective perception has received considerable attention as a promising approach to overcome occlusions and limited sensing ranges of vehicle-local perception in autonomous driving. In order to develop and test novel collective perception technologies, appropriate datasets are required. These datasets must include not only different environmental conditions, as they strongly influence the perception capabilities, but also a wide range of scenarios with different road users as well as realistic sensor models. Therefore, we propose the Synthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic multi-modal dataset that incorporates realistic camera and LiDAR models as well as parameterized and physically accurate weather simulations for both sensor types. The dataset contains 17,600 frames from over 40 diverse scenarios with up to 24 collaborative agents, infrastructure sensors, and passive traffic, including cyclists an
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Masked Gaussian Fields (MGFs)的框架，用于针对多视图图像进行更快且更准确的建筑物表面重建。通过有效的空间采样和COLMAP的多层次建筑物掩码生成，MGFs能够在保持三维细节的同时，显著减少噪声，提高重建效率。</title><link>https://arxiv.org/abs/2408.03060</link><description>&lt;p&gt;
MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03060
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Masked Gaussian Fields (MGFs)的框架，用于针对多视图图像进行更快且更准确的建筑物表面重建。通过有效的空间采样和COLMAP的多层次建筑物掩码生成，MGFs能够在保持三维细节的同时，显著减少噪声，提高重建效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03060v1 Announce Type: new  Abstract: Over the last few decades, image-based building surface reconstruction has garnered substantial research interest and has been applied across various fields, such as heritage preservation, architectural planning, etc. Compared to the traditional photogrammetric and NeRF-based solutions, recently, Gaussian fields-based methods have exhibited significant potential in generating surface meshes due to their time-efficient training and detailed 3D information preservation. However, most gaussian fields-based methods are trained with all image pixels, encompassing building and nonbuilding areas, which results in a significant noise for building meshes and degeneration in time efficiency. This paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to generate accurate surface reconstruction for building in a time-efficient way. The framework first applies EfficientSAM and COLMAP to generate multi-level masks of building and t
&lt;/p&gt;</description></item><item><title>该文章提出的CPD框架通过集成层次依赖问题解决的“comb”步骤，实现了模型结构无关的层级依赖突破，并通过适应性去除参数和知识蒸馏的方法保证了在去除冗余参数的同时保持了模型的学习信息，从而使模型压缩更加高效和通用。</title><link>https://arxiv.org/abs/2408.03046</link><description>&lt;p&gt;
Comb, Prune, Distill: Towards Unified Pruning for Vision Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03046
&lt;/p&gt;
&lt;p&gt;
该文章提出的CPD框架通过集成层次依赖问题解决的“comb”步骤，实现了模型结构无关的层级依赖突破，并通过适应性去除参数和知识蒸馏的方法保证了在去除冗余参数的同时保持了模型的学习信息，从而使模型压缩更加高效和通用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03046v1 Announce Type: new  Abstract: Lightweight and effective models are essential for devices with limited resources, such as intelligent vehicles. Structured pruning offers a promising approach to model compression and efficiency enhancement. However, existing methods often tie pruning techniques to specific model architectures or vision tasks. To address this limitation, we propose a novel unified pruning framework Comb, Prune, Distill (CPD), which addresses both model-agnostic and task-agnostic concerns simultaneously. Our framework employs a combing step to resolve hierarchical layer-wise dependency issues, enabling architecture independence. Additionally, the pruning pipeline adaptively remove parameters based on the importance scoring metrics regardless of vision tasks. To support the model in retaining its learned information, we introduce knowledge distillation during the pruning step. Extensive experiments demonstrate the generalizability of our framework, encomp
&lt;/p&gt;</description></item><item><title>该文章提出了针对性的视觉提示方法，以增强多模态大型语言模型在医学视觉问答任务中对图像的局部理解和整体理解能力，为这些模型提供了区域化问题的回答能力。</title><link>https://arxiv.org/abs/2408.03043</link><description>&lt;p&gt;
Targeted Visual Prompting for Medical Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03043
&lt;/p&gt;
&lt;p&gt;
该文章提出了针对性的视觉提示方法，以增强多模态大型语言模型在医学视觉问答任务中对图像的局部理解和整体理解能力，为这些模型提供了区域化问题的回答能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03043v1 Announce Type: new  Abstract: With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation. However, simple visual errors cast doubt on the actual visual understanding abilities of these models. To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to se
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Free-Echo的无训练条件视频扩散模型，用于从单个心动周期分割图合成具有空间-语义特性的超声心动图，无需额外的训练数据。该方法基于3D-Unet模型，使用训练自由的条件方法SDEdit对分割图进行条件约束，并在CAMUS和EchoNet-Dynamic两个公开的超声心动图数据集上取得了与训练依赖的CDM相当的性能。该方法为合成合乎实际的超声心动图开辟了新的可能性。</title><link>https://arxiv.org/abs/2408.03035</link><description>&lt;p&gt;
Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03035
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Free-Echo的无训练条件视频扩散模型，用于从单个心动周期分割图合成具有空间-语义特性的超声心动图，无需额外的训练数据。该方法基于3D-Unet模型，使用训练自由的条件方法SDEdit对分割图进行条件约束，并在CAMUS和EchoNet-Dynamic两个公开的超声心动图数据集上取得了与训练依赖的CDM相当的性能。该方法为合成合乎实际的超声心动图开辟了新的可能性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03035v1 Announce Type: cross  Abstract: Conditional video diffusion models (CDM) have shown promising results for video synthesis, potentially enabling the generation of realistic echocardiograms to address the problem of data scarcity. However, current CDMs require a paired segmentation map and echocardiogram dataset. We present a new method called Free-Echo for generating realistic echocardiograms from a single end-diastolic segmentation map without additional training data. Our method is based on the 3D-Unet with Temporal Attention Layers model and is conditioned on the segmentation map using a training-free conditioning method based on SDEdit. We evaluate our model on two public echocardiogram datasets, CAMUS and EchoNet-Dynamic. We show that our model can generate plausible echocardiograms that are spatially aligned with the input segmentation map, achieving performance comparable to training-based CDMs. Our work opens up new possibilities for generating echocardiograms
&lt;/p&gt;</description></item><item><title>该文章通过结合背景信息并用对比学习的方法重新审视通道注意机制，解决了夜间行人检测中的低光性能问题，提出了FBCA（Fore-Background Contrast Attention）模型，该模型通过利用区域背景和行人对象的区别来提高检测性能。</title><link>https://arxiv.org/abs/2408.03030</link><description>&lt;p&gt;
Nighttime Pedestrian Detection Based on Fore-Background Contrast Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03030
&lt;/p&gt;
&lt;p&gt;
该文章通过结合背景信息并用对比学习的方法重新审视通道注意机制，解决了夜间行人检测中的低光性能问题，提出了FBCA（Fore-Background Contrast Attention）模型，该模型通过利用区域背景和行人对象的区别来提高检测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03030v1 Announce Type: new  Abstract: The significance of background information is frequently overlooked in contemporary research concerning channel attention mechanisms. This study addresses the issue of suboptimal single-spectral nighttime pedestrian detection performance under low-light conditions by incorporating background information into the channel attention mechanism. Despite numerous studies focusing on the development of efficient channel attention mechanisms, the relevance of background information has been largely disregarded. By adopting a contrast learning approach, we reexamine channel attention with regard to pedestrian objects and background information for nighttime pedestrian detection, resulting in the proposed Fore-Background Contrast Attention (FBCA). FBCA possesses two primary attributes: (1) channel descriptors form remote dependencies with global spatial feature information; (2) the integration of background information enhances the distinction bet
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Cleansed k-Nearest Neighbor (CKNN)的全新方法，针对视频异常检测中的“Anomaly Cluster”问题，通过数据清洗技术有效排除不良训练样本，为Unsupervised Video Anomaly Detection提供了改进的解决方案。</title><link>https://arxiv.org/abs/2408.03014</link><description>&lt;p&gt;
CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03014
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Cleansed k-Nearest Neighbor (CKNN)的全新方法，针对视频异常检测中的“Anomaly Cluster”问题，通过数据清洗技术有效排除不良训练样本，为Unsupervised Video Anomaly Detection提供了改进的解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03014v1 Announce Type: new  Abstract: In this paper, we address the problem of unsupervised video anomaly detection (UVAD). The task aims to detect abnormal events in test video using unlabeled videos as training data. The presence of anomalies in the training data poses a significant challenge in this task, particularly because they form clusters in the feature space. We refer to this property as the "Anomaly Cluster" issue. The condensed nature of these anomalies makes it difficult to distinguish between normal and abnormal data in the training set. Consequently, training conventional anomaly detection techniques using an unlabeled dataset often leads to sub-optimal results. To tackle this difficulty, we propose a new method called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out the Anomaly Clusters by cleansing the training dataset. Following the k-nearest neighbor algorithm in the feature space provides powerful anomaly detection capability. Although the
&lt;/p&gt;</description></item><item><title>该文章提出了一种双路径协作生成网络，用于情感视频 captioning。</title><link>https://arxiv.org/abs/2408.03006</link><description>&lt;p&gt;
Dual-path Collaborative Generation Network for Emotional Video Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03006
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种双路径协作生成网络，用于情感视频 captioning。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03006v1 Announce Type: new  Abstract: Emotional Video Captioning is an emerging task that aims to describe factual content with the intrinsic emotions expressed in videos. The essential of the EVC task is to effectively perceive subtle and ambiguous visual emotional cues during the caption generation, which is neglected by the traditional video captioning. Existing emotional video captioning methods perceive global visual emotional cues at first, and then combine them with the video features to guide the emotional caption generation, which neglects two characteristics of the EVC task. Firstly, their methods neglect the dynamic subtle changes in the intrinsic emotions of the video, which makes it difficult to meet the needs of common scenes with diverse and changeable emotions. Secondly, as their methods incorporate emotional cues into each step, the guidance role of emotion is overemphasized, which makes factual content more or less ignored during generation. To this end, we
&lt;/p&gt;</description></item><item><title>该文章提出了一种称为神经调谐的先进技术，旨在通过模拟人脑中特定神经元激活的模式，使得大型多模态模型能够同时有效处理多个不同的任务，包括图像分段、文本增强、图像描述和文本到图像的生成。这种方法通过一个名为MMUD的全新基准测试，展示了对大规模模型在多个任务的统一调谐和高效执行，为多模态任务性能提升提供了一条新途径。</title><link>https://arxiv.org/abs/2408.03001</link><description>&lt;p&gt;
Multitask and Multimodal Neural Tuning for Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03001
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种称为神经调谐的先进技术，旨在通过模拟人脑中特定神经元激活的模式，使得大型多模态模型能够同时有效处理多个不同的任务，包括图像分段、文本增强、图像描述和文本到图像的生成。这种方法通过一个名为MMUD的全新基准测试，展示了对大规模模型在多个任务的统一调谐和高效执行，为多模态任务性能提升提供了一条新途径。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03001v1 Announce Type: new  Abstract: In recent years, large-scale multimodal models have demonstrated impressive capabilities across various domains. However, enabling these models to effectively perform multiple multimodal tasks simultaneously remains a significant challenge. To address this, we introduce a novel tuning method called neural tuning, designed to handle diverse multimodal tasks concurrently, including reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. Neural tuning emulates sparse distributed representation in human brain, where only specific subsets of neurons are activated for each task. Additionally, we present a new benchmark, MMUD, where each sample is annotated with multiple task labels. By applying neural tuning to pretrained large models on the MMUD benchmark, we achieve simultaneous task handling in a streamlined and efficient manner. All models, code, and datasets will be publicly available after publicat
&lt;/p&gt;</description></item><item><title>该文章提出了DreamLCM方法，通过结合Latent Consistency Model，解决了 text-to-3D 任务中因采样策略和噪声的不确定性导致的对象质量问题。DreamLCM能够生成一致且有高清度的引导信号，为优化目标3D模型提供了精确和详细的梯度，并通过两种策略进一步提升了生成质量。</title><link>https://arxiv.org/abs/2408.02993</link><description>&lt;p&gt;
DreamLCM: Towards High-Quality Text-to-3D Generation via Latent Consistency Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02993
&lt;/p&gt;
&lt;p&gt;
该文章提出了DreamLCM方法，通过结合Latent Consistency Model，解决了 text-to-3D 任务中因采样策略和噪声的不确定性导致的对象质量问题。DreamLCM能够生成一致且有高清度的引导信号，为优化目标3D模型提供了精确和详细的梯度，并通过两种策略进一步提升了生成质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02993v1 Announce Type: new  Abstract: Recently, the text-to-3D task has developed rapidly due to the appearance of the SDS method. However, the SDS method always generates 3D objects with poor quality due to the over-smooth issue. This issue is attributed to two factors: 1) the DDPM single-step inference produces poor guidance gradients; 2) the randomness from the input noises and timesteps averages the details of the 3D contents.In this paper, to address the issue, we propose DreamLCM which incorporates the Latent Consistency Model (LCM). DreamLCM leverages the powerful image generation capabilities inherent in LCM, enabling generating consistent and high-quality guidance, i.e., predicted noises or images. Powered by the improved guidance, the proposed method can provide accurate and detailed gradients to optimize the target 3D models.In addition, we propose two strategies to enhance the generation quality further. Firstly, we propose a guidance calibration strategy, utiliz
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于扩散模型的非范例类增量学习方法，该模型通过自监督学习和扩散模型生成高度逼近真实特征的类代表特征，有效地消除了在无示范增量学习中由于特征生成规则导致的分布差距问题。</title><link>https://arxiv.org/abs/2408.02983</link><description>&lt;p&gt;
Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02983
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于扩散模型的非范例类增量学习方法，该模型通过自监督学习和扩散模型生成高度逼近真实特征的类代表特征，有效地消除了在无示范增量学习中由于特征生成规则导致的分布差距问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02983v1 Announce Type: new  Abstract: Non-exemplar class-incremental learning (NECIL) is to resist catastrophic forgetting without saving old class samples. Prior methodologies generally employ simple rules to generate features for replaying, suffering from large distribution gap between replayed features and real ones. To address the aforementioned issue, we propose a simple, yet effective \textbf{Diff}usion-based \textbf{F}eature \textbf{R}eplay (\textbf{DiffFR}) method for NECIL. First, to alleviate the limited representational capacity caused by fixing the feature extractor, we employ Siamese-based self-supervised learning for initial generalizable features. Second, we devise diffusion models to generate class-representative features highly similar to real features, which provides an effective way for exemplar-free knowledge memorization. Third, we introduce prototype calibration to direct the diffusion model's focus towards learning the distribution shapes of features, 
&lt;/p&gt;</description></item><item><title>该文章提出了一种与样本无关的对抗性扰动方法，旨在增强VLP模型对图像和文本的鲁棒性。作者开创性地探索了通过跨模态决策边界来创建通用的对抗性扰动，并将其应用于任何给定的图像，以此来提高模型的抗攻击能力。通过这种方法，即使在考虑了Top-k准确性的情况下，模型也能够成功抵御攻击。</title><link>https://arxiv.org/abs/2408.02980</link><description>&lt;p&gt;
Sample-agnostic Adversarial Perturbation for Vision-Language Pre-training Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02980
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种与样本无关的对抗性扰动方法，旨在增强VLP模型对图像和文本的鲁棒性。作者开创性地探索了通过跨模态决策边界来创建通用的对抗性扰动，并将其应用于任何给定的图像，以此来提高模型的抗攻击能力。通过这种方法，即使在考虑了Top-k准确性的情况下，模型也能够成功抵御攻击。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02980v1 Announce Type: new  Abstract: Recent studies on AI security have highlighted the vulnerability of Vision-Language Pre-training (VLP) models to subtle yet intentionally designed perturbations in images and texts. Investigating multimodal systems' robustness via adversarial attacks is crucial in this field. Most multimodal attacks are sample-specific, generating a unique perturbation for each sample to construct adversarial samples. To the best of our knowledge, it is the first work through multimodal decision boundaries to explore the creation of a universal, sample-agnostic perturbation that applies to any image. Initially, we explore strategies to move sample points beyond the decision boundaries of linear classifiers, refining the algorithm to ensure successful attacks under the top $k$ accuracy metric. Based on this foundation, in visual-language tasks, we treat visual and textual modalities as reciprocal sample points and decision hyperplanes, guiding image embed
&lt;/p&gt;</description></item><item><title>该文章提出的ASR-enhanced Multimodal Product Representation Learning（AMPere）方法通过使用基于LLM的ASR文本摘要器简化从噪声ASR文本中提取产品相关信息的过程，并将其与视觉数据联合输入到一个多分支网络中，以生成紧凑的跨域产品 multimodal 表示。</title><link>https://arxiv.org/abs/2408.02978</link><description>&lt;p&gt;
ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02978
&lt;/p&gt;
&lt;p&gt;
该文章提出的ASR-enhanced Multimodal Product Representation Learning（AMPere）方法通过使用基于LLM的ASR文本摘要器简化从噪声ASR文本中提取产品相关信息的过程，并将其与视觉数据联合输入到一个多分支网络中，以生成紧凑的跨域产品 multimodal 表示。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02978v1 Announce Type: cross  Abstract: E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive exp
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于上下文残差编码和INR优化的三维点云快速几何压缩方法，通过KNN方法确定原始表面点的邻域关系，有效利用算术编码对3D点特征进行压缩，并且设计了一个双层压缩架构，通过对粗糙结构和精细细节的层次处理，实现了模型复杂度和压缩率的平衡优化。</title><link>https://arxiv.org/abs/2408.02966</link><description>&lt;p&gt;
Fast Point Cloud Geometry Compression with Context-based Residual Coding and INR-based Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02966
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于上下文残差编码和INR优化的三维点云快速几何压缩方法，通过KNN方法确定原始表面点的邻域关系，有效利用算术编码对3D点特征进行压缩，并且设计了一个双层压缩架构，通过对粗糙结构和精细细节的层次处理，实现了模型复杂度和压缩率的平衡优化。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02966v1 Announce Type: new  Abstract: Compressing a set of unordered points is far more challenging than compressing images/videos of regular sample grids, because of the difficulties in characterizing neighboring relations in an irregular layout of points. Many researchers resort to voxelization to introduce regularity, but this approach suffers from quantization loss. In this research, we use the KNN method to determine the neighborhoods of raw surface points. This gives us a means to determine the spatial context in which the latent features of 3D points are compressed by arithmetic coding. As such, the conditional probability model is adaptive to local geometry, leading to significant rate reduction. Additionally, we propose a dual-layer architecture where a non-learning base layer reconstructs the main structures of the point cloud at low complexity, while a learned refinement layer focuses on preserving fine details. This design leads to reductions in model complexity 
&lt;/p&gt;</description></item><item><title>该文章提出的记忆增强自注意力模块（MATR）能够利用受限于固定大小的输入视频片段来识别多个动作实例，通过记忆队列保存过去片段的特征来有效利用长期上下文信息，并提出了一种新的动作定位方法来预测动作结束时间和从记忆队列中估计动作开始时间，该方法的性能优于已存在的方法，在THUMOS14和MUSES两个数据集上均取得显著提升，甚至在某些情况下超越了某些传统的动作识别方法。</title><link>https://arxiv.org/abs/2408.02957</link><description>&lt;p&gt;
Online Temporal Action Localization with Memory-Augmented Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02957
&lt;/p&gt;
&lt;p&gt;
该文章提出的记忆增强自注意力模块（MATR）能够利用受限于固定大小的输入视频片段来识别多个动作实例，通过记忆队列保存过去片段的特征来有效利用长期上下文信息，并提出了一种新的动作定位方法来预测动作结束时间和从记忆队列中估计动作开始时间，该方法的性能优于已存在的方法，在THUMOS14和MUSES两个数据集上均取得显著提升，甚至在某些情况下超越了某些传统的动作识别方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02957v1 Announce Type: new  Abstract: Online temporal action localization (On-TAL) is the task of identifying multiple action instances given a streaming video. Since existing methods take as input only a video segment of fixed size per iteration, they are limited in considering long-term context and require tuning the segment size carefully. To overcome these limitations, we propose memory-augmented transformer (MATR). MATR utilizes the memory queue that selectively preserves the past segment features, allowing to leverage long-term context for inference. We also propose a novel action localization method that observes the current input segment to predict the end time of the ongoing action and accesses the memory queue to estimate the start time of the action. Our method outperformed existing methods on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the online setting but also some offline TAL methods.
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为FakeMix的评估基准，旨在分析和检测视频和音频中的片段级深伪数据，并引入了Temporal Accuracy（TA）和Frame-wise Discrimination Metric（FDM）等新型评估指标，以改善对深伪检测模型的动态检测性能，从而填补现有视频级分类方法的不足，并在测试不同深伪攻击时展现出更好地效果。</title><link>https://arxiv.org/abs/2408.02954</link><description>&lt;p&gt;
WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal Deepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02954
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为FakeMix的评估基准，旨在分析和检测视频和音频中的片段级深伪数据，并引入了Temporal Accuracy（TA）和Frame-wise Discrimination Metric（FDM）等新型评估指标，以改善对深伪检测模型的动态检测性能，从而填补现有视频级分类方法的不足，并在测试不同深伪攻击时展现出更好地效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02954v1 Announce Type: new  Abstract: All current benchmarks for multimodal deepfake detection manipulate entire frames using various generation techniques, resulting in oversaturated detection accuracies exceeding 94% at the video-level classification. However, these benchmarks struggle to detect dynamic deepfake attacks with challenging frame-by-frame alterations presented in real-world scenarios. To address this limitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed at identifying manipulated segments within both video and audio, providing insight into the origins of deepfakes. Furthermore, we propose novel evaluation metrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to assess the robustness of deepfake detection models. Evaluating state-of-the-art models against diverse deepfake benchmarks, particularly FakeMix, demonstrates the effectiveness of our approach comprehensively. Specifically, while achieving an Average Preci
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了两种简单的标注策略——Multi-Size Labeling（MSL）和Distance-Based Labeling（DBL），通过整合到不同类型的神经网络中，显著提高了对小脑梗塞区域进行精确分割的准确性。通过在ATLAS v2.0数据集上的实验评估，展示了这两种策略集成为Ensemble后，对于召回率、F1分数以及Dice分数的提升效果，特别是在处理小型病变方面，显著提高了诊断的准确性。</title><link>https://arxiv.org/abs/2408.02929</link><description>&lt;p&gt;
Segmenting Small Stroke Lesions with Novel Labeling Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02929
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了两种简单的标注策略——Multi-Size Labeling（MSL）和Distance-Based Labeling（DBL），通过整合到不同类型的神经网络中，显著提高了对小脑梗塞区域进行精确分割的准确性。通过在ATLAS v2.0数据集上的实验评估，展示了这两种策略集成为Ensemble后，对于召回率、F1分数以及Dice分数的提升效果，特别是在处理小型病变方面，显著提高了诊断的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02929v1 Announce Type: new  Abstract: Deep neural networks have demonstrated exceptional efficacy in stroke lesion segmentation. However, the delineation of small lesions, critical for stroke diagnosis, remains a challenge. In this study, we propose two straightforward yet powerful approaches that can be seamlessly integrated into a variety of networks: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL), with the aim of enhancing the segmentation accuracy of small lesions. MSL divides lesion masks into various categories based on lesion volume while DBL emphasizes the lesion boundaries. Experimental evaluations on the Anatomical Tracings of Lesions After Stroke (ATLAS) v2.0 dataset showcase that an ensemble of MSL and DBL achieves consistently better or equal performance on recall (3.6% and 3.7%), F1 (2.4% and 1.5%), and Dice scores (1.3% and 0.0%) compared to the top-1 winner of the 2022 MICCAI ATLAS Challenge on both the subset only containing small lesions and th
&lt;/p&gt;</description></item><item><title>该文章阐述了Meta开发的Segment Anything Model 2 (SAM2)在海洋科学研究中的应用前景，特别是在解决水面以下的实例分割问题上。实验结果表明，当使用地面真实边界框作为提示时，SAM2在水底实例分割领域表现出色。然而，当SAM2在没有提示的情况下运行自动分割时，其感知和分割水面以下实例的能力受到了限制。</title><link>https://arxiv.org/abs/2408.02924</link><description>&lt;p&gt;
Evaluation of Segment Anything Model 2: The Role of SAM2 in the Underwater Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02924
&lt;/p&gt;
&lt;p&gt;
该文章阐述了Meta开发的Segment Anything Model 2 (SAM2)在海洋科学研究中的应用前景，特别是在解决水面以下的实例分割问题上。实验结果表明，当使用地面真实边界框作为提示时，SAM2在水底实例分割领域表现出色。然而，当SAM2在没有提示的情况下运行自动分割时，其感知和分割水面以下实例的能力受到了限制。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02924v1 Announce Type: new  Abstract: With breakthroughs in large-scale modeling, the Segment Anything Model (SAM) and its extensions have been attempted for applications in various underwater visualization tasks in marine sciences, and have had a significant impact on the academic community. Recently, Meta has further developed the Segment Anything Model 2 (SAM2), which significantly improves running speed and segmentation accuracy compared to its predecessor. This report aims to explore the potential of SAM2 in marine science by evaluating it on the underwater instance segmentation benchmark datasets UIIS and USIS10K. The experiments show that the performance of SAM2 is extremely dependent on the type of user-provided prompts. When using the ground truth bounding box as prompt, SAM2 performed excellently in the underwater instance segmentation domain. However, when running in automatic mode, SAM2's ability with point prompts to sense and segment underwater instances is sig
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Pose Magic的混合Mamba-GCN网络，实现了高效且时间一致的人体姿势估计，通过结合Mamba的高质量长范围建模能力和GCN的局部增强效果，从而在保持准确性的同时提高了计算效率。</title><link>https://arxiv.org/abs/2408.02922</link><description>&lt;p&gt;
Pose Magic: Efficient and Temporally Consistent Human Pose Estimation with a Hybrid Mamba-GCN Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02922
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Pose Magic的混合Mamba-GCN网络，实现了高效且时间一致的人体姿势估计，通过结合Mamba的高质量长范围建模能力和GCN的局部增强效果，从而在保持准确性的同时提高了计算效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02922v1 Announce Type: new  Abstract: Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are primarily based on Transformers. However, existing Transformer-based 3D HPE backbones often encounter a trade-off between accuracy and computational efficiency. To resolve the above dilemma, in this work, leveraging recent advances in state space models, we utilize Mamba for high-quality and efficient long-range modeling. Nonetheless, Mamba still faces challenges in precisely exploiting the local dependencies between joints. To address these issues, we propose a new attention-free hybrid spatiotemporal architecture named Hybrid Mamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN by capturing relationships between neighboring joints, thus producing new representations to complement Mamba's outputs. By adaptively fusing representations from Mamba and GCN, Pose Magic demonstrates superior capability in learning the underlying 3D structu
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为“双视图金字塔池化”（Dual-View Pyramid Pooling，DVPP）的方法，旨在对深度神经网络中的空间特征和像素级特征进行全面的聚合，以提高医学图像分类的准确性并校准分类器的信心。通过分析空间特征和像素级特征的区别，该研究来解决传统池化方法在特征表达和分类精度上存在的问题，从而提供更好的医学图像分类结果。</title><link>https://arxiv.org/abs/2408.02906</link><description>&lt;p&gt;
Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical Image Classification and Confidence Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02906
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为“双视图金字塔池化”（Dual-View Pyramid Pooling，DVPP）的方法，旨在对深度神经网络中的空间特征和像素级特征进行全面的聚合，以提高医学图像分类的准确性并校准分类器的信心。通过分析空间特征和像素级特征的区别，该研究来解决传统池化方法在特征表达和分类精度上存在的问题，从而提供更好的医学图像分类结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02906v1 Announce Type: new  Abstract: Spatial pooling (SP) and cross-channel pooling (CCP) operators have been applied to aggregate spatial features and pixel-wise features from feature maps in deep neural networks (DNNs), respectively. Their main goal is to reduce computation and memory overhead without visibly weakening the performance of DNNs. However, SP often faces the problem of losing the subtle feature representations, while CCP has a high possibility of ignoring salient feature representations, which may lead to both miscalibration of confidence issues and suboptimal medical classification results. To address these problems, we propose a novel dual-view framework, the first to systematically investigate the relative roles of SP and CCP by analyzing the difference between spatial features and pixel-wise features. Based on this framework, we propose a new pooling method, termed dual-view pyramid pooling (DVPP), to aggregate multi-scale dual-view features. DVPP aims to
&lt;/p&gt;</description></item><item><title>该文章提出了一种新颖的两阶段框架，用于准确识别埃及车辆牌照上的阿拉伯文字。该框架首先通过图像处理技术可靠地定位牌照，然后使用定制设计的深度学习模型进行阿拉伯字符识别。该系统在多样化数据集上取得了99.3%的准确率，超过现有方法。其潜在应用包括智能交通管理，如交通违规检测和停车场优化。未来研究将进一步通过架构改进、扩大数据集和解决系统依赖问题来增强系统的性能。</title><link>https://arxiv.org/abs/2408.02904</link><description>&lt;p&gt;
Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02904
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新颖的两阶段框架，用于准确识别埃及车辆牌照上的阿拉伯文字。该框架首先通过图像处理技术可靠地定位牌照，然后使用定制设计的深度学习模型进行阿拉伯字符识别。该系统在多样化数据集上取得了99.3%的准确率，超过现有方法。其潜在应用包括智能交通管理，如交通违规检测和停车场优化。未来研究将进一步通过架构改进、扩大数据集和解决系统依赖问题来增强系统的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02904v1 Announce Type: new  Abstract: This paper introduces a novel two-stage framework for accurate Egyptian Vehicle License Plate Recognition (EVLPR). The first stage employs image processing techniques to reliably localize license plates, while the second stage utilizes a custom-designed deep learning model for robust Arabic character recognition. The proposed system achieves a remarkable 99.3% accuracy on a diverse dataset, surpassing existing approaches. Its potential applications extend to intelligent traffic management, including traffic violation detection and parking optimization. Future research will focus on enhancing the system's capabilities through architectural refinements, expanded datasets, and addressing system dependencies.
&lt;/p&gt;</description></item><item><title>该文章提出Lighthouse，一个综合性的、可复现的视频时刻检索和突出检测（MR-HD）用户友好型库。Lighthouse旨在解决现有研究中缺乏跨方法、数据集和视频文本特征的全面和可复现实验的问题，并在统一训练和评估代码库中涵盖多种设置，同时通过提供一个可复用的代码基、六个模型、三种特征和五个数据集以及在API和网页演示上的易访问性，使得研究者和开发者能够轻松地使用这些方法。</title><link>https://arxiv.org/abs/2408.02901</link><description>&lt;p&gt;
Lighthouse: A User-Friendly Library for Reproducible Video Moment Retrieval and Highlight Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02901
&lt;/p&gt;
&lt;p&gt;
该文章提出Lighthouse，一个综合性的、可复现的视频时刻检索和突出检测（MR-HD）用户友好型库。Lighthouse旨在解决现有研究中缺乏跨方法、数据集和视频文本特征的全面和可复现实验的问题，并在统一训练和评估代码库中涵盖多种设置，同时通过提供一个可复用的代码基、六个模型、三种特征和五个数据集以及在API和网页演示上的易访问性，使得研究者和开发者能够轻松地使用这些方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02901v1 Announce Type: new  Abstract: We propose Lighthouse, a user-friendly library for reproducible video moment retrieval and highlight detection (MR-HD). Although researchers proposed various MR-HD approaches, the research community holds two main issues. The first is a lack of comprehensive and reproducible experiments across various methods, datasets, and video-text features. This is because no unified training and evaluation codebase covers multiple settings. The second is user-unfriendly design. Because previous works use different libraries, researchers set up individual environments. In addition, most works release only the training codes, requiring users to implement the whole inference process of MR-HD. Lighthouse addresses these issues by implementing a unified reproducible codebase that includes six models, three features, and five datasets. In addition, it provides an inference API and web demo to make these methods easily accessible for researchers and develo
&lt;/p&gt;</description></item><item><title>该文章介绍了MedTrinity-25M，这是一个涵盖了2500万张图像的医学多模态大型数据集，其包含10种不同的模态，且被标注了超过65种疾病的多层次信息。数据集中的标注不仅包括全球性的文本信息，如疾病/病变类型、模态、特定区域描述，还包括对感兴趣区域（ROIs）的详细本地标注，包括边界框和分割掩码。与那些仅限于图像-文本对的数据集相比，我们开发了一种自动化的管线，能够在不依赖任何文本描述的情况下，生成多层次的视觉和文本标注（以图像-ROI-描述三元组的形式）。此外，数据来自于90多个不同的源头，已经过预处理和基于领域专家的模型进行定位。</title><link>https://arxiv.org/abs/2408.02900</link><description>&lt;p&gt;
MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02900
&lt;/p&gt;
&lt;p&gt;
该文章介绍了MedTrinity-25M，这是一个涵盖了2500万张图像的医学多模态大型数据集，其包含10种不同的模态，且被标注了超过65种疾病的多层次信息。数据集中的标注不仅包括全球性的文本信息，如疾病/病变类型、模态、特定区域描述，还包括对感兴趣区域（ROIs）的详细本地标注，包括边界框和分割掩码。与那些仅限于图像-文本对的数据集相比，我们开发了一种自动化的管线，能够在不依赖任何文本描述的情况下，生成多层次的视觉和文本标注（以图像-ROI-描述三元组的形式）。此外，数据来自于90多个不同的源头，已经过预处理和基于领域专家的模型进行定位。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02900v1 Announce Type: new  Abstract: This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert model
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于扩散模型的数据增强方法，能够在维护数据语义协调性的同时，显著提升数据集的多样性，并通过实验证明了其在物体检测任务中的有效性和优越性。</title><link>https://arxiv.org/abs/2408.02891</link><description>&lt;p&gt;
Diverse Generation while Maintaining Semantic Coordination: A Diffusion-Based Data Augmentation Method for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02891
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于扩散模型的数据增强方法，能够在维护数据语义协调性的同时，显著提升数据集的多样性，并通过实验证明了其在物体检测任务中的有效性和优越性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02891v1 Announce Type: new  Abstract: Recent studies emphasize the crucial role of data augmentation in enhancing the performance of object detection models. However,existing methodologies often struggle to effectively harmonize dataset diversity with semantic coordination.To bridge this gap, we introduce an innovative augmentation technique leveraging pre-trained conditional diffusion models to mediate this balance. Our approach encompasses the development of a Category Affinity Matrix, meticulously designed to enhance dataset diversity, and a Surrounding Region Alignment strategy, which ensures the preservation of semantic coordination in the augmented images. Extensive experimental evaluations confirm the efficacy of our method in enriching dataset diversity while seamlessly maintaining semantic coordination. Our method yields substantial average improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives on three distinct object detection models, respectively.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为VizECGNet的模型，它使用视觉ECG图像网络来通过结合多模态训练和知识蒸馏技术，仅使用打印的ECG图形便能够准确地对多种心血管疾病进行分类。</title><link>https://arxiv.org/abs/2408.02888</link><description>&lt;p&gt;
VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02888
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为VizECGNet的模型，它使用视觉ECG图像网络来通过结合多模态训练和知识蒸馏技术，仅使用打印的ECG图形便能够准确地对多种心血管疾病进行分类。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02888v1 Announce Type: new  Abstract: An electrocardiogram (ECG) captures the heart's electrical signal to assess various heart conditions. In practice, ECG data is stored as either digitized signals or printed images. Despite the emergence of numerous deep learning models for digitized signals, many hospitals prefer image storage due to cost considerations. Recognizing the unavailability of raw ECG signals in many clinical settings, we propose VizECGNet, which uses only printed ECG graphics to determine the prognosis of multiple cardiovascular diseases. During training, cross-modal attention modules (CMAM) are used to integrate information from two modalities - image and signal, while self-modality attention modules (SMAM) capture inherent long-range dependencies in ECG data of each modality. Additionally, we utilize knowledge distillation to improve the similarity between two distinct predictions from each modality stream. This innovative multi-modal deep learning architec
&lt;/p&gt;</description></item><item><title>该文章提出了一种能够模拟现实人物行为的全端到端网络，包括言语和非言语的交流能力，能够进行实时双工交流，填补了先前系统存在的现实人类代理的空白。</title><link>https://arxiv.org/abs/2408.02879</link><description>&lt;p&gt;
Body of Her: A Preliminary Study on End-to-End Humanoid Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02879
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种能够模拟现实人物行为的全端到端网络，包括言语和非言语的交流能力，能够进行实时双工交流，填补了先前系统存在的现实人类代理的空白。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02879v1 Announce Type: new  Abstract: Interactive virtual humanoid agent is a crucial interface with the physical world. A relatively complete humanoid agent first needs to have face and body, then possess both verbal and non-verbal (such as eye contact, facial expression, lip motion, gesture, and manipulation) abilities, and finally, it is capable of real-time duplex communication, e.g., the ability to actively interrupt conversations. Most prior systems typically only consider a subset of these elements, leaving a gap from realistic humanoid agent. In this work, we propose a real-time, duplex, interactive end-to-end network capable of modeling realistic agent behaviors, including speech, full-body movements for talking, responding, idling, and manipulation. This system is a multimodal model integrating audio and visual inputs, extended from a pre-trained large language model (LLM). We collect approximately 200,000 hours of audio, around 130,000 hours of video data, and abo
&lt;/p&gt;</description></item><item><title>该文章描述了一个名为VisionUnite的全新面向眼科的视觉语言基础模型，它增强了临床知识，并在1.24亿图像文本对的大型数据集上进行了预训练。VisionUnite在诊断能力上超越了现有的生成性基础模型，如GPT-4V和Gemini Pro，并且在多病种诊断、临床解释和病人互动等临床场景中表现出色，有望成为辅助专科医师的有力工具。</title><link>https://arxiv.org/abs/2408.02865</link><description>&lt;p&gt;
VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02865
&lt;/p&gt;
&lt;p&gt;
该文章描述了一个名为VisionUnite的全新面向眼科的视觉语言基础模型，它增强了临床知识，并在1.24亿图像文本对的大型数据集上进行了预训练。VisionUnite在诊断能力上超越了现有的生成性基础模型，如GPT-4V和Gemini Pro，并且在多病种诊断、临床解释和病人互动等临床场景中表现出色，有望成为辅助专科医师的有力工具。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02865v1 Announce Type: cross  Abstract: The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly vers
&lt;/p&gt;</description></item><item><title>该文章提出了Madeleine（Madeleine），一种多模态预训练策略，用于从包括免疫组化在内的多标记染色切片中获取丰富的任务无关训练信号，从而在海平面巨像素全切片图像中学习全面的、可转移的表示形式。</title><link>https://arxiv.org/abs/2408.02859</link><description>&lt;p&gt;
Multistain Pretraining for Slide Representation Learning in Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02859
&lt;/p&gt;
&lt;p&gt;
该文章提出了Madeleine（Madeleine），一种多模态预训练策略，用于从包括免疫组化在内的多标记染色切片中获取丰富的任务无关训练信号，从而在海平面巨像素全切片图像中学习全面的、可转移的表示形式。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02859v1 Announce Type: cross  Abstract: Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&amp;amp;E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224 x 224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce Madeleine, a multimodal pretraining strategy for slide representation
&lt;/p&gt;</description></item><item><title>该文章创新地比较了利用RGB-D相机捕捉的3D运动数据和通过OpenPose、BlazePose等算法从2DRGB视频中估计的3D人体姿势数据的性能，提出了对物理康复训练中患者动作进行评估的新方法。通过使用Gaussian Mixture Model（GMM）作为评估工具，该研究为数据分析效率和机器人教练系统中算法性能的分析提供了新的见解。</title><link>https://arxiv.org/abs/2408.02855</link><description>&lt;p&gt;
Analyzing Data Efficiency and Performance of Machine Learning Algorithms for Assessing Low Back Pain Physical Rehabilitation Exercises
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02855
&lt;/p&gt;
&lt;p&gt;
该文章创新地比较了利用RGB-D相机捕捉的3D运动数据和通过OpenPose、BlazePose等算法从2DRGB视频中估计的3D人体姿势数据的性能，提出了对物理康复训练中患者动作进行评估的新方法。通过使用Gaussian Mixture Model（GMM）作为评估工具，该研究为数据分析效率和机器人教练系统中算法性能的分析提供了新的见解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02855v1 Announce Type: cross  Abstract: Analyzing human motion is an active research area, with various applications. In this work, we focus on human motion analysis in the context of physical rehabilitation using a robot coach system. Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose estimation from RGB images had made impressive improvements, we aim to compare the assessment of physical rehabilitation exercises using movement data obtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB videos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is employed from position (and orientation) features, with performance metrics defined based on the log-likelihood values from GMM. The evaluation is performed on a medical database of 
&lt;/p&gt;</description></item><item><title>该文章提出了GAReT方法，通过使用地理适配器(GeoAdapter)和自回归变换器(Auto-Regressive Transformers)，实现了无需相机和里程计数据的跨视角视频地理定位，解决了现有CVGL方法面临的数据获取困难、计算效率低以及预测结果时间不一致等问题。</title><link>https://arxiv.org/abs/2408.02840</link><description>&lt;p&gt;
GAReT: Cross-view Video Geolocalization with Adapters and Auto-Regressive Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02840
&lt;/p&gt;
&lt;p&gt;
该文章提出了GAReT方法，通过使用地理适配器(GeoAdapter)和自回归变换器(Auto-Regressive Transformers)，实现了无需相机和里程计数据的跨视角视频地理定位，解决了现有CVGL方法面临的数据获取困难、计算效率低以及预测结果时间不一致等问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02840v1 Announce Type: new  Abstract: Cross-view video geo-localization (CVGL) aims to derive GPS trajectories from street-view videos by aligning them with aerial-view images. Despite their promising performance, current CVGL methods face significant challenges. These methods use camera and odometry data, typically absent in real-world scenarios. They utilize multiple adjacent frames and various encoders for feature extraction, resulting in high computational costs. Moreover, these approaches independently predict each street-view frame's location, resulting in temporally inconsistent GPS trajectories. To address these challenges, in this work, we propose GAReT, a fully transformer-based method for CVGL that does not require camera and odometry data. We introduce GeoAdapter, a transformer-adapter module designed to efficiently aggregate image-level representations and adapt them for video inputs. Specifically, we train a transformer encoder on video frames and aerial images
&lt;/p&gt;</description></item><item><title>该文章介绍了一个名为DaCapo的模块化深度学习框架，该框架能够加速针对大规模近等轴对称图像数据的机器学习方法训练和应用。DaCapo框架特别优化了对于这一领域的特性，包括其模块化结构、有效的实验管理工具以及可扩展的部署能力。文章强调了DaCapo在改善对大规模等轴对称图像分割技术的访问方面的潜力，并鼓励社区对该开源项目进行探索和贡献。</title><link>https://arxiv.org/abs/2408.02834</link><description>&lt;p&gt;
DaCapo: a modular deep learning framework for scalable 3D image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02834
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一个名为DaCapo的模块化深度学习框架，该框架能够加速针对大规模近等轴对称图像数据的机器学习方法训练和应用。DaCapo框架特别优化了对于这一领域的特性，包括其模块化结构、有效的实验管理工具以及可扩展的部署能力。文章强调了DaCapo在改善对大规模等轴对称图像分割技术的访问方面的潜力，并鼓励社区对该开源项目进行探索和贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02834v1 Announce Type: new  Abstract: DaCapo is a specialized deep learning library tailored to expedite the training and application of existing machine learning approaches on large, near-isotropic image data. In this correspondence, we introduce DaCapo's unique features optimized for this specific domain, highlighting its modular structure, efficient experiment management tools, and scalable deployment capabilities. We discuss its potential to improve access to large-scale, isotropic image segmentation and invite the community to explore and contribute to this open-source initiative.
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于模型信心分数的方法，用于评估客户端模型更新的不确定性，从而检测并防御恶意客户端的攻击。这种方法能够有效地应对模型中毒和数据中毒等多种恶意攻击，并提高了对潜在数据中毒攻击的检测和处理能力，如标签翻转和标签混乱。此外，该方法能够适当地平衡精度和鲁棒性，从而在对抗不同类型的攻击时保持较高的检测准确率。</title><link>https://arxiv.org/abs/2408.02813</link><description>&lt;p&gt;
Mitigating Malicious Attacks in Federated Learning via Confidence-aware Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02813
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于模型信心分数的方法，用于评估客户端模型更新的不确定性，从而检测并防御恶意客户端的攻击。这种方法能够有效地应对模型中毒和数据中毒等多种恶意攻击，并提高了对潜在数据中毒攻击的检测和处理能力，如标签翻转和标签混乱。此外，该方法能够适当地平衡精度和鲁棒性，从而在对抗不同类型的攻击时保持较高的检测准确率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02813v1 Announce Type: cross  Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm that allows multiple clients to collaboratively train a global model without sharing private local data. However, FL systems are vulnerable to attacks from malicious clients, who can degrade the global model performance through data poisoning and model poisoning. Existing defense methods typically focus on a single type of attack, such as Byzantine attacks or backdoor attacks, and are often ineffective against potential data poisoning attacks like label flipping and label shuffling. Additionally, these methods often lack accuracy and robustness in detecting and handling malicious updates. To address these issues, we propose a novel method based on model confidence scores, which evaluates the uncertainty of client model updates to detect and defend against malicious clients. Our approach is comprehensively effective for both model poisoning and data poisoning a
&lt;/p&gt;</description></item><item><title>该文章描述了一种名为SiCo的虚拟试衣新方法，它通过考虑用户的身体尺寸和衣物尺寸之间的相互作用，为用户提供了更准确的衣物展示效果，从而帮助用户做出更有信息支持的购买决策。</title><link>https://arxiv.org/abs/2408.02803</link><description>&lt;p&gt;
SiCo: A Size-Controllable Virtual Try-On Approach for Informed Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02803
&lt;/p&gt;
&lt;p&gt;
该文章描述了一种名为SiCo的虚拟试衣新方法，它通过考虑用户的身体尺寸和衣物尺寸之间的相互作用，为用户提供了更准确的衣物展示效果，从而帮助用户做出更有信息支持的购买决策。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02803v1 Announce Type: cross  Abstract: Virtual try-on (VTO) applications aim to improve the online shopping experience by allowing users to preview garments, before making purchase decisions. However, many VTO tools fail to consider the crucial relationship between a garment's size and the user's body size, often employing a one-size-fits-all approach when visualizing a clothing item. This results in poor size recommendations and purchase decisions leading to increased return rates. To address this limitation, we introduce SiCo, an online VTO system, where users can upload images of themselves and visualize how different sizes of clothing would look on their body to help make better-informed purchase decisions. Our user study shows SiCo's superiority over baseline VTO. The results indicate that our approach significantly enhances user ability to gauge the appearance of outfits on their bodies and boosts their confidence in selecting clothing sizes that match desired goals. 
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于高斯混合模型的 evidential learning 方法，该模型认为图像数据应由多个高斯分布组成，并采用逆伽马分布作为中间先验，以提高深度估计的精确度和模型不确定性捕捉能力，在 Scene Flow、KITTI 2015 和 Middlebury 2014 数据集上进行了有效性验证。</title><link>https://arxiv.org/abs/2408.02796</link><description>&lt;p&gt;
Gaussian Mixture based Evidential Learning for Stereo Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02796
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于高斯混合模型的 evidential learning 方法，该模型认为图像数据应由多个高斯分布组成，并采用逆伽马分布作为中间先验，以提高深度估计的精确度和模型不确定性捕捉能力，在 Scene Flow、KITTI 2015 和 Middlebury 2014 数据集上进行了有效性验证。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02796v1 Announce Type: new  Abstract: In this paper, we introduce a novel Gaussian mixture based evidential learning solution for robust stereo matching. Diverging from previous evidential deep learning approaches that rely on a single Gaussian distribution, our framework posits that individual image data adheres to a mixture-of-Gaussian distribution in stereo matching. This assumption yields more precise pixel-level predictions and more accurately mirrors the real-world image distribution. By further employing the inverse-Gamma distribution as an intermediary prior for each mixture component, our probabilistic model achieves improved depth estimation compared to its counterpart with the single Gaussian and effectively captures the model uncertainty, which enables a strong cross-domain generation ability. We evaluated our method for stereo matching by training the model using the Scene Flow dataset and testing it on KITTI 2015 and Middlebury 2014. The experiment results cons
&lt;/p&gt;</description></item><item><title>该文章提出了一种深度学习模型，能够准确预测皮肤病变从皮肤图像中获得的抬升标签，并通过在一个新收集的Dataset上进行测试，证明了该模型在跨域识别方面的有效性。</title><link>https://arxiv.org/abs/2408.02792</link><description>&lt;p&gt;
Lesion Elevation Prediction from Skin Images Improves Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02792
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种深度学习模型，能够准确预测皮肤病变从皮肤图像中获得的抬升标签，并通过在一个新收集的Dataset上进行测试，证明了该模型在跨域识别方面的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02792v1 Announce Type: new  Abstract: While deep learning-based computer-aided diagnosis for skin lesion image analysis is approaching dermatologists' performance levels, there are several works showing that incorporating additional features such as shape priors, texture, color constancy, and illumination further improves the lesion diagnosis performance. In this work, we look at another clinically useful feature, skin lesion elevation, and investigate the feasibility of predicting and leveraging skin lesion elevation labels. Specifically, we use a deep learning model to predict image-level lesion elevation labels from 2D skin lesion images. We test the elevation prediction accuracy on the derm7pt dataset, and use the elevation prediction model to estimate elevation labels for images from five other datasets: ISIC 2016, 2017, and 2018 Challenge datasets, MSK, and DermoFit. We evaluate cross-domain generalization by using these estimated elevation labels as auxiliary inputs t
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为GazeXplain的方法，该方法通过学习来预测视觉扫描路径的自然语言解释，填补了传统扫描模型仅预测视线转移而不提供解释的空白。该方法包括对视线固定的人自然语言解释进行注释，并提出了一个包含注意力-语言解码器的通用模型，该模型能够同时预测扫描路径和生成解释。通过整合一种独特的语义对齐机制来提高解释与视线固定的一致性，以及一种跨数据集协同训练方法来提高泛化能力。这些创新为可解释的人类视觉扫描路径预测提供了一个全面的、可适应的解决方案。</title><link>https://arxiv.org/abs/2408.02788</link><description>&lt;p&gt;
GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02788
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为GazeXplain的方法，该方法通过学习来预测视觉扫描路径的自然语言解释，填补了传统扫描模型仅预测视线转移而不提供解释的空白。该方法包括对视线固定的人自然语言解释进行注释，并提出了一个包含注意力-语言解码器的通用模型，该模型能够同时预测扫描路径和生成解释。通过整合一种独特的语义对齐机制来提高解释与视线固定的一致性，以及一种跨数据集协同训练方法来提高泛化能力。这些创新为可解释的人类视觉扫描路径预测提供了一个全面的、可适应的解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02788v1 Announce Type: new  Abstract: While exploring visual scenes, humans' scanpaths are driven by their underlying attention processes. Understanding visual scanpaths is essential for various applications. Traditional scanpath models predict the where and when of gaze shifts without providing explanations, creating a gap in understanding the rationale behind fixations. To bridge this gap, we introduce GazeXplain, a novel study of visual scanpath prediction and explanation. This involves annotating natural-language explanations for fixations across eye-tracking datasets and proposing a general model with an attention-language decoder that jointly predicts scanpaths and generates explanations. It integrates a unique semantic alignment mechanism to enhance the consistency between fixations and explanations, alongside a cross-dataset co-training approach for generalization. These novelties present a comprehensive and adaptable solution for explainable human visual scanpath pr
&lt;/p&gt;</description></item><item><title>该文章提出了StyleSeg方法，该方法通过学习多样化的分割风格，在没有任何标注者对应知识的情况下，从皮肤病变图像及其掩膜对中提高分割性能。该方法在四个公开的皮肤病变分割数据集上表现出了优于其他方法的性能，并且对最大的多标注者皮肤病变分割数据集（ISIC-MultiAnnot）进行了情感一致性的预测，并揭示了预测风格与标注者偏好之间的强相关性。</title><link>https://arxiv.org/abs/2408.02787</link><description>&lt;p&gt;
Segmentation Style Discovery: Application to Skin Lesion Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02787
&lt;/p&gt;
&lt;p&gt;
该文章提出了StyleSeg方法，该方法通过学习多样化的分割风格，在没有任何标注者对应知识的情况下，从皮肤病变图像及其掩膜对中提高分割性能。该方法在四个公开的皮肤病变分割数据集上表现出了优于其他方法的性能，并且对最大的多标注者皮肤病变分割数据集（ISIC-MultiAnnot）进行了情感一致性的预测，并揭示了预测风格与标注者偏好之间的强相关性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02787v1 Announce Type: new  Abstract: Variability in medical image segmentation, arising from annotator preferences, expertise, and their choice of tools, has been well documented. While the majority of multi-annotator segmentation approaches focus on modeling annotator-specific preferences, they require annotator-segmentation correspondence. In this work, we introduce the problem of segmentation style discovery, and propose StyleSeg, a segmentation method that learns plausible, diverse, and semantically consistent segmentation styles from a corpus of image-mask pairs without any knowledge of annotator correspondence. StyleSeg consistently outperforms competing methods on four publicly available skin lesion segmentation (SLS) datasets. We also curate ISIC-MultiAnnot, the largest multi-annotator SLS dataset with annotator correspondence, and our results show a strong alignment, using our newly proposed measure AS2, between the predicted styles and annotator preferences. The c
&lt;/p&gt;</description></item><item><title>该文章提出了一个轻量级且鲁棒的红外小目标检测网络LR-Net，通过构建轻量级的特征提取注意力模块和简单的特征细化传输模块，实现了对目标特征的有效提取和信息交流，显著提高了网络的特征精细提取能力，同时在资源消耗上取得了较好的平衡。</title><link>https://arxiv.org/abs/2408.02780</link><description>&lt;p&gt;
LR-Net: A Lightweight and Robust Network for Infrared Small Target Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02780
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个轻量级且鲁棒的红外小目标检测网络LR-Net，通过构建轻量级的特征提取注意力模块和简单的特征细化传输模块，实现了对目标特征的有效提取和信息交流，显著提高了网络的特征精细提取能力，同时在资源消耗上取得了较好的平衡。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02780v1 Announce Type: new  Abstract: Limited by equipment limitations and the lack of target intrinsic features, existing infrared small target detection methods have difficulty meeting actual comprehensive performance requirements. Therefore, we propose an innovative lightweight and robust network (LR-Net), which abandons the complex structure and achieves an effective balance between detection accuracy and resource consumption. Specifically, to ensure the lightweight and robustness, on the one hand, we construct a lightweight feature extraction attention (LFEA) module, which can fully extract target features and strengthen information interaction across channels. On the other hand, we construct a simple refined feature transfer (RFT) module. Compared with direct cross-layer connections, the RFT module can improve the network's feature refinement extraction capability with little resource consumption. Meanwhile, to solve the problem of small target loss in high-level featu
&lt;/p&gt;</description></item><item><title>该文章提出了一种具有单点监督的精炼红外小目标检测方案，通过引入评估标签进化框架，结合TTA和CRF技术提升了目标检测的准确性，并通过可调灵敏度策略提高了检测率。</title><link>https://arxiv.org/abs/2408.02773</link><description>&lt;p&gt;
Refined Infrared Small Target Detection Scheme with Single-Point Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02773
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种具有单点监督的精炼红外小目标检测方案，通过引入评估标签进化框架，结合TTA和CRF技术提升了目标检测的准确性，并通过可调灵敏度策略提高了检测率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02773v1 Announce Type: new  Abstract: Recently, infrared small target detection with single-point supervision has attracted extensive attention. However, the detection accuracy of existing methods has difficulty meeting actual needs. Therefore, we propose an innovative refined infrared small target detection scheme with single-point supervision, which has excellent segmentation accuracy and detection rate. Specifically, we introduce label evolution with single point supervision (LESPS) framework and explore the performance of various excellent infrared small target detection networks based on this framework. Meanwhile, to improve the comprehensive performance, we construct a complete post-processing strategy. On the one hand, to improve the segmentation accuracy, we use a combination of test-time augmentation (TTA) and conditional random field (CRF) for post-processing. On the other hand, to improve the detection rate, we introduce an adjustable sensitivity (AS) strategy for
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为"Anticipation via Recognition and Reasoning (ARR)"的全新端到端视频建模架构，它通过分解动作预测任务为动作识别和序列推理任务，并结合下一步动作预测（NAP）方法，有效学习了不同动作之间的统计关系。与现有基于时间聚合的方法相比，ARR能够在学习动作之间的统计联系方面表现得更为有效。</title><link>https://arxiv.org/abs/2408.02769</link><description>&lt;p&gt;
From Recognition to Prediction: Leveraging Sequence Reasoning for Action Anticipation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02769
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为"Anticipation via Recognition and Reasoning (ARR)"的全新端到端视频建模架构，它通过分解动作预测任务为动作识别和序列推理任务，并结合下一步动作预测（NAP）方法，有效学习了不同动作之间的统计关系。与现有基于时间聚合的方法相比，ARR能够在学习动作之间的统计联系方面表现得更为有效。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02769v1 Announce Type: new  Abstract: The action anticipation task refers to predicting what action will happen based on observed videos, which requires the model to have a strong ability to summarize the present and then reason about the future. Experience and common sense suggest that there is a significant correlation between different actions, which provides valuable prior knowledge for the action anticipation task. However, previous methods have not effectively modeled this underlying statistical relationship. To address this issue, we propose a novel end-to-end video modeling architecture that utilizes attention mechanisms, named Anticipation via Recognition and Reasoning (ARR). ARR decomposes the action anticipation task into action recognition and sequence reasoning tasks, and effectively learns the statistical relationship between actions by next action prediction (NAP). In comparison to existing temporal aggregation strategies, ARR is able to extract more effective
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为ConDL的深学习框架，用于估计稠密图像对应关系，通过在全卷积模型中生成图像的稠密特征图，每个像素都被赋予了能够跨多张图像匹配的描述符。与以往的方法不同，该模型在包含各种扭曲的合成数据上进行了训练，包括视角变化、光照差异、阴影和反光亮点等。该文章使用对比学习来增强特征图对这些扭曲的鲁棒性，从而实现匹配结果的稳固性。此外，该方法的独特之处在于它不需要关键点检测器，这显著区别于许多现有的图像匹配技术。</title><link>https://arxiv.org/abs/2408.02766</link><description>&lt;p&gt;
ConDL: Detector-Free Dense Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02766
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为ConDL的深学习框架，用于估计稠密图像对应关系，通过在全卷积模型中生成图像的稠密特征图，每个像素都被赋予了能够跨多张图像匹配的描述符。与以往的方法不同，该模型在包含各种扭曲的合成数据上进行了训练，包括视角变化、光照差异、阴影和反光亮点等。该文章使用对比学习来增强特征图对这些扭曲的鲁棒性，从而实现匹配结果的稳固性。此外，该方法的独特之处在于它不需要关键点检测器，这显著区别于许多现有的图像匹配技术。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02766v1 Announce Type: new  Abstract: In this work, we introduce a deep-learning framework designed for estimating dense image correspondences. Our fully convolutional model generates dense feature maps for images, where each pixel is associated with a descriptor that can be matched across multiple images. Unlike previous methods, our model is trained on synthetic data that includes significant distortions, such as perspective changes, illumination variations, shadows, and specular highlights. Utilizing contrastive learning, our feature maps achieve greater invariance to these distortions, enabling robust matching. Notably, our method eliminates the need for a keypoint detector, setting it apart from many existing image-matching techniques.
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于余弦相似度的半监督正则化策略，用于缓解自监督学习中可能出现的过拟合并降低数据依赖性。通过结合监督标签和自监督任务的弱标签，该方法能够在不增加额外标注成本的前提下，增强模型对未见过的输入数据的泛化能力。实验结果表明，该方法在多种视觉任务上取得了显著的性能提升，包括图像分类、目标检测和语义分割，并且在具有挑战性的域外分布数据集上表现出了较强的适应性。</title><link>https://arxiv.org/abs/2408.02761</link><description>&lt;p&gt;
Dimensionality Reduction and Nearest Neighbors for Improving Out-of-Distribution Detection in Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02761
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于余弦相似度的半监督正则化策略，用于缓解自监督学习中可能出现的过拟合并降低数据依赖性。通过结合监督标签和自监督任务的弱标签，该方法能够在不增加额外标注成本的前提下，增强模型对未见过的输入数据的泛化能力。实验结果表明，该方法在多种视觉任务上取得了显著的性能提升，包括图像分类、目标检测和语义分割，并且在具有挑战性的域外分布数据集上表现出了较强的适应性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02761v1 Announce Type: new  Abstract: Clinically deployed deep learning-based segmentation models are known to fail on data outside of their training distributions. While clinicians review the segmentations, these models tend to perform well in most instances, which could exacerbate automation bias. Therefore, detecting out-of-distribution images at inference is critical to warn the clinicians that the model likely failed. This work applied the Mahalanobis distance (MD) post hoc to the bottleneck features of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted magnetic resonance imaging and computed tomography. By reducing the dimensions of the bottleneck features with either principal component analysis or uniform manifold approximation and projection, images the models failed on were detected with high performance and minimal computational load. In addition, this work explored a non-parametric alternative to the MD, a k-th nearest neighbors distance (
&lt;/p&gt;</description></item><item><title>该文章提出将用于图像合成的生成模型转化为数据挖掘工具，通过预先训练的扩散模型来评估训练数据中视觉元素的典型性，这种方法能够高效地通过生成模型来概括数据，并对于特定标签（如地理位置、时间戳、标签或疾病）进行可视化分析，展示了在处理大量数据时的高效性。</title><link>https://arxiv.org/abs/2408.02752</link><description>&lt;p&gt;
Diffusion Models as Data Mining Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02752
&lt;/p&gt;
&lt;p&gt;
该文章提出将用于图像合成的生成模型转化为数据挖掘工具，通过预先训练的扩散模型来评估训练数据中视觉元素的典型性，这种方法能够高效地通过生成模型来概括数据，并对于特定标签（如地理位置、时间戳、标签或疾病）进行可视化分析，展示了在处理大量数据时的高效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02752v1 Announce Type: new  Abstract: This paper demonstrates how to use generative models trained for image synthesis as tools for visual data mining. Our insight is that since contemporary generative models learn an accurate representation of their training data, we can use them to summarize the data by mining for visual patterns. Concretely, we show that after finetuning conditional diffusion models to synthesize images from a specific dataset, we can use these models to define a typicality measure on that dataset. This measure assesses how typical visual elements are for different data labels, such as geographic location, time stamps, semantic labels, or even the presence of a disease. This analysis-by-synthesis approach to data mining has two key advantages. First, it scales much better than traditional correspondence-based approaches since it does not require explicitly comparing all pairs of visual elements. Second, while most previous works on visual data mining focu
&lt;/p&gt;</description></item><item><title>该文章提出了一种仅使用合成的、不泄露身份信息的红外图片的隐私安全红外呈现攻击检测方法。通过合成符合ISO/IEC 19794-6标准的红外图片，以及使用两种不同的生成模型来模拟真实身份和无身份泄露的合成样本来检测攻击。通过这种方式，该方法在图像合成模型上进行训练，并在使用传统方法评价时展现出了良好的性能。</title><link>https://arxiv.org/abs/2408.02750</link><description>&lt;p&gt;
Privacy-Safe Iris Presentation Attack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02750
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种仅使用合成的、不泄露身份信息的红外图片的隐私安全红外呈现攻击检测方法。通过合成符合ISO/IEC 19794-6标准的红外图片，以及使用两种不同的生成模型来模拟真实身份和无身份泄露的合成样本来检测攻击。通过这种方式，该方法在图像合成模型上进行训练，并在使用传统方法评价时展现出了良好的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02750v1 Announce Type: new  Abstract: This paper proposes a framework for a privacy-safe iris presentation attack detection (PAD) method, designed solely with synthetically-generated, identity-leakage-free iris images. Once trained, the method is evaluated in a classical way using state-of-the-art iris PAD benchmarks. We designed two generative models for the synthesis of ISO/IEC 19794-6-compliant iris images. The first model synthesizes bona fide-looking samples. To avoid ``identity leakage,'' the generated samples that accidentally matched those used in the model's training were excluded. The second model synthesizes images of irises with textured contact lenses and is conditioned by a given contact lens brand to have better control over textured contact lens appearance when forming the training set. Our experiments demonstrate that models trained solely on synthetic data achieve a lower but still reasonable performance when compared to solutions trained with iris images c
&lt;/p&gt;</description></item><item><title>该文章开发了一种名为MMIU的基准测试，旨在评估大型视觉语言模型的多模态多图像理解能力，并通过7种多图像关系、52个任务和11,000多个精心策划的多选择问题，提供了迄今为止最全面的benchmark，揭示了包括顶级模型在内的多图像理解挑战，并且在多图像理解和空间理解任务上表现不佳。</title><link>https://arxiv.org/abs/2408.02718</link><description>&lt;p&gt;
MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02718
&lt;/p&gt;
&lt;p&gt;
该文章开发了一种名为MMIU的基准测试，旨在评估大型视觉语言模型的多模态多图像理解能力，并通过7种多图像关系、52个任务和11,000多个精心策划的多选择问题，提供了迄今为止最全面的benchmark，揭示了包括顶级模型在内的多图像理解挑战，并且在多图像理解和空间理解任务上表现不佳。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02718v1 Announce Type: new  Abstract: The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Thro
&lt;/p&gt;</description></item><item><title>该文章介绍了RCDM（Robust Conditional Diffusion Model），一种通过控制理论动态减少噪声影响并显著增强模型鲁棒的轻量级解决方案，旨在改善条件扩散模型（CDM）在处理不当的特定输入时出现的缺陷问题，并使其在各种复杂任务中更加适用。</title><link>https://arxiv.org/abs/2408.02710</link><description>&lt;p&gt;
RCDM: Enabling Robustness for Conditional Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02710
&lt;/p&gt;
&lt;p&gt;
该文章介绍了RCDM（Robust Conditional Diffusion Model），一种通过控制理论动态减少噪声影响并显著增强模型鲁棒的轻量级解决方案，旨在改善条件扩散模型（CDM）在处理不当的特定输入时出现的缺陷问题，并使其在各种复杂任务中更加适用。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02710v1 Announce Type: cross  Abstract: The conditional diffusion model (CDM) enhances the standard diffusion model by providing more control, improving the quality and relevance of the outputs, and making the model adaptable to a wider range of complex tasks. However, inaccurate conditional inputs in the inverse process of CDM can easily lead to generating fixed errors in the neural network, which diminishes the adaptability of a well-trained model. The existing methods like data augmentation, adversarial training, robust optimization can improve the robustness, while they often face challenges such as high computational complexity, limited applicability to unknown perturbations, and increased training difficulty. In this paper, we propose a lightweight solution, the Robust Conditional Diffusion Model (RCDM), based on control theory to dynamically reduce the impact of noise and significantly enhance the model's robustness. RCDM leverages the collaborative interaction betwee
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于手写笔交互分割的医学 hyperspectral 图像方法，该方法利用用户知识结合临床洞察，成功克服了深度学习方法在分割 hyperspectral 图像时面临的限制，实现了精确的分割结果。</title><link>https://arxiv.org/abs/2408.02708</link><description>&lt;p&gt;
Scribble-Based Interactive Segmentation of Medical Hyperspectral Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02708
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于手写笔交互分割的医学 hyperspectral 图像方法，该方法利用用户知识结合临床洞察，成功克服了深度学习方法在分割 hyperspectral 图像时面临的限制，实现了精确的分割结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02708v1 Announce Type: cross  Abstract: Hyperspectral imaging (HSI) is an advanced medical imaging modality that captures optical data across a broad spectral range, providing novel insights into the biochemical composition of tissues. HSI may enable precise differentiation between various tissue types and pathologies, making it particularly valuable for tumour detection, tissue classification, and disease diagnosis.   Deep learning-based segmentation methods have shown considerable advancements, offering automated and accurate results. However, these methods face challenges with HSI datasets due to limited annotated data and discrepancies from hardware and acquisition techniques~\cite{clancy2020surgical,studier2023heiporspectral}. Variability in clinical protocols also leads to different definitions of structure boundaries. Interactive segmentation methods, utilizing user knowledge and clinical insights, can overcome these issues and achieve precise segmentation results \ci
&lt;/p&gt;</description></item><item><title>该文章提出了一个Compositional Physical Reasoning (ComPhy)数据集，旨在通过有限的视频内容让模型能够推断出对象的物理属性（如质量和电荷），并预测这些属性在交互中的动态效果。此外，研究还评价了模型回答相关问题的能力。通过训练和测试模型，该方法在理解和模拟自然界的物理过程方面显示出潜在的贡献。</title><link>https://arxiv.org/abs/2408.02687</link><description>&lt;p&gt;
Compositional Physical Reasoning of Objects and Events from Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02687
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个Compositional Physical Reasoning (ComPhy)数据集，旨在通过有限的视频内容让模型能够推断出对象的物理属性（如质量和电荷），并预测这些属性在交互中的动态效果。此外，研究还评价了模型回答相关问题的能力。通过训练和测试模型，该方法在理解和模拟自然界的物理过程方面显示出潜在的贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02687v1 Announce Type: new  Abstract: Understanding and reasoning about objects' physical properties in the natural world is a fundamental challenge in artificial intelligence. While some properties like colors and shapes can be directly observed, others, such as mass and electric charge, are hidden from the objects' visual appearance. This paper addresses the unique challenge of inferring these hidden physical properties from objects' motion and interactions and predicting corresponding dynamics based on the inferred physical properties. We first introduce the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes limited videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions. Besides the synthetic videos from simulators, we also collect a real-wo
&lt;/p&gt;</description></item><item><title>该文章发现并分析了英国生物银行数据库中基于眼底图像的疾病分类模型存在的巨大偏差问题，即使在整体模型表现出色的情况下，不同评估中心的个体仍然面临显著的不公平表现。这项研究不仅揭示了数据标准化过程中潜在的歧视问题，而且对比了多种现有偏差缓解方法的适用性，发现针对不同的偏差问题，不同的缓解方法效果差异甚大，这表明需要开发针对特定问题定制化的缓解偏差的方法。研究最终表明，当前的缓解手段在提高模型的公平性方面效果有限，进一步强调了对公平性算法的高要求和继续研究的重要性。</title><link>https://arxiv.org/abs/2408.02676</link><description>&lt;p&gt;
On Biases in a UK Biobank-based Retinal Image Classification Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02676
&lt;/p&gt;
&lt;p&gt;
该文章发现并分析了英国生物银行数据库中基于眼底图像的疾病分类模型存在的巨大偏差问题，即使在整体模型表现出色的情况下，不同评估中心的个体仍然面临显著的不公平表现。这项研究不仅揭示了数据标准化过程中潜在的歧视问题，而且对比了多种现有偏差缓解方法的适用性，发现针对不同的偏差问题，不同的缓解方法效果差异甚大，这表明需要开发针对特定问题定制化的缓解偏差的方法。研究最终表明，当前的缓解手段在提高模型的公平性方面效果有限，进一步强调了对公平性算法的高要求和继续研究的重要性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02676v1 Announce Type: cross  Abstract: Recent work has uncovered alarming disparities in the performance of machine learning models in healthcare. In this study, we explore whether such disparities are present in the UK Biobank fundus retinal images by training and evaluating a disease classification model on these images. We assess possible disparities across various population groups and find substantial differences despite strong overall performance of the model. In particular, we discover unfair performance for certain assessment centres, which is surprising given the rigorous data standardisation protocol. We compare how these differences emerge and apply a range of existing bias mitigation methods to each one. A key insight is that each disparity has unique properties and responds differently to the mitigation methods. We also find that these methods are largely unable to enhance fairness, highlighting the need for better bias mitigation methods tailored to the specif
&lt;/p&gt;</description></item><item><title>该文章提出了一种在机器学习系统对抗攻击中应用意图混淆技术的新策略，通过在目标对象邻近添加非重叠的扰动对象来隐藏攻击意图。研究团队进行了随机实验，以测试5种流行的对象检测器（YOLOv3、SSD、RetinaNet、Faster R-CNN和Cascade R-CNN）对这种攻击的响应。实验结果表明，所有检测器在面对目标和非目标攻击时均实现了成功。研究还分析了成功因素，包括目标对象的可信度和扰动对象的大小，并建议攻击者利用这些因素来进一步提高攻击的成功率。</title><link>https://arxiv.org/abs/2408.02674</link><description>&lt;p&gt;
On Feasibility of Intent Obfuscating Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02674
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种在机器学习系统对抗攻击中应用意图混淆技术的新策略，通过在目标对象邻近添加非重叠的扰动对象来隐藏攻击意图。研究团队进行了随机实验，以测试5种流行的对象检测器（YOLOv3、SSD、RetinaNet、Faster R-CNN和Cascade R-CNN）对这种攻击的响应。实验结果表明，所有检测器在面对目标和非目标攻击时均实现了成功。研究还分析了成功因素，包括目标对象的可信度和扰动对象的大小，并建议攻击者利用这些因素来进一步提高攻击的成功率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02674v1 Announce Type: cross  Abstract: Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose incorporating intent obfuscation in generating adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors -- YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN -- using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attack
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EI-VLG的长期视频语言地面（VLG）方法，该方法通过利用多模态大型语言模型提供的大量文本信息来模拟人类经验，有效排除无关视频片段，增强了视频-语言共同表征的能力，解决了长视频语言地面问题中忽略非相关信息的问题。</title><link>https://arxiv.org/abs/2408.02336</link><description>&lt;p&gt;
Infusing Environmental Captions for Long-Form Video Language Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02336
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EI-VLG的长期视频语言地面（VLG）方法，该方法通过利用多模态大型语言模型提供的大量文本信息来模拟人类经验，有效排除无关视频片段，增强了视频-语言共同表征的能力，解决了长视频语言地面问题中忽略非相关信息的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.
&lt;/p&gt;</description></item><item><title>该文章提出了一个无需源数据的新型方法，用于在两个不同数据分布的源和目标域之间进行有效的性能预测。通过使用生成模型进行校准，并借助不确定性和温度调节，该文章展现了在缺乏源数据的情况下进行准确性评估的方法，并在对象识别基准数据集上得到了验证。</title><link>https://arxiv.org/abs/2408.02209</link><description>&lt;p&gt;
Source-Free Domain-Invariant Performance Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02209
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个无需源数据的新型方法，用于在两个不同数据分布的源和目标域之间进行有效的性能预测。通过使用生成模型进行校准，并借助不确定性和温度调节，该文章展现了在缺乏源数据的情况下进行准确性评估的方法，并在对象识别基准数据集上得到了验证。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02209v1 Announce Type: new  Abstract: Accurately estimating model performance poses a significant challenge, particularly in scenarios where the source and target domains follow different data distributions. Most existing performance prediction methods heavily rely on the source data in their estimation process, limiting their applicability in a more realistic setting where only the trained model is accessible. The few methods that do not require source data exhibit considerably inferior performance. In this work, we propose a source-free approach centred on uncertainty-based estimation, using a generative model for calibration in the absence of source data. We establish connections between our approach for unsupervised calibration and temperature scaling. We then employ a gradient-based strategy to evaluate the correctness of the calibrated predictions. Our experiments on benchmark object recognition datasets reveal that existing source-based methods fall short with limited
&lt;/p&gt;</description></item><item><title>该文章聚焦于数据评估和选择在指令微调大型语言模型中的应用，全面梳理了现有文献，为这类策略提供了一种分类清晰、层级精细的体系结构，以提高指令微调的效率和效果。</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
该文章聚焦于数据评估和选择在指令微调大型语言模型中的应用，全面梳理了现有文献，为这类策略提供了一种分类清晰、层级精细的体系结构，以提高指令微调的效率和效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>该文章系统评估了针对物体检测的开放源攻击方法和模型鲁棒性，并提供了增强对攻击效果和相应缓解措施理解的观察结果。同时，文章还指出了未来在保护自动化物体检测系统安全中需要解决的重大研究挑战。</title><link>https://arxiv.org/abs/2408.01934</link><description>&lt;p&gt;
A Survey and Evaluation of Adversarial Attacks for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01934
&lt;/p&gt;
&lt;p&gt;
该文章系统评估了针对物体检测的开放源攻击方法和模型鲁棒性，并提供了增强对攻击效果和相应缓解措施理解的观察结果。同时，文章还指出了未来在保护自动化物体检测系统安全中需要解决的重大研究挑战。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01934v1 Announce Type: new  Abstract: Deep learning models excel in various computer vision tasks but are susceptible to adversarial examples-subtle perturbations in input data that lead to incorrect predictions. This vulnerability poses significant risks in safety-critical applications such as autonomous vehicles, security surveillance, and aircraft health monitoring. While numerous surveys focus on adversarial attacks in image classification, the literature on such attacks in object detection is limited. This paper offers a comprehensive taxonomy of adversarial attacks specific to object detection, reviews existing adversarial robustness evaluation metrics, and systematically assesses open-source attack methods and model robustness. Key observations are provided to enhance the understanding of attack effectiveness and corresponding countermeasures. Additionally, we identify crucial research challenges to guide future efforts in securing automated object detection systems.
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为FBSDiff的新方法，它可以将频率带替换到扩散特征中，用于高度可控的文本驱动图像翻译。通过这种方法，可以无需模型训练、微调和在线优化，直接将大型预训练文本到图像扩散模型转换为图像到图像模型，实现高质量和灵活的文本驱动的图像翻译。</title><link>https://arxiv.org/abs/2408.00998</link><description>&lt;p&gt;
FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00998
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为FBSDiff的新方法，它可以将频率带替换到扩散特征中，用于高度可控的文本驱动图像翻译。通过这种方法，可以无需模型训练、微调和在线优化，直接将大型预训练文本到图像扩散模型转换为图像到图像模型，实现高质量和灵活的文本驱动的图像翻译。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00998v2 Announce Type: replace  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I gener
&lt;/p&gt;</description></item><item><title>该文章介绍了Segment Anything Model 2 (SAM 2)在2D和3D医学图像中的广泛应用，这是对现有技术的显著扩展。通过对18种医学图像数据集的评估，证明了SAM 2在多帧3D分割和单帧2D分割中的有效性，展示了其在CT、MRI、PET等3D和X光、超声等2D医学图像分割领域的创新贡献。</title><link>https://arxiv.org/abs/2408.00756</link><description>&lt;p&gt;
Segment anything model 2: an application to 2D and 3D medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00756
&lt;/p&gt;
&lt;p&gt;
该文章介绍了Segment Anything Model 2 (SAM 2)在2D和3D医学图像中的广泛应用，这是对现有技术的显著扩展。通过对18种医学图像数据集的评估，证明了SAM 2在多帧3D分割和单帧2D分割中的有效性，展示了其在CT、MRI、PET等3D和X光、超声等2D医学图像分割领域的创新贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00756v2 Announce Type: replace  Abstract: Segment Anything Model (SAM) has gained significant attention because of its ability to segment varous objects in images given a prompt. The recently developed SAM 2 has extended this ability to video inputs. This opens an opportunity to apply SAM to 3D images, one of the fundamental tasks in the medical imaging field. In this paper, we extensively evaluate SAM 2's ability to segment both 2D and 3D medical images by first collecting 18 medical imaging datasets, including common 3D modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) as well as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of SAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are provided to one or multiple slice(s) selected from the volume, and (2) single-frame 2D segmentation, where prompts are provided to each slice. The former is only applicable to 3D modaliti
&lt;/p&gt;</description></item><item><title>该文章描述了在诊断和评估患者组织样本方面，病理学的重要性及其在手术和活检样本中所起的作用。随着整个滑片扫描器和深度学习技术的进步，病理学AI技术得到了显著的发展，减轻了病理学家的负担，并为治疗计划提供了支持。文章提到，与传统AI相比，更准确和广泛应用于各种任务的AI模型，即基础模型（FMs），最近在病理学领域取得了进展。文章指出，这类基础模型已经在病理学中得到了广泛的应用，包括疾病诊断、罕见疾病的诊断、患者生存预后预测和生物标志物表达研究等领域。</title><link>https://arxiv.org/abs/2407.21317</link><description>&lt;p&gt;
Pathology Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21317
&lt;/p&gt;
&lt;p&gt;
该文章描述了在诊断和评估患者组织样本方面，病理学的重要性及其在手术和活检样本中所起的作用。随着整个滑片扫描器和深度学习技术的进步，病理学AI技术得到了显著的发展，减轻了病理学家的负担，并为治疗计划提供了支持。文章提到，与传统AI相比，更准确和广泛应用于各种任务的AI模型，即基础模型（FMs），最近在病理学领域取得了进展。文章指出，这类基础模型已经在病理学中得到了广泛的应用，包括疾病诊断、罕见疾病的诊断、患者生存预后预测和生物标志物表达研究等领域。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21317v2 Announce Type: replace  Abstract: Pathology has played a crucial role in the diagnosis and evaluation of patient tissue samples obtained from surgeries and biopsies for many years. The advent of Whole Slide Scanners and the development of deep learning technologies have significantly advanced the field, leading to extensive research and development in pathology AI (Artificial Intelligence). These advancements have contributed to reducing the workload of pathologists and supporting decision-making in treatment plans. Recently, large-scale AI models known as Foundation Models (FMs), which are more accurate and applicable to a wide range of tasks compared to traditional AI, have emerged, and expanded their application scope in the healthcare field. Numerous FMs have been developed in pathology, and there are reported cases of their application in various tasks, such as disease diagnosis, rare cancer diagnosis, patient survival prognosis prediction, biomarker expression 
&lt;/p&gt;</description></item><item><title>该文章提出的MMTrail是一个含大量预告片视频的大规模多模态视频-语言数据集，其中每个视频都对应有详细的视觉描述和多模态描述。这些描述涵盖了预告片的多种主题和管理，包括情节、视觉帧和背景音乐，这使得数据集在探索跨模态研究方面具有很高的价值。</title><link>https://arxiv.org/abs/2407.20962</link><description>&lt;p&gt;
MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20962
&lt;/p&gt;
&lt;p&gt;
该文章提出的MMTrail是一个含大量预告片视频的大规模多模态视频-语言数据集，其中每个视频都对应有详细的视觉描述和多模态描述。这些描述涵盖了预告片的多种主题和管理，包括情节、视觉帧和背景音乐，这使得数据集在探索跨模态研究方面具有很高的价值。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20962v2 Announce Type: replace  Abstract: Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions. Such ignorance results in the difficulty of multiple cross-modality studies. To fulfill this gap, we present MMTrail, a large-scale multi-modality video-language dataset incorporating more than 20M trailer clips with visual captions, and 2M high-quality clips with multimodal captions. Trailers preview full-length video works and integrate context, visual frames, and background music. In particular, the trailer has two main advantages: (1) the topics are diverse, and the content characters 
&lt;/p&gt;</description></item><item><title>该文章提出了一种通过自我监督的扩散过程训练CLIP模型以克服其视觉短板的简单方法。通过引入DIVA（DIffusion model as a Visual Assistant for CLIP），本文展示了使用扩散模型作为CLIP的视觉助手，显著提升了CLIP在视觉感知方面的性能。</title><link>https://arxiv.org/abs/2407.20171</link><description>&lt;p&gt;
Diffusion Feedback Helps CLIP See Better
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20171
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种通过自我监督的扩散过程训练CLIP模型以克服其视觉短板的简单方法。通过引入DIVA（DIffusion model as a Visual Assistant for CLIP），本文展示了使用扩散模型作为CLIP的视觉助手，显著提升了CLIP在视觉感知方面的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20171v2 Announce Type: replace  Abstract: Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedb
&lt;/p&gt;</description></item><item><title>该文章提出了一个新型的基于联合训练的图像模型，它能够解释深度模型的预测。通过在训练中同时优化一个选择器和预测器，该模型能够在不损害重要特征的情况下，有效地识别并解释图像中的关键区域。这种方法通过一个特定的目标函数，解决了现有的方法中存在的特征完全性和互锁问题，从而提供了更加强大且解释性的视觉模型。</title><link>https://arxiv.org/abs/2407.19308</link><description>&lt;p&gt;
Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19308
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个新型的基于联合训练的图像模型，它能够解释深度模型的预测。通过在训练中同时优化一个选择器和预测器，该模型能够在不损害重要特征的情况下，有效地识别并解释图像中的关键区域。这种方法通过一个特定的目标函数，解决了现有的方法中存在的特征完全性和互锁问题，从而提供了更加强大且解释性的视觉模型。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19308v2 Announce Type: replace  Abstract: As deep vision models' popularity rapidly increases, there is a growing emphasis on explanations for model predictions. The inherently explainable attribution method aims to enhance the understanding of model behavior by identifying the important regions in images that significantly contribute to predictions. It is achieved by cooperatively training a selector (generating an attribution map to identify important features) and a predictor (making predictions using the identified features). Despite many advancements, existing methods suffer from the incompleteness problem, where discriminative features are masked out, and the interlocking problem, where the non-optimized selector initially selects noise, causing the predictor to fit on this noise and perpetuate the cycle. To address these problems, we introduce a new objective that discourages the presence of discriminative features in the masked-out regions thus enhancing the comprehe
&lt;/p&gt;</description></item><item><title>该文章提出了一种带有多个轻量级解码器的多解码器场景表示网络（MDSRN），能够为输入坐标生成多个可能预测，并计算它们的均值作为多解码器集合的预测，以及它们的方差作为可信度分数，从而支持推理时预测质量评估。</title><link>https://arxiv.org/abs/2407.19082</link><description>&lt;p&gt;
Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19082
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种带有多个轻量级解码器的多解码器场景表示网络（MDSRN），能够为输入坐标生成多个可能预测，并计算它们的均值作为多解码器集合的预测，以及它们的方差作为可信度分数，从而支持推理时预测质量评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19082v2 Announce Type: replace-cross  Abstract: Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN) ensemble architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CityX的全新多模式可控的程序化内容生成方法，可使用多种布局指令生成现实感十足、规模不受限的3D虚拟城市，并能实现对城市布局的精细控制。</title><link>https://arxiv.org/abs/2407.17572</link><description>&lt;p&gt;
CityX: Controllable Procedural Content Generation for Unbounded 3D Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17572
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CityX的全新多模式可控的程序化内容生成方法，可使用多种布局指令生成现实感十足、规模不受限的3D虚拟城市，并能实现对城市布局的精细控制。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17572v3 Announce Type: replace  Abstract: Generating a realistic, large-scale 3D virtual city remains a complex challenge due to the involvement of numerous 3D assets, various city styles, and strict layout constraints. Existing approaches provide promising attempts at procedural content generation to create large-scale scenes using Blender agents. However, they face crucial issues such as difficulties in scaling up generation capability and achieving fine-grained control at the semantic layout level. To address these problems, we propose a novel multi-modal controllable procedural content generation method, named CityX, which enhances realistic, unbounded 3D city generation guided by multiple layout conditions, including OSM, semantic maps, and satellite images. Specifically, the proposed method contains a general protocol for integrating various PCG plugins and a multi-agent framework for transforming instructions into executable Blender actions. Through this effective fra
&lt;/p&gt;</description></item><item><title>该文章提出MMRA (Multi-Granularity and Multi-Image Relational Association) 基准，用于评估大型视觉语言模型在多维度和多图像关系关联方面的能力。通过精心编制的1024个样本，文章推动了图像间关联关系的任务研究，通过11个不同层次的子任务，如“使用相似性”和“子事件”，对当前的视觉语言模型进行全面的评估。</title><link>https://arxiv.org/abs/2407.17379</link><description>&lt;p&gt;
MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image Relational Association Capabilities in Large Visual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17379
&lt;/p&gt;
&lt;p&gt;
该文章提出MMRA (Multi-Granularity and Multi-Image Relational Association) 基准，用于评估大型视觉语言模型在多维度和多图像关系关联方面的能力。通过精心编制的1024个样本，文章推动了图像间关联关系的任务研究，通过11个不同层次的子任务，如“使用相似性”和“子事件”，对当前的视觉语言模型进行全面的评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17379v2 Announce Type: replace  Abstract: Given the remarkable success that large visual language models (LVLMs) have achieved in image perception tasks, the endeavor to make LVLMs perceive the world like humans is drawing increasing attention. Current multi-modal benchmarks primarily focus on facts or specific topic-related knowledge contained within individual images. However, they often overlook the associative relations between multiple images, which require the identification and analysis of similarities among entities or content present in different images. Therefore, we propose the multi-image relation association task and a meticulously curated Multi-granularity Multi-image Relational Association (MMRA) benchmark, comprising 1,024 samples. In order to systematically and comprehensively evaluate current LVLMs, we establish an associational relation system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent) at two granularity levels (i.e., image and 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为多模态共学习（MMCL）的框架，用于在训练阶段通过结合多种模态特征进行共学习，并在推理阶段只使用简洁的骨骼信息。这种方法整合了大型语言模型来辅助骨骼动作识别的训练，以充分利用不同模态特征之间的互补性，同时保持了系统的效率。通过这种方式，该研究旨在提高骨骼动作识别的性能，并减少对其他详细身体信息的依赖。</title><link>https://arxiv.org/abs/2407.15706</link><description>&lt;p&gt;
Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15706
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为多模态共学习（MMCL）的框架，用于在训练阶段通过结合多种模态特征进行共学习，并在推理阶段只使用简洁的骨骼信息。这种方法整合了大型语言模型来辅助骨骼动作识别的训练，以充分利用不同模态特征之间的互补性，同时保持了系统的效率。通过这种方式，该研究旨在提高骨骼动作识别的性能，并减少对其他详细身体信息的依赖。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15706v5 Announce Type: replace  Abstract: Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为DiffX的扩散模型，这是一种用于跨模态生成的布局指导模型。DiffX模型在共享 latent 空间中执行扩散和去噪过程，并通过引入具有门控注意力机制的Joint-Modality Embedder（JME）和用于长描述嵌入的Long-CLIP，有效地增强了布局和文本条件的交互。</title><link>https://arxiv.org/abs/2407.15488</link><description>&lt;p&gt;
DiffX: Guide Your Layout to Cross-Modal Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15488
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为DiffX的扩散模型，这是一种用于跨模态生成的布局指导模型。DiffX模型在共享 latent 空间中执行扩散和去噪过程，并通过引入具有门控注意力机制的Joint-Modality Embedder（JME）和用于长描述嵌入的Long-CLIP，有效地增强了布局和文本条件的交互。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15488v3 Announce Type: replace  Abstract: Diffusion models have made significant strides in language-driven and layout-driven image generation. However, most diffusion models are limited to visible RGB image generation. In fact, human perception of the world is enriched by diverse viewpoints, such as chromatic contrast, thermal illumination, and depth information. In this paper, we introduce a novel diffusion model for general layout-guided cross-modal generation, called DiffX. Notably, DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space. Moreover, we introduce the Joint-Modality Embedder (JME) to enhance interaction between layout and text conditions by incorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is employed for long caption embedding for user instruction. To facilitate the user-instructed generative training, we construct the cro
&lt;/p&gt;</description></item><item><title>该文章提出了一种利用跨相关性捕捉物体时序信息的新学习方法，解决了探测器与特征提取器之间的竞争问题，并通过在连续帧的热图上学习更丰富的运动特征，改善了直接将重识别任务嵌入到多目标跟踪中的问题，使提取的特征具有更高的辨别能力，从而提高了跟踪性能。</title><link>https://arxiv.org/abs/2407.14086</link><description>&lt;p&gt;
Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.14086
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种利用跨相关性捕捉物体时序信息的新学习方法，解决了探测器与特征提取器之间的竞争问题，并通过在连续帧的热图上学习更丰富的运动特征，改善了直接将重识别任务嵌入到多目标跟踪中的问题，使提取的特征具有更高的辨别能力，从而提高了跟踪性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.14086v2 Announce Type: replace  Abstract: Joint Detection and Embedding (JDE) trackers have demonstrated excellent performance in Multi-Object Tracking (MOT) tasks by incorporating the extraction of appearance features as auxiliary tasks through embedding Re-Identification task (ReID) into the detector, achieving a balance between inference speed and tracking performance. However, solving the competition between the detector and the feature extractor has always been a challenge. Meanwhile, the issue of directly embedding the ReID task into MOT has remained unresolved. The lack of high discriminability in appearance features results in their limited utility. In this paper, a new learning approach using cross-correlation to capture temporal information of objects is proposed. The feature extraction network is no longer trained solely on appearance features from each frame but learns richer motion features by utilizing feature heatmaps from consecutive frames, which addresses t
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为IMAGDressing-v1的系统，它借助先进的生成模型实现了更真实的虚拟试衣效果，特别地，该系统可以让用户自由编辑人物图像，并可根据需求添加或更改服装、脸部、姿势和背景场景。此外，文章还引入了一种名为CAMI的综合性一致性评估指标，用以验证所生成图像与参考服装的一致性。通过一个独特的混合注意力模块，IMAGDressing-v1能够将服装的特征与图像的其他特征有效地结合，从而提供了高保真度的虚拟试衣体验。</title><link>https://arxiv.org/abs/2407.12705</link><description>&lt;p&gt;
IMAGDressing-v1: Customizable Virtual Dressing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.12705
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为IMAGDressing-v1的系统，它借助先进的生成模型实现了更真实的虚拟试衣效果，特别地，该系统可以让用户自由编辑人物图像，并可根据需求添加或更改服装、脸部、姿势和背景场景。此外，文章还引入了一种名为CAMI的综合性一致性评估指标，用以验证所生成图像与参考服装的一致性。通过一个独特的混合注意力模块，IMAGDressing-v1能够将服装的特征与图像的其他特征有效地结合，从而提供了高保真度的虚拟试衣体验。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.12705v2 Announce Type: replace  Abstract: Latest advances have achieved realistic virtual try-on (VTON) through localized garment inpainting using latent diffusion models, significantly enhancing consumers' online shopping experience. However, existing VTON technologies neglect the need for merchants to showcase garments comprehensively, including flexible control over garments, optional faces, poses, and scenes. To address this issue, we define a virtual dressing (VD) task focused on generating freely editable human images with fixed garments and optional conditions. Meanwhile, we design a comprehensive affinity metric index (CAMI) to evaluate the consistency between generated images and reference garments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet that captures semantic features from CLIP and texture features from VAE. We present a hybrid attention module, including a frozen self-attention and a trainable cross-attention, to integrate garment feat
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CCVA-FL的系统，用于在医疗影像领域中通过联邦学习解决来自不同客户端的图像数据之间的差异化问题，并介绍了一种旨在最小化跨客户端差异的方法，涉及使用Scalable Diffusion Models with Transformers（DiT）生成反映目标客户端图像数据的合成图像，这些合成图像被用于共享以帮助其他客户端的图像数据进入目标客户端的图像空间，从而在分布式医疗影像数据中实现更好的模型训练效果。</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CCVA-FL的系统，用于在医疗影像领域中通过联邦学习解决来自不同客户端的图像数据之间的差异化问题，并介绍了一种旨在最小化跨客户端差异的方法，涉及使用Scalable Diffusion Models with Transformers（DiT）生成反映目标客户端图像数据的合成图像，这些合成图像被用于共享以帮助其他客户端的图像数据进入目标客户端的图像空间，从而在分布式医疗影像数据中实现更好的模型训练效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v4 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>该文章提出了VCHAR框架，一种基于variances驱动的复杂人类活动识别方法，该方法通过生成表示来有效识别和解释视频中的复杂活动，从而减少了传统的标签化负担。</title><link>https://arxiv.org/abs/2407.03291</link><description>&lt;p&gt;
VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.03291
&lt;/p&gt;
&lt;p&gt;
该文章提出了VCHAR框架，一种基于variances驱动的复杂人类活动识别方法，该方法通过生成表示来有效识别和解释视频中的复杂活动，从而减少了传统的标签化负担。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.03291v2 Announce Type: replace-cross  Abstract: Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evalu
&lt;/p&gt;</description></item><item><title>该文章创新性地提出一个名为InterleavedBench的基准测试，这是首个专门用于评估随意交织文本和图像生成的工具，涵盖了多种现实世界场景。同时，文章还开发了InterleavedEval，一个基于GPT-4o的强大无基准评估指标，以确保准确性和开放性场景的质量评估。</title><link>https://arxiv.org/abs/2406.14643</link><description>&lt;p&gt;
Holistic Evaluation for Interleaved Text-and-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.14643
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出一个名为InterleavedBench的基准测试，这是首个专门用于评估随意交织文本和图像生成的工具，涵盖了多种现实世界场景。同时，文章还开发了InterleavedEval，一个基于GPT-4o的强大无基准评估指标，以确保准确性和开放性场景的质量评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.14643v2 Announce Type: replace  Abstract: Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为OpenECAD的模型，通过精细调整预训练模型，它能够生成具有编辑能力的3D CAD设计，从而提高了制造过程的效率。</title><link>https://arxiv.org/abs/2406.09913</link><description>&lt;p&gt;
OpenECAD: An Efficient Visual Language Model for Editable 3D-CAD Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.09913
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为OpenECAD的模型，通过精细调整预训练模型，它能够生成具有编辑能力的3D CAD设计，从而提高了制造过程的效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.09913v3 Announce Type: replace  Abstract: Computer-aided design (CAD) tools are utilized in the manufacturing industry for modeling everything from cups to spacecraft. These programs are complex to use and typically require years of training and experience to master. Structured and well-constrained 2D sketches and 3D constructions are crucial components of CAD modeling. A well-executed CAD model can be seamlessly integrated into the manufacturing process, thereby enhancing production efficiency. Deep generative models of 3D shapes and 3D object reconstruction models have garnered significant research interest. However, most of these models produce discrete forms of 3D objects that are not editable. Moreover, the few models based on CAD operations often have substantial input restrictions. In this work, we fine-tuned pre-trained models to create OpenECAD models (0.55B, 0.89B, 2.4B and 3.1B), leveraging the visual, logical, coding, and general capabilities of visual language m
&lt;/p&gt;</description></item><item><title>该文章提出一个开放平台GenAI-Arena，旨在通过用户参与评估不同类型的生成模型，如文本到图像、文本到视频以及图像编辑，以提供更民主和准确的评估模型性能的方法。</title><link>https://arxiv.org/abs/2406.04485</link><description>&lt;p&gt;
GenAI Arena: An Open Evaluation Platform for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04485
&lt;/p&gt;
&lt;p&gt;
该文章提出一个开放平台GenAI-Arena，旨在通过用户参与评估不同类型的生成模型，如文本到图像、文本到视频以及图像编辑，以提供更民主和准确的评估模型性能的方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04485v2 Announce Type: replace-cross  Abstract: Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source g
&lt;/p&gt;</description></item><item><title>该文章探讨了检索增强的图像 Captioning 模型在实践中存在的不稳定性问题，即模型可能因获取的误导性信息而产生错误。通过分析SmallCap模型，作者发现模型容易受到频繁出现在检索结果中的单词的影响，而这些单词也倾向于被模型复制到生成的描述中。为此，作者提出了一种解决方法，即通过从更具多样性的检索结果中采样，来减少模型学习复制高频单词的概率，从而提高了模型在类似领域任务中的性能。</title><link>https://arxiv.org/abs/2406.02265</link><description>&lt;p&gt;
Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.02265
&lt;/p&gt;
&lt;p&gt;
该文章探讨了检索增强的图像 Captioning 模型在实践中存在的不稳定性问题，即模型可能因获取的误导性信息而产生错误。通过分析SmallCap模型，作者发现模型容易受到频繁出现在检索结果中的单词的影响，而这些单词也倾向于被模型复制到生成的描述中。为此，作者提出了一种解决方法，即通过从更具多样性的检索结果中采样，来减少模型学习复制高频单词的概率，从而提高了模型在类似领域任务中的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.02265v3 Announce Type: replace  Abstract: Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain 
&lt;/p&gt;</description></item><item><title>该文章提出PT43D模型，这是一个概率变换器，能够从单个包含高度模糊视觉信息的RGB图像中生成3D形状。通过模拟训练样本和采用交叉注意力机制来识别输入图像中最重要的区域，该模型有效应对了现实场景中的遮挡和视角限制。</title><link>https://arxiv.org/abs/2405.11914</link><description>&lt;p&gt;
PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single Highly-Ambiguous RGB Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11914
&lt;/p&gt;
&lt;p&gt;
该文章提出PT43D模型，这是一个概率变换器，能够从单个包含高度模糊视觉信息的RGB图像中生成3D形状。通过模拟训练样本和采用交叉注意力机制来识别输入图像中最重要的区域，该模型有效应对了现实场景中的遮挡和视角限制。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11914v2 Announce Type: replace  Abstract: Generating 3D shapes from single RGB images is essential in various applications such as robotics. Current approaches typically target images containing clear and complete visual descriptions of the object, without considering common realistic cases where observations of objects that are largely occluded or truncated. We thus propose a transformer-based autoregressive model to generate the probabilistic distribution of 3D shapes conditioned on an RGB image containing potentially highly ambiguous observations of the object. To handle realistic scenarios such as occlusion or field-of-view truncation, we create simulated image-to-shape training pairs that enable improved fine-tuning for real-world scenarios. We then adopt cross-attention to effectively identify the most relevant region of interest from the input image for shape generation. This enables inference of sampled shapes with reasonable diversity and strong alignment with the i
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Dual-Task Vision Transformer（DTViT）的神经网络结构，旨在自动快速准确地分类脑CT图像中的出血性中风类型，并能根据出血部位的不同（如深部、脑叶下和脑叶出血）提供精确诊断，以支持及早制定个性化的治疗方案。</title><link>https://arxiv.org/abs/2405.06814</link><description>&lt;p&gt;
Dual-Task Vision Transformer for Rapid and Accurate Intracerebral Hemorrhage CT Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.06814
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Dual-Task Vision Transformer（DTViT）的神经网络结构，旨在自动快速准确地分类脑CT图像中的出血性中风类型，并能根据出血部位的不同（如深部、脑叶下和脑叶出血）提供精确诊断，以支持及早制定个性化的治疗方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.06814v3 Announce Type: replace  Abstract: Intracerebral hemorrhage (ICH) is a severe and sudden medical condition caused by the rupture of blood vessels in the brain, leading to permanent damage to brain tissue and often resulting in functional disabilities or death in patients. Diagnosis and analysis of ICH typically rely on brain CT imaging. Given the urgency of ICH conditions, early treatment is crucial, necessitating rapid analysis of CT images to formulate tailored treatment plans. However, the complexity of ICH CT images and the frequent scarcity of specialist radiologists pose significant challenges. Therefore, we collect a dataset from the real world for ICH and normal classification and three types of ICH image classification based on the hemorrhage location, i.e., Deep, Subcortical, and Lobar. In addition, we propose a neural network structure, dual-task vision transformer (DTViT), for the automated classification and diagnosis of ICH images. The DTViT deploys the 
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用连续布朗运动桥段扩散的帧插值方法，旨在确保视频帧插值的输出与ground truth一致，尽管Latent Diffusion Models在运行多个实例时会产生多样化的图像。</title><link>https://arxiv.org/abs/2405.05953</link><description>&lt;p&gt;
Frame Interpolation with Consecutive Brownian Bridge Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.05953
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用连续布朗运动桥段扩散的帧插值方法，旨在确保视频帧插值的输出与ground truth一致，尽管Latent Diffusion Models在运行多个实例时会产生多样化的图像。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.05953v4 Announce Type: replace  Abstract: Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, result
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CoReX的概念和关系基于的解释方法，用以在复杂的真实世界领域中（如生物学）揭示和评估卷积神经网络（CNN）的决策过程。该方法通过屏蔽不相关的概念和限制预测模型中可解释的替代模型中的关系，能够提供更深入的解释。此外，文章进行了多种图像数据集的实验，以评估CoReX在解释和评估CNN模型决策中的有效性。</title><link>https://arxiv.org/abs/2405.01661</link><description>&lt;p&gt;
When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01661
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CoReX的概念和关系基于的解释方法，用以在复杂的真实世界领域中（如生物学）揭示和评估卷积神经网络（CNN）的决策过程。该方法通过屏蔽不相关的概念和限制预测模型中可解释的替代模型中的关系，能够提供更深入的解释。此外，文章进行了多种图像数据集的实验，以评估CoReX在解释和评估CNN模型决策中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat
&lt;/p&gt;</description></item><item><title>该文章提出了一种通过调整概念瓶颈模型中的概念对齐来改进干预效果的方法。通过训练一个概念干预对齐模块，作者发现可以改善人类专家在模型决策过程中的干预效果，通过最小化对每个图像的干预次数，从而减少了获取人类反馈的成本。</title><link>https://arxiv.org/abs/2405.01531</link><description>&lt;p&gt;
Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01531
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种通过调整概念瓶颈模型中的概念对齐来改进干预效果的方法。通过训练一个概念干预对齐模块，作者发现可以改善人类专家在模型决策过程中的干预效果，通过最小化对每个图像的干预次数，从而减少了获取人类反馈的成本。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01531v2 Announce Type: replace-cross  Abstract: Concept Bottleneck Models (CBMs) ground image classification on human-understandable concepts to allow for interpretable model decisions. Crucially, the CBM design inherently allows for human interventions, in which expert users are given the ability to modify potentially misaligned concept choices to influence the decision behavior of the model in an interpretable fashion. However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human feedback is expensive. In this paper, we find that this is noticeably driven by an independent treatment of concepts during intervention, wherein a change of one concept does not influence the use of other ones in the model's final decision. To address this issue, we introduce a trainable concept intervention realignment module, which leverages concept relations to realign concept ass
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SimEndoGS的新型高效数据驱动的手术场景模拟方法，该方法使用机器人手术视频中的物理嵌入式3D高斯分布来模拟场景。这种方法能够从三维立体内窥镜视频中自动学习手术场景的三维结构，并在保证场景几何正确性的同时，利用深度监督和各向异性约束来防止过拟合，从而提供更真实和可扩大的模拟体验。</title><link>https://arxiv.org/abs/2405.00956</link><description>&lt;p&gt;
SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00956
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SimEndoGS的新型高效数据驱动的手术场景模拟方法，该方法使用机器人手术视频中的物理嵌入式3D高斯分布来模拟场景。这种方法能够从三维立体内窥镜视频中自动学习手术场景的三维结构，并在保证场景几何正确性的同时，利用深度监督和各向异性约束来防止过拟合，从而提供更真实和可扩大的模拟体验。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00956v3 Announce Type: replace  Abstract: Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy r
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为LMM-PCQA的方法，该方法通过给大型多模态模型（LMMs）提供文本监督来实现对点云质量评估的辅助。通过将质量标签转换为文本描述并在微调阶段使用，LMMs能够从2D点云投影中推导出质量评分logits。同时，还提取了结构特征来补偿3D领域感知能力上的损失。最后，将这些质量logits和结构特征结合起来进行回归，以获得质量分数。通过实验验证了该方法的有效性，展现了将LMMs集成到点云质量评估中的一种新颖方式，从而提高了模型在评估点云质量时的性能。</title><link>https://arxiv.org/abs/2404.18203</link><description>&lt;p&gt;
LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18203
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为LMM-PCQA的方法，该方法通过给大型多模态模型（LMMs）提供文本监督来实现对点云质量评估的辅助。通过将质量标签转换为文本描述并在微调阶段使用，LMMs能够从2D点云投影中推导出质量评分logits。同时，还提取了结构特征来补偿3D领域感知能力上的损失。最后，将这些质量logits和结构特征结合起来进行回归，以获得质量分数。通过实验验证了该方法的有效性，展现了将LMMs集成到点云质量评估中的一种新颖方式，从而提高了模型在评估点云质量时的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18203v2 Announce Type: replace  Abstract: Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model under
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为SafePaint的抗 forensic 图像修补框架，通过端到端训练有效地提高了图像修补的抗检测能力。</title><link>https://arxiv.org/abs/2404.18136</link><description>&lt;p&gt;
SafePaint: Anti-forensic Image Inpainting with Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18136
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为SafePaint的抗 forensic 图像修补框架，通过端到端训练有效地提高了图像修补的抗检测能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18136v2 Announce Type: replace  Abstract: Existing image inpainting methods have achieved remarkable accomplishments in generating visually appealing results, often accompanied by a trend toward creating more intricate structural textures. However, while these models excel at creating more realistic image content, they often leave noticeable traces of tampering, posing a significant threat to security. In this work, we take the anti-forensic capabilities into consideration, firstly proposing an end-to-end training framework for anti-forensic image inpainting named SafePaint. Specifically, we innovatively formulated image inpainting as two major tasks: semantically plausible content completion and region-wise optimization. The former is similar to current inpainting methods that aim to restore the missing regions of corrupted images. The latter, through domain adaptation, endeavors to reconcile the discrepancies between the inpainted region and the unaltered area to achieve a
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为MambaMOS的基于LiDAR的3D移动对象分割方法，该方法通过引入一个名为Time Clue Bootstrapping Embedding（TCBE）的新嵌入模块来增强点云中时空信息的耦合，并缓解了忽略时空线索的问题。同时，文章还引入了Motion-aware State Space Model（MSSM），以让模型能够理解不同时间步之间同一对象的空间状态，强调了移动对象的运动状态，从而在LiDAR点云数据中实现了更精确的移动对象分割。</title><link>https://arxiv.org/abs/2404.12794</link><description>&lt;p&gt;
MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12794
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为MambaMOS的基于LiDAR的3D移动对象分割方法，该方法通过引入一个名为Time Clue Bootstrapping Embedding（TCBE）的新嵌入模块来增强点云中时空信息的耦合，并缓解了忽略时空线索的问题。同时，文章还引入了Motion-aware State Space Model（MSSM），以让模型能够理解不同时间步之间同一对象的空间状态，强调了移动对象的运动状态，从而在LiDAR点云数据中实现了更精确的移动对象分割。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12794v2 Announce Type: replace-cross  Abstract: LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment moving objects in point clouds of the current scan using motion information from previous scans. Despite the promising results achieved by previous MOS methods, several key issues, such as the weak coupling of temporal and spatial information, still need further study. In this paper, we propose a novel LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model, termed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue Bootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial information in point clouds and alleviate the issue of overlooked temporal clues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to endow the model with the capacity to understand the temporal correlations of the same object across different time steps. Specifically, MSSM emphasizes the motion states of the sa
&lt;/p&gt;</description></item><item><title>该文章提出了一项名为强化学习中具有可泛化性的高斯点积法（Reinforcement Learning with Generalizable Gaussian Splatting）的先进技术，该技术通过3D高斯点积法为视觉增强型强化学习任务提供了高质量的环境表示，解决了之前环境表示存在的复杂几何描述不足、场景泛化能力不强、需要精确前景掩码等问题，并显著提高了表示的解读能力。</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
该文章提出了一项名为强化学习中具有可泛化性的高斯点积法（Reinforcement Learning with Generalizable Gaussian Splatting）的先进技术，该技术通过3D高斯点积法为视觉增强型强化学习任务提供了高质量的环境表示，解决了之前环境表示存在的复杂几何描述不足、场景泛化能力不强、需要精确前景掩码等问题，并显著提高了表示的解读能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为LoSA的特殊长短期范围适配器，用于针对端到端时序动作定位任务，能够有效适应和处理未裁剪的视频。这种适配器特别设计用于时序动作定位，并通过在视频 backbone 的中间层上引入特殊的长短期范围适配器，能够在不增加大量 GPU 内存消耗的情况下，显著减少内存足迹。此外，LoSA 还包括了一种长短期范围门控融合机制，能够有效地结合这些适配器的输出，从而提高了时序动作识别的准确性和效率。</title><link>https://arxiv.org/abs/2404.01282</link><description>&lt;p&gt;
LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01282
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为LoSA的特殊长短期范围适配器，用于针对端到端时序动作定位任务，能够有效适应和处理未裁剪的视频。这种适配器特别设计用于时序动作定位，并通过在视频 backbone 的中间层上引入特殊的长短期范围适配器，能够在不增加大量 GPU 内存消耗的情况下，显著减少内存足迹。此外，LoSA 还包括了一种长短期范围门控融合机制，能够有效地结合这些适配器的输出，从而提高了时序动作识别的准确性和效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01282v2 Announce Type: replace  Abstract: Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Gated Fusion that strategically combines the output of these adapter
&lt;/p&gt;</description></item><item><title>该文章提出了一种改进文本到图像模型中空间一致性的方法，通过创建SPRIGHT——一个用于描述6百万图像的专门用于空间描述的大数据集，并发现使用SPRIGHT数据集的一小部分（约0.25%）就可以显著提高22%的空间关系描述的比例，从而提升模型的空间推理能力。</title><link>https://arxiv.org/abs/2404.01197</link><description>&lt;p&gt;
Getting it Right: Improving Spatial Consistency in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01197
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种改进文本到图像模型中空间一致性的方法，通过创建SPRIGHT——一个用于描述6百万图像的专门用于空间描述的大数据集，并发现使用SPRIGHT数据集的一小部分（约0.25%）就可以显著提高22%的空间关系描述的比例，从而提升模型的空间推理能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01197v2 Announce Type: replace  Abstract: One of the key shortcomings in current text-to-image (T2I) models is their inability to consistently generate images which faithfully follow the spatial relationships specified in the text prompt. In this paper, we offer a comprehensive investigation of this limitation, while also developing datasets and methods that support algorithmic solutions to improve spatial reasoning in T2I models. We find that spatial relationships are under-represented in the image descriptions found in current vision-language datasets. To alleviate this data bottleneck, we create SPRIGHT, the first spatially focused, large-scale dataset, by re-captioning 6 million images from 4 widely used vision datasets and through a 3-fold evaluation and analysis pipeline, show that SPRIGHT improves the proportion of spatial relationships in existing datasets. We show the efficacy of SPRIGHT data by showing that using only $\sim$0.25% of SPRIGHT results in a 22% improve
&lt;/p&gt;</description></item><item><title>该文章针对 egocentric 手部互动中存在的挑战，尤其强调了 3D 手部-物体重建任务的困难性，并提出了进行此类任务在机器人、AR/VR、动作识别和运动生成等领域中的重要性。文章详细介绍了基于几何图形和图像识别的包围盒回归方法，对于修正因头部运动产生的模糊效应和解决 egocentric 相机特有的畸变问题提供了深入的分析。同时，文章展示了当前状态下最新基线和提交方法的表现，并对如何更有效解决重构问题进行了探讨。文章通过 HANDS23 挑战评估了在 ARCTIC 和 AssemblyHands 数据集上提出的解决方法，并强调了高容量 Transformer 在学习复杂手-物体交互方面的有效性，以及集成不同预测方法对提高重建精度具有重要意义。通过这一系列的分析，文章为未来研究提供了参考，特别是在手-物体交互和 egocentric 三维重建领域，强调了在重新审视基准方法、算法创新和技术融合方面的迫切需求。</title><link>https://arxiv.org/abs/2403.16428</link><description>&lt;p&gt;
Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16428
&lt;/p&gt;
&lt;p&gt;
该文章针对 egocentric 手部互动中存在的挑战，尤其强调了 3D 手部-物体重建任务的困难性，并提出了进行此类任务在机器人、AR/VR、动作识别和运动生成等领域中的重要性。文章详细介绍了基于几何图形和图像识别的包围盒回归方法，对于修正因头部运动产生的模糊效应和解决 egocentric 相机特有的畸变问题提供了深入的分析。同时，文章展示了当前状态下最新基线和提交方法的表现，并对如何更有效解决重构问题进行了探讨。文章通过 HANDS23 挑战评估了在 ARCTIC 和 AssemblyHands 数据集上提出的解决方法，并强调了高容量 Transformer 在学习复杂手-物体交互方面的有效性，以及集成不同预测方法对提高重建精度具有重要意义。通过这一系列的分析，文章为未来研究提供了参考，特别是在手-物体交互和 egocentric 三维重建领域，强调了在重新审视基准方法、算法创新和技术融合方面的迫切需求。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16428v2 Announce Type: replace  Abstract: We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3Dunderstanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from differen
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为CR3DT的模型，它结合了摄像头和雷达技术进行三维物体检测和多目标跟踪，通过将雷达数据的加入，显著提高了性能。</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为CR3DT的模型，它结合了摄像头和雷达技术进行三维物体检测和多目标跟踪，通过将雷达数据的加入，显著提高了性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v2 Announce Type: replace  Abstract: To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporat
&lt;/p&gt;</description></item><item><title>该文章提出了一个新颖的跨域无监督学习方法，能够将现有的无监督学习策略推广到未知领域，解决了领域迁移问题。通过引入一种名为Meta Domain Alignment Semantic Refinement（MDASR）的新机制，文中方法能够在保留特异性信息的同时，消除大型语言模型（LLM）输出的冗余类语义中的非特定性信息，从而有效地促进了从已知领域到未知领域的无监督学习迁移。</title><link>https://arxiv.org/abs/2403.14362</link><description>&lt;p&gt;
Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14362
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个新颖的跨域无监督学习方法，能够将现有的无监督学习策略推广到未知领域，解决了领域迁移问题。通过引入一种名为Meta Domain Alignment Semantic Refinement（MDASR）的新机制，文中方法能够在保留特异性信息的同时，消除大型语言模型（LLM）输出的冗余类语义中的非特定性信息，从而有效地促进了从已知领域到未知领域的无监督学习迁移。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14362v3 Announce Type: replace  Abstract: Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes. However, existing GZSL is still limited to seen domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains. Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains. Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs), we present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic sem
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RRL-Net的模型，通过构建一个自动编码器来充分捕获个体图像块的特性，并通过一个特征交互学习模块来提取深层次的特征关系，同时通过一个轻量级的多维全局到局部注意力模块进一步挖掘个体特征，以在高光谱数据匹配中实现更精确的匹配。</title><link>https://arxiv.org/abs/2403.11751</link><description>&lt;p&gt;
Relational Representation Learning Network for Cross-Spectral Image Patch Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11751
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RRL-Net的模型，通过构建一个自动编码器来充分捕获个体图像块的特性，并通过一个特征交互学习模块来提取深层次的特征关系，同时通过一个轻量级的多维全局到局部注意力模块进一步挖掘个体特征，以在高光谱数据匹配中实现更精确的匹配。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11751v2 Announce Type: replace  Abstract: Recently, feature relation learning has drawn widespread attention in cross-spectral image patch matching. However, existing related research focuses on extracting diverse relations between image patch features and ignores sufficient intrinsic feature representations of individual image patches. Therefore, we propose an innovative relational representation learning idea that simultaneously focuses on sufficiently mining the intrinsic features of individual image patches and the relations between image patch features. Based on this, we construct a Relational Representation Learning Network (RRL-Net). Specifically, we innovatively construct an autoencoder to fully characterize the individual intrinsic features, and introduce a feature interaction learning (FIL) module to extract deep-level feature relations. To further fully mine individual intrinsic features, a lightweight multi-dimensional global-to-local attention (MGLA) module is c
&lt;/p&gt;</description></item><item><title>该文章提出了一个新颖的框架，旨在通过整合来自不同领域的标签去噪策略来解决skeleton-based人类动作识别中的标签噪声问题，并在处理稀疏骨架数据时显著提高了识别性能。</title><link>https://arxiv.org/abs/2403.09975</link><description>&lt;p&gt;
Skeleton-Based Human Action Recognition with Noisy Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09975
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个新颖的框架，旨在通过整合来自不同领域的标签去噪策略来解决skeleton-based人类动作识别中的标签噪声问题，并在处理稀疏骨架数据时显著提高了识别性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09975v2 Announce Type: replace-cross  Abstract: Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model's training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodolo
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为EchoTrack的端到端框架，用于在自动驾驶场景中的音频指代多对象跟踪（AR-MOT）。EchoTrack通过双向频率域跨注意力融合模块（Bi-FCFM）实现了音频和视频特征的双流端到端融合，解决了传统音频视频融合方法和文本依赖的多对象跟踪在应用中的局限性。</title><link>https://arxiv.org/abs/2402.18302</link><description>&lt;p&gt;
EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18302
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为EchoTrack的端到端框架，用于在自动驾驶场景中的音频指代多对象跟踪（AR-MOT）。EchoTrack通过双向频率域跨注意力融合模块（Bi-FCFM）实现了音频和视频特征的双流端到端融合，解决了传统音频视频融合方法和文本依赖的多对象跟踪在应用中的局限性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18302v2 Announce Type: replace-cross  Abstract: This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from 
&lt;/p&gt;</description></item><item><title>该文章的核心创新在于提出了一种名为EMO的框架，它使用了一种直接的音频-视频合成方法，无需中间的3D模型或面部特征点，能够增强说话和唱歌视频生成中的现实感和表达力。该方法确保了视频中帧的连续性以及身份的一致性，生成了高度逼真和自然的动画。实验结果表明，EMO在表达性和领先技术上均得到了显著提升。</title><link>https://arxiv.org/abs/2402.17485</link><description>&lt;p&gt;
EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17485
&lt;/p&gt;
&lt;p&gt;
该文章的核心创新在于提出了一种名为EMO的框架，它使用了一种直接的音频-视频合成方法，无需中间的3D模型或面部特征点，能够增强说话和唱歌视频生成中的现实感和表达力。该方法确保了视频中帧的连续性以及身份的一致性，生成了高度逼真和自然的动画。实验结果表明，EMO在表达性和领先技术上均得到了显著提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17485v2 Announce Type: replace  Abstract: In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of express
&lt;/p&gt;</description></item><item><title>该文章提出了一种改进的扩散模型图像恢复方法，通过在每个生成步骤选择与测量相同的样本，并采用自适应候选样本数来增强输出稳定性，实现了更为精准的图像恢复效果。</title><link>https://arxiv.org/abs/2402.16907</link><description>&lt;p&gt;
Diffusion Posterior Proximal Sampling for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16907
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种改进的扩散模型图像恢复方法，通过在每个生成步骤选择与测量相同的样本，并采用自适应候选样本数来增强输出稳定性，实现了更为精准的图像恢复效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16907v2 Announce Type: replace-cross  Abstract: Diffusion models have demonstrated remarkable efficacy in generating high-quality samples. Existing diffusion-based image restoration algorithms exploit pre-trained diffusion models to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we present a refined paradigm for diffusion-based image restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. The number of candidate samples used for selection is adaptively determined based on the signal-to-noise ratio of the timestep. Additionally, we start the restoration process with an initialization combined with the meas
&lt;/p&gt;</description></item><item><title>该文章提出GAOKAO-MM，一个基于中文高考的全新多模态评估基准，它要求模型展现出对图片、图表、函数图和其他10种类型图片的正确理解和分析能力，同时要求模型具有推理和处理高级认知任务的能力。在GAOKAO-MM基准中，10个大型视觉语言模型（LVLMs）表现不佳，它们的正确率都低于50%。这表明现有的LVLMs在处理复杂多模态任务方面仍有较大的提升空间。</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
该文章提出GAOKAO-MM，一个基于中文高考的全新多模态评估基准，它要求模型展现出对图片、图表、函数图和其他10种类型图片的正确理解和分析能力，同时要求模型具有推理和处理高级认知任务的能力。在GAOKAO-MM基准中，10个大型视觉语言模型（LVLMs）表现不佳，它们的正确率都低于50%。这表明现有的LVLMs在处理复杂多模态任务方面仍有较大的提升空间。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v2 Announce Type: replace-cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs ha
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用可解释机器学习自动化量子点测量分析的方法，这一创新贡献能够帮助快速有效地对量子计算设备进行更精确的调校，特别是在对测量得到的图像进行分析时，提供了一种能够进行有效指导的自动化工具，从而加速量子计算技术的进步。</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
Automation of Quantum Dot Measurement Analysis via Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用可解释机器学习自动化量子点测量分析的方法，这一创新贡献能够帮助快速有效地对量子计算设备进行更精确的调校，特别是在对测量得到的图像进行分析时，提供了一种能够进行有效指导的自动化工具，从而加速量子计算技术的进步。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v4 Announce Type: replace  Abstract: The rapid development of quantum dot (QD) devices for quantum computing has necessitated more efficient and automated methods for device characterization and tuning. Many of the measurements acquired during the tuning process come in the form of images that need to be properly analyzed to guide the subsequent tuning steps. By design, features present in such images capture certain behaviors or states of the measured QD devices. When considered carefully, such features can aid the control and calibration of QD devices. An important example of such images are so-called \textit{triangle plots}, which visually represent current flow and reveal characteristics important for QD device calibration. While image-based classification tools, such as convolutional neural networks (CNNs), can be used to verify whether a given measurement is \textit{good} and thus warrants the initiation of the next phase of tuning, they do not provide any insight
&lt;/p&gt;</description></item><item><title>该文章提出了使用NeRF（神经辐射场）技术进行植物三维几何重建，在室内外多种复杂环境中验证了其有效性，特别是在户外场景中，其重建精度已达到74.6%，证实了NeRF模型的能力。并且通过一个早停训练技巧，大大减少了训练时间，并仅损失了7.4%的平均F1分数来优化了NeRF模型的性能。</title><link>https://arxiv.org/abs/2402.10344</link><description>&lt;p&gt;
Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry Reconstruction in Field Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10344
&lt;/p&gt;
&lt;p&gt;
该文章提出了使用NeRF（神经辐射场）技术进行植物三维几何重建，在室内外多种复杂环境中验证了其有效性，特别是在户外场景中，其重建精度已达到74.6%，证实了NeRF模型的能力。并且通过一个早停训练技巧，大大减少了训练时间，并仅损失了7.4%的平均F1分数来优化了NeRF模型的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10344v3 Announce Type: replace  Abstract: We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D reconstruction of plants in varied environments, from indoor settings to outdoor fields. Traditional methods usually fail to capture the complex geometric details of plants, which is crucial for phenotyping and breeding studies. We evaluate the reconstruction fidelity of NeRFs in three scenarios with increasing complexity and compare the results with the point cloud obtained using LiDAR as ground truth. In the most realistic field scenario, the NeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU, highlighting the efficacy of NeRFs for 3D reconstruction in challenging environments. Additionally, we propose an early stopping technique for NeRF training that almost halves the training time while achieving only a reduction of 7.4% in the average F1 score. This optimization process significantly enhances the speed and efficiency of 3D recon
&lt;/p&gt;</description></item><item><title>该文章引入了ColorSwap数据集，旨在通过评估和改善 multimodal 模型在不同对象与颜色匹配方面的性能来推动这一领域的研究发展。通过自动化生成题目和图像以及人类后续的校对，这一数据集包含了2,000组图片和句子，每组包含原始的图片和句子以及颜色对调的配对，从而为模型在颜色感知和理解上的准确性和灵活性提供了挑战。即便使用了诸如 GPT-4V 和 LLaVA 等较先进的模型，在处理这类问题时模型的表现也只能达到72%和42%，而且通过更高级的提示技术可能会进一步提升。此外，对比性模型诸如 CLIP 和 SigLIP 在处理这类任务方面的表现也得到了研究。</title><link>https://arxiv.org/abs/2402.04492</link><description>&lt;p&gt;
ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04492
&lt;/p&gt;
&lt;p&gt;
该文章引入了ColorSwap数据集，旨在通过评估和改善 multimodal 模型在不同对象与颜色匹配方面的性能来推动这一领域的研究发展。通过自动化生成题目和图像以及人类后续的校对，这一数据集包含了2,000组图片和句子，每组包含原始的图片和句子以及颜色对调的配对，从而为模型在颜色感知和理解上的准确性和灵活性提供了挑战。即便使用了诸如 GPT-4V 和 LLaVA 等较先进的模型，在处理这类问题时模型的表现也只能达到72%和42%，而且通过更高级的提示技术可能会进一步提升。此外，对比性模型诸如 CLIP 和 SigLIP 在处理这类任务方面的表现也得到了研究。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.04492v2 Announce Type: replace  Abstract: This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to cha
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用形式化方法验证深度神经网络在飞机滑行阶段视觉分类器时应对图像扰动情况的鲁棒性，其创新和贡献在于为航空行业中深度学习模型的安全性认证提供了新的技术路径，旨在确保在具有高安全要求的环境中使用的神经网络不会出现重大误判。</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用形式化方法验证深度神经网络在飞机滑行阶段视觉分类器时应对图像扰动情况的鲁棒性，其创新和贡献在于为航空行业中深度学习模型的安全性认证提供了新的技术路径，旨在确保在具有高安全要求的环境中使用的神经网络不会出现重大误判。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.00035v4 Announce Type: replace  Abstract: As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we there
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为PathoDuet的基础模型，用于对H&amp;E和IHC染色的病理切片进行分析。这些模型通过新的自监督学习框架进行预训练，该方法利用了特定于病理图像的特征，如不同倍率的图像和不同染色的关系。通过两个预训练任务——跨尺度定位和跨染色转换，研究者能够在H&amp;E图像上预训练模型，并将其有效迁移到IHC图像任务上。</title><link>https://arxiv.org/abs/2312.09894</link><description>&lt;p&gt;
PathoDuet: Foundation Models for Pathological Slide Analysis of H&amp;amp;E and IHC Stains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09894
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为PathoDuet的基础模型，用于对H&amp;amp;E和IHC染色的病理切片进行分析。这些模型通过新的自监督学习框架进行预训练，该方法利用了特定于病理图像的特征，如不同倍率的图像和不同染色的关系。通过两个预训练任务——跨尺度定位和跨染色转换，研究者能够在H&amp;amp;E图像上预训练模型，并将其有效迁移到IHC图像任务上。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09894v2 Announce Type: replace  Abstract: Large amounts of digitized histopathological data display a promising future for developing pathological foundation models via self-supervised learning methods. Foundation models pretrained with these methods serve as a good basis for downstream tasks. However, the gap between natural and histopathological images hinders the direct application of existing methods. In this work, we present PathoDuet, a series of pretrained models on histopathological images, and a new self-supervised learning framework in histopathology. The framework is featured by a newly-introduced pretext token and later task raisers to explicitly utilize certain relations between images, like multiple magnifications and multiple stains. Based on this, two pretext tasks, cross-scale positioning and cross-stain transferring, are designed to pretrain the model on Hematoxylin and Eosin (H&amp;amp;E) images and transfer the model to immunohistochemistry (IHC) images, respecti
&lt;/p&gt;</description></item><item><title>该文章提出了一种称为深度去学习（Deep Unlearning）的新方法，该方法能够不依赖梯度信息地高效、快速地解决类别遗忘问题，从而实现了无需重训模型即可为每个删除请求进行精确的类别更新，在不违背隐私的情况下提高了数据处理效率。</title><link>https://arxiv.org/abs/2312.00761</link><description>&lt;p&gt;
Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00761
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种称为深度去学习（Deep Unlearning）的新方法，该方法能够不依赖梯度信息地高效、快速地解决类别遗忘问题，从而实现了无需重训模型即可为每个删除请求进行精确的类别更新，在不违背隐私的情况下提高了数据处理效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00761v4 Announce Type: replace-cross  Abstract: Machine unlearning is a prominent and challenging field, driven by regulatory demands for user data deletion and heightened privacy awareness. Existing approaches involve retraining model or multiple finetuning steps for each deletion request, often constrained by computational limits and restricted data access. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate specific classes from the learned model. Our algorithm first estimates the Retain and the Forget Spaces using Singular Value Decomposition on the layerwise activations for a small subset of samples from the retain and unlearn classes, respectively. We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space. Finally, we obtain the unlearned model by updating the weights to suppress the class discriminatory features from the activation spaces. 
&lt;/p&gt;</description></item><item><title>该文章提出的InceptionHuman框架通过结合不同模态的指令输入（如文本、姿态、边缘、分割图等），能够控制生成具有高保真度的3D人类模型。该框架实现了在不具有独特特征、不自然的阴影、不自然的姿势/服饰、有限的视角等问题的同时，逐步细化神经辐射场空间，并利用两个新颖模块——迭代姿态感知精修（IPAR）和渐进式增强重建（PAR），实现了3D人类模型的稳定生成。</title><link>https://arxiv.org/abs/2311.16499</link><description>&lt;p&gt;
InceptionHuman: Controllable Prompt-to-NeRF for Photorealistic 3D Human Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16499
&lt;/p&gt;
&lt;p&gt;
该文章提出的InceptionHuman框架通过结合不同模态的指令输入（如文本、姿态、边缘、分割图等），能够控制生成具有高保真度的3D人类模型。该框架实现了在不具有独特特征、不自然的阴影、不自然的姿势/服饰、有限的视角等问题的同时，逐步细化神经辐射场空间，并利用两个新颖模块——迭代姿态感知精修（IPAR）和渐进式增强重建（PAR），实现了3D人类模型的稳定生成。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16499v2 Announce Type: replace  Abstract: This paper presents InceptionHuman, a prompt-to-NeRF framework that allows easy control via a combination of prompts in different modalities (e.g., text, poses, edge, segmentation map, etc) as inputs to generate photorealistic 3D humans. While many works have focused on generating 3D human models, they suffer one or more of the following: lack of distinctive features, unnatural shading/shadows, unnatural poses/clothes, limited views, etc. InceptionHuman achieves consistent 3D human generation within a progressively refined NeRF space with two novel modules, Iterative Pose-Aware Refinement (IPAR) and Progressive-Augmented Reconstruction (PAR). IPAR iteratively refines the diffusion-generated images and synthesizes high-quality 3D-aware views considering the close-pose RGB values. PAR employs a pretrained diffusion prior to augment the generated synthetic views and adds regularization for view-independent appearance. Overall, the synth
&lt;/p&gt;</description></item><item><title>该文章提出了一个基于稳定性的初始化技术，旨在提升神经ODE模型的训练效率和预测性能，尤其是在不同学习任务和工业应用中均取得了显著的成果。</title><link>https://arxiv.org/abs/2311.15890</link><description>&lt;p&gt;
Stability-Informed Initialization of Neural Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15890
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个基于稳定性的初始化技术，旨在提升神经ODE模型的训练效率和预测性能，尤其是在不同学习任务和工业应用中均取得了显著的成果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15890v3 Announce Type: replace-cross  Abstract: This paper addresses the training of Neural Ordinary Differential Equations (neural ODEs), and in particular explores the interplay between numerical integration techniques, stability regions, step size, and initialization techniques. It is shown how the choice of integration technique implicitly regularizes the learned model, and how the solver's corresponding stability region affects training and prediction performance. From this analysis, a stability-informed parameter initialization technique is introduced. The effectiveness of the initialization method is displayed across several learning benchmarks and industrial applications.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为RSB-Pose的短基线双目三维人体姿态估计方法，该方法通过引入立体多边形特征和立体共关键点估计模块，有效缓解了由于基线缩短而导致的三维姿态估计中的两个主要问题：2D关键点估计的鲁棒性和摄像头之间的遮挡问题。</title><link>https://arxiv.org/abs/2311.14242</link><description>&lt;p&gt;
RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with Occlusion Handling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14242
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为RSB-Pose的短基线双目三维人体姿态估计方法，该方法通过引入立体多边形特征和立体共关键点估计模块，有效缓解了由于基线缩短而导致的三维姿态估计中的两个主要问题：2D关键点估计的鲁棒性和摄像头之间的遮挡问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14242v2 Announce Type: replace  Abstract: In the domain of 3D Human Pose Estimation, which finds widespread daily applications, the requirement for convenient acquisition equipment continues to grow. To satisfy this demand, we set our sights on a short-baseline binocular setting that offers both portability and a geometric measurement property that radically mitigates depth ambiguity. However, as the binocular baseline shortens, two serious challenges emerge: first, the robustness of 3D reconstruction against 2D errors deteriorates; and second, occlusion reoccurs due to the limited visual differences between two views. To address the first challenge, we propose the Stereo Co-Keypoints Estimation module to improve the view consistency of 2D keypoints and enhance the 3D robustness. In this module, the disparity is utilized to represent the correspondence of binocular 2D points and the Stereo Volume Feature is introduced to contain binocular features across different disparitie
&lt;/p&gt;</description></item><item><title>该文章提出了一种通过神经网络学习变形场的方法，用于视觉数据的可重定向编辑，该方法能够在保留内容的同时强化低信息内容的区域变形，适用于图像、3D场景（神经辐射场）和 polygon 网格等多种视觉数据格式，相比以往方法在内容自适应重塑方面表现更佳。</title><link>https://arxiv.org/abs/2311.13297</link><description>&lt;p&gt;
Retargeting Visual Data with Deformation Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13297
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种通过神经网络学习变形场的方法，用于视觉数据的可重定向编辑，该方法能够在保留内容的同时强化低信息内容的区域变形，适用于图像、3D场景（神经辐射场）和 polygon 网格等多种视觉数据格式，相比以往方法在内容自适应重塑方面表现更佳。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13297v2 Announce Type: replace  Abstract: Seam carving is an image editing method that enable content-aware resizing, including operations like removing objects. However, the seam-finding strategy based on dynamic programming or graph-cut limits its applications to broader visual data formats and degrees of freedom for editing. Our observation is that describing the editing and retargeting of images more generally by a displacement field yields a generalisation of content-aware deformations. We propose to learn a deformation with a neural network that keeps the output plausible while trying to deform it only in places with low information content. This technique applies to different kinds of visual data, including images, 3D scenes given as neural radiance fields, or even polygon meshes. Experiments conducted on different visual data show that our method achieves better content-aware retargeting compared to previous methods.
&lt;/p&gt;</description></item><item><title>该文章提出了一种多智能体三维环境重建和微重力条件下变化检测的方法，使用自由飞行的机器人来维护太空基地。其中一个机器人通过图像和深度信息重建环境的三维模型，另一个机器人定期检查环境的变化，使用真实数据进行验证，适用于未来的太空探索活动。</title><link>https://arxiv.org/abs/2311.02558</link><description>&lt;p&gt;
Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02558
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种多智能体三维环境重建和微重力条件下变化检测的方法，使用自由飞行的机器人来维护太空基地。其中一个机器人通过图像和深度信息重建环境的三维模型，另一个机器人定期检查环境的变化，使用真实数据进行验证，适用于未来的太空探索活动。
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02558v3 Announce Type: replace  Abstract: Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction sys
&lt;/p&gt;</description></item><item><title>该文章探讨了一种基于数据驱动的方法，用于在多Agent协作环境中解决由于视角变化造成的视觉定位问题。研究工作通过对比现有方法和提出的多个基线，提出了一种新颖的数据驱动策略，旨在选择在特定位置进行最佳的视角选择。实验结果表明，该策略在模拟环境中超越了现有方法的表现。</title><link>https://arxiv.org/abs/2310.02650</link><description>&lt;p&gt;
Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02650
&lt;/p&gt;
&lt;p&gt;
该文章探讨了一种基于数据驱动的方法，用于在多Agent协作环境中解决由于视角变化造成的视觉定位问题。研究工作通过对比现有方法和提出的多个基线，提出了一种新颖的数据驱动策略，旨在选择在特定位置进行最佳的视角选择。实验结果表明，该策略在模拟环境中超越了现有方法的表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02650v3 Announce Type: replace  Abstract: Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation
&lt;/p&gt;</description></item><item><title>该文章提出一种仅依赖查询点标注的弱 supervision学习算法，用于训练仅需少量的查询点标注即可进行卫星图像语义分割的深度学习模型，有效降低了高精度像素级标注的成本和时间。</title><link>https://arxiv.org/abs/2309.05490</link><description>&lt;p&gt;
Learning Semantic Segmentation with Query Points Supervision on Aerial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05490
&lt;/p&gt;
&lt;p&gt;
该文章提出一种仅依赖查询点标注的弱 supervision学习算法，用于训练仅需少量的查询点标注即可进行卫星图像语义分割的深度学习模型，有效降低了高精度像素级标注的成本和时间。
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05490v2 Announce Type: replace  Abstract: Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models supervised with imag
&lt;/p&gt;</description></item><item><title>该文章创新地提出了Adv3D，一种使用Neural Radiance Fields（NeRF）生成3D对抗性样本的方法，专门针对3D物体检测在驾驶场景中的应用。通过最小化相邻对象在训练集中预测的信心，Adv3D利用NeRF的3D准确度和逼真外观特性，创造出一种新的物理上可实现的对抗性威胁，展示了在渲染NeRF时对3D检测器的显著性能影响。</title><link>https://arxiv.org/abs/2309.01351</link><description>&lt;p&gt;
Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in Driving Scenarios with NeRF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01351
&lt;/p&gt;
&lt;p&gt;
该文章创新地提出了Adv3D，一种使用Neural Radiance Fields（NeRF）生成3D对抗性样本的方法，专门针对3D物体检测在驾驶场景中的应用。通过最小化相邻对象在训练集中预测的信心，Adv3D利用NeRF的3D准确度和逼真外观特性，创造出一种新的物理上可实现的对抗性威胁，展示了在渲染NeRF时对3D检测器的显著性能影响。
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01351v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adver
&lt;/p&gt;</description></item><item><title>该文章提出了一种动态融合卷积神经网络（CNN）和手工特征的方法，用于检测指纹呈现攻击，并在已知和未知材料条件下提高了检测性能。</title><link>https://arxiv.org/abs/2308.10015</link><description>&lt;p&gt;
DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10015
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种动态融合卷积神经网络（CNN）和手工特征的方法，用于检测指纹呈现攻击，并在已知和未知材料条件下提高了检测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10015v2 Announce Type: replace  Abstract: Automatic fingerprint recognition systems suffer from the threat of presentation attacks due to their wide range of deployment in areas including national borders and commercial applications. A presentation attack can be performed by creating a spoof of a user's fingerprint with or without their consent. This paper presents a dynamic ensemble of deep CNN and handcrafted features to detect presentation attacks in known-material and unknown-material protocols of the livness detection competition. The proposed presentation attack detection model, in this way, utilizes the capabilities of both deep CNN and handcrafted features techniques and exhibits better performance than their individual performances. The proposed method is validated using benchmark databases from the Liveness Detection Competition in 2015, 2017, and 2019, yielding overall accuracy of 96.10\%, 96.49\%, and 94.99\% on them, respectively. The proposed method outperforms
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为FS-CDIS的框架，通过两个专门的损失函数对少样本学习方法进行了优化，有效提高了对于具有高度相似背景的隐身对象（如野生动物）的检测和分割能力。</title><link>https://arxiv.org/abs/2304.07444</link><description>&lt;p&gt;
The Art of Camouflage: Few-Shot Learning for Animal Detection and Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.07444
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为FS-CDIS的框架，通过两个专门的损失函数对少样本学习方法进行了优化，有效提高了对于具有高度相似背景的隐身对象（如野生动物）的检测和分割能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.07444v4 Announce Type: replace  Abstract: Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data on concealed objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. As camouflaged instances are challenging to recognize due to their similarity compared to the surroundings, we guide our models to obtain camouflaged features that highly distinguish the instances from the background. In this work, we propose FS-CDIS, a framework to efficiently detect and segment camouflaged instances via two loss functions contributing to the training process. Firstly, the instance triplet loss with the characteristic of differentiating the anchor, which is the mean of all camouflaged foreground points, and the background poi
&lt;/p&gt;</description></item><item><title>该文章提出了使用深度学习辅助的智能手机显微镜图像检测和量化Giardia和Cryptosporidium（oo）囊氏的方法，解决了传统显微镜检测成本高、不易携以及缺乏专业技术人员的问题，具有重要的公共卫生意义。</title><link>https://arxiv.org/abs/2304.05339</link><description>&lt;p&gt;
Deep-learning Assisted Detection and Quantification of (oo)cysts of Giardia and Cryptosporidium on Smartphone Microscopy Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.05339
&lt;/p&gt;
&lt;p&gt;
该文章提出了使用深度学习辅助的智能手机显微镜图像检测和量化Giardia和Cryptosporidium（oo）囊氏的方法，解决了传统显微镜检测成本高、不易携以及缺乏专业技术人员的问题，具有重要的公共卫生意义。
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.05339v2 Announce Type: replace-cross  Abstract: The consumption of microbial-contaminated food and water is responsible for the deaths of millions of people annually. Smartphone-based microscopy systems are portable, low-cost, and more accessible alternatives for the detection of Giardia and Cryptosporidium than traditional brightfield microscopes. However, the images from smartphone microscopes are noisier and require manual cyst identification by trained technicians, usually unavailable in resource-limited settings. Automatic detection of (oo)cysts using deep-learning-based object detection could offer a solution for this limitation. We evaluate the performance of four state-of-the-art object detectors to detect (oo)cysts of Giardia and Cryptosporidium on a custom dataset that includes both smartphone and brightfield microscopic images from vegetable samples. Faster RCNN, RetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer (Deformable DETR) deep-l
&lt;/p&gt;</description></item><item><title>该文章提出了一种针对开放环境下运动中激活肌肉组估计（AMGE）的新任务，并创建了一个大规模的MuscleMap数据集，包含多种类型运动视频和多种物运动中激活肌肉组的标注信息。通过这一数据集和提出的解决方案，该研究旨在开发可在真实环境中有效工作的视频分析工具，尤其是在体育和康复医疗领域。</title><link>https://arxiv.org/abs/2303.00952</link><description>&lt;p&gt;
Towards Activated Muscle Group Estimation in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00952
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种针对开放环境下运动中激活肌肉组估计（AMGE）的新任务，并创建了一个大规模的MuscleMap数据集，包含多种类型运动视频和多种物运动中激活肌肉组的标注信息。通过这一数据集和提出的解决方案，该研究旨在开发可在真实环境中有效工作的视频分析工具，尤其是在体育和康复医疗领域。
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00952v5 Announce Type: replace-cross  Abstract: In this paper, we tackle the new task of video-based Activated Muscle Group Estimation (AMGE) aiming at identifying active muscle regions during physical activity in the wild. To this intent, we provide the MuscleMap dataset featuring &amp;gt;15K video clips with 135 different activities and 20 labeled muscle groups. This dataset opens the vistas to multiple video-based applications in sports and rehabilitation medicine under flexible environment constraints. The proposed MuscleMap dataset is constructed with YouTube videos, specifically targeting High-Intensity Interval Training (HIIT) physical exercise in the wild. To make the AMGE model applicable in real-life situations, it is crucial to ensure that the model can generalize well to numerous types of physical activities not present during training and involving new combinations of activated muscles. To achieve this, our benchmark also covers an evaluation setting where the model is
&lt;/p&gt;</description></item></channel></rss>