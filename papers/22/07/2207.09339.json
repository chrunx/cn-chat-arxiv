{
    "title": "Vision Transformers: From Semantic Segmentation to Dense Prediction",
    "abstract": "arXiv:2207.09339v4 Announce Type: replace  Abstract: The emergence of vision transformers (ViTs) in image classification has shifted the methodologies for visual representation learning. In particular, ViTs learn visual representation at full receptive field per layer across all the image patches, in comparison to the increasing receptive fields of CNNs across layers and other alternatives (e.g., large kernels and atrous convolution). In this work, for the first time we explore the global context learning potentials of ViTs for dense visual prediction (e.g., semantic segmentation). Our motivation is that through learning global context at full receptive field layer by layer, ViTs may capture stronger long-range dependency information, critical for dense prediction tasks. We first demonstrate that encoding an image as a sequence of patches, a vanilla ViT without local convolution and resolution reduction can yield stronger visual representation for semantic segmentation. For example, ou",
    "link": "https://arxiv.org/abs/2207.09339",
    "context": "Title: Vision Transformers: From Semantic Segmentation to Dense Prediction\nAbstract: arXiv:2207.09339v4 Announce Type: replace  Abstract: The emergence of vision transformers (ViTs) in image classification has shifted the methodologies for visual representation learning. In particular, ViTs learn visual representation at full receptive field per layer across all the image patches, in comparison to the increasing receptive fields of CNNs across layers and other alternatives (e.g., large kernels and atrous convolution). In this work, for the first time we explore the global context learning potentials of ViTs for dense visual prediction (e.g., semantic segmentation). Our motivation is that through learning global context at full receptive field layer by layer, ViTs may capture stronger long-range dependency information, critical for dense prediction tasks. We first demonstrate that encoding an image as a sequence of patches, a vanilla ViT without local convolution and resolution reduction can yield stronger visual representation for semantic segmentation. For example, ou",
    "path": "papers/22/07/2207.09339.json",
    "total_tokens": 878,
    "translated_title": "视觉变换器：从语义分割到密集预测",
    "translated_abstract": "arXiv:2207.09339v4 宣布类型：替换  摘要：在图像分类领域，视觉变换器（ViTs）的出现改变了视觉表示学习的方法论。特别是，ViTs在第层对所有图像块的全视场范围内学习视觉表示，而与卷积神经网络（CNNs）层间和其他替代方法的逐渐扩大视场相比。在这项工作中，我们首次探索了ViTs在全球上下文中对密集视觉预测（如语义分割）的能力。我们的动机是，通过在每一层学习全视场的全局上下文，ViTs可能捕获更强的长距离依赖信息，这对于密集预测任务至关重要。我们首先证明，将图像编码为序列块，一种没有局部卷积和分辨率降级的原始ViT，可以为语义分割提供更强的视觉表示。例如，我们的实顶实验结果表明，与传统的分割网络相比，ViT即使在没有预训练的情况下，在语义分割任务上也表现出与离散网络匹÷的能力。 This study's findings not only provide a robust and compelling baseline for the dense prediction domain, but also demonstrate the feasibility of using ViTs for the task in a completely unsupervised manner. Further experiments on various dense prediction tasks also show that ViTs are highly competitive and generalize well beyond the conventional segmentation tasks to other real-world applications, such as new medical diagnosis scripts based on fine-grained biomedical image analysis.通过进一步在各种密集预测任务上的实验，我们发现ViT在常规分割任务之外的许多实际应用中也非常有竞争力，并展示了良好的泛化能力，例如，基于精细生物医学图像分析的新医学诊断脚本。",
    "tldr": "本研究不仅为密集预测领域提供了一个强大且有力的基线，而且还证明了在没有预训练的情况下，使用ViT进行任务的可行性。在各种密集预测任务上的进一步实验还表明，ViT不仅在常规分割任务中与其他模型竞争，而且还能很好地泛化到其他现实世界应用中，如基于精细生物医学图像分析的新医学诊断脚本。",
    "en_tdlr": "This study not only provides a robust and compelling baseline for the dense prediction domain and demonstrates the feasibility of using Vision Transformers (ViTs) for the task in an unsupervised manner, but also shows ViTs' competitiveness and strong generalization beyond conventional segmentation tasks to other real-world applications, such as new medical diagnosis scripts based on fine-grained biomedical image analysis."
}