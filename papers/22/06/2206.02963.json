{
    "title": "Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding",
    "abstract": "arXiv:2206.02963v3 Announce Type: replace-cross  Abstract: Knowledge Graph Embedding (KGE), which projects entities and relations into continuous vector spaces, has garnered significant attention. Although high-dimensional KGE methods offer better performance, they come at the expense of significant computation and memory overheads. Decreasing embedding dimensions significantly deteriorates model performance. While several recent efforts utilize knowledge distillation or non-Euclidean representation learning to augment the effectiveness of low-dimensional KGE, they either necessitate a pre-trained high-dimensional teacher model or involve complex non-Euclidean operations, thereby incurring considerable additional computational costs. To address this, this work proposes Confidence-aware Self-Knowledge Distillation (CSD) that learns from the model itself to enhance KGE in a low-dimensional space. Specifically, CSD extracts knowledge from embeddings in previous iterations, which would be ",
    "link": "https://arxiv.org/abs/2206.02963",
    "context": "Title: Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding\nAbstract: arXiv:2206.02963v3 Announce Type: replace-cross  Abstract: Knowledge Graph Embedding (KGE), which projects entities and relations into continuous vector spaces, has garnered significant attention. Although high-dimensional KGE methods offer better performance, they come at the expense of significant computation and memory overheads. Decreasing embedding dimensions significantly deteriorates model performance. While several recent efforts utilize knowledge distillation or non-Euclidean representation learning to augment the effectiveness of low-dimensional KGE, they either necessitate a pre-trained high-dimensional teacher model or involve complex non-Euclidean operations, thereby incurring considerable additional computational costs. To address this, this work proposes Confidence-aware Self-Knowledge Distillation (CSD) that learns from the model itself to enhance KGE in a low-dimensional space. Specifically, CSD extracts knowledge from embeddings in previous iterations, which would be ",
    "path": "papers/22/06/2206.02963.json",
    "total_tokens": 699,
    "translated_title": "基于自语义蒸馏的知识图嵌入置信度感知",
    "translated_abstract": "arXiv:2206.02963v3 公告类型：替换交叉 摘要：知识图嵌入（KGE），即将实体和关系映射到连续的向量空间中，已经吸引了大量关注。虽然高维KGE方法提供了更好的性能，但它们以显著的计算和内存开销为代价。降低嵌入维度显著降低了模型性能。尽管最近的一些努力利用知识蒸馏或非欧几里得表示学习来提高低维KGE的有效性，但它们要么需要一个预先训练的高维教师模型，要么涉及复杂的非欧几里得操作，从而增加了相当多的额外计算成本。为了解决这一问题，这项工作提出了一种基于置信度感知自我知识蒸馏（CSD）的方法，该方法在低维空间中从自身学习以增强KGE。具体来说，CSD从以前迭代的嵌入中提取知识，这些知识可以根据学习到的模型性能视为正确的或错误的，从而改进了低维KGE模型。通过这种方式，CSD为低维KGE的性能提升提供了一种实用且引人注目的解决方案。",
    "tldr": "本文提出了一种基于自语义蒸馏的知识图嵌入方法，该方法利用自我知识蒸馏增强KGE在低维空间中的性能，无需预训练的高维模型，且操作简单。"
}