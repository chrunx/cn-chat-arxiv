{
    "title": "Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding",
    "abstract": "arXiv:2206.02963v3 Announce Type: replace-cross  Abstract: Knowledge Graph Embedding (KGE), which projects entities and relations into continuous vector spaces, has garnered significant attention. Although high-dimensional KGE methods offer better performance, they come at the expense of significant computation and memory overheads. Decreasing embedding dimensions significantly deteriorates model performance. While several recent efforts utilize knowledge distillation or non-Euclidean representation learning to augment the effectiveness of low-dimensional KGE, they either necessitate a pre-trained high-dimensional teacher model or involve complex non-Euclidean operations, thereby incurring considerable additional computational costs. To address this, this work proposes Confidence-aware Self-Knowledge Distillation (CSD) that learns from the model itself to enhance KGE in a low-dimensional space. Specifically, CSD extracts knowledge from embeddings in previous iterations, which would be ",
    "link": "https://arxiv.org/abs/2206.02963",
    "context": "Title: Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding\nAbstract: arXiv:2206.02963v3 Announce Type: replace-cross  Abstract: Knowledge Graph Embedding (KGE), which projects entities and relations into continuous vector spaces, has garnered significant attention. Although high-dimensional KGE methods offer better performance, they come at the expense of significant computation and memory overheads. Decreasing embedding dimensions significantly deteriorates model performance. While several recent efforts utilize knowledge distillation or non-Euclidean representation learning to augment the effectiveness of low-dimensional KGE, they either necessitate a pre-trained high-dimensional teacher model or involve complex non-Euclidean operations, thereby incurring considerable additional computational costs. To address this, this work proposes Confidence-aware Self-Knowledge Distillation (CSD) that learns from the model itself to enhance KGE in a low-dimensional space. Specifically, CSD extracts knowledge from embeddings in previous iterations, which would be ",
    "path": "papers/22/06/2206.02963.json",
    "total_tokens": 375,
    "tldr": "该文章提出了一种称为Confidence-aware Self-Knowledge Distillation (CSD)的方法，该方法通过从模型自身中提取知识来提高低维空间中的知识图嵌入（KGE）性能。这种方法可以在低维空间中学习，同时避免了使用预先训练的高维教师模型或复杂的非欧几里得操作所带来的额外计算成本。通过这种方式，CSD提高了KGE的效率和性能，为处理大规模知识图谱提供了新的解决方案。"
}