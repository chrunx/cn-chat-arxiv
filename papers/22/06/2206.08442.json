{
    "title": "A Look at Value-Based Decision-Time vs. Background Planning Methods Across Different Settings",
    "abstract": "arXiv:2206.08442v2 Announce Type: replace-cross  Abstract: In model-based reinforcement learning (RL), an agent can leverage a learned model to improve its way of behaving in different ways. Two of the prevalent ways to do this are through decision-time and background planning methods. In this study, we are interested in understanding how the value-based versions of these two planning methods will compare against each other across different settings. Towards this goal, we first consider the simplest instantiations of value-based decision-time and background planning methods and provide theoretical results on which one will perform better in the regular RL and transfer learning settings. Then, we consider the modern instantiations of them and provide hypotheses on which one will perform better in the same settings. Finally, we perform illustrative experiments to validate these theoretical results and hypotheses. Overall, our findings suggest that even though value-based versions of the ",
    "link": "https://arxiv.org/abs/2206.08442",
    "context": "Title: A Look at Value-Based Decision-Time vs. Background Planning Methods Across Different Settings\nAbstract: arXiv:2206.08442v2 Announce Type: replace-cross  Abstract: In model-based reinforcement learning (RL), an agent can leverage a learned model to improve its way of behaving in different ways. Two of the prevalent ways to do this are through decision-time and background planning methods. In this study, we are interested in understanding how the value-based versions of these two planning methods will compare against each other across different settings. Towards this goal, we first consider the simplest instantiations of value-based decision-time and background planning methods and provide theoretical results on which one will perform better in the regular RL and transfer learning settings. Then, we consider the modern instantiations of them and provide hypotheses on which one will perform better in the same settings. Finally, we perform illustrative experiments to validate these theoretical results and hypotheses. Overall, our findings suggest that even though value-based versions of the ",
    "path": "papers/22/06/2206.08442.json",
    "total_tokens": 700,
    "translated_title": "不同环境中决策时刻与背景规划方法的基于价值决策比较",
    "translated_abstract": "arXiv:2206.08442v2 公告类型：替换交叉 摘要：在基于模型的强化学习（RL）中，代理可以利用学习到的模型以不同的方式改进其行为。两者的常见方式是通过决策时刻规划和背景规划方法。本研究旨在了解在不同的环境中，基于价值的决策时刻规划和背景规划方法之间的比较。为此目标，我们首先考虑基于价值的决策时刻和背景规划方法的简化实例，并提供理论结果，说明在常规的RL和转移学习环境中，哪一种方法会表现得更好。然后，我们考虑它们在同一环境中的现代实例，并提出关于在同一环境中哪一种方法会表现更好的假设。最后，我们进行 illustrative 实验来验证这些理论结果和假设。总体来看，我们的发现表明，尽管基于价值的决策时刻规划方法可能会在某些环境中比背景规划方法表现得更好，但实际情况可能会根据不同的环境和任务而有所不同。在一个特定的任务上，决策时刻规划可能会优于背景规划，而在另一个任务上，背景规划可能会更有优势。总的来说，我们的研究表明在不同的设置中考虑基于价值的决策时刻规划和背景规划方法时，需要进行具体的评估和比较。",
    "tldr": "本研究比较了不同环境中基于价值的决策时刻规划和背景规划方法的有效性，并在理论上和实验中验证了哪一种方法在改进代理行为方面表现更好。"
}