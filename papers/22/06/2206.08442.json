{
    "title": "A Look at Value-Based Decision-Time vs. Background Planning Methods Across Different Settings",
    "abstract": "arXiv:2206.08442v2 Announce Type: replace-cross  Abstract: In model-based reinforcement learning (RL), an agent can leverage a learned model to improve its way of behaving in different ways. Two of the prevalent ways to do this are through decision-time and background planning methods. In this study, we are interested in understanding how the value-based versions of these two planning methods will compare against each other across different settings. Towards this goal, we first consider the simplest instantiations of value-based decision-time and background planning methods and provide theoretical results on which one will perform better in the regular RL and transfer learning settings. Then, we consider the modern instantiations of them and provide hypotheses on which one will perform better in the same settings. Finally, we perform illustrative experiments to validate these theoretical results and hypotheses. Overall, our findings suggest that even though value-based versions of the ",
    "link": "https://arxiv.org/abs/2206.08442",
    "context": "Title: A Look at Value-Based Decision-Time vs. Background Planning Methods Across Different Settings\nAbstract: arXiv:2206.08442v2 Announce Type: replace-cross  Abstract: In model-based reinforcement learning (RL), an agent can leverage a learned model to improve its way of behaving in different ways. Two of the prevalent ways to do this are through decision-time and background planning methods. In this study, we are interested in understanding how the value-based versions of these two planning methods will compare against each other across different settings. Towards this goal, we first consider the simplest instantiations of value-based decision-time and background planning methods and provide theoretical results on which one will perform better in the regular RL and transfer learning settings. Then, we consider the modern instantiations of them and provide hypotheses on which one will perform better in the same settings. Finally, we perform illustrative experiments to validate these theoretical results and hypotheses. Overall, our findings suggest that even though value-based versions of the ",
    "path": "papers/22/06/2206.08442.json",
    "total_tokens": 332,
    "tldr": "该文章研究了模型基于强化学习（RL）中决策时间与背景规划方法在不同环境中的性能对比，尤其关注基于价值的两种方法在常规RL和转移学习场景中的表现差异，并进行了相应的实验验证。"
}