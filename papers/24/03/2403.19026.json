{
    "title": "EgoNav: Egocentric Scene-aware Human Trajectory Prediction",
    "abstract": "arXiv:2403.19026v3 Announce Type: replace  Abstract: Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to constantly adapt to the surrounding scene based on egocentric vision, and predict the ego motion of the wearer. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We then present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. To that end, we introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-g",
    "link": "https://arxiv.org/abs/2403.19026",
    "context": "Title: EgoNav: Egocentric Scene-aware Human Trajectory Prediction\nAbstract: arXiv:2403.19026v3 Announce Type: replace  Abstract: Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to constantly adapt to the surrounding scene based on egocentric vision, and predict the ego motion of the wearer. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We then present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. To that end, we introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-g",
    "path": "papers/24/03/2403.19026.json",
    "total_tokens": 377,
    "tldr": "该文章提出了一种名为EgoNav的方法，通过融合身体佩戴的摄像头和传感器数据，实现了在人造环境中对人类行走轨迹的预测，并考虑到了周围静态环境的影响。通过使用一种扩散模型，该技术能够生成一系列可能的未来行走轨迹分布，并且能够有效地将用户的视觉环境记忆纳入考量。此外，文章还介绍了用于训练和验证算法的数据集，为未来的相关研究提供了支持。"
}