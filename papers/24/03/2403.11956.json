{
    "title": "Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment",
    "abstract": "arXiv:2403.11956v5 Announce Type: replace  Abstract: With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video's corresponding mean opinion score. Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language ",
    "link": "https://arxiv.org/abs/2403.11956",
    "context": "Title: Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment\nAbstract: arXiv:2403.11956v5 Announce Type: replace  Abstract: With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video's corresponding mean opinion score. Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language ",
    "path": "papers/24/03/2403.11956.json",
    "total_tokens": 380,
    "tldr": "该文章构建了首个大规模文本到视频质量评估数据库T2VQA-DB，包含10,000个由9种不同文本到视频模型生成的视频，并对每个视频进行了主观评测，在此基础上提出了一个基于自监督学习的文本到视频质量评估模型。"
}