{
    "title": "Dissecting Deep RL with High Update Ratios: Combatting Value Divergence",
    "abstract": "arXiv:2403.05996v3 Announce Type: replace-cross  Abstract: We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer moment",
    "link": "https://arxiv.org/abs/2403.05996",
    "context": "Title: Dissecting Deep RL with High Update Ratios: Combatting Value Divergence\nAbstract: arXiv:2403.05996v3 Announce Type: replace-cross  Abstract: We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer moment",
    "path": "papers/24/03/2403.05996.json",
    "total_tokens": 647,
    "translated_title": "高更新比深度 RL 解析：抗击值分歧",
    "translated_abstract": "本文表明，在环境样本数量远少于梯度更新次数的情况下，深度强化学习算法可以通过对抗值函数分歧，并不reset网络参数，而保持学习能力。Nikishin 等人的一项近期研究（2022年）指出在一个较大的更新数据比下，出现了优先偏见，即代理对早期的交互过度拟合，而忽视后来的交互，从而损害了其学习能力。在本研究中，我们对导致优先偏见的现象进行了调查。我们检查了训练的前期阶段，这些阶段被认为是导致无法学习的原因，并发现一个根本的挑战是长期存在的值函数分歧。不仅仅是在未知分布的数据上，甚至在已知分布的数据上，我们也发现了过度膨胀的Q值，并且发现这种现象与在未见过的动作预测上的过估计有关，这种过估计是由优化器的力推动的。",
    "tldr": "研究表明，在强化学习中，当梯度更新次数远远超过环境样本时，通过对抗值函数分歧，算法能够保持学习能力，避免了优先偏见的出现。"
}