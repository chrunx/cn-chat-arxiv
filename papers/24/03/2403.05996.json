{
    "title": "Dissecting Deep RL with High Update Ratios: Combatting Value Divergence",
    "abstract": "arXiv:2403.05996v3 Announce Type: replace-cross  Abstract: We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer moment",
    "link": "https://arxiv.org/abs/2403.05996",
    "context": "Title: Dissecting Deep RL with High Update Ratios: Combatting Value Divergence\nAbstract: arXiv:2403.05996v3 Announce Type: replace-cross  Abstract: We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer moment",
    "path": "papers/24/03/2403.05996.json",
    "total_tokens": 339,
    "tldr": "该文章揭示了在深度强化学习中，当更新的次数远超过环境样本时，算法能够抵抗价值偏差并保持学习能力，有效避免了在大型更新-数据比下可能导致的价值函数稀释现象。"
}