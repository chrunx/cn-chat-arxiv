{
    "title": "Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects",
    "abstract": "arXiv:2403.16428v2 Announce Type: replace  Abstract: We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3Dunderstanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from differen",
    "link": "https://arxiv.org/abs/2403.16428",
    "context": "Title: Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects\nAbstract: arXiv:2403.16428v2 Announce Type: replace  Abstract: We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3Dunderstanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from differen",
    "path": "papers/24/03/2403.16428.json",
    "total_tokens": 511,
    "tldr": "该文章针对 egocentric 手部互动中存在的挑战，尤其强调了 3D 手部-物体重建任务的困难性，并提出了进行此类任务在机器人、AR/VR、动作识别和运动生成等领域中的重要性。文章详细介绍了基于几何图形和图像识别的包围盒回归方法，对于修正因头部运动产生的模糊效应和解决 egocentric 相机特有的畸变问题提供了深入的分析。同时，文章展示了当前状态下最新基线和提交方法的表现，并对如何更有效解决重构问题进行了探讨。文章通过 HANDS23 挑战评估了在 ARCTIC 和 AssemblyHands 数据集上提出的解决方法，并强调了高容量 Transformer 在学习复杂手-物体交互方面的有效性，以及集成不同预测方法对提高重建精度具有重要意义。通过这一系列的分析，文章为未来研究提供了参考，特别是在手-物体交互和 egocentric 三维重建领域，强调了在重新审视基准方法、算法创新和技术融合方面的迫切需求。"
}