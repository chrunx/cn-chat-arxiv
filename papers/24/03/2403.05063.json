{
    "title": "Aligning Large Language Models for Controllable Recommendations",
    "abstract": "arXiv:2403.05063v2 Announce Type: replace-cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method",
    "link": "https://arxiv.org/abs/2403.05063",
    "context": "Title: Aligning Large Language Models for Controllable Recommendations\nAbstract: arXiv:2403.05063v2 Announce Type: replace-cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method",
    "path": "papers/24/03/2403.05063.json",
    "total_tokens": 612,
    "translated_title": "大型语言模型在可控推荐系统中的对齐",
    "translated_abstract": "arXiv:2403.05063v2 Announce Type: replace-cross 摘要：在大型语言模型（LLMs）表现出卓越的一般智能的启发下，研究人员已经开始探索其在开创下一代推荐系统方面的应用——这些系统是会话性的、可解释的，并且是可控的。然而，现有的文献主要集中在将领域特定的知识整合到LLMs中以提高准确性，往往忽视了遵循指令的能力。为了解决这一不足，我们首先介绍了一系列监督学习任务，这些任务被增加了由传统推荐模型得出的标签，旨在显着提高LLMs在遵循推荐特定指令方面的能力。随后，我们还开发了一种基于强化学习的对齐过程，以进一步增强LLMs在响应用户意图和减少格式错误方面的能力。通过在两个真实世界数据集上的广泛实验，我们的方法证明了这种方法的有效性和实用性，为开发更智能、更可控的推荐系统奠定了基础。",
    "tldr": "我们提出了一种通过强化学习对语言模型进行微调和适应以提高它们遵循推荐系统和人类意图的能力的方法。"
}