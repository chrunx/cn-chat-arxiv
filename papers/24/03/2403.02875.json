{
    "title": "Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples",
    "abstract": "arXiv:2403.02875v2 Announce Type: replace  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show s",
    "link": "https://arxiv.org/abs/2403.02875",
    "context": "Title: Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples\nAbstract: arXiv:2403.02875v2 Announce Type: replace  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show s",
    "path": "papers/24/03/2403.02875.json",
    "total_tokens": 448,
    "tldr": "该文章提出了一种新方法，通过嵌入合成困难负面文本例子到预训练中，增强了基于对比学习的多模态模型在概念理解方面的精细度。这些困难的负面样本通过随机打乱与视觉概念相关的词汇来创建，这对于促进更精细的语言和视觉概念对齐至关重要。此外，研究小组还开发了一个新的困难数据集“InpaintCOCO”，用于评估视觉语言模型在颜色、物体和大小的精细概念对齐方面的能力。通过从COCO图像中生成并嵌入这些数据集，研究人员能够通过改变视觉概念来去除图像对及其原标题之间的匹配关系，从而进一步考验了模型的理解能力。实验结果表明，这种新方法的引入极大地提高了模型对视觉概念的理解和准确度，为多模态对比学习提供了新的见解。"
}