{
    "title": "NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator",
    "abstract": "arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a",
    "link": "https://arxiv.org/abs/2403.02411",
    "context": "Title: NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator\nAbstract: arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a",
    "path": "papers/24/03/2403.02411.json",
    "total_tokens": 646,
    "translated_title": "NiNformer: 一种使用 Token 混合作为门控函数生成器的网络在网络 transformer",
    "translated_abstract": "arXiv:2403.02411v5 公告类型：替换 摘要：自引入以来，注意机制一直是 transformer 架构的主要组件，并引领了深度学习领域的重大进步，涵盖多个领域和多种任务。注意机制被用作计算机视觉中的 Vision Transformer ViT，其应用扩展到了多个视觉领域任务，如分类、分割、物体检测和图像生成。这个机制虽然表达能力非常强，但它也有一些不足，如计算成本高昂，且需要较大的数据集才能有效优化。为了解决这些问题，文献中提出了许多设计，旨在减少计算负担，并缓解对数据大小的要求。在计算机视觉领域中，一些这方面的尝试包括 MLP-Mixer、Conv-Mixer、Perciver-IO 等。本论文介绍了作为门控函数生成器的新的计算块。",
    "tldr": "本文提出了 NiNformer，一种结合了网络在网络 (Network in Network) 和 Transformer 架构的新模型，使用 Token 混合作为门控函数生成器，以提高计算效率和减少数据集大小要求。"
}