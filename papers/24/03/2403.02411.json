{
    "title": "NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator",
    "abstract": "arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a",
    "link": "https://arxiv.org/abs/2403.02411",
    "context": "Title: NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator\nAbstract: arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a",
    "path": "papers/24/03/2403.02411.json",
    "total_tokens": 327,
    "tldr": "该文章提出一个新的计算模块作为注意力机制的替代，该模块利用网络在网络架构中的局部感受野和全局注意力机制，通过一种新颖的递归混合机制来提升Transformer的效率和性能。"
}