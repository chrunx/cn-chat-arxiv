{
    "title": "On the Efficacy of Text-Based Input Modalities for Action Anticipation",
    "abstract": "arXiv:2401.12972v2 Announce Type: replace  Abstract: Anticipating future actions is a highly challenging task due to the diversity and scale of potential future actions; yet, information from different modalities help narrow down plausible action choices. Each modality can provide diverse and often complementary context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text descriptions of actions and objects can also lead to more accurate action anticipation by providing additional contextual cues, e.g., about the environment and its contents. We propose a Multi-modal Contrastive Anticipative Transformer (M-CAT), a video transformer architecture that jointly learns from multi-modal features and text descriptions of actions and objects. We train our model in two stages, where the model first learns to align video clips with descriptions of future actions, and is subsequently fine-tuned ",
    "link": "https://arxiv.org/abs/2401.12972",
    "context": "Title: On the Efficacy of Text-Based Input Modalities for Action Anticipation\nAbstract: arXiv:2401.12972v2 Announce Type: replace  Abstract: Anticipating future actions is a highly challenging task due to the diversity and scale of potential future actions; yet, information from different modalities help narrow down plausible action choices. Each modality can provide diverse and often complementary context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text descriptions of actions and objects can also lead to more accurate action anticipation by providing additional contextual cues, e.g., about the environment and its contents. We propose a Multi-modal Contrastive Anticipative Transformer (M-CAT), a video transformer architecture that jointly learns from multi-modal features and text descriptions of actions and objects. We train our model in two stages, where the model first learns to align video clips with descriptions of future actions, and is subsequently fine-tuned ",
    "path": "papers/24/01/2401.12972.json",
    "total_tokens": 389,
    "tldr": "该文章提出了一个名为M-CAT（Multi-modal Contrastive Anticipative Transformer）的模型，该模型是一种视频变换器架构，它结合了多模态特征和动作及物体描述的文本信息。文章通过两个阶段的训练方法，首先训练模型将视频片段与未来动作的描述进行对齐，然后对其进行微调，以提高动作预测的准确性。此外，文章还探讨了文本描述在限定潜在动作选择中的作用，证明了文本描述在动作预测中的有效性。"
}