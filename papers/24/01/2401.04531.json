{
    "title": "MERA: A Comprehensive LLM Evaluation in Russian",
    "abstract": "arXiv:2401.04531v3 Announce Type: replace-cross  Abstract: Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settin",
    "link": "https://arxiv.org/abs/2401.04531",
    "context": "Title: MERA: A Comprehensive LLM Evaluation in Russian\nAbstract: arXiv:2401.04531v3 Announce Type: replace-cross  Abstract: Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settin",
    "path": "papers/24/01/2401.04531.json",
    "total_tokens": 647,
    "translated_title": "MERA：一种全面的俄语LLM评估",
    "translated_abstract": "arXiv:2401.04531v3 公告类型：替换交叉  翻译摘要：在过去几年中，人工智能研究中最显著的进步之一是在于基础模型（FMs）的崛起，尤其是语言模型（LMs）的发展。随着模型规模的增加，LMs在可量化的方面取得了进步，并在定性特征上有所新发展。然而，尽管研究人员给予了关注，并且LM的应用迅速增长，但我们对这些模型的能力、局限性和相关风险的理解仍需要进一步加深。为了解决这些问题，我们推出了一个开放的Multimodal Evaluation of Russian-language Architectures（MERA），这是一个旨在评估面向俄语语言的基础模型的新指令基准。该基准包括21个评估任务，涵盖了11个技能领域的生成模型，并且设计成为一个黑盒测试，以确保排除数据泄漏。该论文介绍了一种评估FM和LM的方法论，尤其是在零样本和few-shot固定指令设置中。",
    "tldr": "MERA是一个用于评估面向俄语语言的基础模型的新指令基准，它包括21个评估任务，旨在更好地理解这些模型的能力、局限性和风险。"
}