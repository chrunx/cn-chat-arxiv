{
    "title": "Accelerating Multilingual Language Model for Excessively Tokenized Languages",
    "abstract": "arXiv:2401.10660v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) have remarkably enhanced performances on a variety of tasks in multiple languages. However, tokenizers in LLMs trained primarily on English-centric corpora often overly fragment a text into character or Unicode-level tokens in non-Roman alphabetic languages, leading to inefficient text generation. We introduce a simple yet effective framework to accelerate text generation in such languages. Our approach involves employing a new language model head with a vocabulary set tailored to a specific target language for a pre-trained LLM. This is followed by fine-tuning the new head while incorporating a verification step to ensure the model's performance is preserved. We show that this targeted fine-tuning, while freezing other model parameters, effectively reduces token fragmentation for the target language. Our extensive experiments demonstrate that the proposed framework increases ",
    "link": "https://arxiv.org/abs/2401.10660",
    "context": "Title: Accelerating Multilingual Language Model for Excessively Tokenized Languages\nAbstract: arXiv:2401.10660v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) have remarkably enhanced performances on a variety of tasks in multiple languages. However, tokenizers in LLMs trained primarily on English-centric corpora often overly fragment a text into character or Unicode-level tokens in non-Roman alphabetic languages, leading to inefficient text generation. We introduce a simple yet effective framework to accelerate text generation in such languages. Our approach involves employing a new language model head with a vocabulary set tailored to a specific target language for a pre-trained LLM. This is followed by fine-tuning the new head while incorporating a verification step to ensure the model's performance is preserved. We show that this targeted fine-tuning, while freezing other model parameters, effectively reduces token fragmentation for the target language. Our extensive experiments demonstrate that the proposed framework increases ",
    "path": "papers/24/01/2401.10660.json",
    "total_tokens": 322,
    "tldr": "该文章提供了一种简单有效的方法，能够在不影响原始模型性能的前提下，通过针对特定目标语言的重新训练和一个验证步骤，降低大型语言模型在非罗马字母语言中造成的文本过度碎片化问题。"
}