{
    "title": "Rethinking Jailbreaking through the Lens of Representation Engineering",
    "abstract": "arXiv:2401.06824v3 Announce Type: replace-cross  Abstract: The recent surge in jailbreaking methods has revealed the vulnerability of Large Language Models (LLMs) to malicious inputs. While earlier research has primarily concentrated on increasing the success rates of jailbreaking attacks, the underlying mechanism for safeguarding LLMs remains underexplored. This study investigates the vulnerability of safety-aligned LLMs by uncovering specific activity patterns within the representation space generated by LLMs. Such ``safety patterns'' can be identified with only a few pairs of contrastive queries in a simple method and function as ``keys'' (used as a metaphor for security defense capability) that can be used to open or lock Pandora's Box of LLMs. Extensive experiments demonstrate that the robustness of LLMs against jailbreaking can be lessened or augmented by attenuating or strengthening the identified safety patterns. These findings deepen our understanding of jailbreaking phenomena",
    "link": "https://arxiv.org/abs/2401.06824",
    "context": "Title: Rethinking Jailbreaking through the Lens of Representation Engineering\nAbstract: arXiv:2401.06824v3 Announce Type: replace-cross  Abstract: The recent surge in jailbreaking methods has revealed the vulnerability of Large Language Models (LLMs) to malicious inputs. While earlier research has primarily concentrated on increasing the success rates of jailbreaking attacks, the underlying mechanism for safeguarding LLMs remains underexplored. This study investigates the vulnerability of safety-aligned LLMs by uncovering specific activity patterns within the representation space generated by LLMs. Such ``safety patterns'' can be identified with only a few pairs of contrastive queries in a simple method and function as ``keys'' (used as a metaphor for security defense capability) that can be used to open or lock Pandora's Box of LLMs. Extensive experiments demonstrate that the robustness of LLMs against jailbreaking can be lessened or augmented by attenuating or strengthening the identified safety patterns. These findings deepen our understanding of jailbreaking phenomena",
    "path": "papers/24/01/2401.06824.json",
    "total_tokens": 347,
    "tldr": "该文章通过代表工程学视角重新审视越狱问题，揭示了针对大型语言模型（LLMs）安全攻击的成功率提高背后的机制，同时提出了通过在LLMs产生的表示空间中识别和调节特定安全模式的方法，可能有助于增强或减弱LLMs对抗越狱攻击的抵抗力。"
}