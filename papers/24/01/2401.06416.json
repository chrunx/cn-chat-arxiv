{
    "title": "Mission: Impossible Language Models",
    "abstract": "arXiv:2401.06416v2 Announce Type: replace-cross  Abstract: Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially",
    "link": "https://arxiv.org/abs/2401.06416",
    "context": "Title: Mission: Impossible Language Models\nAbstract: arXiv:2401.06416v2 Announce Type: replace-cross  Abstract: Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially",
    "path": "papers/24/01/2401.06416.json",
    "total_tokens": 346,
    "tldr": "该文章展示了如何使用大型语言模型来学习各种复杂的合成不可能语言，这些语言是根据非自然的词序和语法规则对英语数据的系统性更改。作者报告了对于GPT-2小型模型在这些偶然被认为是不可学习语言上的广泛评估，并探索了语言学习的界限，以及语言模型在不同复杂性下学习不可能语言的能力。"
}