{
    "title": "Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives",
    "abstract": "arXiv:2404.11317v2 Announce Type: replace  Abstract: The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be ef",
    "link": "https://arxiv.org/abs/2404.11317",
    "context": "Title: Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives\nAbstract: arXiv:2404.11317v2 Announce Type: replace  Abstract: The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be ef",
    "path": "papers/24/04/2404.11317.json",
    "total_tokens": 345,
    "tldr": "该文章提出了一种通过对比学习对正负样本进行比例缩放的方法，改进了基于复合图像检索的任务，使用生成数据的方法解决正样本不足的问题，同时通过两阶段训练框架增加负样本数量，优化了模型在检索任务中的表现。"
}