{
    "title": "Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",
    "abstract": "arXiv:2404.11129v2 Announce Type: replace  Abstract: The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks. Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision. Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales th",
    "link": "https://arxiv.org/abs/2404.11129",
    "context": "Title: Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales\nAbstract: arXiv:2404.11129v2 Announce Type: replace  Abstract: The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks. Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision. Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales th",
    "path": "papers/24/04/2404.11129.json",
    "total_tokens": 750,
    "translated_title": "论文标题: Fact: 使用忠实、简洁且可转移的论据教 LLM 多模态语言模型",
    "translated_abstract": "论文摘要: arXiv:2404.11129v2 公告类型: 替换 论文摘要: 多模态大型语言模型 (MLLMs) 的卓越性能无可争议地展示了它们在处理广泛的视觉任务方面的理解能力。然而，它们的黑盒推理过程仍然是未解之谜，使它们不可解释，并且在幻觉方面挣扎。这些模型执行复杂的组成推理任务的能力也受到了限制，最终导致这些模型学习的停滞不前。在这项工作中，我们介绍了一种名为Fact的新范式，用于为多模态语言模型生成忠实、简洁且可转移的论据。这种范式使用了可验证的视觉编程来生成可执行代码，保证了忠实性和精确性。随后，通过一系列操作，包括修剪、合并和桥接，理由增强了其简洁性。此外，我们还过滤了智能体的论据，以确保它们拥有更加丰富的知识结构。我们还构建了一个多个视觉任务的数据集，集成了MLLMs+Fact的优势，证明了我们方法的具体优势，以及对未来的研究方向进行了一系列探讨。事实这一范式提供了一种革命性的解决方案，以此来解锁大型模态语言模型在视觉任务中的深层理解和推理过程。",
    "tldr": "论文要点: Fact 范式通过生成多模态语言模型的忠实、简洁、可转移论据，解锁了他们在视觉任务中的深层理解和推理过程。努力解决当前的MLLMs在透明性、可解释性以及执行复杂任务方面的挑战。",
    "en_tdlr": "论文要点: Fact introduces a novel framework for generating faithful, concise, and transferable rationales to teach multimodal LLMs, overcoming their current challenges in transparency, interpretability, and executing complex tasks."
}