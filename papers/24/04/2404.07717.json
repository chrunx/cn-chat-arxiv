{
    "title": "Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics",
    "abstract": "arXiv:2404.07717v3 Announce Type: replace  Abstract: Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's ref",
    "link": "https://arxiv.org/abs/2404.07717",
    "context": "Title: Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics\nAbstract: arXiv:2404.07717v3 Announce Type: replace  Abstract: Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's ref",
    "path": "papers/24/04/2404.07717.json",
    "total_tokens": 429,
    "tldr": "该文章展示了大型语言模型（LLMs）和视觉-语言模型（VLMs）在机器人中用于低级别认知任务的潜力，特别是在对物体反射率的估计上。通过利用语言分布性语义中蕴含的人类知识，以及VLMs在图像基础反射率估计方面的泛化能力，研究人员成功地通过文本输入对物体的反射率进行了预测，并且证明这一过程可以通过实际的机器人臂的行为来验证，从而提高了机器人在复杂环境中的感知-动作一致性。"
}