{
    "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data",
    "abstract": "arXiv:2404.05530v2 Announce Type: replace-cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or ne",
    "link": "https://arxiv.org/abs/2404.05530",
    "context": "Title: Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data\nAbstract: arXiv:2404.05530v2 Announce Type: replace-cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or ne",
    "path": "papers/24/04/2404.05530.json",
    "total_tokens": 417,
    "tldr": "该文章研究了如何通过向用于训练语言模型（LM）的人类偏好数据中注入不良偏好对，来攻击基于人类反馈的强化学习（RLHF）。作者提出了一种策略，通过构建不良偏好对，并测试其对两个广泛使用的偏好数据的污染效果。结果表明，偏好污染是高度有效的：即使注入的不良数据仅占原始数据集的一小部分（1%-5%），也可以成功操纵LM生成目标实体并带有目标情感。这说明恶意行为者有可能通过污染偏好数据来影响语言模型的输出。"
}