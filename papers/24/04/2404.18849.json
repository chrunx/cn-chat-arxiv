{
    "title": "MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection",
    "abstract": "arXiv:2404.18849v2 Announce Type: replace  Abstract: In real-world scenarios, using multiple modalities like visible (RGB) and infrared (IR) can greatly improve the performance of a predictive task such as object detection (OD). Multimodal learning is a common way to leverage these modalities, where multiple modality-specific encoders and a fusion module are used to improve performance. In this paper, we tackle a different way to employ RGB and IR modalities, where only one modality or the other is observed by a single shared vision encoder. This realistic setting requires a lower memory footprint and is more suitable for applications such as autonomous driving and surveillance, which commonly rely on RGB and IR data. However, when learning a single encoder on multiple modalities, one modality can dominate the other, producing uneven recognition results. This work investigates how to efficiently leverage RGB and IR modalities to train a common transformer-based OD vision encoder, while",
    "link": "https://arxiv.org/abs/2404.18849",
    "context": "Title: MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection\nAbstract: arXiv:2404.18849v2 Announce Type: replace  Abstract: In real-world scenarios, using multiple modalities like visible (RGB) and infrared (IR) can greatly improve the performance of a predictive task such as object detection (OD). Multimodal learning is a common way to leverage these modalities, where multiple modality-specific encoders and a fusion module are used to improve performance. In this paper, we tackle a different way to employ RGB and IR modalities, where only one modality or the other is observed by a single shared vision encoder. This realistic setting requires a lower memory footprint and is more suitable for applications such as autonomous driving and surveillance, which commonly rely on RGB and IR data. However, when learning a single encoder on multiple modalities, one modality can dominate the other, producing uneven recognition results. This work investigates how to efficiently leverage RGB and IR modalities to train a common transformer-based OD vision encoder, while",
    "path": "papers/24/04/2404.18849.json",
    "total_tokens": 663,
    "translated_title": "MiPa：混合补丁红外-可见模式 agnostic 对象检测",
    "translated_abstract": "arXiv:2404.18849v2 公告类型：替换  译文摘要：在实际场景中，使用多种模式，如可见（RGB）和红外（IR），可以大大提高预测任务，如对象检测（OD）的性能。多模态学习是一种常见的方法，利用这些模式，其中多个模态特定的编码器和融合模块用于提高性能。在本文中，我们探讨了一种不同的方式来利用RGB和IR模式，其中只有一个模式或另一个模式被单个共享视觉编码器观察到。这种现实的情况要求较小的内存占用，并且更适合依赖于RGB和IR数据的应用程序，如自动驾驶和监控。然而，在多个模式上训练单一编码器时，一种模式可能会主导另一种模式，产生不均衡的识别结果。本文研究了如何高效地利用RGB和IR模式，以训练一个基于transformer的OD视觉编码器，同时...",
    "tldr": "该研究探索了一种共享的视觉编码器如何在单个模型中有效地整合RGB和IR数据，以提高对象检测的性能，同时减少内存占用，适用于自动驾驶和监控等实际应用。"
}