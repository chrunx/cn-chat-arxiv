{
    "title": "Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph",
    "abstract": "arXiv:2404.03623v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis hi",
    "link": "https://arxiv.org/abs/2404.03623",
    "context": "Title: Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph\nAbstract: arXiv:2404.03623v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis hi",
    "path": "papers/24/04/2404.03623.json",
    "total_tokens": 467,
    "tldr": "该文章探讨了大型语言模型在事实知识存储与处理方面的能力，并通过提出一种框架，能够解析出LLM在处理语句时所依赖的知识图，揭示了知识在模型内部从矢量形式到谓词集合的转化过程。文章使用了一种名为激活调制的工具，它允许实时改变模型在推理过程中对单个词元的响应，以提取隐藏在这些词元内部的实体信息。尽管这些实体知识并不直接由训练获得，而是在与输入信号交互的过程中逐步构建，但文章已经展示了如何通过这种调制的手段提取并解析这些知识，并在两个不同的句级事实判断数据集上实施了测试，探索了知识片段在模型不同层次上的体现。\n\n简而言之，该研究揭示了LLM处理语句时所根据的知识图谱，实现了事实信息的提取解析，并且证明了通过一种新型激活调制方法可以对模型内部知识进行有效的挖掘。"
}