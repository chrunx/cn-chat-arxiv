{
    "title": "Tell Me What's Next: Textual Foresight for Generic UI Representations",
    "abstract": "arXiv:2406.07822v2 Announce Type: replace  Abstract: Mobile app user interfaces (UIs) are rich with action, text, structure, and image content that can be utilized to learn generic UI representations for tasks like automating user commands, summarizing content, and evaluating the accessibility of user interfaces. Prior work has learned strong visual representations with local or global captioning losses, but fails to retain both granularities. To combat this, we propose Textual Foresight, a novel pretraining objective for learning UI screen representations. Textual Foresight generates global text descriptions of future UI states given a current UI and local action taken. Our approach requires joint reasoning over elements and entire screens, resulting in improved UI features: on generation tasks, UI agents trained with Textual Foresight outperform state-of-the-art by 2% with 28x fewer images. We train with our newly constructed mobile app dataset, OpenApp, which results in the first pu",
    "link": "https://arxiv.org/abs/2406.07822",
    "context": "Title: Tell Me What's Next: Textual Foresight for Generic UI Representations\nAbstract: arXiv:2406.07822v2 Announce Type: replace  Abstract: Mobile app user interfaces (UIs) are rich with action, text, structure, and image content that can be utilized to learn generic UI representations for tasks like automating user commands, summarizing content, and evaluating the accessibility of user interfaces. Prior work has learned strong visual representations with local or global captioning losses, but fails to retain both granularities. To combat this, we propose Textual Foresight, a novel pretraining objective for learning UI screen representations. Textual Foresight generates global text descriptions of future UI states given a current UI and local action taken. Our approach requires joint reasoning over elements and entire screens, resulting in improved UI features: on generation tasks, UI agents trained with Textual Foresight outperform state-of-the-art by 2% with 28x fewer images. We train with our newly constructed mobile app dataset, OpenApp, which results in the first pu",
    "path": "papers/24/06/2406.07822.json",
    "total_tokens": 400,
    "tldr": "该文章提出了一种名为“文本前瞻”的预训练目标，用于学习用户界面的表示，通过生成未来用户界面状态的全球文本描述来改善界面元素和整个屏幕的联合推理。这种方法在生成任务上取得了显著的改进，在较少的图像数量下，与现有技术相比，生成了更精确的用户界面描述。此外，该研究还构建了名为“OpenApp”的移动应用程序数据集，为该领域提供了首个公开的移动应用程序数据集。"
}