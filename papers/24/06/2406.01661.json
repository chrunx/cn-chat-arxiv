{
    "title": "A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization",
    "abstract": "arXiv:2406.01661v2 Announce Type: replace-cross  Abstract: Learning to sample from intractable distributions over discrete sets without relying on corresponding training data is a central problem in a wide range of fields, including Combinatorial Optimization. Currently, popular deep learning-based approaches rely primarily on generative models that yield exact sample likelihoods. This work introduces a method that lifts this restriction and opens the possibility to employ highly expressive latent variable models like diffusion models. Our approach is conceptually based on a loss that upper bounds the reverse Kullback-Leibler divergence and evades the requirement of exact sample likelihoods. We experimentally validate our approach in data-free Combinatorial Optimization and demonstrate that our method achieves a new state-of-the-art on a wide range of benchmark problems.",
    "link": "https://arxiv.org/abs/2406.01661",
    "context": "Title: A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization\nAbstract: arXiv:2406.01661v2 Announce Type: replace-cross  Abstract: Learning to sample from intractable distributions over discrete sets without relying on corresponding training data is a central problem in a wide range of fields, including Combinatorial Optimization. Currently, popular deep learning-based approaches rely primarily on generative models that yield exact sample likelihoods. This work introduces a method that lifts this restriction and opens the possibility to employ highly expressive latent variable models like diffusion models. Our approach is conceptually based on a loss that upper bounds the reverse Kullback-Leibler divergence and evades the requirement of exact sample likelihoods. We experimentally validate our approach in data-free Combinatorial Optimization and demonstrate that our method achieves a new state-of-the-art on a wide range of benchmark problems.",
    "path": "papers/24/06/2406.01661.json",
    "total_tokens": 303,
    "tldr": "该文章提出了一种不依赖于对应训练数据的学习从离散集合分布中采样的新框架，该方法使用深在学习技术和扩散模型进行无监督的组合优化，开创了使用高度灵活的潜在变量模型的新可能性。"
}