{
    "title": "Investigating and Defending Shortcut Learning in Personalized Diffusion Models",
    "abstract": "arXiv:2406.18944v3 Announce Type: replace  Abstract: Personalized diffusion models have gained popularity for adapting pre-trained text-to-image models to generate images of specific topics with minimal training data. However, these models are vulnerable to minor adversarial perturbations, leading to degraded performance on corrupted datasets. Such vulnerabilities are further exploited to craft protective perturbations on sensitive images like portraits that prevent unauthorized generation. In response, diffusion-based purification methods have been proposed to remove these perturbations and retain generation performance. However, existing works turn to over-purifying the images, which causes information loss. In this paper, we take a closer look at the fine-tuning process of personalized diffusion models through the lens of shortcut learning. And we propose a hypothesis explaining the manipulation mechanisms of existing perturbation methods, demonstrating that perturbed images signifi",
    "link": "https://arxiv.org/abs/2406.18944",
    "context": "Title: Investigating and Defending Shortcut Learning in Personalized Diffusion Models\nAbstract: arXiv:2406.18944v3 Announce Type: replace  Abstract: Personalized diffusion models have gained popularity for adapting pre-trained text-to-image models to generate images of specific topics with minimal training data. However, these models are vulnerable to minor adversarial perturbations, leading to degraded performance on corrupted datasets. Such vulnerabilities are further exploited to craft protective perturbations on sensitive images like portraits that prevent unauthorized generation. In response, diffusion-based purification methods have been proposed to remove these perturbations and retain generation performance. However, existing works turn to over-purifying the images, which causes information loss. In this paper, we take a closer look at the fine-tuning process of personalized diffusion models through the lens of shortcut learning. And we propose a hypothesis explaining the manipulation mechanisms of existing perturbation methods, demonstrating that perturbed images signifi",
    "path": "papers/24/06/2406.18944.json",
    "total_tokens": 306,
    "tldr": "该文章揭示了个性化扩散模型在对抗性攻击下的脆弱性，并提出了一种保护隐私的新方法，该方法通过在图像生成过程中引入特定的过滤技术，可以在抵抗对抗性攻击的同时最小化信息损失。"
}