{
    "title": "Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation",
    "abstract": "arXiv:2406.07867v2 Announce Type: replace  Abstract: In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. ",
    "link": "https://arxiv.org/abs/2406.07867",
    "context": "Title: Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation\nAbstract: arXiv:2406.07867v2 Announce Type: replace  Abstract: In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. ",
    "path": "papers/24/06/2406.07867.json",
    "total_tokens": 827,
    "translated_title": "现实对话：面向面对面对话的口语对话模型",
    "translated_abstract": "在本文中，我们介绍了一种新型的面向面对面对话的口语对话模型。该模型处理用户的音频-视觉言语输入，并产生音频-视觉言语作为回应，标志着朝着创建不依赖中间文本的虚拟助手迈出了第一步。为此，我们新引入了MultiDialog，这是首个大规模的多模态（即声音和视觉）口语对话语料库，含约340小时的9,000多个对话的平行音频-视觉记录，它们是基于开放域对话数据集TopicalChat录制的。MultiDialog包含了对话伙伴根据给定脚本进行角色扮演并进行情感标注的音频-视频对话记录，我们期待这些记录将为多模态合成研究开辟新的机会。我们的面向面对面对话的口语对话模型结合了经过文本预训练的大型语言模型，并将其适应到音频-视觉口语对话领域，通过将语音-文本联合预训练纳入模型之中。",
    "tldr": "我们介绍了一个新型的面向面对面对话的口语对话模型，该模型能够处理音频-视觉言语输入并产生相应的回应。这是创建不依赖中间文本虚拟助手的第一步。我们新引入了MultiDialog，这是首个大规模的多模态口语对话语料库，含约340小时的9,000多个对话的平行音频-视觉记录，这些记录是基于开放域对话数据集TopicalChat录制的。"
}