{
    "title": "Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation",
    "abstract": "arXiv:2406.09305v2 Announce Type: replace  Abstract: In subject-driven text-to-image generation, recent works have achieved superior performance by training the model on synthetic datasets containing numerous image pairs. Trained on these datasets, generative models can produce text-aligned images for specific subject from arbitrary testing image in a zero-shot manner. They even outperform methods which require additional fine-tuning on testing images. However, the cost of creating such datasets is prohibitive for most researchers. To generate a single training pair, current methods fine-tune a pre-trained text-to-image model on the subject image to capture fine-grained details, then use the fine-tuned model to create images for the same subject based on creative text prompts. Consequently, constructing a large-scale dataset with millions of subjects can require hundreds of thousands of GPU hours. To tackle this problem, we propose Toffee, an efficient method to construct datasets for ",
    "link": "https://arxiv.org/abs/2406.09305",
    "context": "Title: Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation\nAbstract: arXiv:2406.09305v2 Announce Type: replace  Abstract: In subject-driven text-to-image generation, recent works have achieved superior performance by training the model on synthetic datasets containing numerous image pairs. Trained on these datasets, generative models can produce text-aligned images for specific subject from arbitrary testing image in a zero-shot manner. They even outperform methods which require additional fine-tuning on testing images. However, the cost of creating such datasets is prohibitive for most researchers. To generate a single training pair, current methods fine-tune a pre-trained text-to-image model on the subject image to capture fine-grained details, then use the fine-tuned model to create images for the same subject based on creative text prompts. Consequently, constructing a large-scale dataset with millions of subjects can require hundreds of thousands of GPU hours. To tackle this problem, we propose Toffee, an efficient method to construct datasets for ",
    "path": "papers/24/06/2406.09305.json",
    "total_tokens": 369,
    "tldr": "该文章提出了一种名为Toffee的效率百万级别的数据集构建方法，用于针对特定主题的文本到图像生成。通过该算法，可以高效地训练文本到图像生成模型，使其能够在无需对测试图像进行进一步微调的情况下，直接根据任意文本生成主题相关的图像，从而降低创造大型规模数据集所需的大量GPU计算资源。"
}