{
    "title": "Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model",
    "abstract": "arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo",
    "link": "https://arxiv.org/abs/2406.19905",
    "context": "Title: Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model\nAbstract: arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo",
    "path": "papers/24/06/2406.19905.json",
    "total_tokens": 354,
    "tldr": "该文章提出的STGC方法通过分析专家中的token级别梯度，有效解决了Mixture-of-Experts方法在处理Large Vision-Language Models时产生的token级梯度冲突问题。通过这种方式，可以提高专家处理tokens的效率和性能。"
}