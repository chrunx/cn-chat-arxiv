{
    "title": "Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model",
    "abstract": "arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo",
    "link": "https://arxiv.org/abs/2406.19905",
    "context": "Title: Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model\nAbstract: arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo",
    "path": "papers/24/06/2406.19905.json",
    "total_tokens": 767,
    "translated_title": "解决混合专家模型中的大视觉语言模型中的项目标梯度冲突",
    "translated_abstract": "arXiv:2406.19905v2 公告类型：替换 摘要：在研究大型视觉语言模型（LVLMs）的过程中，混合专家（MoE）模型引起了越来越多的关注。它使用稀疏模型来替换稠密模型，在推理期间激活较少的数据量和参数，从而显著降低了推理成本。在LVLMs中的现有MoE方法鼓励不同的专家处理不同的项目标，它们通常使用一个路由器来预测每个项目标的派遣。然而，这些预测仅基于示例特征，并不能真正揭示被分配到专家中的项目的优化方向。这可能在分配给同一个专家的不同项目中引起严重的优化干扰。为了解决这个问题，本文提出了一种基于项目标级梯度分析的新方法，即解决项目标梯度冲突（STGC）。具体来说，我们首先使用项目标级梯度来识别专家中的冲突项目标。然后，我们为每个冲突项目标添加了一个专门的学习项，并通过使用一组初步的部署标签，它们的更新将成为主监督的补充。在我们的实验中，SimP已成功应用于Transformer-XL和T5模型，并验证了其有效性。它在不增加模型复杂性的情况下，可提高模型性能，尤其是语义下游任务的性能。此外，我们还发现，通过逐步从低层级到高层级传递预测标签，可以进一步提升性能。",
    "tldr": "本文提出的方法旨在解决在大视觉语言模型中使用混合专家模式时出现的项目标梯度冲突问题，通过使用专门的学习项和初步的部署标签来优化模型性能。"
}