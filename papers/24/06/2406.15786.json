{
    "title": "What Matters in Transformers? Not All Attention is Needed",
    "abstract": "arXiv:2406.15786v4 Announce Type: replace-cross  Abstract: Scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks. However, it also introduces redundant structures, posing challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different modules, such as MLP and Attention layers, is under-explored. In this work, we investigate the varying redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. This metric operates on the premise that redundant structures produce outputs highly similar to their inputs. Surprisingly, while attention layers are essential for transformers and distinguish them from other mainstream architectures, we found that a large proportion of attention layers exhibit excessively high similarity and can be safely pruned without degrading performance, leading to reduc",
    "link": "https://arxiv.org/abs/2406.15786",
    "context": "Title: What Matters in Transformers? Not All Attention is Needed\nAbstract: arXiv:2406.15786v4 Announce Type: replace-cross  Abstract: Scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks. However, it also introduces redundant structures, posing challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different modules, such as MLP and Attention layers, is under-explored. In this work, we investigate the varying redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. This metric operates on the premise that redundant structures produce outputs highly similar to their inputs. Surprisingly, while attention layers are essential for transformers and distinguish them from other mainstream architectures, we found that a large proportion of attention layers exhibit excessively high similarity and can be safely pruned without degrading performance, leading to reduc",
    "path": "papers/24/06/2406.15786.json",
    "total_tokens": 360,
    "tldr": "该文章研究发现，在Transformer模型中，虽然注意力层对于模型的重要性不言而喻，但并不是所有的注意力层都需要保持。通过对模型内部不同模块（如MLP和注意力层）的相似性进行分析，作者发现存在大量的重复结构，这些重复结构在模型的输出中表现得与输入高度相似。这表明在模型优化过程中可以安全地去除一些不必要的注意力层，而不会显著影响模型性能。"
}