{
    "title": "Towards Synchronous Memorizability and Generalizability with Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation",
    "abstract": "arXiv:2406.18037v2 Announce Type: replace  Abstract: The ability to learn sequentially from different data sites is crucial for a deep network in solving practical medical image diagnosis problems due to privacy restrictions and storage limitations. However, adapting on incoming site leads to catastrophic forgetting on past sites and decreases generalizablity on unseen sites. Existing Continual Learning (CL) and Domain Generalization (DG) methods have been proposed to solve these two challenges respectively, but none of them can address both simultaneously. Recognizing this limitation, this paper proposes a novel training paradigm, learning towards Synchronous Memorizability and Generalizability (SMG-Learning). To achieve this, we create the orientational gradient alignment to ensure memorizability on previous sites, and arbitrary gradient alignment to enhance generalizability on unseen sites. This approach is named as Parallel Gradient Alignment (PGA). Furthermore, we approximate the ",
    "link": "https://arxiv.org/abs/2406.18037",
    "context": "Title: Towards Synchronous Memorizability and Generalizability with Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation\nAbstract: arXiv:2406.18037v2 Announce Type: replace  Abstract: The ability to learn sequentially from different data sites is crucial for a deep network in solving practical medical image diagnosis problems due to privacy restrictions and storage limitations. However, adapting on incoming site leads to catastrophic forgetting on past sites and decreases generalizablity on unseen sites. Existing Continual Learning (CL) and Domain Generalization (DG) methods have been proposed to solve these two challenges respectively, but none of them can address both simultaneously. Recognizing this limitation, this paper proposes a novel training paradigm, learning towards Synchronous Memorizability and Generalizability (SMG-Learning). To achieve this, we create the orientational gradient alignment to ensure memorizability on previous sites, and arbitrary gradient alignment to enhance generalizability on unseen sites. This approach is named as Parallel Gradient Alignment (PGA). Furthermore, we approximate the ",
    "path": "papers/24/06/2406.18037.json",
    "total_tokens": 386,
    "tldr": "该文章提出了一种名为SMG-Learning的学习范式，旨在同时提高模型对不同数据源的记忆能力和对新数据源的泛化能力。文章通过创建定向梯度对齐策略，确保模型能够记忆过去数据源的信息，并通过任意梯度对齐策略提高对新数据源的泛化能力。通过这种平行梯度对齐技术，模型能够在不遗忘先前学习内容的同时，在新数据源上表现出良好的泛化性能。"
}