{
    "title": "CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition",
    "abstract": "arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea",
    "link": "https://arxiv.org/abs/2406.11340",
    "context": "Title: CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition\nAbstract: arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea",
    "path": "papers/24/06/2406.11340.json",
    "total_tokens": 753,
    "translated_title": "CM2-Net：不断的多模态映射网络用于驾驶员行为识别",
    "translated_abstract": "arXiv:2406.11340v3 公告类型：替换  翻译：驾驶员行为识别在通过整合多种模态（如红外和深度）来增强驾驶员与车辆互动并确保行车安全方面取得了显著进展。然而，与仅使用RGB模态相比，在车舱环境中收集所有类型非RGB模态的数据总是既费时又昂贵。因此， previous works 建议独立地通过在RGB视频上预先训练的模型来微调每个非RGB模态，但这些方法在面对新的模态时提取有信息特征时不太有效，因为存在较大的领域差距。相反，我们提出了一个不断的多模态映射网络（CM2-Net），可以在不断学习的每种新模态上提供从先前学习的模态的指导性提示。特别是，我们开发了一种累积的跨模态映射提示（ACMP）技术，用于将辨别性和信息丰富的特征从不同模态映射出来。为了有效地解决多个模态的跨界学习难题，我们创新地提出了一种模态级和通道级的多级映射策略，该策略能够捕获和整合来自不同模态和通道多尺度、多角度的空间和时间特征。通过在真实世界数据集上的实验结果证明，CM2-Net不仅能够不断学习新模态的特征表示，而且还能够在多种模态的数据上训练出一个泛化性强的模型，实现更好的驾驶员行为识别精度。",
    "tldr": "CM2-Net提出了一种创新的不断学习新模态特征表示的多模态映射网络，能够在多种模态数据上提高驾驶员行为识别的精度。"
}