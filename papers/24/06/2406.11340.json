{
    "title": "CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition",
    "abstract": "arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea",
    "link": "https://arxiv.org/abs/2406.11340",
    "context": "Title: CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition\nAbstract: arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea",
    "path": "papers/24/06/2406.11340.json",
    "total_tokens": 343,
    "tldr": "该文章提出了一种名为CM2-Net的连续跨模态映射网络，该网络能够持续学习新模态信息，并通过从以往模态获得的指示信息来增强特征提取能力，从而有效地处理不同模态之间的显著域间隙。"
}