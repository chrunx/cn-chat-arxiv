{
    "title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
    "abstract": "arXiv:2405.15750v2 Announce Type: replace-cross  Abstract: This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.",
    "link": "https://arxiv.org/abs/2405.15750",
    "context": "Title: Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence\nAbstract: arXiv:2405.15750v2 Announce Type: replace-cross  Abstract: This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.",
    "path": "papers/24/05/2405.15750.json",
    "total_tokens": 380,
    "tldr": "该文章提出了Filtered Corpus Training（FiCT）方法，这是一种训练语言模型（LMs）的新方法，通过从训练数据中过滤掉特定的句法结构来训练模型，并检验了语言模型对基于间接证据的语言泛化能力。通过在LSTM和Transformer两种不同类型的语言模型上进行实验，该研究开发了一系列针对各种语言现象的过滤后的语料库。实验结果显示，尽管Transformer在理解文本和生成源代码方面表现得更好，但两种类型的语言模型在基于间接证据的语言泛化测试中表现出了几乎相同的优异水平，这表明了语言模型具有出色的间接泛化能力。"
}