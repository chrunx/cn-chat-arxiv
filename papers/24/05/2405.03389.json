{
    "title": "Don't Waste Your Time: Early Stopping Cross-Validation",
    "abstract": "arXiv:2405.03389v2 Announce Type: replace-cross  Abstract: State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit. However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration. While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget. We aim to make model selection with cross-validation more effective. Therefore, we study early stopping the process of cross-validation during model selection. We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets. We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds. In addition, we inve",
    "link": "https://arxiv.org/abs/2405.03389",
    "context": "Title: Don't Waste Your Time: Early Stopping Cross-Validation\nAbstract: arXiv:2405.03389v2 Announce Type: replace-cross  Abstract: State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit. However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration. While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget. We aim to make model selection with cross-validation more effective. Therefore, we study early stopping the process of cross-validation during model selection. We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets. We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds. In addition, we inve",
    "path": "papers/24/05/2405.03389.json",
    "total_tokens": 716,
    "translated_title": "不要浪费您的宝贵时间：早期停止交叉验证",
    "translated_abstract": "arXiv:2405.03389v2 Announce Type: 替换样本摘要：在表数据自动化机器学习系统中，标准的机器学习技术通常采用交叉验证；确保测量的性能能够推广到未见过的数据，或者随后的集合学习不会过拟合。然而，与留出验证相比，使用k-折交叉验证显著增加了验证单个配置的计算成本。虽然确保了更好的泛化能力，并且由此增强了性能，但额外的成本往往超过了在时间预算内进行有效模型选择的能力。我们的目标是使带有交叉验证的模型选择更加有效。因此，我们研究了在模型选择过程中对交叉验证进行早期停顿的影响。我们调查了在两类算法（多层感知机和随机森林）和36个分类数据集上对随机搜索的早期停顿的影响。此外，我们还分析了在考虑3-、5-和10-折交叉验证时，折叠数量对早期停顿的影响。此外，我们还研究了性能指标的敏感性，以确定何时停止交叉验证最有效。实验结果表明，在模型选择过程中采用早期停顿的交叉验证是可行的，它可以显著减少训练时间和成本，同时保持或提高性能。",
    "tldr": "通过早期停止交叉验证的过程，研究人员减少了模型选择中不必要的计算成本，同时保持或提高了性能，这对于在有限的时间预算内有效进行模型选择至关重要。"
}