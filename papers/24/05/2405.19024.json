{
    "title": "Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory",
    "abstract": "arXiv:2405.19024v3 Announce Type: replace-cross  Abstract: We consider inverse reinforcement learning problems with concave utilities. Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function. CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others. Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent. There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions. However, inverse RL for CURL problems has not been considered previously. In this paper we show that most of the standard IRL",
    "link": "https://arxiv.org/abs/2405.19024",
    "context": "Title: Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory\nAbstract: arXiv:2405.19024v3 Announce Type: replace-cross  Abstract: We consider inverse reinforcement learning problems with concave utilities. Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function. CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others. Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent. There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions. However, inverse RL for CURL problems has not been considered previously. In this paper we show that most of the standard IRL",
    "path": "papers/24/05/2405.19024.json",
    "total_tokens": 660,
    "translated_title": "逆凹效用强化学习是逆游戏理论",
    "translated_abstract": "arXiv:2405.19024v3 公告类型：替换-交叉  摘要：我们考虑具有凹效用的逆强化学习问题。凹效用强化学习（CURL）是标准RL目标的一般化，它使用了状态的占用度量凹函数，而不是线性函数。CURL因其能够代表许多重要应用实例，包括标准的RL例如模仿学习、纯探索、受限MDP、离线RL、人造行为规范化RL等而最近受到了关注。逆强化学习是一种强大的范式，它专注于恢复一个未知的行为奖励函数，它能够为观察到的行为提供合理的解释。到目前为止，关于逆强化学习的理论研究已经取得了一些进展，其中问题被形式化为识别可行的奖励函数集合。然而，对于CURL问题的逆强化学习尚未被考虑。在这篇论文中，我们展示了大多数标准的IRL研究方法都可以应用于逆凹效用强化学习问题，并且提供了一个新的统一框架来解决此类问题，即使在凹效用设置中也能够确保可行奖励函数的识别。",
    "tldr": "这篇论文提出了一个新的统一框架，能够解决逆凹效用强化学习问题，即使在凹效用设置中也能够确保可行奖励函数的识别。",
    "en_tdlr": "This paper presents a new unified framework for solving inverse concave-utility reinforcement learning problems, ensuring the identification of feasible reward functions even in the case of concave utility settings."
}