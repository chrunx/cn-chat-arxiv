{
    "title": "Opinion-Guided Reinforcement Learning",
    "abstract": "arXiv:2405.17287v2 Announce Type: replace-cross  Abstract: Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence can be produced. Thus, guiding reinforcement learning agents by way of opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle) and human advisors, at different levels of uncertainty, and under multiple advice strategies. Ou",
    "link": "https://arxiv.org/abs/2405.17287",
    "context": "Title: Opinion-Guided Reinforcement Learning\nAbstract: arXiv:2405.17287v2 Announce Type: replace-cross  Abstract: Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence can be produced. Thus, guiding reinforcement learning agents by way of opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle) and human advisors, at different levels of uncertainty, and under multiple advice strategies. Ou",
    "path": "papers/24/05/2405.17287.json",
    "total_tokens": 341,
    "tldr": "该文章提出了一种通过意见指导强化学习Agent的端到端方法，通过建模和管理指导者的意见来增强学习过程的性能。在评估中，无论是基于人工代理还是人类导师，该方法在不确定性和不同指导策略下均显示出了有价值的性能提升。"
}