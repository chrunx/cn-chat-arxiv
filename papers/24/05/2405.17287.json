{
    "title": "Opinion-Guided Reinforcement Learning",
    "abstract": "arXiv:2405.17287v2 Announce Type: replace-cross  Abstract: Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence can be produced. Thus, guiding reinforcement learning agents by way of opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle) and human advisors, at different levels of uncertainty, and under multiple advice strategies. Ou",
    "link": "https://arxiv.org/abs/2405.17287",
    "context": "Title: Opinion-Guided Reinforcement Learning\nAbstract: arXiv:2405.17287v2 Announce Type: replace-cross  Abstract: Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence can be produced. Thus, guiding reinforcement learning agents by way of opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle) and human advisors, at different levels of uncertainty, and under multiple advice strategies. Ou",
    "path": "papers/24/05/2405.17287.json",
    "total_tokens": 691,
    "translated_title": "观点指导的强化学习",
    "translated_abstract": "arXiv:2405.17287v2 宣布类型：替换交叉  翻译摘要：在强化学习中，人类指导往往能够提升学习代理的性能。然而，人类指导通常是基于不完全的信息和猜测，而不是系统的论证。虽然这些指导有一定的不确定性，例如由于对问题的部分了解或一无所知而引起，但它们也会比可以得到的硬证据早出现。因此，通过观点指导强化学习代理，有可能实现更有效的学习过程，但也带来了对观点进行建模和管理的问题。本文介绍了一种通过观点指导强化学习代理的方法。为此，我们提供了一个端到端的方法来管理和建模顾问的指导。为此，我们评估了在不同水平的不确定性下，通过多种建议策略，与合成（或acle）和人类顾问的实用性。我们的方法在多个环境下展示了对不同质量和不确定性的指导的不依赖性，并能够提高强化学习代理的性能。同时，方法在经济效率方面也表现出色，在合成顾问条件下，可以避免高达90%的人工决策，而在人类顾问条件下也能够缩小决策的错误率。",
    "tldr": "本文提出了一种通过观点指导强化学习的方法，通过模型的提供者，我们可以更有效的优化强化学习代理在不同的环境和不确定性条件下的表现，同时对于在人工和人类指导下的经济效率都取得了积极的结果。"
}