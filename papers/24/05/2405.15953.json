{
    "title": "Activator: GLU Activation Function as the Core Component of a Vision Transformer",
    "abstract": "arXiv:2405.15953v2 Announce Type: replace  Abstract: Transformer architecture currently represents the main driver behind many successes in a variety of tasks addressed by deep learning, especially the recent advances in natural language processing (NLP) culminating with large language models (LLM). In addition, transformer architecture has found a wide spread of interest from computer vision (CV) researchers and practitioners, allowing for many advancements in vision-related tasks and opening the door for multi-task and multi-modal deep learning architectures that share the same principle of operation. One drawback to these architectures is their reliance on the scaled dot product attention mechanism with the softmax activation function, which is computationally expensive and requires large compute capabilities both for training and inference. This paper investigates substituting the attention mechanism usually adopted for transformer architecture with an architecture incorporating ga",
    "link": "https://arxiv.org/abs/2405.15953",
    "context": "Title: Activator: GLU Activation Function as the Core Component of a Vision Transformer\nAbstract: arXiv:2405.15953v2 Announce Type: replace  Abstract: Transformer architecture currently represents the main driver behind many successes in a variety of tasks addressed by deep learning, especially the recent advances in natural language processing (NLP) culminating with large language models (LLM). In addition, transformer architecture has found a wide spread of interest from computer vision (CV) researchers and practitioners, allowing for many advancements in vision-related tasks and opening the door for multi-task and multi-modal deep learning architectures that share the same principle of operation. One drawback to these architectures is their reliance on the scaled dot product attention mechanism with the softmax activation function, which is computationally expensive and requires large compute capabilities both for training and inference. This paper investigates substituting the attention mechanism usually adopted for transformer architecture with an architecture incorporating ga",
    "path": "papers/24/05/2405.15953.json",
    "total_tokens": 320,
    "tldr": "该文章提出了一种以GELU激活函数为核心组件的Vision Transformer架构，旨在通过替换传统的注意力机制以降低计算成本，提高了模型在计算机视觉任务中的效率和速度，同时保持了性能，展示了激活函数的创新应用在深度学习模型中的优化潜力。"
}