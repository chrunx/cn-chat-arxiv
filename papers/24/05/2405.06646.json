{
    "title": "Diffusion-based Human Motion Style Transfer with Semantic Guidance",
    "abstract": "arXiv:2405.06646v2 Announce Type: replace-cross  Abstract: 3D Human motion style transfer is a fundamental problem in computer graphic and animation processing. Existing AdaIN- based methods necessitate datasets with balanced style distribution and content/style labels to train the clustered latent space. However, we may encounter a single unseen style example in practical scenarios, but not in sufficient quantity to constitute a style cluster for AdaIN-based methods. Therefore, in this paper, we propose a novel two-stage framework for few-shot style transfer learning based on the diffusion model. Specifically, in the first stage, we pre-train a diffusion-based text-to-motion model as a generative prior so that it can cope with various content motion inputs. In the second stage, based on the single style example, we fine-tune the pre-trained diffusion model in a few-shot manner to make it capable of style transfer. The key idea is regarding the reverse process of diffusion as a motion-",
    "link": "https://arxiv.org/abs/2405.06646",
    "context": "Title: Diffusion-based Human Motion Style Transfer with Semantic Guidance\nAbstract: arXiv:2405.06646v2 Announce Type: replace-cross  Abstract: 3D Human motion style transfer is a fundamental problem in computer graphic and animation processing. Existing AdaIN- based methods necessitate datasets with balanced style distribution and content/style labels to train the clustered latent space. However, we may encounter a single unseen style example in practical scenarios, but not in sufficient quantity to constitute a style cluster for AdaIN-based methods. Therefore, in this paper, we propose a novel two-stage framework for few-shot style transfer learning based on the diffusion model. Specifically, in the first stage, we pre-train a diffusion-based text-to-motion model as a generative prior so that it can cope with various content motion inputs. In the second stage, based on the single style example, we fine-tune the pre-trained diffusion model in a few-shot manner to make it capable of style transfer. The key idea is regarding the reverse process of diffusion as a motion-",
    "path": "papers/24/05/2405.06646.json",
    "total_tokens": 343,
    "tldr": "该文章提出一种基于扩散模型的少数样例风格转移学习框架，该框架首先通过文本到动作的模型进行预训练，然后在有限样例基础上对其进行微调，以实现动作风格转移目标。"
}