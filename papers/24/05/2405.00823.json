{
    "title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
    "abstract": "arXiv:2405.00823v2 Announce Type: replace-cross  Abstract: We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being s",
    "link": "https://arxiv.org/abs/2405.00823",
    "context": "Title: WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting\nAbstract: arXiv:2405.00823v2 Announce Type: replace-cross  Abstract: We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being s",
    "path": "papers/24/05/2405.00823.json",
    "total_tokens": 488,
    "tldr": "该文章引入了一个名为WorkBench的基准数据集，用于评估代理在现实工作环境中的任务执行能力。该数据集包含了一个模拟环境，其中包含五个数据库、26种工具和690个任务，这些任务代表常见的业务活动。WorkBench的任务设计具有挑战性，因为它们需要规划和工具选择，以及有时需要多个动作。当任务成功执行时，数据库的值可能会发生变化。每个任务的正确结果是唯一的和无可争议的，这使得自动化评估变得可靠。文章的关键贡献是提出了基于结果的评估方法，并通过五个现有的ReAct代理在WorkBench上的评估展示了这一方法的有效性。尽管GPT-4的表现最佳，但完成的任务也只有43%。文章还揭示了代理在执行错误任务时可能导致的结果，比如发送错误的电子邮件。"
}