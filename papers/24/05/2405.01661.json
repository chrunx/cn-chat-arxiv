{
    "title": "When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX",
    "abstract": "arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat",
    "link": "https://arxiv.org/abs/2405.01661",
    "context": "Title: When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX\nAbstract: arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat",
    "path": "papers/24/05/2405.01661.json",
    "total_tokens": 388,
    "tldr": "该文章提出了一个名为CoReX的概念和关系基于的解释方法，用以在复杂的真实世界领域中（如生物学）揭示和评估卷积神经网络（CNN）的决策过程。该方法通过屏蔽不相关的概念和限制预测模型中可解释的替代模型中的关系，能够提供更深入的解释。此外，文章进行了多种图像数据集的实验，以评估CoReX在解释和评估CNN模型决策中的有效性。"
}