{
    "title": "When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX",
    "abstract": "arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat",
    "link": "https://arxiv.org/abs/2405.01661",
    "context": "Title: When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX\nAbstract: arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat",
    "path": "papers/24/05/2405.01661.json",
    "total_tokens": 717,
    "translated_title": "CoReX：基于关系的概念解释器在探索和评价分类器决定中的应用",
    "translated_abstract": "arXiv:2405.01661v2公告类型：替换交叉 摘要：基于输入像素相关性的卷积神经网络（CNN）解释可能过于不具体，无法评估哪些和如何输入特征影响模型决策。特别是在生物等复杂现实世界领域中，特定概念的存在及其之间的关系可能会区分于不同的类。像素相关性不足以传达此类信息。因此，模型的评价受到限制，数据中存在的相关方面以及影响模型决策的因素可能会被忽略。本工作提出了一种新的方法来解释和评价CNN模型，该方法使用基于概念和关系的解释器（CoReX）。它通过从决策过程中屏蔽（不）相关的概念以及通过限制学习到的可解释的替代模型中的关系，对一组图像的预测行为进行解释。我们使用多种图像数据集对我们的方法进行了测试，并且证明CoReX能够为决策提供更明确的概念和关系的解释，从而有助于更深入理解模型预测的内在机制。此外，我们还通过实验验证了CoReX能够在生物医学图像分析等领域中利用概念和关系的解释来有效提高模型的透明性和可解释性。",
    "tldr": "CoReX是一种新的基于概念和关系的解释器，可以帮助更深入理解模型预测的内在机制，并提高模型在生物医学图像分析等领域的透明性和可解释性。"
}