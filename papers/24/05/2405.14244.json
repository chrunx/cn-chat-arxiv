{
    "title": "Tell me why: Training preferences-based RL with human preferences and step-level explanations",
    "abstract": "arXiv:2405.14244v2 Announce Type: replace  Abstract: Human-in-the-loop reinforcement learning allows the training of agents through various interfaces, even for non-expert humans. Recently, preference-based methods (PbRL), where the human has to give his preference over two trajectories, increased in popularity since they allow training in domains where more direct feedback is hard to formulate. However, the current PBRL methods have limitations and do not provide humans with an expressive interface for giving feedback. With this work, we propose a new preference-based learning method that provides humans with a more expressive interface to provide their preference over trajectories and a factual explanation (or annotation of why they have this preference). These explanations allow the human to explain what parts of the trajectory are most relevant for the preference. We allow the expression of the explanations over individual trajectory steps. We evaluate our method in various simulat",
    "link": "https://arxiv.org/abs/2405.14244",
    "context": "Title: Tell me why: Training preferences-based RL with human preferences and step-level explanations\nAbstract: arXiv:2405.14244v2 Announce Type: replace  Abstract: Human-in-the-loop reinforcement learning allows the training of agents through various interfaces, even for non-expert humans. Recently, preference-based methods (PbRL), where the human has to give his preference over two trajectories, increased in popularity since they allow training in domains where more direct feedback is hard to formulate. However, the current PBRL methods have limitations and do not provide humans with an expressive interface for giving feedback. With this work, we propose a new preference-based learning method that provides humans with a more expressive interface to provide their preference over trajectories and a factual explanation (or annotation of why they have this preference). These explanations allow the human to explain what parts of the trajectory are most relevant for the preference. We allow the expression of the explanations over individual trajectory steps. We evaluate our method in various simulat",
    "path": "papers/24/05/2405.14244.json",
    "total_tokens": 387,
    "tldr": "该文章提出了一种新的偏好基于的强化学习方法，该方法允许人类通过提供对轨迹的偏好以及对其偏好为什么成立的详尽解释来指导训练。这种对单个轨迹步骤的解释提供了一种更为表达力的反馈机制，使人类能够清楚地表达哪些轨迹组成部分对其偏好至关重要。通过这种方式，文章创新性地提高了人类在强化学习过程中的参与度和互动的深度，使得人类不仅能够决定一个行为策略的优劣，还能提供更深层次的解释和指导，从而提高了学习过程的效率和质量。"
}