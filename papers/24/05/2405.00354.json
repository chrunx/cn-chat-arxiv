{
    "title": "CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with Perturbation Strategies and Knowledge Distillation",
    "abstract": "arXiv:2405.00354v2 Announce Type: replace  Abstract: Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data. Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy. In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's learning from both labeled and unlabeled data. CrossMatch employs multiple encoders and decoders to generate diverse data streams, which undergo self-knowledge distillation to enhance consistency and reliability of predictions across varied perturbations. Our method significantly surpasses other state-of-the-art techniques in standard benchmarks by effectively minimizing the gap between training on labeled and unlabeled data and improving edge ",
    "link": "https://arxiv.org/abs/2405.00354",
    "context": "Title: CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with Perturbation Strategies and Knowledge Distillation\nAbstract: arXiv:2405.00354v2 Announce Type: replace  Abstract: Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data. Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy. In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's learning from both labeled and unlabeled data. CrossMatch employs multiple encoders and decoders to generate diverse data streams, which undergo self-knowledge distillation to enhance consistency and reliability of predictions across varied perturbations. Our method significantly surpasses other state-of-the-art techniques in standard benchmarks by effectively minimizing the gap between training on labeled and unlabeled data and improving edge ",
    "path": "papers/24/05/2405.00354.json",
    "total_tokens": 365,
    "tldr": "该文章提出了一个名为CrossMatch的框架，通过结合图像级别的和特征级别的数据扰动策略，以及知识蒸馏技术，显著提升了半监督学习在医学图像分割任务中的效果。这一创新方法通过多分支编码器和解码器生成多样化的数据流，并在数据流之间进行了自我知识蒸馏，从而提高了预测的一致性和可靠性，并在标准基准测试中大幅超越了现有最先进的半监督学习算法。"
}