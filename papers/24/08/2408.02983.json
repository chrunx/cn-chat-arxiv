{
    "title": "Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond",
    "abstract": "arXiv:2408.02983v1 Announce Type: new  Abstract: Non-exemplar class-incremental learning (NECIL) is to resist catastrophic forgetting without saving old class samples. Prior methodologies generally employ simple rules to generate features for replaying, suffering from large distribution gap between replayed features and real ones. To address the aforementioned issue, we propose a simple, yet effective \\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR}) method for NECIL. First, to alleviate the limited representational capacity caused by fixing the feature extractor, we employ Siamese-based self-supervised learning for initial generalizable features. Second, we devise diffusion models to generate class-representative features highly similar to real features, which provides an effective way for exemplar-free knowledge memorization. Third, we introduce prototype calibration to direct the diffusion model's focus towards learning the distribution shapes of features, ",
    "link": "https://arxiv.org/abs/2408.02983",
    "context": "Title: Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond\nAbstract: arXiv:2408.02983v1 Announce Type: new  Abstract: Non-exemplar class-incremental learning (NECIL) is to resist catastrophic forgetting without saving old class samples. Prior methodologies generally employ simple rules to generate features for replaying, suffering from large distribution gap between replayed features and real ones. To address the aforementioned issue, we propose a simple, yet effective \\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR}) method for NECIL. First, to alleviate the limited representational capacity caused by fixing the feature extractor, we employ Siamese-based self-supervised learning for initial generalizable features. Second, we devise diffusion models to generate class-representative features highly similar to real features, which provides an effective way for exemplar-free knowledge memorization. Third, we introduce prototype calibration to direct the diffusion model's focus towards learning the distribution shapes of features, ",
    "path": "papers/24/08/2408.02983.json",
    "total_tokens": 354,
    "tldr": "该文章提出了一种基于扩散模型的非范例类增量学习方法，该模型通过自监督学习和扩散模型生成高度逼近真实特征的类代表特征，有效地消除了在无示范增量学习中由于特征生成规则导致的分布差距问题。"
}