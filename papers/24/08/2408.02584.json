{
    "title": "Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization",
    "abstract": "arXiv:2408.02584v1 Announce Type: cross  Abstract: The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-relat",
    "link": "https://arxiv.org/abs/2408.02584",
    "context": "Title: Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization\nAbstract: arXiv:2408.02584v1 Announce Type: cross  Abstract: The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-relat",
    "path": "papers/24/08/2408.02584.json",
    "total_tokens": 703,
    "translated_title": "利用LLMs的力量：针对高质量基于属性的摘要的高质量微调方法",
    "translated_abstract": "arXiv:2408.02584v1 公告类型：交叉  摘要：信息数字化时代的不断增长迫切需要高效的方法，以便用户从长篇文章中提取关键洞察。基于属性的摘要提供了一种针对性的方法，生成重点关注特定方面在文件内的摘要。尽管基于属性的摘要研究在总结领域取得了进步，但我们仍在不断寻求更好的模型性能。鉴于大型语言模型（LLMs）在自然语言处理领域的不同任务中展示了潜在的革命性发展，特别是在总结问题中，本文探讨了LLMs在基于属性的摘要任务上的微调潜力。我们评估了公开源基础LLMs，如Llama2、Mistral、Gemma和Aya的微调对一个公开可用的特定领域基于属性的摘要数据集的影响。我们的假设是这种方法将使这些模型能够有效地识别和提取与属性的相关内容，并且在实际摘要任务中显著提高性能。",
    "tldr": "本文研究了如何通过微调大型语言模型来提高基于属性的摘要的质量，并将在特定领域摘要数据集上的实验结果与实际性能提升进行了评估。"
}