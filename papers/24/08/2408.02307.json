{
    "title": "Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped Convolution",
    "abstract": "arXiv:2408.02307v1 Announce Type: new  Abstract: Recent advancements in low-cost ensemble learning have demonstrated improved efficiency for image classification. However, the existing low-cost ensemble methods show relatively lower accuracy compared to conventional ensemble learning. In this paper, we propose a new low-cost ensemble learning, which can simultaneously achieve high efficiency and classification performance. A CNN is transformed into a multi-branch structure without introduction of additional components, which maintains the computational complexity as that of the original single model and also enhances diversity among the branches' outputs via sufficient separation between different pathways of the branches. In addition, we propose a new strategy that applies grouped convolution in the branches with different numbers of groups in different branches, which boosts the diversity of the branches' outputs. For training, we employ knowledge distillation using the ensemble of t",
    "link": "https://arxiv.org/abs/2408.02307",
    "context": "Title: Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped Convolution\nAbstract: arXiv:2408.02307v1 Announce Type: new  Abstract: Recent advancements in low-cost ensemble learning have demonstrated improved efficiency for image classification. However, the existing low-cost ensemble methods show relatively lower accuracy compared to conventional ensemble learning. In this paper, we propose a new low-cost ensemble learning, which can simultaneously achieve high efficiency and classification performance. A CNN is transformed into a multi-branch structure without introduction of additional components, which maintains the computational complexity as that of the original single model and also enhances diversity among the branches' outputs via sufficient separation between different pathways of the branches. In addition, we propose a new strategy that applies grouped convolution in the branches with different numbers of groups in different branches, which boosts the diversity of the branches' outputs. For training, we employ knowledge distillation using the ensemble of t",
    "path": "papers/24/08/2408.02307.json",
    "total_tokens": 316,
    "tldr": "该文章提出了一种新的低成本超参数学习方法，通过将CNN转化为多分支结构并采用不同数目组的分组卷积，有效提升了分支间的输出多样性，即使在维持原始单一模型计算复杂性的同时，也显著提高了分类性能。"
}