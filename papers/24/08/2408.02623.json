{
    "title": "YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition",
    "abstract": "arXiv:2408.02623v1 Announce Type: new  Abstract: In this paper, we propose a new framework called YOWOv3, which is an improved version of YOWOv2, designed specifically for the task of Human Action Detection and Recognition. This framework is designed to facilitate extensive experimentation with different configurations and supports easy customization of various components within the model, reducing efforts required for understanding and modifying the code. YOWOv3 demonstrates its superior performance compared to YOWOv2 on two widely used datasets for Human Action Detection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor model YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model - YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33% and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that YOWOv3 significantly reduces the number",
    "link": "https://arxiv.org/abs/2408.02623",
    "context": "Title: YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition\nAbstract: arXiv:2408.02623v1 Announce Type: new  Abstract: In this paper, we propose a new framework called YOWOv3, which is an improved version of YOWOv2, designed specifically for the task of Human Action Detection and Recognition. This framework is designed to facilitate extensive experimentation with different configurations and supports easy customization of various components within the model, reducing efforts required for understanding and modifying the code. YOWOv3 demonstrates its superior performance compared to YOWOv2 on two widely used datasets for Human Action Detection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor model YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model - YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33% and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that YOWOv3 significantly reduces the number",
    "path": "papers/24/08/2408.02623.json",
    "total_tokens": 437,
    "tldr": "该文章提出了一种名为YOWOv3的改进框架，用于人类动作检测和识别任务，相较于YOWOv2版本，YOWOv3在相同的两大数据集上展现出更高的准确率，同时参数数和计算量显著减少，提高了效率和性能。"
}