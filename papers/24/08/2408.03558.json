{
    "title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods",
    "abstract": "arXiv:2408.03558v1 Announce Type: new  Abstract: In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We t",
    "link": "https://arxiv.org/abs/2408.03558",
    "context": "Title: D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods\nAbstract: arXiv:2408.03558v1 Announce Type: new  Abstract: In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We t",
    "path": "papers/24/08/2408.03558.json",
    "total_tokens": 376,
    "tldr": "该文章提出了一个称为D²Styler的框架，该框架使用VQ-GANs的离散表示能力和离散差分的优势，如稳定的训练和无模式塌陷，结合了自适应实例归一化（AdaIN），以在内容和风格图像之间平滑地转移特征，显著提高了风格转移图像的视觉质量。"
}