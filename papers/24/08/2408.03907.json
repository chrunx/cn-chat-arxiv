{
    "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
    "abstract": "arXiv:2408.03907v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation meth",
    "link": "https://arxiv.org/abs/2408.03907",
    "context": "Title: Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models\nAbstract: arXiv:2408.03907v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation meth",
    "path": "papers/24/08/2408.03907.json",
    "total_tokens": 377,
    "tldr": "该文章提出了一种自动化方法，用于检测大型语言模型（LLMs）中的性别偏见，并通过利用机器学习（ML）模型和高级语言模型（LLM）来评估和识别模型中的潜在偏见。通过训练模型生成对抗性提示来诱导模型产生涉及性别偏见的反应，该文章研究了自动评价方法的有效性和提出了新的评估指标，分析了不同模型家族的性能特点，评估了现有自动评估方法的优势和局限性，并在此基础上提出了改进建议。"
}