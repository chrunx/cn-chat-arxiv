{
    "title": "Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization",
    "abstract": "arXiv:2408.03149v1 Announce Type: new  Abstract: The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge ",
    "link": "https://arxiv.org/abs/2408.03149",
    "context": "Title: Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization\nAbstract: arXiv:2408.03149v1 Announce Type: new  Abstract: The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge ",
    "path": "papers/24/08/2408.03149.json",
    "total_tokens": 382,
    "tldr": "该文章提出了一个名为EGMS的模型，通过集成细粒度实体知识，该模型可以更精细地整合视觉信息，从而增强文本摘要的生成，并在选取图像方面进行精炼处理。在BART的基础上，EGMS模型采用了双模态编码器，并利用共享权重来处理文本-图像和实体-图像信息，有效地结合了视觉数据以生成更高质量的文本摘要。"
}