{
    "title": "On Feasibility of Intent Obfuscating Attacks",
    "abstract": "arXiv:2408.02674v1 Announce Type: cross  Abstract: Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose incorporating intent obfuscation in generating adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors -- YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN -- using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attack",
    "link": "https://arxiv.org/abs/2408.02674",
    "context": "Title: On Feasibility of Intent Obfuscating Attacks\nAbstract: arXiv:2408.02674v1 Announce Type: cross  Abstract: Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose incorporating intent obfuscation in generating adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors -- YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN -- using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attack",
    "path": "papers/24/08/2408.02674.json",
    "total_tokens": 419,
    "tldr": "该文章提出了一种在机器学习系统对抗攻击中应用意图混淆技术的新策略，通过在目标对象邻近添加非重叠的扰动对象来隐藏攻击意图。研究团队进行了随机实验，以测试5种流行的对象检测器（YOLOv3、SSD、RetinaNet、Faster R-CNN和Cascade R-CNN）对这种攻击的响应。实验结果表明，所有检测器在面对目标和非目标攻击时均实现了成功。研究还分析了成功因素，包括目标对象的可信度和扰动对象的大小，并建议攻击者利用这些因素来进一步提高攻击的成功率。"
}