{
    "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
    "abstract": "arXiv:2408.03281v1 Announce Type: cross  Abstract: Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities.",
    "link": "https://arxiv.org/abs/2408.03281",
    "context": "Title: StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation\nAbstract: arXiv:2408.03281v1 Announce Type: cross  Abstract: Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities.",
    "path": "papers/24/08/2408.03281.json",
    "total_tokens": 327,
    "tldr": "该文章提出的StructEval框架通过在多个认知水平和关键概念上进行结构化评估，深化和拓宽了大型语言模型评估，提供了一种可靠的工具来抵抗数据污染的风险和减少潜在偏见的干扰，从而更可靠和一致地得出有关模型能力的结果。"
}