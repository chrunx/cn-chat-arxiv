{
    "title": "Better Alignment with Instruction Back-and-Forth Translation",
    "abstract": "arXiv:2408.04614v1 Announce Type: cross  Abstract: We propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by Li et al.(2023a), and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (backtranslated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space. Further analysis shows that our backtranslated instructions are of higher quality than other sources of synth",
    "link": "https://arxiv.org/abs/2408.04614",
    "context": "Title: Better Alignment with Instruction Back-and-Forth Translation\nAbstract: arXiv:2408.04614v1 Announce Type: cross  Abstract: We propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by Li et al.(2023a), and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (backtranslated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space. Further analysis shows that our backtranslated instructions are of higher quality than other sources of synth",
    "path": "papers/24/08/2408.04614.json",
    "total_tokens": 465,
    "tldr": "该文章提出一种新的方法，即指令双向翻译，用于构建高质量的基于世界知识的合成数据，以对大型语言模型进行对齐。通过使用Li等人的反向翻译方法，我们对文档数据生成了高质量的合成指令，并对响应进行了重写，以进一步提高其质量。使用由此产生的（反向翻译指令，重写响应）对进行微调，在AlpacaEval上的胜率比使用其他常用指令数据集（如Humpback、ShareGPT、Open Orca、Alpaca-GPT4和Self-instruct）要高。我们还展示了使用LLM重写响应比直接蒸馏效果更好，并且生成的两个文本分布在外部嵌入空间中表现出显著的区别。进一步分析表明，我们的反向翻译指令在质量上优于其他合成源的指令。"
}