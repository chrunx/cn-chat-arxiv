{
    "title": "Multistain Pretraining for Slide Representation Learning in Pathology",
    "abstract": "arXiv:2408.02859v1 Announce Type: cross  Abstract: Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224 x 224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce Madeleine, a multimodal pretraining strategy for slide representation",
    "link": "https://arxiv.org/abs/2408.02859",
    "context": "Title: Multistain Pretraining for Slide Representation Learning in Pathology\nAbstract: arXiv:2408.02859v1 Announce Type: cross  Abstract: Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224 x 224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce Madeleine, a multimodal pretraining strategy for slide representation",
    "path": "papers/24/08/2408.02859.json",
    "total_tokens": 355,
    "tldr": "该文章提出了Madeleine（Madeleine），一种多模态预训练策略，用于从包括免疫组化在内的多标记染色切片中获取丰富的任务无关训练信号，从而在海平面巨像素全切片图像中学习全面的、可转移的表示形式。"
}