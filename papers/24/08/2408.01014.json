{
    "title": "EIUP: A Training-Free Approach to Erase Non-Compliant Concepts Conditioned on Implicit Unsafe Prompts",
    "abstract": "arXiv:2408.01014v1 Announce Type: new  Abstract: Text-to-image diffusion models have shown the ability to learn a diverse range of concepts. However, it is worth noting that they may also generate undesirable outputs, consequently giving rise to significant security concerns. Specifically, issues such as Not Safe for Work (NSFW) content and potential violations of style copyright may be encountered. Since image generation is conditioned on text, prompt purification serves as a straightforward solution for content safety. Similar to the approach taken by LLM, some efforts have been made to control the generation of safe outputs by purifying prompts. However, it is also important to note that even with these efforts, non-toxic text still carries a risk of generating non-compliant images, which is referred to as implicit unsafe prompts. Furthermore, some existing works fine-tune the models to erase undesired concepts from model weights. This type of method necessitates multiple training i",
    "link": "https://arxiv.org/abs/2408.01014",
    "context": "Title: EIUP: A Training-Free Approach to Erase Non-Compliant Concepts Conditioned on Implicit Unsafe Prompts\nAbstract: arXiv:2408.01014v1 Announce Type: new  Abstract: Text-to-image diffusion models have shown the ability to learn a diverse range of concepts. However, it is worth noting that they may also generate undesirable outputs, consequently giving rise to significant security concerns. Specifically, issues such as Not Safe for Work (NSFW) content and potential violations of style copyright may be encountered. Since image generation is conditioned on text, prompt purification serves as a straightforward solution for content safety. Similar to the approach taken by LLM, some efforts have been made to control the generation of safe outputs by purifying prompts. However, it is also important to note that even with these efforts, non-toxic text still carries a risk of generating non-compliant images, which is referred to as implicit unsafe prompts. Furthermore, some existing works fine-tune the models to erase undesired concepts from model weights. This type of method necessitates multiple training i",
    "path": "papers/24/08/2408.01014.json",
    "total_tokens": 699,
    "translated_title": "EIUP：一种基于隐式不安全提示的条件擦除非合规概念训练免费方法",
    "translated_abstract": "arXiv:2408.01014v1 公告类型：新  翻译摘要：文本到图像扩散模型已经显示了学习广泛概念的能力。然而，值得注意的是，它们也可能产生不希望的输出，这引起了重大的安全问题。特别是，可能遇到的问题包括不适合工作（NSFW）的内容和潜在的样式版权违规。由于图像生成是根据文本来条件的，纯洁提示对于内容安全来说是一种简单有效的解决方案。类似于大型语言模型（LLM）所采取的方法，已经有一些努力旨在通过纯洁提示来控制生成安全的输出。然而，需要注意的是，即使进行了这些努力，非有毒文本仍然存在风险，可能导致生成非合规的图像，这种情况被称为隐式不安全提示。此外，一些现有工作通过调整模型来从模型权重中擦除不希望的概念。这种方法需要多个训练周期。我们的贡献点在于提出了一种新的训练无需方法EIUP，通过隐式不安全提示来消除非合规概念'''与原文中提到的性能问题进行了相应调整，以确保输出内容的准确性。",
    "tldr": "EIUP提出了一种新的训练无需方法，通过隐式不安全提示来消除非合规概念，解决了文本到图像扩散模型可能产生的安全问题。"
}