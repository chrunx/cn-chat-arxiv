{
    "title": "SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models",
    "abstract": "arXiv:2408.00655v2 Announce Type: replace  Abstract: Contemporary large language models (LLMs) primarily rely on next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a Sentence Encoder and a Sentence Decoder. The encoder effectively condenses the information within a sentence into a singular token, while the decoder reconstructs this compressed data back into its original sentential form. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference approach, markedly accelerating inference speeds. SentenceVAE also maintains the integrity of the original semantic content by segmenting the text into sentences, thereby improving a",
    "link": "https://arxiv.org/abs/2408.00655",
    "context": "Title: SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models\nAbstract: arXiv:2408.00655v2 Announce Type: replace  Abstract: Contemporary large language models (LLMs) primarily rely on next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a Sentence Encoder and a Sentence Decoder. The encoder effectively condenses the information within a sentence into a singular token, while the decoder reconstructs this compressed data back into its original sentential form. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference approach, markedly accelerating inference speeds. SentenceVAE also maintains the integrity of the original semantic content by segmenting the text into sentences, thereby improving a",
    "path": "papers/24/08/2408.00655.json",
    "total_tokens": 693,
    "translated_title": "SentenceVAE：通过下一个句子预测为大型语言模型提供更快、更长和更准确的推理",
    "translated_abstract": "arXiv:2408.00655v2 公告类型：替换  摘要：现代大型语言模型（LLM）主要依靠下一个token预测方法进行推理，这严重阻碍了它们的处理速度。在这项研究中，我们提出了一种新的推理方法，称为下一个句子预测，旨在提高LLM的推理效率。我们介绍了句式变分自动编码器（SentenceVAE），这是一个由句式编码器和句式解码器组成的tiny模型。编码器能够有效地将句子中的信息压缩成一个单一的代币，而解码器重建这个压缩的数据，使其恢复到原始的句子形式。通过将SentenceVAE集成到LLM的输入和输出层中，我们开发了句子级别的LLM（Sentence-level LLMs, SLLMs），这些模型采用了一种按句子处理的推理方法，显著提高了推理速度。SentenceVAE还通过将文本分割成句子，保持了原始语义内容的完整性，因此提高了推理的准确性和流畅性。",
    "tldr": "SentenceVAE是一种创新的模型，它通过将大型语言模型的推理过程改为由句子逐个处理的策略，大幅提高了这些模型的推理速度和准确性。",
    "en_tdlr": "SentenceVAE is an innovative model that significantly enhances the inference speed and accuracy of large language models by changing their inference process to one that processes sentences individually."
}