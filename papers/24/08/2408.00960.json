{
    "title": "PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting",
    "abstract": "arXiv:2408.00960v1 Announce Type: cross  Abstract: Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and t",
    "link": "https://arxiv.org/abs/2408.00960",
    "context": "Title: PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting\nAbstract: arXiv:2408.00960v1 Announce Type: cross  Abstract: Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and t",
    "path": "papers/24/08/2408.00960.json",
    "total_tokens": 569,
    "translated_title": "PERSOMA个性化软提示适配器架构在个性化语言提示中的应用",
    "translated_abstract": "论文介绍了一种名为PERSOMA的个性化软提示适配器架构，旨在大型语言模型中对用户的广泛交互历史进行准确和个性化的理解。与现有的大型语言模型个性化提示方法相比，PERSOMA提供了一种新的高效捕获用户历史的方式。论文通过使用用户交互的操作减少和压缩算法，将用户交互信息转化为特征丰富的软提示嵌入表示。此外，论文还通过各种评估方法验证了PERSOMA架构，包括判断各类参数压缩和调整方法的效率，如低秩注意力（LoRA）等方法。通过严格的测试后，结果表明PERSOMA在处理复杂和大量的用户交互历史方面优于现有基于嵌入和提示的方法。",
    "tldr": "PERSOMA是一种用于个性化语言提示的软提示适配器架构，能高效处理和个性化用户交互历史。"
}