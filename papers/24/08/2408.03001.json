{
    "title": "Multitask and Multimodal Neural Tuning for Large Models",
    "abstract": "arXiv:2408.03001v1 Announce Type: new  Abstract: In recent years, large-scale multimodal models have demonstrated impressive capabilities across various domains. However, enabling these models to effectively perform multiple multimodal tasks simultaneously remains a significant challenge. To address this, we introduce a novel tuning method called neural tuning, designed to handle diverse multimodal tasks concurrently, including reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. Neural tuning emulates sparse distributed representation in human brain, where only specific subsets of neurons are activated for each task. Additionally, we present a new benchmark, MMUD, where each sample is annotated with multiple task labels. By applying neural tuning to pretrained large models on the MMUD benchmark, we achieve simultaneous task handling in a streamlined and efficient manner. All models, code, and datasets will be publicly available after publicat",
    "link": "https://arxiv.org/abs/2408.03001",
    "context": "Title: Multitask and Multimodal Neural Tuning for Large Models\nAbstract: arXiv:2408.03001v1 Announce Type: new  Abstract: In recent years, large-scale multimodal models have demonstrated impressive capabilities across various domains. However, enabling these models to effectively perform multiple multimodal tasks simultaneously remains a significant challenge. To address this, we introduce a novel tuning method called neural tuning, designed to handle diverse multimodal tasks concurrently, including reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. Neural tuning emulates sparse distributed representation in human brain, where only specific subsets of neurons are activated for each task. Additionally, we present a new benchmark, MMUD, where each sample is annotated with multiple task labels. By applying neural tuning to pretrained large models on the MMUD benchmark, we achieve simultaneous task handling in a streamlined and efficient manner. All models, code, and datasets will be publicly available after publicat",
    "path": "papers/24/08/2408.03001.json",
    "total_tokens": 363,
    "tldr": "该文章提出了一种称为神经调谐的先进技术，旨在通过模拟人脑中特定神经元激活的模式，使得大型多模态模型能够同时有效处理多个不同的任务，包括图像分段、文本增强、图像描述和文本到图像的生成。这种方法通过一个名为MMUD的全新基准测试，展示了对大规模模型在多个任务的统一调谐和高效执行，为多模态任务性能提升提供了一条新途径。"
}