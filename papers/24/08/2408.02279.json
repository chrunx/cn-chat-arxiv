{
    "title": "DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting",
    "abstract": "arXiv:2408.02279v1 Announce Type: cross  Abstract: Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution feat",
    "link": "https://arxiv.org/abs/2408.02279",
    "context": "Title: DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting\nAbstract: arXiv:2408.02279v1 Announce Type: cross  Abstract: Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution feat",
    "path": "papers/24/08/2408.02279.json",
    "total_tokens": 647,
    "translated_title": "DRFormer：利用多样 receptive field 的多尺度 Transformer 长时序预测",
    "translated_abstract": "arXiv:2408.02279v1 Announce Type: cross  摘要：长时序预测（LTSF）在金融、交通预测等诸多领域得到了广泛应用。近期，基于片段的 Transformer 模型作为一种前景的解决方案受到关注，它将数据分为底层片段，作为输入的键。然而，现有的方法主要依赖于预先设定的片段长度，这需要行业专家的知识，并且在捕捉不同尺度的数据特征方面面临挑战。此外，时间序列数据在不同的时间尺度上表现出不同的变化和波动模式，这传统方法难以有效建模。在本文中，我们提出了一种以动态学习算法为指导的动态 Tokenizer，用于捕捉时间序列数据中的多样 receptive field 和稀疏模式。为了建立层级化的 receptive field，我们开发了一种多尺度 Transformer 模型，结合了多尺度序列抽取，能够捕捉多尺度特征。模型可以有效地抽取不同时间尺度的信息，从而在长期预测任务中获得更好性能。",
    "tldr": "本研究提出了一种名为 DRFormer 的多尺度 Transformer 模型，创新性地使用动态 Tokenizer 和动态稀疏学习算法来捕捉时间序列数据的多样 receptive field 和稀疏模式。该模型能够构建层级化的 receptive field，并有效抽取不同时间尺度的信息，显著提升了长期时间序列预测任务中的性能。"
}