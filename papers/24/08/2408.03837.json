{
    "title": "WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models",
    "abstract": "arXiv:2408.03837v1 Announce Type: cross  Abstract: WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledevalA.",
    "link": "https://arxiv.org/abs/2408.03837",
    "context": "Title: WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models\nAbstract: arXiv:2408.03837v1 Announce Type: cross  Abstract: WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledevalA.",
    "path": "papers/24/08/2408.03837.json",
    "total_tokens": 304,
    "tldr": "该工具为研究者提供了一种开放性高、功能全面的评估大型语言模型安全性的方法，并且覆盖了多种不同类型的安全测试场景，促进了相关模型安全性的深入研究。"
}