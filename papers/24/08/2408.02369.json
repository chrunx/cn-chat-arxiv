{
    "title": "The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024",
    "abstract": "arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta",
    "link": "https://arxiv.org/abs/2408.02369",
    "context": "Title: The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024\nAbstract: arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta",
    "path": "papers/24/08/2408.02369.json",
    "total_tokens": 747,
    "translated_title": "《CNVSRC 2024 视觉说话人识别系统的 NPU-ASLP 描述》",
    "translated_abstract": "本文详细介绍了 NPU-ASLP（团队237）在本届中连续视觉说话人识别挑战赛（CNVSRC 2024）中提出的视觉说话人识别（VSR）系统，并参与了所有四个赛道，包括单音说话人VSR任务和多音说话人VSR任务的固定和开放赛道。在数据处理方面，我们采用了基线提供的唇动提取器来生成多尺度视频数据。此外，训练期间应用了各种数据增强技术，包括速度扰动、随机旋转、水平翻转和颜色变换。VSR模型采用了端到端的架构，结合了CTC/注意力损失，引入了增强的ResNet3D视觉前端、E-Branchformer编码器和双向Transformer解码器。我们的方法为单音说话人任务和多音说话人任务分别达到了30.47%和34.30%的错误率，在单音说话人任务的开放赛道中获得了第二名。",
    "tldr": "本文介绍了NPU-ASLP团队在CNVSRC 2024竞赛中对VSR系统的技术细节，包括数据处理、模型架构和实现效果，并获得了良好成绩。",
    "en_tdlr": "This paper outlines the technical details of the NPU-ASLP team's VSR system for the CNVSRC 2024 competition, including data processing, model architecture, and implementation results, and achieved good results."
}