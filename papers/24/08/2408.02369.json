{
    "title": "The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024",
    "abstract": "arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta",
    "link": "https://arxiv.org/abs/2408.02369",
    "context": "Title: The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024\nAbstract: arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta",
    "path": "papers/24/08/2408.02369.json",
    "total_tokens": 578,
    "tldr": "该文章详细介绍了以NPU-ASLP命名的用于CNVSRC 2024视觉语音识别挑战的所有四个轨道的系统设计，包括单一说话者VSR任务和多说话者VSR任务的固定和开放轨道。数据处理方面，该系统采用基础线上的唇动提取器产生了多尺度视频数据。在训练期间，各种增强技术被应用，包括速度偏移、随机旋转、水平翻转和颜色变换。VSR模型采用端到端架构，带有联合CTC/注意力损失，引入了增强的ResNet3D视觉前端、E-Branchformer编码器和双方向Transformer解码器。该系统在单一说话者任务的开放轨道上取得了30.47%的错误率（CER），在多说话者任务的开放轨道上取得了34.30%的CER，在单一说话者任务的固定轨道上取得了32.51%的CER，在多说话者任务的固定轨道上取得了35.28%的CER，从而在该挑战中赢得了第二名，证明了其创新的语音识别系统的有效性和实用性。"
}