{
    "title": "Unlocking the Non-Native Language Context Limitation: Native Language Prompting Facilitates Knowledge Elicitation",
    "abstract": "arXiv:2408.03544v1 Announce Type: cross  Abstract: Multilingual large language models (MLLMs) struggle to answer questions posed in non-dominant languages, even though they have already acquired the relevant knowledge from their dominant language corpus. In contrast, human multilinguals can overcome this issue by invoking the relatively rich knowledge acquired from native language texts through Positive Native Language Transfer (PNLT). Inspired by this, we analogize the dominant language of MLLMs to the native language of human multilinguals, and propose Native Language Prompting (NatLan) to simulate the PNLT observed in human multilinguals. It explicitly creates native language contexts for MLLMs to facilitate the elicitation of the rich native language knowledge during question-answering, unlocking the limitations imposed by non-native language contexts on the effective application of knowledge. By employing multi-MLLM collaboration, NatLan reduces the workload on each MLLM in simula",
    "link": "https://arxiv.org/abs/2408.03544",
    "context": "Title: Unlocking the Non-Native Language Context Limitation: Native Language Prompting Facilitates Knowledge Elicitation\nAbstract: arXiv:2408.03544v1 Announce Type: cross  Abstract: Multilingual large language models (MLLMs) struggle to answer questions posed in non-dominant languages, even though they have already acquired the relevant knowledge from their dominant language corpus. In contrast, human multilinguals can overcome this issue by invoking the relatively rich knowledge acquired from native language texts through Positive Native Language Transfer (PNLT). Inspired by this, we analogize the dominant language of MLLMs to the native language of human multilinguals, and propose Native Language Prompting (NatLan) to simulate the PNLT observed in human multilinguals. It explicitly creates native language contexts for MLLMs to facilitate the elicitation of the rich native language knowledge during question-answering, unlocking the limitations imposed by non-native language contexts on the effective application of knowledge. By employing multi-MLLM collaboration, NatLan reduces the workload on each MLLM in simula",
    "path": "papers/24/08/2408.03544.json",
    "total_tokens": 375,
    "tldr": "该文章提出了一种名为Native Language Prompting（NatLan）的策略，通过在机器学习模型中模拟人类多语言者使用母语知识的方式，使得模型即使在接收到的文本内容不在其主要熟语言范围内也可以有效提取相关知识，解决了传统机器学习模型在处理非主要熟语言内容时的困境。"
}