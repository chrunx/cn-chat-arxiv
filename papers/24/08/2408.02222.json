{
    "title": "Cross-modulated Attention Transformer for RGBT Tracking",
    "abstract": "arXiv:2408.02222v1 Announce Type: new  Abstract: Existing Transformer-based RGBT trackers achieve remarkable performance benefits by leveraging self-attention to extract uni-modal features and cross-attention to enhance multi-modal feature interaction and template-search correlation computation. Nevertheless, the independent search-template correlation calculations ignore the consistency between branches, which can result in ambiguous and inappropriate correlation weights. It not only limits the intra-modal feature representation, but also harms the robustness of cross-attention for multi-modal feature interaction and search-template correlation computation. To address these issues, we propose a novel approach called Cross-modulated Attention Transformer (CAFormer), which performs intra-modality self-correlation, inter-modality feature interaction, and search-template correlation computation in a unified attention model, for RGBT tracking. In particular, we first independently generate",
    "link": "https://arxiv.org/abs/2408.02222",
    "context": "Title: Cross-modulated Attention Transformer for RGBT Tracking\nAbstract: arXiv:2408.02222v1 Announce Type: new  Abstract: Existing Transformer-based RGBT trackers achieve remarkable performance benefits by leveraging self-attention to extract uni-modal features and cross-attention to enhance multi-modal feature interaction and template-search correlation computation. Nevertheless, the independent search-template correlation calculations ignore the consistency between branches, which can result in ambiguous and inappropriate correlation weights. It not only limits the intra-modal feature representation, but also harms the robustness of cross-attention for multi-modal feature interaction and search-template correlation computation. To address these issues, we propose a novel approach called Cross-modulated Attention Transformer (CAFormer), which performs intra-modality self-correlation, inter-modality feature interaction, and search-template correlation computation in a unified attention model, for RGBT tracking. In particular, we first independently generate",
    "path": "papers/24/08/2408.02222.json",
    "total_tokens": 450,
    "tldr": "原文标题：用于RGBT跟踪的跨调制注意变压器\n摘要：现有基于Transformer的RGBT跟踪器通过利用自注意力来提取单模态特征并利用交叉注意力来增强多模态特征交互和模板搜索相关性计算，从而表现出了显著的性能提升。然而，独立的多模态特征交互和模板搜索相关性计算忽视了分支间的相互依赖性，这种忽视会导致不明确的和不恰当的相关性权重计算。这不仅限制了单模态特征的表现，也伤害了多模态特征交互和模板搜索相关性计算的鲁棒性。为了解决这些问题，我们提出了一种新颖的跨调制注意变压器（CAFormer）方法，它在一个统一的注意力模型中执行单模态自我相关性计算、双模态特征交互计算和模板搜索相关性计算，用于RGBT跟踪。特别是，我们首先独立生成"
}