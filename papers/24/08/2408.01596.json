{
    "title": "Trustworthy Machine Learning under Social and Adversarial Data Sources",
    "abstract": "arXiv:2408.01596v1 Announce Type: cross  Abstract: Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing th",
    "link": "https://arxiv.org/abs/2408.01596",
    "context": "Title: Trustworthy Machine Learning under Social and Adversarial Data Sources\nAbstract: arXiv:2408.01596v1 Announce Type: cross  Abstract: Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing th",
    "path": "papers/24/08/2408.01596.json",
    "total_tokens": 663,
    "translated_title": "标题：社会和对抗性数据源下的可信赖机器学习",
    "translated_abstract": "摘要：近年来，机器学习领域取得了显著的突破。随着机器学习越来越多地渗透到生活的各个方面，个人和组织越来越频繁地与这些系统互动，表现出各种社会和对抗性的行为。这些行为可能对机器学习系统的行为和性能产生显著影响。特别地，在这些互动过程中，数据可能是由战略性的个体产生的，由自私的数据收集者收集，可能被敌对的攻击者篡改，并且被用来建立满足多个目标的预测器、模型和政策。因此，机器学习系统的输出可能会恶化，比如深度神经网络对对抗性示例的易感性（Shafahi et al., 2018; Szegedy et al., 2013）以及在存在战略个体的情境下经典算法的性能下降（Ahmadi et al., 2021）。解决这些问题要求开发全新的理论框架和技术，这些框架和技术能够确保机器学习系统的输出在面对道德和战略性的数据源时仍然是可信赖的。",
    "tldr": "本文探讨了在社会和对抗性数据源影响下如何确保机器学习系统的输出仍然是可信赖的，提出了全新的理论框架和技术。",
    "en_tdlr": "This paper explores how to ensure the output of machine learning systems remains trustworthy despite the impact of social and adversarial data sources, proposing novel theoretical frameworks and techniques."
}