{
    "title": "Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning",
    "abstract": "arXiv:2408.01076v1 Announce Type: new  Abstract: Deep neural networks (DNNs) excel on fixed datasets but struggle with incremental and shifting data in real-world scenarios. Continual learning addresses this challenge by allowing models to learn from new data while retaining previously learned knowledge. Existing methods mainly rely on visual features, often neglecting the rich semantic information encoded in text. The semantic knowledge available in the label information of the images, offers important semantic information that can be related with previously acquired knowledge of semantic classes. Consequently, effectively leveraging this information throughout continual learning is expected to be beneficial. To address this, we propose integrating semantic guidance within and across tasks by capturing semantic similarity using text embeddings. We start from a pre-trained CLIP model, employ the \\emph{Semantically-guided Representation Learning (SG-RL)} module for a soft-assignment tow",
    "link": "https://arxiv.org/abs/2408.01076",
    "context": "Title: Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning\nAbstract: arXiv:2408.01076v1 Announce Type: new  Abstract: Deep neural networks (DNNs) excel on fixed datasets but struggle with incremental and shifting data in real-world scenarios. Continual learning addresses this challenge by allowing models to learn from new data while retaining previously learned knowledge. Existing methods mainly rely on visual features, often neglecting the rich semantic information encoded in text. The semantic knowledge available in the label information of the images, offers important semantic information that can be related with previously acquired knowledge of semantic classes. Consequently, effectively leveraging this information throughout continual learning is expected to be beneficial. To address this, we propose integrating semantic guidance within and across tasks by capturing semantic similarity using text embeddings. We start from a pre-trained CLIP model, employ the \\emph{Semantically-guided Representation Learning (SG-RL)} module for a soft-assignment tow",
    "path": "papers/24/08/2408.01076.json",
    "total_tokens": 762,
    "translated_title": "使用预训练文本编码器语义知识进行持续学习",
    "translated_abstract": "arXiv:2408.01076v1 新闻类型：新报告类型 摘要：深度神经网络（DNNs）在固定数据集上表现出色，但在现实世界中不断变化的场景中学习新数据时却遇到困难。持续学习这一挑战旨在允许模型在新知识学习的同时保留以前学到的知识。现有的方法主要依赖于视觉特征，经常忽视图像标签信息中的丰富语义信息。图像标签中的语义信息提供了与先前获得的分类知识相关的宝贵信息。因此，在实际应用中有效地利用这些信息应该是有益的。为了解决这个问题，我们提出了一种通过使用文本嵌入捕捉语义相似性的方法，该方法能够在任务之间和任务内部整合语义引导。我们从CLIP模型开始，使用“Semantically-guided Representation Learning（SG-RL）”模块进行软分配，在任务T的监督下得分最好的文本embedding，该嵌入与新的任务T'中检索到的类别相关的图片embedding相似。我们对一个多任务学习网络进行实验，改网络在分割数据集上进行训练，并在迁移到流行的CIFAR-100数据集上时能够进行可迁移的学习。实验结果表明，与没有语义指导的持续学习传统方法相比，我们的方法具有更好的保持能力。",
    "tldr": "本文提出了一种使用预训练文本编码器语义知识进行持续学习的策略，通过在任务之间和任务内部整合语义引导，提高了模型对新数据的可迁移学习能力，并在实验中取得了优于传统方法的保持效果。"
}