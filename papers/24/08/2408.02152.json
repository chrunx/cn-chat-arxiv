{
    "title": "Generative Retrieval with Few-shot Indexing",
    "abstract": "arXiv:2408.02152v1 Announce Type: cross  Abstract: Existing generative retrieval (GR) approaches rely on training-based indexing, i.e., fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has three limitations: high training overhead, under-utilization of the pre-trained knowledge of large language models (LLMs), and challenges in adapting to a dynamic document corpus. To address the above issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR). It has a novel few-shot indexing process, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Few-Shot GR relies solely on prompting an LLM without req",
    "link": "https://arxiv.org/abs/2408.02152",
    "context": "Title: Generative Retrieval with Few-shot Indexing\nAbstract: arXiv:2408.02152v1 Announce Type: cross  Abstract: Existing generative retrieval (GR) approaches rely on training-based indexing, i.e., fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has three limitations: high training overhead, under-utilization of the pre-trained knowledge of large language models (LLMs), and challenges in adapting to a dynamic document corpus. To address the above issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR). It has a novel few-shot indexing process, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Few-Shot GR relies solely on prompting an LLM without req",
    "path": "papers/24/08/2408.02152.json",
    "total_tokens": 707,
    "translated_title": "基于少样本索引的生成检索",
    "translated_abstract": "arXiv:2408.02152v1 公告类型：交叉摘要：现有的生成检索（GR）方法依赖于基于训练的数据库索引方法，即对模型进行微调，以记住查询与其相关文档标识符（docid）之间的关联。基于训练的索引有三个局限性：训练成本高、大型语言模型（LLM）预训练知识的利用率不足以及动态文档集合上的适应性问题。为了解决这些问题，我们提出了一种新的基于少样本索引的生成检索框架（Few-Shot GR）。它具有一种新的少样本索引过程，在过程中，我们将整个文档集合中的所有文档提供给LLM进行提示，最终创建一个包含所有文档的文档标识符银行。在检索过程中，我们将查询提供给相同的LLM，并限制其产生的文档标识符位于在索引期间创建的文档标识符银行之内，然后将生成的docid映射回其对应的文档。Few-Shot GR依赖于对LLM进行提示，而无需要求其进行训练",
    "tldr": "论文提出了一种新的基于少样本索引的生成检索框架，它通过提示LLM创建文档标识符银行，并在检索过程中限制模型产生的docid，以提高检索性能。"
}