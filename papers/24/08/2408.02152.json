{
    "title": "Generative Retrieval with Few-shot Indexing",
    "abstract": "arXiv:2408.02152v1 Announce Type: cross  Abstract: Existing generative retrieval (GR) approaches rely on training-based indexing, i.e., fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has three limitations: high training overhead, under-utilization of the pre-trained knowledge of large language models (LLMs), and challenges in adapting to a dynamic document corpus. To address the above issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR). It has a novel few-shot indexing process, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Few-Shot GR relies solely on prompting an LLM without req",
    "link": "https://arxiv.org/abs/2408.02152",
    "context": "Title: Generative Retrieval with Few-shot Indexing\nAbstract: arXiv:2408.02152v1 Announce Type: cross  Abstract: Existing generative retrieval (GR) approaches rely on training-based indexing, i.e., fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has three limitations: high training overhead, under-utilization of the pre-trained knowledge of large language models (LLMs), and challenges in adapting to a dynamic document corpus. To address the above issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR). It has a novel few-shot indexing process, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Few-Shot GR relies solely on prompting an LLM without req",
    "path": "papers/24/08/2408.02152.json",
    "total_tokens": 422,
    "tldr": "该文章提出了一种新型的基于极少样本来索引的生成式检索方法（Few-Shot GR），该方法通过提示语言模型为整个文档库生成文档标识符（docid），从而创建一个包含整个文档库的docid银行，并在检索过程中限制模型生成的docid必须位于该银行中，进而将生成的docid映射回对应的文档，这种方法摒弃了对下游文档集进行训练的需求，显著降低了检索任务的工作量，并提高了对动态文档库的适应性。"
}