{
    "title": "Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling",
    "abstract": "arXiv:2408.03695v1 Announce Type: new  Abstract: Recent image generation models excel at creating high-quality images from brief captions. However, they fail to maintain consistency of multiple instances across images when encountering lengthy contexts. This inconsistency is largely due to in existing training datasets the absence of granular instance feature labeling in existing training datasets. To tackle these issues, we introduce Openstory++, a large-scale dataset combining additional instance-level annotations with both images and text. Furthermore, we develop a training methodology that emphasizes entity-centric image-text generation, ensuring that the models learn to effectively interweave visual and textual information. Specifically, Openstory++ streamlines the process of keyframe extraction from open-domain videos, employing vision-language models to generate captions that are then polished by a large language model for narrative continuity. It surpasses previous datasets by ",
    "link": "https://arxiv.org/abs/2408.03695",
    "context": "Title: Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling\nAbstract: arXiv:2408.03695v1 Announce Type: new  Abstract: Recent image generation models excel at creating high-quality images from brief captions. However, they fail to maintain consistency of multiple instances across images when encountering lengthy contexts. This inconsistency is largely due to in existing training datasets the absence of granular instance feature labeling in existing training datasets. To tackle these issues, we introduce Openstory++, a large-scale dataset combining additional instance-level annotations with both images and text. Furthermore, we develop a training methodology that emphasizes entity-centric image-text generation, ensuring that the models learn to effectively interweave visual and textual information. Specifically, Openstory++ streamlines the process of keyframe extraction from open-domain videos, employing vision-language models to generate captions that are then polished by a large language model for narrative continuity. It surpasses previous datasets by ",
    "path": "papers/24/08/2408.03695.json",
    "total_tokens": 331,
    "tldr": "该文章的主要贡献在于开发了一项大型数据集Openstory++以及一套新的训练方法，该方法通过引入实例级标注和更清晰的训练准则来改进对开放域视觉故事叙说中多实例统一性的维护。"
}