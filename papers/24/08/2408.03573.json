{
    "title": "Active Testing of Large Language Model via Multi-Stage Sampling",
    "abstract": "arXiv:2408.03573v1 Announce Type: cross  Abstract: Performance evaluation plays a crucial role in the development life cycle of large language models (LLMs). It estimates the model's capability, elucidates behavior characteristics, and facilitates the identification of potential issues and limitations, thereby guiding further improvement. Given that LLMs' diverse task-handling abilities stem from large volumes of training data, a comprehensive evaluation also necessitates abundant, well-annotated, and representative test data to assess LLM performance across various downstream tasks. However, the demand for high-quality test data often entails substantial time, computational resources, and manual efforts, sometimes causing the evaluation to be inefficient or impractical. To address these challenges, researchers propose active testing, which estimates the overall performance by selecting a subset of test data. Nevertheless, the existing active testing methods tend to be inefficient, eve",
    "link": "https://arxiv.org/abs/2408.03573",
    "context": "Title: Active Testing of Large Language Model via Multi-Stage Sampling\nAbstract: arXiv:2408.03573v1 Announce Type: cross  Abstract: Performance evaluation plays a crucial role in the development life cycle of large language models (LLMs). It estimates the model's capability, elucidates behavior characteristics, and facilitates the identification of potential issues and limitations, thereby guiding further improvement. Given that LLMs' diverse task-handling abilities stem from large volumes of training data, a comprehensive evaluation also necessitates abundant, well-annotated, and representative test data to assess LLM performance across various downstream tasks. However, the demand for high-quality test data often entails substantial time, computational resources, and manual efforts, sometimes causing the evaluation to be inefficient or impractical. To address these challenges, researchers propose active testing, which estimates the overall performance by selecting a subset of test data. Nevertheless, the existing active testing methods tend to be inefficient, eve",
    "path": "papers/24/08/2408.03573.json",
    "total_tokens": 319,
    "tldr": "该文章提出了一个通过多阶段采样技术进行大型语言模型主动测试的方法，该技术能够高效地评估模型的整体性能，同时减少了高质数据需求和测试成本，对LLM的性能评估具有重要的创新和贡献。"
}