{
    "title": "A Survey of Mamba",
    "abstract": "arXiv:2408.01129v1 Announce Type: cross  Abstract: Deep learning, as a vital technique, has sparked a notable revolution in artificial intelligence. As the most representative architecture, Transformers have empowered numerous advanced models, especially the large language models that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models, has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domain",
    "link": "https://arxiv.org/abs/2408.01129",
    "context": "Title: A Survey of Mamba\nAbstract: arXiv:2408.01129v1 Announce Type: cross  Abstract: Deep learning, as a vital technique, has sparked a notable revolution in artificial intelligence. As the most representative architecture, Transformers have empowered numerous advanced models, especially the large language models that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models, has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domain",
    "path": "papers/24/08/2408.01129.json",
    "total_tokens": 520,
    "translated_title": "Mamba架构综述",
    "translated_abstract": "深度学习是一门至关重要的技术，已经在人工智能领域引发了一场显著的革命。作为最典型的架构，Transformer已经赋能了大量的先进模型，尤其是在包含数十亿参数的大型语言模型方面，它们已经成为深度学习领域的基石。尽管取得了显着成就，但Transformer仍然面临固有的局限性，特别是由于注意力计算的二次计算复杂性导致的耗时的推理过程。最近，一种名为Mamba的新型架构，借鉴了经典的状态空间模型，作为一种发展基础模型的潜在替代方案而受到关注，它在保持序列长度相关近线性可扩展性的同时，能够提供与Transformer相当的建模能力。这一发现激励了越来越多的研究积极探索Mamba在各种领域实现卓越表现的可能性。",
    "tldr": "Mamba架构以经典状态空间模型的灵感为基础，提出了一种新型解决方案，可以在保持序列长度相关近线性可扩展性的同时，提供与Transformer相当的建模能力，有望在人工智能领域带来新的发展。"
}