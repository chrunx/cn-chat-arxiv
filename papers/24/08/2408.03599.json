{
    "title": "Activations Through Extensions: A Framework To Boost Performance Of Neural Networks",
    "abstract": "arXiv:2408.03599v1 Announce Type: cross  Abstract: Activation functions are non-linearities in neural networks that allow them to learn complex mapping between inputs and outputs. Typical choices for activation functions are ReLU, Tanh, Sigmoid etc., where the choice generally depends on the application domain. In this work, we propose a framework/strategy that unifies several works on activation functions and theoretically explains the performance benefits of these works. We also propose novel techniques that originate from the framework and allow us to obtain ``extensions'' (i.e. special generalizations of a given neural network) of neural networks through operations on activation functions. We theoretically and empirically show that ``extensions'' of neural networks have performance benefits compared to vanilla neural networks with insignificant space and time complexity costs on standard test functions. We also show the benefits of neural network ``extensions'' in the time-series d",
    "link": "https://arxiv.org/abs/2408.03599",
    "context": "Title: Activations Through Extensions: A Framework To Boost Performance Of Neural Networks\nAbstract: arXiv:2408.03599v1 Announce Type: cross  Abstract: Activation functions are non-linearities in neural networks that allow them to learn complex mapping between inputs and outputs. Typical choices for activation functions are ReLU, Tanh, Sigmoid etc., where the choice generally depends on the application domain. In this work, we propose a framework/strategy that unifies several works on activation functions and theoretically explains the performance benefits of these works. We also propose novel techniques that originate from the framework and allow us to obtain ``extensions'' (i.e. special generalizations of a given neural network) of neural networks through operations on activation functions. We theoretically and empirically show that ``extensions'' of neural networks have performance benefits compared to vanilla neural networks with insignificant space and time complexity costs on standard test functions. We also show the benefits of neural network ``extensions'' in the time-series d",
    "path": "papers/24/08/2408.03599.json",
    "total_tokens": 347,
    "tldr": "该文章提出了一种新的框架，用于通过操作激活函数来扩展神经网络，以此来提升神经网络的性能。这种拓展不仅对标准测试函数有显著的性能提升，并且在时间序列数据上的表现也得到了验证。此外，与传统的神经网络相比，这种拓展来的神经网络在空间和时间复杂性上几乎没有增加。"
}