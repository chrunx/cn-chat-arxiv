{
    "title": "Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration",
    "abstract": "arXiv:2408.01099v1 Announce Type: cross  Abstract: Recently, pre-trained model and efficient parameter tuning have achieved remarkable success in natural language processing and high-level computer vision with the aid of masked modeling and prompt tuning. In low-level computer vision, however, there have been limited investigations on pre-trained models and even efficient fine-tuning strategy has not yet been explored despite its importance and benefit in various real-world tasks such as alleviating memory inflation issue when integrating new tasks on AI edge devices. Here, we propose a novel efficient parameter tuning approach dubbed contribution-based low-rank adaptation (CoLoRA) for multiple image restorations along with effective pre-training method with random order degradations (PROD). Unlike prior arts that tune all network parameters, our CoLoRA effectively fine-tunes small amount of parameters by leveraging LoRA (low-rank adaptation) for each new vision task with our contribut",
    "link": "https://arxiv.org/abs/2408.01099",
    "context": "Title: Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration\nAbstract: arXiv:2408.01099v1 Announce Type: cross  Abstract: Recently, pre-trained model and efficient parameter tuning have achieved remarkable success in natural language processing and high-level computer vision with the aid of masked modeling and prompt tuning. In low-level computer vision, however, there have been limited investigations on pre-trained models and even efficient fine-tuning strategy has not yet been explored despite its importance and benefit in various real-world tasks such as alleviating memory inflation issue when integrating new tasks on AI edge devices. Here, we propose a novel efficient parameter tuning approach dubbed contribution-based low-rank adaptation (CoLoRA) for multiple image restorations along with effective pre-training method with random order degradations (PROD). Unlike prior arts that tune all network parameters, our CoLoRA effectively fine-tunes small amount of parameters by leveraging LoRA (low-rank adaptation) for each new vision task with our contribut",
    "path": "papers/24/08/2408.01099.json",
    "total_tokens": 808,
    "translated_title": "基于贡献的低秩适应性与预训练模型在真实图像修复中的应用",
    "translated_abstract": "arXiv:2408.01099v1 公告类型: 交叉  摘要: 最近，在自然语言处理和高层次计算机视觉中，借助掩码建模和提示调优，预训练模型和高效参数调优取得了显著的成功。然而，在低层次计算机视觉领域，对预训练模型的研究有限，尽管其在各种现实世界任务（如减轻AI边缘设备上新任务时的内存膨胀问题）中的重要性与收益备受关注，高效的小批量参数调优策略尚未被探索。这里，我们提出了一个新颖的有效参数调优方法，称为贡献性低秩适应性（CoLoRA），用于多个图像修复任务，以及一种有效的预训练方法，名为随机顺序失真（PROD）。与之前所有网络参数调优的工作不同，我们的CoLoRA通过利用LoRA（低秩适应性）针对每个新的视觉任务来有效地调优少量参数。我们采用一种贡献测量机制，通过分析网络的感知变化权重来指导网络参数的选定和优化，极大地简化了模型的复杂性，同时保持了高效的性能。我们证明，在多个真实世界图像修复任务上，通过PROD和CoLoRA的协同作用，我们的方法在精度和内存需求方面均优于现有的预训练和调优方法。我们的工作丰富了图像处理领域的预训练范式，并为AI边缘设备上的新任务集成提供了有效的方法。",
    "tldr": "本文提出了一种高效参数调优方法CoLoRA，通过随机失真预训练（PROD）和贡献性低秩适应性，针对多个真实图像修复任务，大幅简化了模型的复杂性，并在精度与内存需求上超越了现有方法。"
}