{
    "title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models",
    "abstract": "arXiv:2408.02032v1 Announce Type: new  Abstract: While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the `hallucination' problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing auxiliary analysis networks, which inevitable incur additional costs. Another approach, known as contrastive decoding, induces hallucinations by manually disturbing the vision or instruction raw inputs and mitigates them by contrasting the outputs of the disturbed and original LVLMs. However, these approaches rely on empirical holistic input disturbances and double the inference cost. To avoid these issues, we propose a simple yet effective method named Self-Introspective Decoding (SID). Our empirical investigation reveals that pretrained LVLMs ca",
    "link": "https://arxiv.org/abs/2408.02032",
    "context": "Title: Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models\nAbstract: arXiv:2408.02032v1 Announce Type: new  Abstract: While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the `hallucination' problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing auxiliary analysis networks, which inevitable incur additional costs. Another approach, known as contrastive decoding, induces hallucinations by manually disturbing the vision or instruction raw inputs and mitigates them by contrasting the outputs of the disturbed and original LVLMs. However, these approaches rely on empirical holistic input disturbances and double the inference cost. To avoid these issues, we propose a simple yet effective method named Self-Introspective Decoding (SID). Our empirical investigation reveals that pretrained LVLMs ca",
    "path": "papers/24/08/2408.02032.json",
    "total_tokens": 356,
    "tldr": "该文章提出的Self-Introspective Decoding (SID)方法通过在解码过程中进行自我审视，成功减轻了大型视觉语言模型在生成过程中发生的不符合真实语境的文本（即“幻觉”现象），从而提升了模型的描述准确性，促进了其在现实应用中的有效部署。"
}