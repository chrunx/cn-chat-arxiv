{
    "title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs",
    "abstract": "arXiv:2408.01420v1 Announce Type: cross  Abstract: Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jai",
    "link": "https://arxiv.org/abs/2408.01420",
    "context": "Title: Mission Impossible: A Statistical Perspective on Jailbreaking LLMs\nAbstract: arXiv:2408.01420v1 Announce Type: cross  Abstract: Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jai",
    "path": "papers/24/08/2408.01420.json",
    "total_tokens": 737,
    "translated_title": "《不可能的任务：从统计视角看大型语言模型逃逸》",
    "translated_abstract": "arXiv:2408.01420v1 公告类型：十字交叉 摘要：大型语言模型（LLMs）在有限的质控条件下接受了海量文本数据的训练。因此，LLMs可能会出现意料之外的或甚至是有害的行为，例如泄露信息、散播虚假新闻或传播仇恨言论。抵御措施，通常被称为偏好对齐，包括使用精心编写的文本实例，对预训练的LLMs进行微调，这些实例体现了期望的行为。即使这样，经验证据表明，偏好对齐的LLMs可能会被引诱从事有害的行为。这种大型语言模型的逃逸现象通常是通过修改输入的提示文本来实现的，这种提示文本具有对LLM的恶意的修改。我们的论文从统计学视角为偏好对齐和逃逸现象提供了理论洞察。在我们构建的理论框架下，首先证明了如果训练数据集中存在有害行为，预训练的LLMs将模仿这些行为。基于同一个理论框架，我们还提出了对齐行为的统计概念，并对逃逸行为进行了下界估计。在面对对齐策略的潜在弱点时，我们的研究表明，通过适当的统计方法已经可以很好地理解并且尝试避免大型语言模型的这些潜在威胁。",
    "tldr": "本研究发现，大型语言模型训练数据中的不良行为可能导致模型在偏好对齐策略下仍可能表现出有害行为，并提出从统计学角度理解和避免这种“逃逸”现象的方法。"
}