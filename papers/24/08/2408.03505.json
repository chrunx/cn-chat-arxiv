{
    "title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation",
    "abstract": "arXiv:2408.03505v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking t",
    "link": "https://arxiv.org/abs/2408.03505",
    "context": "Title: Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation\nAbstract: arXiv:2408.03505v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking t",
    "path": "papers/24/08/2408.03505.json",
    "total_tokens": 333,
    "tldr": "该文章提出的Optimus系统通过优化GPU处理任务的方式，能在大型多模态语言模型训练中减少“GPU气泡”（GPU bubbles），从而加速训练过程。"
}