{
    "title": "Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs",
    "abstract": "arXiv:2408.01008v1 Announce Type: cross  Abstract: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks, such as question-answering, sentiment analysis, text summarization, and machine translation. However, the ever-growing complexity of LLMs demands immense computational resources, hindering the broader research and application of these models. To address this, various parameter-efficient fine-tuning strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been developed. Despite their potential, these methods often face limitations in compressibility. Specifically, LoRA struggles to scale effectively with the increasing number of trainable parameters in modern large scale LLMs. Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which utilizes tensor train decomposition, has not yet achieved the level of compression necessary for fine-tuning very large scale mo",
    "link": "https://arxiv.org/abs/2408.01008",
    "context": "Title: Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs\nAbstract: arXiv:2408.01008v1 Announce Type: cross  Abstract: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks, such as question-answering, sentiment analysis, text summarization, and machine translation. However, the ever-growing complexity of LLMs demands immense computational resources, hindering the broader research and application of these models. To address this, various parameter-efficient fine-tuning strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been developed. Despite their potential, these methods often face limitations in compressibility. Specifically, LoRA struggles to scale effectively with the increasing number of trainable parameters in modern large scale LLMs. Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which utilizes tensor train decomposition, has not yet achieved the level of compression necessary for fine-tuning very large scale mo",
    "path": "papers/24/08/2408.01008.json",
    "total_tokens": 672,
    "translated_title": "张量束低秩近似 (TT-LoRA): 加速 LLMs 以民主化人工智能",
    "translated_abstract": "arXiv:2408.01008v1 公告类型: 交叉  翻译摘要: 在最近几年中，大型语言模型 (LLMs) 在广泛的自然语言处理 (NLP) 任务中展示了惊人的能力，例如问答、情感分析、文本摘要和机器翻译。然而，LLMs日益复杂的需求巨大计算资源，阻碍了这些模型的更广泛研究和应用。为了解决这一问题，已经开发了各种参数高效的微调策略，如Low-Rank Approximation（LoRA）和Adapters。尽管这些方法具有潜力，但它们通常在可压缩性方面存在限制。具体来说，LoRA在现代大型LLM中越来越增多的可训练参数上难以有效扩展。此外，虽然Low-Rank Economic Tensor-Train Adaptation（LoRETTA）利用张量束分解，但它尚未实现对非常大模型进行压缩的必要水平。",
    "tldr": "本文提出了一种名为TT-LoRA的低秩张量束近似方法，旨在通过更快的LLMs微调加速人工智能民主化。"
}