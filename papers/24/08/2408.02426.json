{
    "title": "FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification",
    "abstract": "arXiv:2408.02426v1 Announce Type: new  Abstract: The success of large-scale pre-trained models has established fine-tuning as a standard method for achieving significant improvements in downstream tasks. However, fine-tuning the entire parameter set of a pre-trained model is costly. Parameter-efficient transfer learning (PETL) has recently emerged as a cost-effective alternative for adapting pre-trained models to downstream tasks. Despite its advantages, the increasing model size and input resolution present challenges for PETL, as the training memory consumption is not reduced as effectively as the parameter usage. In this paper, we introduce Fine-grained Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical image classification, which significantly reduces memory consumption compared to other PETL methods. FPT+ performs transfer learning by training a lightweight side network and accessing pre-trained knowledge from a large pre-trained model (LPM) through fine",
    "link": "https://arxiv.org/abs/2408.02426",
    "context": "Title: FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification\nAbstract: arXiv:2408.02426v1 Announce Type: new  Abstract: The success of large-scale pre-trained models has established fine-tuning as a standard method for achieving significant improvements in downstream tasks. However, fine-tuning the entire parameter set of a pre-trained model is costly. Parameter-efficient transfer learning (PETL) has recently emerged as a cost-effective alternative for adapting pre-trained models to downstream tasks. Despite its advantages, the increasing model size and input resolution present challenges for PETL, as the training memory consumption is not reduced as effectively as the parameter usage. In this paper, we introduce Fine-grained Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical image classification, which significantly reduces memory consumption compared to other PETL methods. FPT+ performs transfer learning by training a lightweight side network and accessing pre-trained knowledge from a large pre-trained model (LPM) through fine",
    "path": "papers/24/08/2408.02426.json",
    "total_tokens": 696,
    "translated_title": "FPT+: 一种针对高分辨率医学图像分类的高效参数和内存转移学习方法",
    "translated_abstract": "arXiv:2408.02426v1 公告类型：新篇 摘要：大规模预训练模型在下游任务上取得了巨大成功，建立了fine-tuning作为实现显著改进的标准方法。然而，将整个预训练模型的参数集进行fine-tuning代价高昂。近年来，高效参数转移学习（PETL）作为一种成本效益更高的替代方法，用于将预训练模型适应到下游任务上。尽管高效参数转移学习的优势显而易见，但随着模型大小的增加和输入分辨率的提高，PETL面临的挑战也随之增加，因为训练内存消耗并没有像参数使用那样得到有效降低。在此篇论文中，我们介绍了Fine-grained Prompt Tuning plus（FPT+），一种针对高分辨率医学图像分类的高效参数和内存转移学习方法，与其他的PETL方法相比，FPT+在减少内存消耗方面取得了显著进步。FPT+通过训练一个小型的侧网络并利用来自大型预训练模型（LPM）的精细粗粒度提示调优技术，访问预训练知识来实现高效迁移学习。",
    "tldr": "FPT+是一种高效的转移学习方法，特别适用于高分辨率医学图像分类，能够在减少内存消耗的同时，从大型预训练模型中学习知识。"
}