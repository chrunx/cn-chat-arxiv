{
    "title": "Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation",
    "abstract": "arXiv:2408.01363v1 Announce Type: cross  Abstract: Vision--Language Models (VLMs) have demonstrated success across diverse applications, yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs, including CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc} retrieval task tailored for multimedia content creation in a zero-shot fashion. Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V, encompassing open-source and closed-source visual-instruction-tuned Large Language Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared to human relevance judgments, surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems. (3) GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's $\\kappa$ value of around 0.08, which outperforms CLIPScore at approx",
    "link": "https://arxiv.org/abs/2408.01363",
    "context": "Title: Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation\nAbstract: arXiv:2408.01363v1 Announce Type: cross  Abstract: Vision--Language Models (VLMs) have demonstrated success across diverse applications, yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs, including CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc} retrieval task tailored for multimedia content creation in a zero-shot fashion. Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V, encompassing open-source and closed-source visual-instruction-tuned Large Language Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared to human relevance judgments, surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems. (3) GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's $\\kappa$ value of around 0.08, which outperforms CLIPScore at approx",
    "path": "papers/24/08/2408.01363.json",
    "total_tokens": 794,
    "translated_title": "使用视觉语言模型对图像文本检索评价自动相关性判断的研究",
    "translated_abstract": "arXiv:2408.01363v1 Announce Type: 交叉 摘要: 视觉语言模型（VLMs）在各种应用中表现出色，但它们在相关性判断方面的潜力尚不明确。本文在针对多媒体内容创建设定的大规模“non-determined”检索任务中，评估了VLMs，包括CLIP、LLaVA和GPT-4V的相关性估计能力。初步实验表明：(1)当与人类的相关性判断相比较时，无论是开源还是闭源的视觉指导性调整的大型语言模型（LLMs），LLaVA和GPT-4V都能达到显著的 Kendall’s τ ∼ 0.4。这一分数超过了CLIPScore指标。(2)虽然CLIPScore更受青睐，但LLMs对基于CLIP的检索系统的偏见较小。(3)与其他模型相比，GPT-4V的分数分布与人类判断更为吻合，实现了Cohen’s κ值在0.08左右，这超过了CLIPScore的评估，其中GPT-4V的评估分数分布与人类判断更为接近，表现出较好的相关性判断能力。",
    "tldr": "这项研究评估了基于视觉语言模型的自动相关性判断在图像文本检索评价中的潜力，发现GPT-4V在相关性判断方面表现优于CLIP和其他模型，但CLIPScore指标在量化评价中更为优越。"
}