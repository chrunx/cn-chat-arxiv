{
    "title": "Sampling for View Synthesis: From Local Light Field Fusion to Neural Radiance Fields and Beyond",
    "abstract": "arXiv:2408.04586v1 Announce Type: cross  Abstract: Capturing and rendering novel views of complex real-world scenes is a long-standing problem in computer graphics and vision, with applications in augmented and virtual reality, immersive experiences and 3D photography. The advent of deep learning has enabled revolutionary advances in this area, classically known as image-based rendering. However, previous approaches require intractably dense view sampling or provide little or no guidance for how users should sample views of a scene to reliably render high-quality novel views. Local light field fusion proposes an algorithm for practical view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image scene representation, then renders novel views by blending adjacent local light fields. Crucially, we extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should s",
    "link": "https://arxiv.org/abs/2408.04586",
    "context": "Title: Sampling for View Synthesis: From Local Light Field Fusion to Neural Radiance Fields and Beyond\nAbstract: arXiv:2408.04586v1 Announce Type: cross  Abstract: Capturing and rendering novel views of complex real-world scenes is a long-standing problem in computer graphics and vision, with applications in augmented and virtual reality, immersive experiences and 3D photography. The advent of deep learning has enabled revolutionary advances in this area, classically known as image-based rendering. However, previous approaches require intractably dense view sampling or provide little or no guidance for how users should sample views of a scene to reliably render high-quality novel views. Local light field fusion proposes an algorithm for practical view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image scene representation, then renders novel views by blending adjacent local light fields. Crucially, we extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should s",
    "path": "papers/24/08/2408.04586.json",
    "total_tokens": 340,
    "tldr": "该文章提出了一种新的算法，用于从场景的不规则采样网格中合成新的视角，这种方法首先将每个采样的视角扩展到一个局部光场，然后通过混合相邻的局部光场来渲染新的视角，为用户提供了如何可靠地合成高质量新视角的具体指导。"
}