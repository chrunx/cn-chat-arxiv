{
    "title": "Whether to trust: the ML leap of faith",
    "abstract": "arXiv:2408.00786v1 Announce Type: cross  Abstract: Human trust is critical for trustworthy AI adoption. Trust is commonly understood as an attitude, but we cannot accurately measure this, nor manage it. We conflate trust in the overall system, ML, and ML's component parts; so most users do not understand the leap of faith they take when they trust ML. Current efforts to build trust explain ML's process, which can be hard for non-ML experts to comprehend because it is complex, and explanations are unrelated to their own (unarticulated) mental models. We propose an innovative way of directly building intrinsic trust in ML, by discerning and measuring the Leap of Faith (LoF) taken when a user trusts ML. Our LoF matrix identifies where an ML model aligns to a user's own mental model. This match is rigorously yet practically identified by feeding the user's data and objective function both into an ML model and an expert-validated rules-based AI model, a verified point of reference that can ",
    "link": "https://arxiv.org/abs/2408.00786",
    "context": "Title: Whether to trust: the ML leap of faith\nAbstract: arXiv:2408.00786v1 Announce Type: cross  Abstract: Human trust is critical for trustworthy AI adoption. Trust is commonly understood as an attitude, but we cannot accurately measure this, nor manage it. We conflate trust in the overall system, ML, and ML's component parts; so most users do not understand the leap of faith they take when they trust ML. Current efforts to build trust explain ML's process, which can be hard for non-ML experts to comprehend because it is complex, and explanations are unrelated to their own (unarticulated) mental models. We propose an innovative way of directly building intrinsic trust in ML, by discerning and measuring the Leap of Faith (LoF) taken when a user trusts ML. Our LoF matrix identifies where an ML model aligns to a user's own mental model. This match is rigorously yet practically identified by feeding the user's data and objective function both into an ML model and an expert-validated rules-based AI model, a verified point of reference that can ",
    "path": "papers/24/08/2408.00786.json",
    "total_tokens": 745,
    "translated_title": "是否信任：机器学习的信仰飞跃",
    "translated_abstract": "arXiv:2408.00786v1 公告类型：交叉  翻译摘要：对于可信赖的人工智能采纳来说，信任至关重要。信任通常被理解为一种态度，但我们无法准确测量它，也不能对其进行管理。我们将对整个系统的信任与机器学习（ML）及其组成部件的信任混为一谈；因此，大多数用户在信任机器学习时所做的信仰飞跃往往不被理解。当前构建信任的努力解释了ML的过程，这对非ML专家来说可能难以理解，因为这个过程很复杂，而且解释与他们自己未表达的思维模型无关。我们提出了一种创新的方法，直接在ML中构建内在信任，通过辨识和测量用户信任ML时的信仰飞跃（Leap of Faith，LoF）。我们的LoF矩阵识别出ML模型与用户自身思维模型的匹配情况。通过将用户的原始数据和目标函数同时输入到ML模型中以及一个经过专家验证的规则为基础的AI模型中，我们可以严格但实际地识别这种匹配。这种匹配是基于对输入数据的内部逻辑一致性的审查，并使用专家确定的规则作为校准点。通过将用户的反馈与这些校准点进行比较，我们可以推断出用户对ML模型的信任程度。正如我们所定义的，信任意味着在没有充分了解模型内部工作原理的情况下，用户愿意接受模型输出作为决策依据。",
    "tldr": "本文提出了一种创新方法，通过辨识和测量用户信任机器学习时的信仰飞跃，直接在机器学习中构建内在信任。",
    "en_tdlr": "This paper proposes an innovative method to build intrinsic trust in machine learning by identifying and measuring the leap of faith users take when trusting ML, directly addressing the issue of trust in AI adoption."
}