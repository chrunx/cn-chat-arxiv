{
    "title": "Learning Provably Robust Policies in Uncertain Parametric Environments",
    "abstract": "arXiv:2408.03093v1 Announce Type: cross  Abstract: We present a data-driven approach for learning MDP policies that are robust across stochastic environments whose transition probabilities are defined by parameters with an unknown distribution. We produce probably approximately correct (PAC) guarantees for the performance of these learned policies in a new, unseen environment over the unknown distribution. Our approach is based on finite samples of the MDP environments, for each of which we build an approximation of the model as an interval MDP, by exploring a set of generated trajectories. We use the built approximations to synthesise a single policy that performs well (meets given requirements) across the sampled environments, and furthermore bound its risk (of not meeting the given requirements) when deployed in an unseen environment. Our procedure offers a trade-off between the guaranteed performance of the learned policy and the risk of not meeting the guarantee in an unseen envir",
    "link": "https://arxiv.org/abs/2408.03093",
    "context": "Title: Learning Provably Robust Policies in Uncertain Parametric Environments\nAbstract: arXiv:2408.03093v1 Announce Type: cross  Abstract: We present a data-driven approach for learning MDP policies that are robust across stochastic environments whose transition probabilities are defined by parameters with an unknown distribution. We produce probably approximately correct (PAC) guarantees for the performance of these learned policies in a new, unseen environment over the unknown distribution. Our approach is based on finite samples of the MDP environments, for each of which we build an approximation of the model as an interval MDP, by exploring a set of generated trajectories. We use the built approximations to synthesise a single policy that performs well (meets given requirements) across the sampled environments, and furthermore bound its risk (of not meeting the given requirements) when deployed in an unseen environment. Our procedure offers a trade-off between the guaranteed performance of the learned policy and the risk of not meeting the guarantee in an unseen envir",
    "path": "papers/24/08/2408.03093.json",
    "total_tokens": 378,
    "tldr": "该文章提出了一种数据驱动的方法，用于在具有未知分布的随机环境中学习具有跨环境鲁棒性的策略。该方法能够为学习到的策略在新环境中提供概率近似正确的性能保证。通过在MDP环境上进行有限样本探索，构建基于轨迹的区间MDP模型近似，并生成一个能在多个环境上保持良好性能的单一策略。同时，该方法还能够量化策略在新环境中的性能风险，并在探索和决策过程中实现这种风险的风险。"
}