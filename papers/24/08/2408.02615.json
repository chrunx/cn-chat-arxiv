{
    "title": "LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba",
    "abstract": "arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh",
    "link": "https://arxiv.org/abs/2408.02615",
    "context": "Title: LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba\nAbstract: arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh",
    "path": "papers/24/08/2408.02615.json",
    "total_tokens": 661,
    "translated_title": "LaMamba-Diff: 基于局部注意力与Mamba的线性时间高保真扩散模型",
    "translated_abstract": "最近基于Transformer的扩散模型表现出色，这主要归功于自注意力机制能够准确地捕捉输入单词的全连接交互，从而同时处理全局和局部上下文。然而，它们二次方的复杂性对于处理长序列输入构成了严重的计算挑战。相反，一个名为Mamba的状态空间模型通过将滤波后的全局上下文压缩到一个隐藏状态中来提供线性的复杂度。尽管效率很高，但这种压缩不可避免地会丢失局部词汇之间的重要细节信息，这对于有效的视觉生成建模至关重要。基于这些观察，我们提出了LaMamba块，它结合了自注意力和Mamba的优点，以线性复杂度捕获全局和局部细节。利用高效的无监督架构，我们的模型通过使用局部注意力和Mamba技术，能够在处理长序列输入时保持高保真度同时减少计算成本。",
    "tldr": "该论文提出了一种新的线性时间高保真扩散模型LaMamba-Diff，它结合了自注意力机制和Mamba块来同时处理全局和局部上下文，有效提高了处理长序列输入的效率和模型性能。",
    "en_tdlr": "The paper introduces a new linear-time high-fidelity diffusion model, LaMamba-Diff, which combines self-attention and Mamba blocks to capture both global and local contexts, effectively enhancing efficiency and model performance when processing long sequences of input."
}