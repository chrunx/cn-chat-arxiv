{
    "title": "LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba",
    "abstract": "arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh",
    "link": "https://arxiv.org/abs/2408.02615",
    "context": "Title: LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba\nAbstract: arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh",
    "path": "papers/24/08/2408.02615.json",
    "total_tokens": 337,
    "tldr": "该文章提出了一种线性时间复杂度的高保真度扩散模型LaMamba-Diff，采用局部注意力和Mamba算法，结合了自注意力和Mamba的优点，能够在不损失局部细节的情况下高效处理长序列输入，适用于高效的视觉生成建模。"
}