{
    "title": "GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer",
    "abstract": "arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info",
    "link": "https://arxiv.org/abs/2408.01826",
    "context": "Title: GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer\nAbstract: arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info",
    "path": "papers/24/08/2408.01826.json",
    "total_tokens": 688,
    "translated_title": "GLDiTalker:基于语音的3D面部动画生成与图谱潜在扩散变换器",
    "translated_abstract": "arXiv:2408.01826v1 公告类型：新 摘要：基于语音的3D面部动画生成在工业应用和学术研究中已受到大量关注。由于现实中面部表情的非言语线索具有不确定性，生成结果应当具备多样性。然而，大多数最近的方法都是确定性模型，无法学习音频与面部动作的许多对许多映射，以至于无法生成多样性的面部动画。为了解决这一问题，我们提出了GLDiTalker，它引入了动作先验以及一定程度的随机性，以减少跨模态映射的不确定性，同时增加面部非言语线索的多样性。特别是，GLDiTalker在第一阶段使用VQ-VAE将面部动作网格序列映射到潜空间中，然后在第二阶段迭代地添加和去除噪声到潜面部动作特征中。为了整合不同层次的空间信息，还使用了分层注意机制来描述空间上下文及其它们之间的关系。通过详细分析和对比现有方法，GLDiTalker展示了更高的语声到表情转换准确性和动画的多样性。",
    "tldr": "GLDiTalker是一种新方法，通过引入动作先验和随机性提高了基于语音的3D面部动画生成模型的多样性。"
}