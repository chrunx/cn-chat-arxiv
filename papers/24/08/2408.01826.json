{
    "title": "GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer",
    "abstract": "arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info",
    "link": "https://arxiv.org/abs/2408.01826",
    "context": "Title: GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer\nAbstract: arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info",
    "path": "papers/24/08/2408.01826.json",
    "total_tokens": 402,
    "tldr": "该文章提出了一种名为GLDiTalker的3D语音驱动面部动画生成方法，该方法利用图潜伏扩散转译器（Graph Latent Diffusion Transformer）能够从音频中生成多样化的非口语面部表情。通过在两个阶段分别使用VQ-VAE对面部动作网格序列进行编码并将噪声逐级添加或去除到面部运动特征中，GLDiTalker能够在保持跨模态映射非确定性特征的同时增加面部表情的非确定性。"
}