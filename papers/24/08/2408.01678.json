{
    "title": "iControl3D: An Interactive System for Controllable 3D Scene Generation",
    "abstract": "arXiv:2408.01678v1 Announce Type: new  Abstract: 3D content creation has long been a complex and time-consuming process, often requiring specialized skills and resources. While recent advancements have allowed for text-guided 3D object and scene generation, they still fall short of providing sufficient control over the generation process, leading to a gap between the user's creative vision and the generated results. In this paper, we present iControl3D, a novel interactive system that empowers users to generate and render customizable 3D scenes with precise control. To this end, a 3D creator interface has been developed to provide users with fine-grained control over the creation process. Technically, we leverage 3D meshes as an intermediary proxy to iteratively merge individual 2D diffusion-generated images into a cohesive and unified 3D scene representation. To ensure seamless integration of 3D meshes, we propose to perform boundary-aware depth alignment before fusing the newly gener",
    "link": "https://arxiv.org/abs/2408.01678",
    "context": "Title: iControl3D: An Interactive System for Controllable 3D Scene Generation\nAbstract: arXiv:2408.01678v1 Announce Type: new  Abstract: 3D content creation has long been a complex and time-consuming process, often requiring specialized skills and resources. While recent advancements have allowed for text-guided 3D object and scene generation, they still fall short of providing sufficient control over the generation process, leading to a gap between the user's creative vision and the generated results. In this paper, we present iControl3D, a novel interactive system that empowers users to generate and render customizable 3D scenes with precise control. To this end, a 3D creator interface has been developed to provide users with fine-grained control over the creation process. Technically, we leverage 3D meshes as an intermediary proxy to iteratively merge individual 2D diffusion-generated images into a cohesive and unified 3D scene representation. To ensure seamless integration of 3D meshes, we propose to perform boundary-aware depth alignment before fusing the newly gener",
    "path": "papers/24/08/2408.01678.json",
    "total_tokens": 700,
    "translated_title": "iControl3D：可控3D场景生成系统",
    "translated_abstract": "arXiv:2408.01678v1 公告类型：新摘要：3D内容创建长期以来一直是一项复杂且耗时的任务，通常需要专业技能和资源。尽管最近的发展允许基于文本的3D对象和场景生成，但它们在提供生成过程的足够控制方面仍然存在不足，导致用户创意愿景与生成结果之间存在差距。在这篇论文中，我们介绍了iControl3D，这是一个全新的交互式系统，它使用户能够以精确的控制生成和渲染可定制的3D场景。为此，我们开发了一个3D创造者界面，为用户提供了对创建过程的精细控制。技术上，我们利用3D网格作为中间代理，将个体2D扩散生成的图像迭代合并为连贯且统一的3D场景表示。为了确保3D网格的无缝集成，我们在融合新生成的图像之前，提出了边界感知深度对齐的方法。",
    "tldr": "iControl3D是一个使用户能够精确控制并生成可定制3D场景的交互式系统。它使用3D网格作为中间代理，通过迭代合并2D扩散生成的图像来创建连贯统一的3D场景表示。"
}