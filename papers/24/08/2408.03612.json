{
    "title": "JARViS: Detecting Actions in Video Using Unified Actor-Scene Context Relation Modeling",
    "abstract": "arXiv:2408.03612v1 Announce Type: new  Abstract: Video action detection (VAD) is a formidable vision task that involves the localization and classification of actions within the spatial and temporal dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD methods utilize a pre-trained person detector to extract the region of interest features, subsequently employing these features for action detection. However, the performance of two-stage VAD methods has been limited as they depend solely on localized actor features to infer action semantics. In this study, we propose a new two-stage VAD framework called Joint Actor-scene context Relation modeling based on Visual Semantics (JARViS), which effectively consolidates cross-modal action semantics distributed globally across spatial and temporal dimensions using Transformer attention. JARViS employs a person detector to produce densely sampled actor features from a keyframe. Concurrently, it uses a video backbone to cre",
    "link": "https://arxiv.org/abs/2408.03612",
    "context": "Title: JARViS: Detecting Actions in Video Using Unified Actor-Scene Context Relation Modeling\nAbstract: arXiv:2408.03612v1 Announce Type: new  Abstract: Video action detection (VAD) is a formidable vision task that involves the localization and classification of actions within the spatial and temporal dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD methods utilize a pre-trained person detector to extract the region of interest features, subsequently employing these features for action detection. However, the performance of two-stage VAD methods has been limited as they depend solely on localized actor features to infer action semantics. In this study, we propose a new two-stage VAD framework called Joint Actor-scene context Relation modeling based on Visual Semantics (JARViS), which effectively consolidates cross-modal action semantics distributed globally across spatial and temporal dimensions using Transformer attention. JARViS employs a person detector to produce densely sampled actor features from a keyframe. Concurrently, it uses a video backbone to cre",
    "path": "papers/24/08/2408.03612.json",
    "total_tokens": 343,
    "tldr": "该文章提出了一个名为JARViS的统一演员-场景上下文关系建模方法，通过利用Transformer注意力机制在全球性的时空维度上有效整合了跨模态动作语义，提高了视频动作检测的性能。"
}