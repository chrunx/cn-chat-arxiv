{
    "title": "Efficient Single Image Super-Resolution with Entropy Attention and Receptive Field Augmentation",
    "abstract": "arXiv:2408.04158v1 Announce Type: cross  Abstract: Transformer-based deep models for single image super-resolution (SISR) have greatly improved the performance of lightweight SISR tasks in recent years. However, they often suffer from heavy computational burden and slow inference due to the complex calculation of multi-head self-attention (MSA), seriously hindering their practical application and deployment. In this work, we present an efficient SR model to mitigate the dilemma between model efficiency and SR performance, which is dubbed Entropy Attention and Receptive Field Augmentation network (EARFA), and composed of a novel entropy attention (EA) and a shifting large kernel attention (SLKA). From the perspective of information theory, EA increases the entropy of intermediate features conditioned on a Gaussian distribution, providing more informative input for subsequent reasoning. On the other hand, SLKA extends the receptive field of SR models with the assistance of channel shifti",
    "link": "https://arxiv.org/abs/2408.04158",
    "context": "Title: Efficient Single Image Super-Resolution with Entropy Attention and Receptive Field Augmentation\nAbstract: arXiv:2408.04158v1 Announce Type: cross  Abstract: Transformer-based deep models for single image super-resolution (SISR) have greatly improved the performance of lightweight SISR tasks in recent years. However, they often suffer from heavy computational burden and slow inference due to the complex calculation of multi-head self-attention (MSA), seriously hindering their practical application and deployment. In this work, we present an efficient SR model to mitigate the dilemma between model efficiency and SR performance, which is dubbed Entropy Attention and Receptive Field Augmentation network (EARFA), and composed of a novel entropy attention (EA) and a shifting large kernel attention (SLKA). From the perspective of information theory, EA increases the entropy of intermediate features conditioned on a Gaussian distribution, providing more informative input for subsequent reasoning. On the other hand, SLKA extends the receptive field of SR models with the assistance of channel shifti",
    "path": "papers/24/08/2408.04158.json",
    "total_tokens": 393,
    "tldr": "该文章提出了一种名为Entropy Attention and Receptive Field Augmentation (EARFA)的模型，旨在解决传统的Transformer-based SISR模型在计算效率和SR性能之间的矛盾。EARFA模型中采用了新颖的Entropy Attention（EA）和Shifted Large Kernel Attention（SLKA）技术，通过增加中间特征的熵来实现更高效的计算，并扩展了模型的感受野。这种方法在保持模型效率的同时，提高了超分辨率任务的性能。"
}