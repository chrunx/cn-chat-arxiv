{
    "title": "The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis",
    "abstract": "arXiv:2408.02379v1 Announce Type: cross  Abstract: Developing and certifying safe - or so-called trustworthy - AI has become an increasingly salient issue, especially in light of upcoming regulation such as the EU AI Act. In this context, the black-box nature of machine learning models limits the use of conventional avenues of approach towards certifying complex technical systems. As a potential solution, methods to give insights into this black-box - devised in the field of eXplainable AI (XAI) - could be used. In this study, the potential and shortcomings of such methods for the purpose of safe AI development and certification are discussed in 15 qualitative interviews with experts out of the areas of (X)AI and certification. We find that XAI methods can be a helpful asset for safe AI development, as they can show biases and failures of ML-models, but since certification relies on comprehensive and correct information about technical systems, their impact is expected to be limited.",
    "link": "https://arxiv.org/abs/2408.02379",
    "context": "Title: The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis\nAbstract: arXiv:2408.02379v1 Announce Type: cross  Abstract: Developing and certifying safe - or so-called trustworthy - AI has become an increasingly salient issue, especially in light of upcoming regulation such as the EU AI Act. In this context, the black-box nature of machine learning models limits the use of conventional avenues of approach towards certifying complex technical systems. As a potential solution, methods to give insights into this black-box - devised in the field of eXplainable AI (XAI) - could be used. In this study, the potential and shortcomings of such methods for the purpose of safe AI development and certification are discussed in 15 qualitative interviews with experts out of the areas of (X)AI and certification. We find that XAI methods can be a helpful asset for safe AI development, as they can show biases and failures of ML-models, but since certification relies on comprehensive and correct information about technical systems, their impact is expected to be limited.",
    "path": "papers/24/08/2408.02379.json",
    "total_tokens": 392,
    "tldr": "该文章讨论了在欧盟AI法案等法规即将出台的背景下，开发和认证安全AI的重要性。文章通过15位来自XAI和认证领域的专家的定性访谈，探讨了可解释AI（XAI）方法在安全AI开发和认证中的潜在作用。研究表明，XAI方法可以帮助揭示机器学习模型的偏见和失败，但考虑到认证需要关于技术系统的全面和准确信息，其影响可能会受到限制。"
}