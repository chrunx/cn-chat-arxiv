{
    "title": "Training LLMs to Recognize Hedges in Spontaneous Narratives",
    "abstract": "arXiv:2408.03319v1 Announce Type: cross  Abstract: Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or \"fuzziness\", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-",
    "link": "https://arxiv.org/abs/2408.03319",
    "context": "Title: Training LLMs to Recognize Hedges in Spontaneous Narratives\nAbstract: arXiv:2408.03319v1 Announce Type: cross  Abstract: Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or \"fuzziness\", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-",
    "path": "papers/24/08/2408.03319.json",
    "total_tokens": 406,
    "tldr": "该文章创建了一个标注有 hedge 的 Roadrunner 动画对白的文本库，并使用 BERT 模型进行微调，以提高准确识别这些主观表达的能力。通过这种方法，人工智能系统能够更好地理解说话者在句子中加入的模糊化表达，如怀疑或宽容程度，从而提高自然语言处理的准确性和对人类话语的理解。"
}