{
    "title": "SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning",
    "abstract": "arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere",
    "link": "https://arxiv.org/abs/2408.01970",
    "context": "Title: SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning\nAbstract: arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere",
    "path": "papers/24/08/2408.01970.json",
    "total_tokens": 394,
    "tldr": "该文章提出了一种名为SR-CIS的自我反思增量系统，它结合了记忆和推理的解耦模块，旨在模仿人类的快速学习和长期记忆能力。系统由一个快速推理的小型模块和一个慢速决策的大型模块组成，通过自适应的Low-Rank Adaptive机制，该系统能够有效处理新的学习任务，同时保留旧的知识记忆。"
}