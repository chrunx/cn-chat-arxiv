{
    "title": "How Well Can Vision Language Models See Image Details?",
    "abstract": "arXiv:2408.03940v1 Announce Type: new  Abstract: Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level remains unclear. In our study, we introduce a pixel value prediction task (PVP) to explore \"How Well Can Vision Language Models See Image Details?\" and to assist VLMs in perceiving more details. Typically, these models comprise a frozen CLIP visual encoder, a large language model, and a connecting module. After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to predict precise pixel values by only fine-tuning the connection module and LLM; and 2) prediction precision is significantly improved when the vision encoder is also adapted. Additionally, our research reveals that incorporating pixel value prediction as one of the VLM pre-training tasks and vision encoder adaptation markedly boosts VLM",
    "link": "https://arxiv.org/abs/2408.03940",
    "context": "Title: How Well Can Vision Language Models See Image Details?\nAbstract: arXiv:2408.03940v1 Announce Type: new  Abstract: Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level remains unclear. In our study, we introduce a pixel value prediction task (PVP) to explore \"How Well Can Vision Language Models See Image Details?\" and to assist VLMs in perceiving more details. Typically, these models comprise a frozen CLIP visual encoder, a large language model, and a connecting module. After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to predict precise pixel values by only fine-tuning the connection module and LLM; and 2) prediction precision is significantly improved when the vision encoder is also adapted. Additionally, our research reveals that incorporating pixel value prediction as one of the VLM pre-training tasks and vision encoder adaptation markedly boosts VLM",
    "path": "papers/24/08/2408.03940.json",
    "total_tokens": 403,
    "tldr": "该文章揭示了现有的大型语言模型在图像细节上的感知能力存在局限性，尤其是在预测图像像素值方面。通过研究并结合像素值预测任务，作者发现仅对连接模块和语言模型进行微调不足以大幅提升预测精度，这表明传统的无差别图像感知方法可能不再适用于当下流行的视觉语言模型。此外，通过将像素值预测集成到视觉语言模型的预训练任务中，并结合视觉编码器的适应性训练，可以使视觉语言模型在细节感知方面得到显著提升。"
}