{
    "title": "Sample-agnostic Adversarial Perturbation for Vision-Language Pre-training Models",
    "abstract": "arXiv:2408.02980v1 Announce Type: new  Abstract: Recent studies on AI security have highlighted the vulnerability of Vision-Language Pre-training (VLP) models to subtle yet intentionally designed perturbations in images and texts. Investigating multimodal systems' robustness via adversarial attacks is crucial in this field. Most multimodal attacks are sample-specific, generating a unique perturbation for each sample to construct adversarial samples. To the best of our knowledge, it is the first work through multimodal decision boundaries to explore the creation of a universal, sample-agnostic perturbation that applies to any image. Initially, we explore strategies to move sample points beyond the decision boundaries of linear classifiers, refining the algorithm to ensure successful attacks under the top $k$ accuracy metric. Based on this foundation, in visual-language tasks, we treat visual and textual modalities as reciprocal sample points and decision hyperplanes, guiding image embed",
    "link": "https://arxiv.org/abs/2408.02980",
    "context": "Title: Sample-agnostic Adversarial Perturbation for Vision-Language Pre-training Models\nAbstract: arXiv:2408.02980v1 Announce Type: new  Abstract: Recent studies on AI security have highlighted the vulnerability of Vision-Language Pre-training (VLP) models to subtle yet intentionally designed perturbations in images and texts. Investigating multimodal systems' robustness via adversarial attacks is crucial in this field. Most multimodal attacks are sample-specific, generating a unique perturbation for each sample to construct adversarial samples. To the best of our knowledge, it is the first work through multimodal decision boundaries to explore the creation of a universal, sample-agnostic perturbation that applies to any image. Initially, we explore strategies to move sample points beyond the decision boundaries of linear classifiers, refining the algorithm to ensure successful attacks under the top $k$ accuracy metric. Based on this foundation, in visual-language tasks, we treat visual and textual modalities as reciprocal sample points and decision hyperplanes, guiding image embed",
    "path": "papers/24/08/2408.02980.json",
    "total_tokens": 361,
    "tldr": "该文章提出了一种与样本无关的对抗性扰动方法，旨在增强VLP模型对图像和文本的鲁棒性。作者开创性地探索了通过跨模态决策边界来创建通用的对抗性扰动，并将其应用于任何给定的图像，以此来提高模型的抗攻击能力。通过这种方法，即使在考虑了Top-k准确性的情况下，模型也能够成功抵御攻击。"
}