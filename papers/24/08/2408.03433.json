{
    "title": "Hybrid diffusion models: combining supervised and generative pretraining for label-efficient fine-tuning of segmentation models",
    "abstract": "arXiv:2408.03433v1 Announce Type: new  Abstract: We are considering in this paper the task of label-efficient fine-tuning of segmentation models: We assume that a large labeled dataset is available and allows to train an accurate segmentation model in one domain, and that we have to adapt this model on a related domain where only a few samples are available. We observe that this adaptation can be done using two distinct methods: The first method, supervised pretraining, is simply to take the model trained on the first domain using classical supervised learning, and fine-tune it on the second domain with the available labeled samples. The second method is to perform self-supervised pretraining on the first domain using a generic pretext task in order to get high-quality representations which can then be used to train a model on the second domain in a label-efficient way. We propose in this paper to fuse these two approaches by introducing a new pretext task, which is to perform simultan",
    "link": "https://arxiv.org/abs/2408.03433",
    "context": "Title: Hybrid diffusion models: combining supervised and generative pretraining for label-efficient fine-tuning of segmentation models\nAbstract: arXiv:2408.03433v1 Announce Type: new  Abstract: We are considering in this paper the task of label-efficient fine-tuning of segmentation models: We assume that a large labeled dataset is available and allows to train an accurate segmentation model in one domain, and that we have to adapt this model on a related domain where only a few samples are available. We observe that this adaptation can be done using two distinct methods: The first method, supervised pretraining, is simply to take the model trained on the first domain using classical supervised learning, and fine-tune it on the second domain with the available labeled samples. The second method is to perform self-supervised pretraining on the first domain using a generic pretext task in order to get high-quality representations which can then be used to train a model on the second domain in a label-efficient way. We propose in this paper to fuse these two approaches by introducing a new pretext task, which is to perform simultan",
    "path": "papers/24/08/2408.03433.json",
    "total_tokens": 337,
    "tldr": "文章提出了一种新的预训练方法，通过结合监督预训练和生成预训练，有效地利用标注数据少的领域进行分割模型的微调，从而提高了在相关领域的标注效率。"
}