{
    "title": "TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling",
    "abstract": "arXiv:2408.01291v1 Announce Type: new  Abstract: Given a 3D mesh, we aim to synthesize 3D textures that correspond to arbitrary textual descriptions. Current methods for generating and assembling textures from sampled views often result in prominent seams or excessive smoothing. To tackle these issues, we present TexGen, a novel multi-view sampling and resampling framework for texture generation leveraging a pre-trained text-to-image diffusion model. For view consistent sampling, first of all we maintain a texture map in RGB space that is parameterized by the denoising step and updated after each sampling step of the diffusion model to progressively reduce the view discrepancy. An attention-guided multi-view sampling strategy is exploited to broadcast the appearance information across views. To preserve texture details, we develop a noise resampling technique that aids in the estimation of noise, generating inputs for subsequent denoising steps, as directed by the text prompt and curre",
    "link": "https://arxiv.org/abs/2408.01291",
    "context": "Title: TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling\nAbstract: arXiv:2408.01291v1 Announce Type: new  Abstract: Given a 3D mesh, we aim to synthesize 3D textures that correspond to arbitrary textual descriptions. Current methods for generating and assembling textures from sampled views often result in prominent seams or excessive smoothing. To tackle these issues, we present TexGen, a novel multi-view sampling and resampling framework for texture generation leveraging a pre-trained text-to-image diffusion model. For view consistent sampling, first of all we maintain a texture map in RGB space that is parameterized by the denoising step and updated after each sampling step of the diffusion model to progressively reduce the view discrepancy. An attention-guided multi-view sampling strategy is exploited to broadcast the appearance information across views. To preserve texture details, we develop a noise resampling technique that aids in the estimation of noise, generating inputs for subsequent denoising steps, as directed by the text prompt and curre",
    "path": "papers/24/08/2408.01291.json",
    "total_tokens": 796,
    "translated_title": "TexGen：具有多视图采样和重采样的文本指导3D纹理生成",
    "translated_abstract": "给定一个3D网格，我们的目的是生成与任意文本描述相对应的3D纹理。当前从采样视图生成的纹理往往导致突出的接缝或过度的平滑处理。为了解决这些问题，我们提出了一种名为TexGen的新的多视图采样和重采样框架，用于利用预训练的文本到图像扩散模型来进行纹理生成。首先，我们在RGB空间保持一张纹理图，该图参数化于去噪步骤，并在扩散模型的每次采样步骤后更新，以逐渐减少视图差异。我们利用了一个注意力指导的多视图采样策略来在视图中传播外观信息。为了保留纹理细节，我们开发了一种噪声重采样技术，它帮助估计噪声，生成对后续去噪步骤有指导性的输入，而这些步骤是由文本提示和当前纹理状态定制的。随后，一种自聚焦金字塔结构被集成到纹理重采样过程中，用于捕获多尺度细节。最后，通过调节纹理的不同特征，纹理的平滑性和细节程度可根据文本描述进行调整。通过在多个基准数据集上的实验，我们展示了TexGen能够高效稳定地在多姿态和多光照条件下生成高质量的纹理，而其多尺度细节捕获和自聚焦机制能够避免在纹理对齐过程中出现显著的接缝和模糊。",
    "tldr": "TexGen是一个用于3D纹理生成的框架，它通过文本描述和预训练的文本到图像扩散模型来生成和组装纹理。这种方法通过多视图采样和重采样技术减少了视图差异，并在保持纹理细节的同时进行了噪声估算和输入指导。",
    "en_tdlr": "TexGen is a novel framework for 3D texture generation that uses text descriptions and a pre-trained text-to-image diffusion model to create and assemble textures. The method reduces view discrepancies through multi-view sampling and resampling techniques, while maintaining texture details by estimating noise and guiding input generation."
}