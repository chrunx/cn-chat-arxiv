{
    "title": "KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks applied to an LHC physics example",
    "abstract": "arXiv:2408.02743v1 Announce Type: cross  Abstract: Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an alternative to multilayer perceptrons, suggesting advantages in performance and interpretability. We study a typical binary event classification task in high-energy physics including high-level features and comment on the performance and interpretability of KANs in this context. We find that the learned activation functions of a one-layer KAN resemble the log-likelihood ratio of the input features. In deeper KANs, the activations in the first KAN layer differ from those in the one-layer KAN, which indicates that the deeper KANs learn more complex representations of the data. We study KANs with different depths and widths and we compare them to multilayer perceptrons in terms of performance and number of trainable parameters. For the chosen classification task, we do not find that KANs are more parameter efficient. However, small KANs may offer advantages in terms of i",
    "link": "https://arxiv.org/abs/2408.02743",
    "context": "Title: KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks applied to an LHC physics example\nAbstract: arXiv:2408.02743v1 Announce Type: cross  Abstract: Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an alternative to multilayer perceptrons, suggesting advantages in performance and interpretability. We study a typical binary event classification task in high-energy physics including high-level features and comment on the performance and interpretability of KANs in this context. We find that the learned activation functions of a one-layer KAN resemble the log-likelihood ratio of the input features. In deeper KANs, the activations in the first KAN layer differ from those in the one-layer KAN, which indicates that the deeper KANs learn more complex representations of the data. We study KANs with different depths and widths and we compare them to multilayer perceptrons in terms of performance and number of trainable parameters. For the chosen classification task, we do not find that KANs are more parameter efficient. However, small KANs may offer advantages in terms of i",
    "path": "papers/24/08/2408.02743.json",
    "total_tokens": 397,
    "tldr": "该文章通过将Kolmogorov-Arnold Networks应用于某LHC物理案例的高级特征二分类任务，研究了其在处理高性能和解释性改进方面的潜力。结果表明，尽管KANs在性能方面没有证明比传统前馈网络更高效，但在某些情况下，尤其是小规模的KAN模型，可能在保持较低参数数量的同时提供更好的数据表示学习能力。"
}