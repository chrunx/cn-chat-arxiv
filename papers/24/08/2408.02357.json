{
    "title": "On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'",
    "abstract": "arXiv:2408.02357v1 Announce Type: new  Abstract: We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the ability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP asserts that consistent reasoning implies fallibility -- in particular, human-like intelligence in AI necessarily comes with human-like fallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to mimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The paradox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will be correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than ",
    "link": "https://arxiv.org/abs/2408.02357",
    "context": "Title: On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'\nAbstract: arXiv:2408.02357v1 Announce Type: new  Abstract: We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the ability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP asserts that consistent reasoning implies fallibility -- in particular, human-like intelligence in AI necessarily comes with human-like fallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to mimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The paradox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will be correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than ",
    "path": "papers/24/08/2408.02357.json",
    "total_tokens": 454,
    "tldr": "该文章提出了“一致推理悖论”(CRP)，指出在处理以不同方式描述但等效的任务时，如“告诉我现在几点钟？”和“现在几点了？”，人类智能的核心要素是能够一致地推理。然而，CRP表明，在推理上的一致性意味着无法避免犯错，因为一个力求模仿人类智力的AI，即使在尝试给出答案时也很可能因推理一致而产生错误（ hallucinations，即看似合理但实际上错误的答案）。该悖论还指出，即使是一个非一致推理的AI（其水平低于人类）也可能在这些问题上给出正确答案。此外，文章还讨论了检测这些错误答案的难度。"
}