{
    "title": "Scaling Laws for Data Poisoning in LLMs",
    "abstract": "arXiv:2408.02946v1 Announce Type: cross  Abstract: Recent work shows that LLMs are vulnerable to data poisoning, in which they are trained on partially corrupted or harmful data. Poisoned data is hard to detect, breaks guardrails, and leads to undesirable and harmful behavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat. We consider three threat models by which data poisoning can occur: malicious fine-tuning, imperfect data curation, and intentional data contamination. Our experiments evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three datasets which speak to each of our threat models. We find that larger LLMs are increasingly vulnerable, learning harmful behavior -- including sleeper agent behavior -- significantly more quickly than smaller LLMs with",
    "link": "https://arxiv.org/abs/2408.02946",
    "context": "Title: Scaling Laws for Data Poisoning in LLMs\nAbstract: arXiv:2408.02946v1 Announce Type: cross  Abstract: Recent work shows that LLMs are vulnerable to data poisoning, in which they are trained on partially corrupted or harmful data. Poisoned data is hard to detect, breaks guardrails, and leads to undesirable and harmful behavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat. We consider three threat models by which data poisoning can occur: malicious fine-tuning, imperfect data curation, and intentional data contamination. Our experiments evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three datasets which speak to each of our threat models. We find that larger LLMs are increasingly vulnerable, learning harmful behavior -- including sleeper agent behavior -- significantly more quickly than smaller LLMs with",
    "path": "papers/24/08/2408.02946.json",
    "total_tokens": 354,
    "tldr": "该文章发现大语言模型（LLMs）在对数据进行污染时表现出了更大的脆弱性，这种污染包括恶意微调、数据质量问题以及数据故意污染。研究显示，随着模型参数的增加，学习有害行为的速度也显著加快，这表明大型模型可能更容易受到有害行为的攻击。"
}