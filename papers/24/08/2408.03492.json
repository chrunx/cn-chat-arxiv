{
    "title": "Automated Theorem Provers Help Improve Large Language Model Reasoning",
    "abstract": "arXiv:2408.03492v1 Announce Type: new  Abstract: In this paper we demonstrate how logic programming systems and Automated first-order logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models (LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM solutions. We first evaluate LLM reasoning on steamroller problems using the PRONTOQA benchmark. We show how accuracy can be improved with a neuro-symbolic architecture where the LLM acts solely as a front-end for translating a given problem into a formal logic language and an automated reasoning engine is called for solving it. However, this approach critically hinges on the correctness of the LLM translation. To assess this translation correctness, we secondly define a framework of syntactic and semantic error categories. We implemented the framework and used it to identify errors that LLMs make in the benchmark domain. Based on these findings, we thirdly extended our method with capabil",
    "link": "https://arxiv.org/abs/2408.03492",
    "context": "Title: Automated Theorem Provers Help Improve Large Language Model Reasoning\nAbstract: arXiv:2408.03492v1 Announce Type: new  Abstract: In this paper we demonstrate how logic programming systems and Automated first-order logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models (LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM solutions. We first evaluate LLM reasoning on steamroller problems using the PRONTOQA benchmark. We show how accuracy can be improved with a neuro-symbolic architecture where the LLM acts solely as a front-end for translating a given problem into a formal logic language and an automated reasoning engine is called for solving it. However, this approach critically hinges on the correctness of the LLM translation. To assess this translation correctness, we secondly define a framework of syntactic and semantic error categories. We implemented the framework and used it to identify errors that LLMs make in the benchmark domain. Based on these findings, we thirdly extended our method with capabil",
    "path": "papers/24/08/2408.03492.json",
    "total_tokens": 380,
    "tldr": "该文章提出使用逻辑编程系统和自动定理证明器改进大型语言模型在逻辑推理任务中的准确度，特别是通过神经符号架构来提高性能。研究人员在评价大型语言模型在蒸汽引擎难题中的表现后，发现通过自动定理证明器来解决相关问题可以大幅提升准确度。文章还介绍了错误分类框架，并在实际应用中发现大型语言模型在翻译过程中的错误，进而优化了方法，提升了结果。"
}