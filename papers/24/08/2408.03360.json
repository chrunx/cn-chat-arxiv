{
    "title": "Prioritize Alignment in Dataset Distillation",
    "abstract": "arXiv:2408.03360v1 Announce Type: cross  Abstract: Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. In this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages. To alleviate this, we propose Prioritize Alignment in Dataset Distillation (PAD), which aligns information from the following two perspectives. 1) We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model. 2) We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level inform",
    "link": "https://arxiv.org/abs/2408.03360",
    "context": "Title: Prioritize Alignment in Dataset Distillation\nAbstract: arXiv:2408.03360v1 Announce Type: cross  Abstract: Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. In this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages. To alleviate this, we propose Prioritize Alignment in Dataset Distillation (PAD), which aligns information from the following two perspectives. 1) We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model. 2) We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level inform",
    "path": "papers/24/08/2408.03360.json",
    "total_tokens": 364,
    "tldr": "该文章提出了一种名为Prioritize Alignment in Dataset Distillation（PAD）的改进方法，旨在提高数据集蒸馏中信息的准确度和有效性。通过首先根据压缩率筛选目标数据集，去除难以被提取的信息，然后仅使用深度模型层来施以蒸馏，以避免引入过多低层次的信息，从而提高了信息提取和嵌入的准确性，提高了压缩数据集的质量。"
}