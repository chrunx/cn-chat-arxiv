{
    "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models",
    "abstract": "arXiv:2408.01933v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability",
    "link": "https://arxiv.org/abs/2408.01933",
    "context": "Title: DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models\nAbstract: arXiv:2408.01933v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability",
    "path": "papers/24/08/2408.01933.json",
    "total_tokens": 353,
    "tldr": "该文章介绍了名为DiReCT的临床笔记诊断推理数据集，它通过521份临床笔记的细致标注，评估了大型语言模型在诊断推理方面的能力以及对人类医生的解释性。数据集包含了一个诊断知识图谱，有助于LLMs进行推理，并突出了这些模型在诊断推理能力上的不足。"
}