{
    "title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
    "abstract": "arXiv:2408.03350v1 Announce Type: new  Abstract: We introduce miniCTX, which tests a model's ability to prove formal mathematical theorems that depend on new definitions, lemmas, or other contextual information that was not observed during training. miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is helpful or needed for the proof. As a baseline for miniCTX, we introduce file-tuning, a simple recipe that trains a model to generate a proof step conditioned on the preceding file contents. File-tuning substantially outperforms the traditional neural theorem proving approach that fine-tunes on states alone. Additionally, our file-tuned model improves performance on the standard miniF2F benchmark, achieving a pass rate of 33.61%, which is a new state-of-the-art for 1.3B para",
    "link": "https://arxiv.org/abs/2408.03350",
    "context": "Title: miniCTX: Neural Theorem Proving with (Long-)Contexts\nAbstract: arXiv:2408.03350v1 Announce Type: new  Abstract: We introduce miniCTX, which tests a model's ability to prove formal mathematical theorems that depend on new definitions, lemmas, or other contextual information that was not observed during training. miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is helpful or needed for the proof. As a baseline for miniCTX, we introduce file-tuning, a simple recipe that trains a model to generate a proof step conditioned on the preceding file contents. File-tuning substantially outperforms the traditional neural theorem proving approach that fine-tunes on states alone. Additionally, our file-tuned model improves performance on the standard miniF2F benchmark, achieving a pass rate of 33.61%, which is a new state-of-the-art for 1.3B para",
    "path": "papers/24/08/2408.03350.json",
    "total_tokens": 488,
    "tldr": "该文章引入了miniCTX，这是一个新的神经定理证明数据集，旨在评估模型在获取训练过程中未出现的定义、引理或其他相关上下文信息的情况下证明数学定理的能力。miniCTX包含了来自真实Lean项目和数学教材的数百个定理，每个定理都对应一个可能包含数万个Token的上下文。模型需要在能够访问与定理相关的代码库的情况下证明定理。文章还介绍了一种名为文件调优的简单技术，它通过基于文件内容条件的生成模型来训练模型以生成证明步骤。与该技术相比，传统仅在状态上进行微调的神经定理证明方法取得了显著的改进。此外，通过文件调优的模型在标准的小型F2F基准测试上也表现出色，获得了一个新的领域最佳成绩，证明步骤的通过率达到了33.61%。"
}