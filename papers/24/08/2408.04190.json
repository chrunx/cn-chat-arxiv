{
    "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning",
    "abstract": "arXiv:2408.04190v1 Announce Type: cross  Abstract: In Reinforcement Learning (RL), designing precise reward functions remains to be a challenge, particularly when aligning with human intent. Preference-based RL (PbRL) was introduced to address this problem by learning reward models from human feedback. However, existing PbRL methods have limitations as they often overlook the second-order preference that indicates the relative strength of preference. In this paper, we propose Listwise Reward Estimation (LiRE), a novel approach for offline PbRL that leverages second-order preference information by constructing a Ranked List of Trajectories (RLT), which can be efficiently built by using the same ternary feedback type as traditional methods. To validate the effectiveness of LiRE, we propose a new offline PbRL dataset that objectively reflects the effect of the estimated rewards. Our extensive experiments on the dataset demonstrate the superiority of LiRE, i.e., outperforming state-of-the-",
    "link": "https://arxiv.org/abs/2408.04190",
    "context": "Title: Listwise Reward Estimation for Offline Preference-based Reinforcement Learning\nAbstract: arXiv:2408.04190v1 Announce Type: cross  Abstract: In Reinforcement Learning (RL), designing precise reward functions remains to be a challenge, particularly when aligning with human intent. Preference-based RL (PbRL) was introduced to address this problem by learning reward models from human feedback. However, existing PbRL methods have limitations as they often overlook the second-order preference that indicates the relative strength of preference. In this paper, we propose Listwise Reward Estimation (LiRE), a novel approach for offline PbRL that leverages second-order preference information by constructing a Ranked List of Trajectories (RLT), which can be efficiently built by using the same ternary feedback type as traditional methods. To validate the effectiveness of LiRE, we propose a new offline PbRL dataset that objectively reflects the effect of the estimated rewards. Our extensive experiments on the dataset demonstrate the superiority of LiRE, i.e., outperforming state-of-the-",
    "path": "papers/24/08/2408.04190.json",
    "total_tokens": 391,
    "tldr": "该文章提出了一种名为Listwise Reward Estimation (LiRE)的新方法，用于改进离线的偏好型强化学习，它能更精准地学习到人类意图的奖励模型。LiRE通过使用与传统方法相同的反馈类型，构建了轨迹的等级列表，充分利用了人类偏好中的第二级信息，从而在实验中证明了其在强化学习任务中能更有效地估计奖励，并显著超越了现有方法。"
}