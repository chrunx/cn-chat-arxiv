{
    "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
    "abstract": "arXiv:2408.02657v1 Announce Type: new  Abstract: We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general m",
    "link": "https://arxiv.org/abs/2408.02657",
    "context": "Title: Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining\nAbstract: arXiv:2408.02657v1 Announce Type: new  Abstract: We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general m",
    "path": "papers/24/08/2408.02657.json",
    "total_tokens": 435,
    "tldr": "该文章创新性地提出了一种名为Lumina-mGPT的模型，它是一种基于多模态预训练的图像生成方法，能够从文本描述中灵活生成高保真的图像。该模型通过在大量文本和图像混合的序列上训练一个简单的前馈神经网络，并结合了多模态的生成预训练，能够在各种视觉和语言任务中表现出优异性能。通过对高质图像文本对进行渐进式监督微调和多模态生成预训练，Lumina-mGPT能够在保持泛化能力和灵活性的同时，在高分辨率下生成具有高美学质量的图像。"
}