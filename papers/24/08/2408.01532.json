{
    "title": "Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization",
    "abstract": "arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo",
    "link": "https://arxiv.org/abs/2408.01532",
    "context": "Title: Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization\nAbstract: arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo",
    "path": "papers/24/08/2408.01532.json",
    "total_tokens": 349,
    "tldr": "该文章提出了一种基于循环神经网络（RNNs）的跨模态注意力框架，用于音频-视频深度伪造检测。该框架能够利用上下文信息学习模态之间的贡献特征，有效克服了音频和视频信号之间的分布性模态差异，提高了检测深度伪造的准确性和鲁棒性。"
}