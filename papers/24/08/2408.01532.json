{
    "title": "Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization",
    "abstract": "arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo",
    "link": "https://arxiv.org/abs/2408.01532",
    "context": "Title: Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization\nAbstract: arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo",
    "path": "papers/24/08/2408.01532.json",
    "total_tokens": 708,
    "translated_title": "基于上下文跨模态注意力的音频-视觉深度伪造检测与定位",
    "translated_abstract": "arXiv:2408.01532v1 公告类型: 交叉 摘要: 在数字时代，深度伪造和合成媒体的出现对社会政治的完整性和诚信构成了重大威胁。基于多模态操纵的深度伪造，如音频-视觉深度伪造，更加逼真，威胁更大。现有的多模态深度伪造检测器通常基于跨模态数据的异构流融合，如音频和视频信号。然而，数据的异构性在有效融合和因此多模态深度伪造检测中创造了分布模态差距。在本论文中，我们提出了一种基于循环神经网络（RNNs）的新颖多模态注意力框架，用于音频-视觉深度伪造的检测。该提出的框架应用注意力到多模态多序列表示上，并学习它们之间的贡献性特征，用于深度伪造检测和位置识别。通过这种方式，我们的模型不仅能够提高深度伪造检测的准确率，而且还能对伪造图像中的不自然区域进行定位。我们的实验结果表明，与现有的多模态方法相比，我们的模型在多个公开数据集上的性能更为优越。",
    "tldr": "本文提出了一种基于RNN的音频-视觉深度伪造检测和定位的新框架，通过跨模态注意力和上下文信息的学习，提高检测准确性并定位伪造图像的不自然区域。"
}