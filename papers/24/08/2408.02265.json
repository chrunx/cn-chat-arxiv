{
    "title": "Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts",
    "abstract": "arXiv:2408.02265v1 Announce Type: new  Abstract: The concept bottleneck model (CBM) is an interpretable-by-design framework that makes decisions by first predicting a set of interpretable concepts, and then predicting the class label based on the given concepts. Existing CBMs are trained with a fixed set of concepts (concepts are either annotated by the dataset or queried from language models). However, this closed-world assumption is unrealistic in practice, as users may wonder about the role of any desired concept in decision-making after the model is deployed. Inspired by the large success of recent vision-language pre-trained models such as CLIP in zero-shot classification, we propose \"OpenCBM\" to equip the CBM with open vocabulary concepts via: (1) Aligning the feature space of a trainable image feature extractor with that of a CLIP's image encoder via a prototype based feature alignment; (2) Simultaneously training an image classifier on the downstream dataset; (3) Reconstructing",
    "link": "https://arxiv.org/abs/2408.02265",
    "context": "Title: Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts\nAbstract: arXiv:2408.02265v1 Announce Type: new  Abstract: The concept bottleneck model (CBM) is an interpretable-by-design framework that makes decisions by first predicting a set of interpretable concepts, and then predicting the class label based on the given concepts. Existing CBMs are trained with a fixed set of concepts (concepts are either annotated by the dataset or queried from language models). However, this closed-world assumption is unrealistic in practice, as users may wonder about the role of any desired concept in decision-making after the model is deployed. Inspired by the large success of recent vision-language pre-trained models such as CLIP in zero-shot classification, we propose \"OpenCBM\" to equip the CBM with open vocabulary concepts via: (1) Aligning the feature space of a trainable image feature extractor with that of a CLIP's image encoder via a prototype based feature alignment; (2) Simultaneously training an image classifier on the downstream dataset; (3) Reconstructing",
    "path": "papers/24/08/2408.02265.json",
    "total_tokens": 395,
    "tldr": "该文章提出了一种名为\"OpenCBM\"的概念瓶颈模型（CBM）的变体，该模型通过对图像特征提取器与CLIP中的图像编码器之间的特征空间进行原型基础的联合训练，使其能够使用开放词汇概念。这一方法允许多种开放概念在决定过程中起作用，而非仅限于预先指定的概念集，提高了模型的实用性和灵活性。"
}