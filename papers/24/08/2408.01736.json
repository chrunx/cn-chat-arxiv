{
    "title": "Can LLMs predict the convergence of Stochastic Gradient Descent?",
    "abstract": "arXiv:2408.01736v1 Announce Type: cross  Abstract: Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.",
    "link": "https://arxiv.org/abs/2408.01736",
    "context": "Title: Can LLMs predict the convergence of Stochastic Gradient Descent?\nAbstract: arXiv:2408.01736v1 Announce Type: cross  Abstract: Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.",
    "path": "papers/24/08/2408.01736.json",
    "total_tokens": 274,
    "tldr": "该文章展示了大型语言模型能够预测随机梯度下降在凸和非凸优化中的收敛点。"
}