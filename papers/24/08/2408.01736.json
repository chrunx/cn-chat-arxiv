{
    "title": "Can LLMs predict the convergence of Stochastic Gradient Descent?",
    "abstract": "arXiv:2408.01736v1 Announce Type: cross  Abstract: Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.",
    "link": "https://arxiv.org/abs/2408.01736",
    "context": "Title: Can LLMs predict the convergence of Stochastic Gradient Descent?\nAbstract: arXiv:2408.01736v1 Announce Type: cross  Abstract: Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.",
    "path": "papers/24/08/2408.01736.json",
    "total_tokens": 560,
    "translated_title": "能用大语言模型预测随机梯度下降的收敛性吗？",
    "translated_abstract": "arXiv:2408.01736v1 公告类型：交叉  摘要：大型语言模型因其在一系列广泛任务中表现出色而闻名。其令人惊讶的一个例子是最近发现的大语言模型理解满足马尔可夫性质的动力系统 governing 原则的能力。在这篇论文中，我们希望通过研究随机梯度下降在凸和非凸优化中的动力学进一步探索这一方向。通过利用随机梯度下降与马尔可夫链之间的理论联系，我们展示了对于之前未见过的起始点，大语言模型在预测 SGD 收敛到的局部最小值方面表现出了惊人的零样本性能。在更广泛的层面上，我们问询了大语言模型是否有潜力用于执行实践中使用的更大深度学习模型的零样本随机试验。",
    "tldr": "论文展示了大型语言模型在预测随机梯度下降收敛到局部最小值方面的出色性能。"
}