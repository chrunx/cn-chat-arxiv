{
    "title": "The Data Addition Dilemma",
    "abstract": "arXiv:2408.04154v1 Announce Type: cross  Abstract: In many machine learning for healthcare tasks, standard datasets are constructed by amassing data across many, often fundamentally dissimilar, sources. But when does adding more data help, and when does it hinder progress on desired model outcomes in real-world settings? We identify this situation as the \\textit{Data Addition Dilemma}, demonstrating that adding training data in this multi-source scaling context can at times result in reduced overall accuracy, uncertain fairness outcomes, and reduced worst-subgroup performance. We find that this possibly arises from an empirically observed trade-off between model performance improvements due to data scaling and model deterioration from distribution shift. We thus establish baseline strategies for navigating this dilemma, introducing distribution shift heuristics to guide decision-making on which data sources to add in data scaling, in order to yield the expected model performance improv",
    "link": "https://arxiv.org/abs/2408.04154",
    "context": "Title: The Data Addition Dilemma\nAbstract: arXiv:2408.04154v1 Announce Type: cross  Abstract: In many machine learning for healthcare tasks, standard datasets are constructed by amassing data across many, often fundamentally dissimilar, sources. But when does adding more data help, and when does it hinder progress on desired model outcomes in real-world settings? We identify this situation as the \\textit{Data Addition Dilemma}, demonstrating that adding training data in this multi-source scaling context can at times result in reduced overall accuracy, uncertain fairness outcomes, and reduced worst-subgroup performance. We find that this possibly arises from an empirically observed trade-off between model performance improvements due to data scaling and model deterioration from distribution shift. We thus establish baseline strategies for navigating this dilemma, introducing distribution shift heuristics to guide decision-making on which data sources to add in data scaling, in order to yield the expected model performance improv",
    "path": "papers/24/08/2408.04154.json",
    "total_tokens": 309,
    "tldr": "该文章探讨了在多源数据集合中扩充训练数据可能导致模型性能降低的问题，并提出了基于分布偏移的策略来优化数据扩充决策以提升模型性能。"
}