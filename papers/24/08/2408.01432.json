{
    "title": "VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance",
    "abstract": "arXiv:2408.01432v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models (LLMs) and pre-trained Vision-Language Models (VLMs) to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boos",
    "link": "https://arxiv.org/abs/2408.01432",
    "context": "Title: VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance\nAbstract: arXiv:2408.01432v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models (LLMs) and pre-trained Vision-Language Models (VLMs) to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boos",
    "path": "papers/24/08/2408.01432.json",
    "total_tokens": 807,
    "translated_title": "VLG-CBM：利用视觉语言指导的训练概念瓶颈模型",
    "translated_abstract": "arXiv:2408.01432v1 公告类型：新  摘要：概念瓶颈模型（CBMs）通过引入一个中间的概念瓶颈层（CBL）提供可解释的预测，该层编码了人类可理解的概念来解释模型的决策。最近的工作提出使用大型语言模型（LLMs）和预训练的视觉语言模型（VLMs）自动训练CBMs，这使得它更加可扩展和自动化。然而，现有方法在两个方面仍有不足：首先，CBL预测的概念经常与输入图像不符，这让人对解释的准确性表示怀疑。其次，已经表明概念值编码了不必要的信息：即使是随机概念集也能获得与当前最先进CBMs相当的测试准确度。为了解决这些关键的局限性，在本工作中，我们提出了一种名为“视觉语言指导的概念瓶颈模型”（VLG-CBM）的全新框架，以实现具有増强解释性的训练，同时享受自动化的收益。通过引入一种新的视觉语言指导机制，VLG-CBM能够确保概念瓶颈层的概念与图像更紧密相关，并增强模型的预测可解释性。此外，我们还发现，概念值中存在冗余信息，这会影响模型的预测决策。通过处理这些额外的信息，VLG-CBM能够进一步提高模型的性能并保证预测解释的一致性和可靠性。我们的实验结果表明，使用VLG-CBM进行训练的模型在保持高准确性的同时，实现了更好的解释性和概念一致性。",
    "tldr": "VLG-CBM是一种新的框架，旨在通过引入视觉语言指导机制，提高概念瓶颈模型的解释性和一致性，并解决概念预测与输入图像不一致的问题。",
    "en_tdlr": "Our proposed framework, VLG-CBM, introduces a novel visual-language guidance mechanism to enhance the interpretability and consistency of concept bottleneck models, addressing the issue of inconsistent concept predictions with input images."
}