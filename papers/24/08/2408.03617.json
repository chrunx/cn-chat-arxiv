{
    "title": "Is Child-Directed Speech Effective Training Data for Language Models?",
    "abstract": "arXiv:2408.03617v1 Announce Type: cross  Abstract: While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 models on 29M words of English-language child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to a heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data support high performance relative to other datasets. The local properties of the data affect model results, but somewhat surprisingly, global properties do not. Further, c",
    "link": "https://arxiv.org/abs/2408.03617",
    "context": "Title: Is Child-Directed Speech Effective Training Data for Language Models?\nAbstract: arXiv:2408.03617v1 Announce Type: cross  Abstract: While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 models on 29M words of English-language child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to a heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data support high performance relative to other datasets. The local properties of the data affect model results, but somewhat surprisingly, global properties do not. Further, c",
    "path": "papers/24/08/2408.03617.json",
    "total_tokens": 372,
    "tldr": "该文章提出并训练了一种基于儿童指令性口语的大语言模型，并对模型在语言理解和学习任务上的表现进行了评估。研究结果显示，儿童指令性口语的局部特性对模型性能有显著影响，但整体特性并未展现出预期的效果。这一发现揭示了儿童在语言学习中所接收到的数据类型对语言模型训练的价值，尤其是其局部语境和对话顺序特征可能在语言模型的发展中扮演了关键角色。"
}