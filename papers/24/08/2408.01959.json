{
    "title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI",
    "abstract": "arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u",
    "link": "https://arxiv.org/abs/2408.01959",
    "context": "Title: Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI\nAbstract: arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u",
    "path": "papers/24/08/2408.01959.json",
    "total_tokens": 376,
    "tldr": "该文章发现43个CLIP视觉语言模型在训练数据集较大时，更能反映社会中的人脸评价偏见，并首次证明社会中普遍的偏见程度与模型中表现出的偏见程度正相关。这表明，随着训练数据量的增加，模型在学习人类“可观察”和“不可观察”特征评价时，更容易复制社会中存在的细微偏见。"
}