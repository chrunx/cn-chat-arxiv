{
    "title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI",
    "abstract": "arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u",
    "link": "https://arxiv.org/abs/2408.01959",
    "context": "Title: Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI\nAbstract: arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u",
    "path": "papers/24/08/2408.01959.json",
    "total_tokens": 788,
    "translated_title": "数据集大小和社会一致性中介面部印象偏见的视觉语言AI",
    "translated_abstract": "arXiv:2408.01959v1公告类型：新摘要：能够将图像与文本关联的多模态AI模型在包括自动图像字幕在内的多个领域具有巨大潜力，甚至对盲人和低视力用户的安全访问应用程序也有帮助。然而，对偏见的不确定性在某些情况下限制了它们的采用和可用性。在本文中，我们对43个CLIP视觉语言模型进行了研究，以确定它们是否像人类一样学习面部印象偏见，并且我们首次发现，这些偏见在三种类型的CLIP模型家族中得到体现。我们还首次展示了，一个偏见的程度在社会中共享，预测了它在CLIP模型中反映的程度的程度。在仅对最大数据集进行训练的模型中，人类似的面部印象才会出现，这表明与未经审核的文化的更好匹配对于复制越来越微妙的社交偏见是必要的。此外，我们还发现，当对图像的某些不可见属性（如可信赖性和性取向）形成印象时，只有在大数据集上下文中基于模型的训练才更加准确。这可能意味着在生成与社会偏好高度一致的文本描述方面，视觉语言模型正在远离对现实数据集的简单映射，并开始学习如何了解和模仿人类的社会形态知觉。这对于理解模型的感知能力和揭示这些偏见对决策制定的潜在影响至关重要。",
    "tldr": "研究揭示了43个CLIP视觉语言模型在面部印象偏见方面的学习情况，证明了社会一致性会中介模型反映的人类偏见程度，并且在数据集大规模训练下，模型更倾向于准确形成那些依赖于不可见属性的印象。"
}