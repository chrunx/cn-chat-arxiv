{
    "title": "Compromising Embodied Agents with Contextual Backdoor Attacks",
    "abstract": "arXiv:2408.02882v1 Announce Type: new  Abstract: Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \\method{}. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ ",
    "link": "https://arxiv.org/abs/2408.02882",
    "context": "Title: Compromising Embodied Agents with Contextual Backdoor Attacks\nAbstract: arXiv:2408.02882v1 Announce Type: new  Abstract: Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \\method{}. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ ",
    "path": "papers/24/08/2408.02882.json",
    "total_tokens": 324,
    "tldr": "该文章提出了一种名为\\method{}的新方法，通过污染少量的上下文演示，攻击者可以在不知不觉中影响大型语言模型（LLMs）的内部知识，导致其生成包含特定环境响应的缺陷代码，从而在未来的运行时触发恶意行为。"
}