{
    "title": "Gemma 2: Improving Open Language Models at a Practical Size",
    "abstract": "arXiv:2408.00118v2 Announce Type: replace-cross  Abstract: In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.",
    "link": "https://arxiv.org/abs/2408.00118",
    "context": "Title: Gemma 2: Improving Open Language Models at a Practical Size\nAbstract: arXiv:2408.00118v2 Announce Type: replace-cross  Abstract: In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.",
    "path": "papers/24/08/2408.00118.json",
    "total_tokens": 665,
    "translated_title": "Gemma 2: 提高实用规模开放语言模型的性能",
    "translated_abstract": "arXiv:2408.00118v2 公告类型: 替换交叉摘要：在本工作中，我们介绍了Gemma 2，Gemma家族的一个新的轻量级、最先进的开源模型系列，参数规模从20亿到270亿不等。在本版本中，我们向Transformer架构应用了几项已知的技术改进，如布雷特吉等人（2020a）提出的本地-全局注意力交叉和埃辛等人（2023）提出的组查询注意力。我们还使用蒸馏知识（Hinton et al.，2015）而不是接下来预测的方式训练了2亿和90亿参数的模型。结果模型在它们的规模上提供了最佳的性能，甚至为比它们大2-3倍的模型提供了竞争性的替代方案。我们将所有模型发布给了社区。",
    "tldr": "Gemma 2是Gemma系列中新型轻量级开放模型，通过改进Transformer架构和应用蒸馏知识培训，在20亿和90亿参数规模上表现最佳。"
}