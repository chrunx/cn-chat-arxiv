{
    "title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models",
    "abstract": "arXiv:2408.02632v1 Announce Type: cross  Abstract: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduc",
    "link": "https://arxiv.org/abs/2408.02632",
    "context": "Title: SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models\nAbstract: arXiv:2408.02632v1 Announce Type: cross  Abstract: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduc",
    "path": "papers/24/08/2408.02632.json",
    "total_tokens": 626,
    "translated_title": "SEAS：大型语言模型自我演化对抗安全优化",
    "translated_abstract": "arXiv:2408.02632v1 预告类型：交叉  译文摘要：随着大型语言模型(LLMs)在能力和影响力方面的不断进步，确保其安全和防止有害输出变得至关重要。一种解决这些问题的 promising（有希望的）方法是训练模型自动生成用于红队演练的对抗性提示。然而，LLMs漏洞的演变微妙性挑战了当前对抗性方法的有效性，这些方法难以专门针对并探索这些模型的弱点。为了应对这些挑战，我们提出了“SEAS：自我演化对抗安全优化”（Self-Evolving Adversarial Safety Optimization）优化框架，它通过利用模型自身生成的数据来增强安全性。SEAS通过三个迭代的阶段：初始化、攻击和对抗性优化，来改进红队和目标模型以提高鲁棒性和安全性。本框架通过自我完善，使得模型更加安全和可靠。",
    "tldr": "SEAS框架通过自我演化对抗优化，提高了大型语言模型在红队演练中的安全性和鲁棒性。",
    "en_tdlr": "The SEAS framework enhances the security and robustness of large language models through self-evolving adversarial optimization for red teaming exercises."
}