{
    "title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models",
    "abstract": "arXiv:2408.02632v1 Announce Type: cross  Abstract: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduc",
    "link": "https://arxiv.org/abs/2408.02632",
    "context": "Title: SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models\nAbstract: arXiv:2408.02632v1 Announce Type: cross  Abstract: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduc",
    "path": "papers/24/08/2408.02632.json",
    "total_tokens": 330,
    "tldr": "该文章提出了SEAS框架，通过迭代地优化初始化、攻击和 adversarial 训练，使得 model 能更有效地抵御通过自身数据生成的攻击，提高 LLM 安全性。"
}