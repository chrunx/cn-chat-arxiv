{
    "title": "DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization",
    "abstract": "arXiv:2408.04400v1 Announce Type: cross  Abstract: This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck a",
    "link": "https://arxiv.org/abs/2408.04400",
    "context": "Title: DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization\nAbstract: arXiv:2408.04400v1 Announce Type: cross  Abstract: This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck a",
    "path": "papers/24/08/2408.04400.json",
    "total_tokens": 330,
    "tldr": "该文章提出了一种名为DIVE的创新方法，通过识别并利用图数据中训练和测试分布之间的子图差异，显著提高了图机器学习模型的分布外泛化能力，解决了传统方法由于假设训练和测试数据分布一致而导致的性能滑坡问题。"
}