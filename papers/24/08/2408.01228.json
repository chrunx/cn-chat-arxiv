{
    "title": "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "abstract": "arXiv:2408.01228v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) combine visual and textual understanding, rendering them well-suited for diverse tasks like generating image captions and answering visual questions across various domains. However, these capabilities are built upon training on large amount of uncurated data crawled from the web. The latter may include sensitive information that VLMs could memorize and leak, raising significant privacy concerns. In this paper, we assess whether these vulnerabilities exist, focusing on identity leakage. Our study leads to three key findings: (i) VLMs leak identity information, even when the vision-language alignment and the fine-tuning use anonymized data; (ii) context has little influence on identity leakage; (iii) simple, widely used anonymization techniques, like blurring, are not sufficient to address the problem. These findings underscore the urgent need for robust privacy protection strategies when deploying VLMs. Ethic",
    "link": "https://arxiv.org/abs/2408.01228",
    "context": "Title: The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models\nAbstract: arXiv:2408.01228v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) combine visual and textual understanding, rendering them well-suited for diverse tasks like generating image captions and answering visual questions across various domains. However, these capabilities are built upon training on large amount of uncurated data crawled from the web. The latter may include sensitive information that VLMs could memorize and leak, raising significant privacy concerns. In this paper, we assess whether these vulnerabilities exist, focusing on identity leakage. Our study leads to three key findings: (i) VLMs leak identity information, even when the vision-language alignment and the fine-tuning use anonymized data; (ii) context has little influence on identity leakage; (iii) simple, widely used anonymization techniques, like blurring, are not sufficient to address the problem. These findings underscore the urgent need for robust privacy protection strategies when deploying VLMs. Ethic",
    "path": "papers/24/08/2408.01228.json",
    "total_tokens": 665,
    "translated_title": "原论文标题：《原力觉醒：揭露视觉语言模型的隐私泄露》",
    "translated_abstract": "原论文摘要：姚恩文，(2024). 随着计算机视觉和自然语言处理的快速发展，视觉语言模型(VLMs)已经成为结合图像理解和文本描述的强大工具，广泛应用于各种任务中，包括生成图像描述和回答视觉相关问题。然而，这种能力建立在一个大型的未经过滤的数据集之上，该数据集可能包含了敏感信息，这些模型有可能学习和泄露这些信息，引起了严重的隐私问题。本文的目的旨在评估这些潜在的隐私泄露问题，特别是关注身份泄露方面。通过一系列实验，我们发现：（1）即使使用匿名的训练数据，VLMs仍然能泄露身份信息；(2) 上下文对身份泄露的影响不大；(3) 目前常见的匿名化技术，如图像模糊，并不能有效地解决这一问题。这些发现凸显了在部署这些模型之前需要采取严格的隐私保护措施的紧迫性。伦理方面，这一研究必须慎重记录和报告所有发现，并向相关利益相关者通报可能的风险。",
    "tldr": "本文揭示了在训练数据中包含敏感信息的视觉语言模型（VLMs）可能会无意中泄露个人身份信息，即便使用匿名化处理。",
    "en_tdlr": "This study highlights the potential for vision-language models trained on sensitive data to inadvertently leak personal identity information, even with anonymization techniques."
}