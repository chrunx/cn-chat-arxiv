{
    "title": "MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning",
    "abstract": "arXiv:2408.04243v1 Announce Type: new  Abstract: With the exponential growth of multimedia data, leveraging multimodal sensors presents a promising approach for improving accuracy in human activity recognition. Nevertheless, accurately identifying these activities using both video data and wearable sensor data presents challenges due to the labor-intensive data annotation, and reliance on external pretrained models or additional data. To address these challenges, we introduce Multimodal Masked Autoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal masked autoencoder with a synchronized masking strategy tailored for wearable sensors. This masking strategy compels the networks to capture more meaningful spatiotemporal features, which enables effective self-supervised pretraining without the need for external data. Furthermore, Mu-MAE leverages the representation extracted from multimodal masked autoencoders as prior information input to a cross-attention multimodal",
    "link": "https://arxiv.org/abs/2408.04243",
    "context": "Title: MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning\nAbstract: arXiv:2408.04243v1 Announce Type: new  Abstract: With the exponential growth of multimedia data, leveraging multimodal sensors presents a promising approach for improving accuracy in human activity recognition. Nevertheless, accurately identifying these activities using both video data and wearable sensor data presents challenges due to the labor-intensive data annotation, and reliance on external pretrained models or additional data. To address these challenges, we introduce Multimodal Masked Autoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal masked autoencoder with a synchronized masking strategy tailored for wearable sensors. This masking strategy compels the networks to capture more meaningful spatiotemporal features, which enables effective self-supervised pretraining without the need for external data. Furthermore, Mu-MAE leverages the representation extracted from multimodal masked autoencoders as prior information input to a cross-attention multimodal",
    "path": "papers/24/08/2408.04243.json",
    "total_tokens": 374,
    "tldr": "该文章提出了一种名为MU-MAE的算法，其结合了多模态 masked autoencoders 用于单例学习，通过同步的遮罩策略强化了传感器数据的特征捕捉，无需额外数据即可实现有效的自监督预训练，同时利用多模态遮罩 autoencoders 的表示作为提供的先验信息输入到跨注意力机制的多模态模型中，有效提升了人类活动识别的准确性。"
}