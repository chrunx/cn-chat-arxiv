{
    "title": "Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation",
    "abstract": "arXiv:2408.03735v1 Announce Type: new  Abstract: This paper presents the first study to explore the potential of parameter quantization for multimodal large language models to alleviate the significant resource constraint encountered during vision-language instruction tuning. We introduce a Quantization-aware Scale LeArning method based on multimodal Warmup, termed QSLAW. This method is grounded in two key innovations: (1) The learning of group-wise scale factors for quantized LLM weights to mitigate the quantization error arising from activation outliers and achieve more effective vision-language instruction tuning; (2) The implementation of a multimodal warmup that progressively integrates linguistic and multimodal training samples, thereby preventing overfitting of the quantized model to multimodal data while ensuring stable adaptation of multimodal large language models to downstream vision-language tasks. Extensive experiments demonstrate that models quantized by QSLAW perform on ",
    "link": "https://arxiv.org/abs/2408.03735",
    "context": "Title: Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation\nAbstract: arXiv:2408.03735v1 Announce Type: new  Abstract: This paper presents the first study to explore the potential of parameter quantization for multimodal large language models to alleviate the significant resource constraint encountered during vision-language instruction tuning. We introduce a Quantization-aware Scale LeArning method based on multimodal Warmup, termed QSLAW. This method is grounded in two key innovations: (1) The learning of group-wise scale factors for quantized LLM weights to mitigate the quantization error arising from activation outliers and achieve more effective vision-language instruction tuning; (2) The implementation of a multimodal warmup that progressively integrates linguistic and multimodal training samples, thereby preventing overfitting of the quantized model to multimodal data while ensuring stable adaptation of multimodal large language models to downstream vision-language tasks. Extensive experiments demonstrate that models quantized by QSLAW perform on ",
    "path": "papers/24/08/2408.03735.json",
    "total_tokens": 407,
    "tldr": "该文章创新性地提出了基于组间尺度学习的方法，结合量化自适应和多模态优化，改进了多模态大型语言模型在视觉语言指导训练中的资源消耗。通过学习量化后的模型权重组间尺度因子，缓解了因激活异常值导致的量化错误，并有效提升了视觉语言指导训练的性能。此外，通过渐进式的多模态训练样本集成，该方法防止了模型对多模态数据的过拟合并保证了多模态大型语言模型在下游视觉语言任务中的稳定适应性。实验结果表明，采用该方法进行量化的模型性能有了显著提升。"
}