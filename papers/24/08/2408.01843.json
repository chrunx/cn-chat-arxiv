{
    "title": "Supervised Image Translation from Visible to Infrared Domain for Object Detection",
    "abstract": "arXiv:2408.01843v1 Announce Type: new  Abstract: This study aims to learn a translation from visible to infrared imagery, bridging the domain gap between the two modalities so as to improve accuracy on downstream tasks including object detection. Previous approaches attempt to perform bi-domain feature fusion through iterative optimization or end-to-end deep convolutional networks. However, we pose the problem as similar to that of image translation, adopting a two-stage training strategy with a Generative Adversarial Network and an object detection model. The translation model learns a conversion that preserves the structural detail of visible images while preserving the texture and other characteristics of infrared images. Images so generated are used to train standard object detection frameworks including Yolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating a super-resolution step into our pipeline to further improve model accuracy, and achieve an improvem",
    "link": "https://arxiv.org/abs/2408.01843",
    "context": "Title: Supervised Image Translation from Visible to Infrared Domain for Object Detection\nAbstract: arXiv:2408.01843v1 Announce Type: new  Abstract: This study aims to learn a translation from visible to infrared imagery, bridging the domain gap between the two modalities so as to improve accuracy on downstream tasks including object detection. Previous approaches attempt to perform bi-domain feature fusion through iterative optimization or end-to-end deep convolutional networks. However, we pose the problem as similar to that of image translation, adopting a two-stage training strategy with a Generative Adversarial Network and an object detection model. The translation model learns a conversion that preserves the structural detail of visible images while preserving the texture and other characteristics of infrared images. Images so generated are used to train standard object detection frameworks including Yolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating a super-resolution step into our pipeline to further improve model accuracy, and achieve an improvem",
    "path": "papers/24/08/2408.01843.json",
    "total_tokens": 717,
    "translated_title": "由可见域到红外域的监督图像翻译方法用于物体检测",
    "translated_abstract": "本研究旨在学习一种从可见域到红外域图像的翻译方法，弥合这两种模态之间的领域差距，以提高包括物体检测在内的下游任务 accuracy。以往的方法试图通过迭代优化或端到端的深卷积网络来实现双域特征融合。然而，我们将问题描述为类似于图像翻译，采用两阶段训练策略，结合生成对抗网络（GAN）和物体检测模型。翻译模型在学习保留可见图像的结构细节的同时，保持了红外图像的纹理和其他特征。所生成的图像被用于训练标准物体检测框架，包括Yolov5、Mask和Faster R-CNN。我们还调查了在我们的管道中集成超分辨率步骤的有用性，以进一步改进模型的准确度，并实现显着改善。",
    "tldr": "本文提出了一种由可见域到红外域的监督图像翻译方法，通过将问题描述为图像翻译并采用两阶段训练策略，改进了物体检测等下游任务的准确性。通过集成生成对抗网络和物体检测模型，该方法学习了一种既能保留可见图像的结构细节又能保持红外图像纹理的转换过程。生成的图像被用于训练标准物体检测框架，实现了显著的准确性提升。"
}