{
    "title": "Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation",
    "abstract": "arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.",
    "link": "https://arxiv.org/abs/2408.01732",
    "context": "Title: Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation\nAbstract: arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.",
    "path": "papers/24/08/2408.01732.json",
    "total_tokens": 320,
    "tldr": "该文章提出了一种基于扩散模型的两阶段方法，结合音频信息和预生成的面部关键点，以生成高质量的唇形同步且时间一致的说话头视频。"
}