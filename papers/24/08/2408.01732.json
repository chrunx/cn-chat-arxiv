{
    "title": "Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation",
    "abstract": "arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.",
    "link": "https://arxiv.org/abs/2408.01732",
    "context": "Title: Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation\nAbstract: arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.",
    "path": "papers/24/08/2408.01732.json",
    "total_tokens": 666,
    "translated_title": "基于特征引导的扩散模型用于高保真度和时间一致性说话人头像生成",
    "translated_abstract": "arXiv:2408.01732v1 公告类型：新  翻译摘要：音频驱动的说话人头像生成是一项重要的但具有挑战性的任务，适用于包括虚拟头像、电影制作和在线会议在内的各种领域。然而，现有的基于GAN的方法过分强调生成与语音同步的唇形，而忽略了生成的帧的视觉质量，而基于扩散的方法则过分强调生成高质量的帧，忽视了唇形匹配，导致嘴部运动出现抖动。为了解决上述问题，我们引入了一种两阶段基于扩散的模型。第一阶段涉及根据给定的语音生成同步的面部特征点。在第二阶段，这些生成的特征点作为条件在去噪过程中使用，旨在优化嘴部抖动问题并生成高保真度、同步和在时间上一致的说话人头像视频。广泛的实验证明，我们的模型达到了最佳性能。",
    "tldr": "我们提出了一个两阶段扩散模型，使用音频同步面部特征点，并在此基础上生成高质量、同步且时间一致的说话人头像视频，解决了当前模型在图像质量和唇形同步方面的不足。"
}