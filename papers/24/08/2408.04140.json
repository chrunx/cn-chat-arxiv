{
    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
    "abstract": "arXiv:2408.04140v1 Announce Type: cross  Abstract: Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
    "link": "https://arxiv.org/abs/2408.04140",
    "context": "Title: UNLEARN Efficient Removal of Knowledge in Large Language Models\nAbstract: arXiv:2408.04140v1 Announce Type: cross  Abstract: Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
    "path": "papers/24/08/2408.04140.json",
    "total_tokens": 390,
    "tldr": "该文章提出了一个名为UNLEARN的全新方法，可以在不重新训练大型语言模型的情况下，通过动态遗忘特定知识的方式来移除模型中的敏感或专有知识，同时保持其他知识的性能，该方法在准确性和有效性上都比以往的策略有显著的提升。此外，文章还提出了一种名为LEARN的方法，可以在不损害相似任务表现的前提下，对模型进行特定知识的定向增加，该方法在匹配低维重构方法(LoRA)的准确性方面表现良好。"
}