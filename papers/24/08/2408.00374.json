{
    "title": "Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving",
    "abstract": "arXiv:2408.00374v2 Announce Type: replace-cross  Abstract: Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated ",
    "link": "https://arxiv.org/abs/2408.00374",
    "context": "Title: Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving\nAbstract: arXiv:2408.00374v2 Announce Type: replace-cross  Abstract: Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated ",
    "path": "papers/24/08/2408.00374.json",
    "total_tokens": 635,
    "translated_title": "基于多视图数据融合的 conformal 轨迹预测在合作驾驶中的应用",
    "translated_abstract": "arXiv:2408.00374v2 Announce Type: replace-cross 摘要: 目前关于轨迹预测的研究主要依赖于车载传感器收集的数据。随着连接的快速发展，如车对车（V2V）和车对基础设施（V2I）通信，通过无线网络收集的有价值的信息变得可用。多视图信息的集成有潜力克服仅从单一视角收集数据的内在局限性，如遮挡和有限视野。在本工作中，我们介绍了 V2INet，一个新颖的轨迹预测框架，旨在通过扩展现有单一视图模型来建模多视图数据。与以前的方法不同，我们的模型支持端到端训练，增强了模型的灵活性和性能。此外，预测的多模态轨迹得到了校正，以：",
    "tldr": "V2INet 提出了一个创新的端到端训练框架，用于结合多角度信息进行轨迹预测，以克服单一视角的局限性，提高校正后的多模态轨迹预测的性能。"
}