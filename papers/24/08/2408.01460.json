{
    "title": "LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models",
    "abstract": "arXiv:2408.01460v1 Announce Type: cross  Abstract: The proliferation of large language models (LLMs) requires robust evaluation of their alignment with local values and ethical standards, especially as existing benchmarks often reflect the cultural, legal, and ideological values of their creators. \\textsc{LocalValueBench}, introduced in this paper, is an extensible benchmark designed to assess LLMs' adherence to Australian values, and provides a framework for regulators worldwide to develop their own LLM benchmarks for local value alignment. Employing a novel typology for ethical reasoning and an interrogation approach, we curated comprehensive questions and utilized prompt engineering strategies to probe LLMs' value alignment. Our evaluation criteria quantified deviations from local values, ensuring a rigorous assessment process. Comparative analysis of three commercial LLMs by USA vendors revealed significant insights into their effectiveness and limitations, demonstrating the critic",
    "link": "https://arxiv.org/abs/2408.01460",
    "context": "Title: LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models\nAbstract: arXiv:2408.01460v1 Announce Type: cross  Abstract: The proliferation of large language models (LLMs) requires robust evaluation of their alignment with local values and ethical standards, especially as existing benchmarks often reflect the cultural, legal, and ideological values of their creators. \\textsc{LocalValueBench}, introduced in this paper, is an extensible benchmark designed to assess LLMs' adherence to Australian values, and provides a framework for regulators worldwide to develop their own LLM benchmarks for local value alignment. Employing a novel typology for ethical reasoning and an interrogation approach, we curated comprehensive questions and utilized prompt engineering strategies to probe LLMs' value alignment. Our evaluation criteria quantified deviations from local values, ensuring a rigorous assessment process. Comparative analysis of three commercial LLMs by USA vendors revealed significant insights into their effectiveness and limitations, demonstrating the critic",
    "path": "papers/24/08/2408.01460.json",
    "total_tokens": 421,
    "tldr": "该文章构建了一个名为LocalValueBench的基准测试，用于评估大型语言模型（LLMs）与澳大利亚价值观的一致性和伦理安全问题。这项基准是可扩展的，为全球各地的监管机构提供了一个框架，以便根据本地价值观定制针对LLMs的评估工具。通过一个创新的伦理推理分类法和调查方法，文章收集了大量的问题，并运用了提示工程策略，以便深入探测模型对本地价值观的响应。评估标准量化了模型的价值观偏离度，确保了评估过程的严格性。通过评估来源自美国的三家商业LLMs的表现，研究揭示了这些模型在效能和局限性方面的关键信息。"
}