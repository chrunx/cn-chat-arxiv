{
    "title": "Targeted Visual Prompting for Medical Visual Question Answering",
    "abstract": "arXiv:2408.03043v1 Announce Type: new  Abstract: With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation. However, simple visual errors cast doubt on the actual visual understanding abilities of these models. To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to se",
    "link": "https://arxiv.org/abs/2408.03043",
    "context": "Title: Targeted Visual Prompting for Medical Visual Question Answering\nAbstract: arXiv:2408.03043v1 Announce Type: new  Abstract: With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation. However, simple visual errors cast doubt on the actual visual understanding abilities of these models. To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to se",
    "path": "papers/24/08/2408.03043.json",
    "total_tokens": 316,
    "tldr": "该文章提出了针对性的视觉提示方法，以增强多模态大型语言模型在医学视觉问答任务中对图像的局部理解和整体理解能力，为这些模型提供了区域化问题的回答能力。"
}