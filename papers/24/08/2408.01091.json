{
    "title": "Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions",
    "abstract": "arXiv:2408.01091v2 Announce Type: replace  Abstract: Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. The dataset and code ",
    "link": "https://arxiv.org/abs/2408.01091",
    "context": "Title: Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions\nAbstract: arXiv:2408.01091v2 Announce Type: replace  Abstract: Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. The dataset and code ",
    "path": "papers/24/08/2408.01091.json",
    "total_tokens": 405,
    "tldr": "该文章介绍了一个名为Self-Contradictory Instructions的基准测试，用于评估大型多模态模型（LMMs）在面对自相矛盾的指令时的表现。通过一个自动化数据集创建框架，生成了20,000个带有矛盾的指令样本，包括语言和视觉两种形式。研究结果表明，当前LMMs在识别多模态指令冲突方面表现不佳，论文提出一种名为Cognitive Awakening Prompting的技术来改善这一情况。"
}