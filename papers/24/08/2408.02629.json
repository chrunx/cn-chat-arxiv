{
    "title": "VidGen-1M: A Large-Scale Dataset for Text-to-video Generation",
    "abstract": "arXiv:2408.02629v1 Announce Type: new  Abstract: The quality of video-text pairs fundamentally determines the upper bound of text-to-video models. Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution. The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data. As a result, there is a lack of appropriate training datasets for text-to-video models. To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models. Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency. When used to train the video generation model, this dataset has led to experimental results that surpass those obta",
    "link": "https://arxiv.org/abs/2408.02629",
    "context": "Title: VidGen-1M: A Large-Scale Dataset for Text-to-video Generation\nAbstract: arXiv:2408.02629v1 Announce Type: new  Abstract: The quality of video-text pairs fundamentally determines the upper bound of text-to-video models. Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution. The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data. As a result, there is a lack of appropriate training datasets for text-to-video models. To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models. Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency. When used to train the video generation model, this dataset has led to experimental results that surpass those obta",
    "path": "papers/24/08/2408.02629.json",
    "total_tokens": 360,
    "tldr": "该文章提出了一个名为VidGen-1M的大型文本到视频生成数据集，该数据集通过迭代过程解决了视频-文本对质量不佳、标签质量差、视频质量低和数据分布不平衡等问题，从而为训练文本到视频模型提供了高质量的训练数据。"
}