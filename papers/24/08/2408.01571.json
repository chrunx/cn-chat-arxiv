{
    "title": "Counterfactual Explanations for Medical Image Classification and Regression using Diffusion Autoencoder",
    "abstract": "arXiv:2408.01571v1 Announce Type: new  Abstract: Counterfactual explanations (CEs) aim to enhance the interpretability of machine learning models by illustrating how alterations in input features would affect the resulting predictions. Common CE approaches require an additional model and are typically constrained to binary counterfactuals. In contrast, we propose a novel method that operates directly on the latent space of a generative model, specifically a Diffusion Autoencoder (DAE). This approach offers inherent interpretability by enabling the generation of CEs and the continuous visualization of the model's internal representation across decision boundaries.   Our method leverages the DAE's ability to encode images into a semantically rich latent space in an unsupervised manner, eliminating the need for labeled data or separate feature extraction models. We show that these latent representations are helpful for medical condition classification and the ordinal regression of severit",
    "link": "https://arxiv.org/abs/2408.01571",
    "context": "Title: Counterfactual Explanations for Medical Image Classification and Regression using Diffusion Autoencoder\nAbstract: arXiv:2408.01571v1 Announce Type: new  Abstract: Counterfactual explanations (CEs) aim to enhance the interpretability of machine learning models by illustrating how alterations in input features would affect the resulting predictions. Common CE approaches require an additional model and are typically constrained to binary counterfactuals. In contrast, we propose a novel method that operates directly on the latent space of a generative model, specifically a Diffusion Autoencoder (DAE). This approach offers inherent interpretability by enabling the generation of CEs and the continuous visualization of the model's internal representation across decision boundaries.   Our method leverages the DAE's ability to encode images into a semantically rich latent space in an unsupervised manner, eliminating the need for labeled data or separate feature extraction models. We show that these latent representations are helpful for medical condition classification and the ordinal regression of severit",
    "path": "papers/24/08/2408.01571.json",
    "total_tokens": 385,
    "tldr": "该文章介绍了使用扩散自编码器（DAE）在医学图像分类和回归中生成有效和可解释的假设解释方法，无需额外模型，通过直接在自编码器的特征空间中操作，该方法极大地提高了模型解释性，并且可以连续地可视化在不同决策边界上的内部表示。通过这种创新方法，文章展示了在无需依赖标签数据或单独的特性提取模型的情况下，如何利用DAE的能力，将图像编码为富有语义的潜在空间，从而增强模型的解释能力。"
}