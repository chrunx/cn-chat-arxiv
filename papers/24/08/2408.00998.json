{
    "title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation",
    "abstract": "arXiv:2408.00998v1 Announce Type: new  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing extraordinary image generation based on natural-language text prompts. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation, for which attention has been focused on leveraging a reference image to control text-to-image synthesis. Due to the close correlation between the reference image and the generated image, this problem can also be regarded as the task of manipulating (or editing) the reference image as per the text, namely text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts the pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven",
    "link": "https://arxiv.org/abs/2408.00998",
    "context": "Title: FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation\nAbstract: arXiv:2408.00998v1 Announce Type: new  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing extraordinary image generation based on natural-language text prompts. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation, for which attention has been focused on leveraging a reference image to control text-to-image synthesis. Due to the close correlation between the reference image and the generated image, this problem can also be regarded as the task of manipulating (or editing) the reference image as per the text, namely text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts the pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven",
    "path": "papers/24/08/2408.00998.json",
    "total_tokens": 680,
    "translated_title": "FBSDiff: 插拔式频率带替代式扩散特征变换的高可控制文本驱动图像翻译",
    "translated_abstract": "arXiv:2408.00998v1 公告类型: 新  摘要: 大规模文本到图像 diffusion 模型在生成人工智能和多模态技术的发展中是一个革命性的里程碑，它允许根据自然语言文本提示生成非凡的图像。然而，这类模型缺乏可控性的问题限制了它们在现实生活中的实际应用，因为注意力已经集中在利用参考图像来控制文本到图像的合成。由于参考图像与生成的图像之间的密切关系，这个问题也可以被看作是根据文本操纵（或者编辑）参考图像的任务，即文本驱动的图像到图像翻译。本文贡献了一种新颖、简练、高效的方法，该方法将预训练的大型文本到图像（T2I）扩散模型调整到图像到图像（I2I）范式中，实现高质量和多变的文本驱动图像编辑。",
    "tldr": "本文提出了一种将预训练的文本到图像扩散模型高效地转换为图像到图像编辑工具的新型方法，使得用户可以根据文本描述灵活地控制图像内容。"
}