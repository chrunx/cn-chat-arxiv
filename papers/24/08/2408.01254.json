{
    "title": "TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks -- Part I: Dataflow and Analytical Modelling",
    "abstract": "arXiv:2408.01254v1 Announce Type: new  Abstract: In order to follow the ever-growing computational complexity and data intensity of state-of-the-art AI models, new computing paradigms are being proposed. These paradigms aim at achieving high energy efficiency, by mitigating the Von Neumann bottleneck that relates to the energy cost of moving data between the processing cores and the memory. Convolutional Neural Networks (CNNs) are particularly susceptible to this bottleneck, given the massive data they have to manage. Systolic Arrays (SAs) are promising architectures to mitigate the data transmission cost, thanks to high data utilization carried out by an array of Processing Elements (PEs). These PEs continuously exchange and process data locally based on specific dataflows (like weight stationary and row stationary), in turn reducing the number of memory accesses to the main memory. The hardware specialization of SAs can meet different workloads, ranging from matrix multiplications to",
    "link": "https://arxiv.org/abs/2408.01254",
    "context": "Title: TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks -- Part I: Dataflow and Analytical Modelling\nAbstract: arXiv:2408.01254v1 Announce Type: new  Abstract: In order to follow the ever-growing computational complexity and data intensity of state-of-the-art AI models, new computing paradigms are being proposed. These paradigms aim at achieving high energy efficiency, by mitigating the Von Neumann bottleneck that relates to the energy cost of moving data between the processing cores and the memory. Convolutional Neural Networks (CNNs) are particularly susceptible to this bottleneck, given the massive data they have to manage. Systolic Arrays (SAs) are promising architectures to mitigate the data transmission cost, thanks to high data utilization carried out by an array of Processing Elements (PEs). These PEs continuously exchange and process data locally based on specific dataflows (like weight stationary and row stationary), in turn reducing the number of memory accesses to the main memory. The hardware specialization of SAs can meet different workloads, ranging from matrix multiplications to",
    "path": "papers/24/08/2408.01254.json",
    "total_tokens": 698,
    "translated_title": "TrIM: 三角形输入运动同步阵列用于卷积神经网络 -- 第一部分：数据流和分析建模",
    "translated_abstract": "arXiv:2408.01254v1 公告类型：新定标题：为了跟上高级AI模型日益复杂的计算需求和数据强度，新的计算范式正在被提出。这些范式旨在通过减少从处理核心到内存的数据传输成本来实现高能效，从而减轻冯诺依曼瓶颈，这是数据传输成本与处理核心到内存能耗相关的一种现象。卷积神经网络（CNNs）特别容易受到这种瓶颈的影响，因为它们需要管理大量数据。同步阵列（SA）是减轻数据传输成本的潜在架构，因为它们能够通过特定的数据流（如权重固定和排固定）减少对主内存的访问次数。这些PEs不断在特定数据流下交换和处理数据，从而减少了对大规模内存的访问次数。SA的硬件特殊化能够适应不同的工作负载，从矩阵乘法到其他类型的计算任务。",
    "tldr": "TrIM 三角形输入运动同步阵列提出了一种新的架构，旨在通过特定数据流减少对大规模内存的访问次数，为卷积神经网络的复杂计算提供更高能效。"
}