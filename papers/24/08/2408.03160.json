{
    "title": "User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance",
    "abstract": "arXiv:2408.03160v1 Announce Type: new  Abstract: Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches -- Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we",
    "link": "https://arxiv.org/abs/2408.03160",
    "context": "Title: User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance\nAbstract: arXiv:2408.03160v1 Announce Type: new  Abstract: Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches -- Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we",
    "path": "papers/24/08/2408.03160.json",
    "total_tokens": 456,
    "tldr": "该文章研究了由大型语言模型（LLMs）驱动的现代多模态推理模型在辅助视觉辅助设备完成多步骤日常活动方面的能力。我们通过在线视频数据集中的动作预测任务，对两种不同类型的多模态LLM方法——Socratic模型和视觉条件语言模型（VCLM）进行了基准测试，以评估其将视觉历史编码化和在中长期预测动作的能力。然而，这些在线视频数据集仅仅允许我们评估辅助设备的前两个能力，并不能评估在用户参与下重新规划的能力。因此，我们设计了一个用户参与式的评估框架，为VCLM提供了实时用户交互的补充评估方式，从而评估了VCLM在中长期预测和动态行动规划方面的实际表现。"
}