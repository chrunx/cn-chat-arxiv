{
    "title": "InPer: Whole-Process Domain Generalization via Causal Intervention and Perturbation",
    "abstract": "arXiv:2408.03608v1 Announce Type: cross  Abstract: Despite the considerable advancements achieved by deep neural networks, their performance tends to degenerate when the test environment diverges from the training ones. Domain generalization (DG) solves this issue by learning representations independent of domain-related information, thus facilitating extrapolation to unseen environments. Existing approaches typically focus on formulating tailored training objectives to extract shared features from the source data. However, the disjointed training and testing procedures may compromise robustness, particularly in the face of unforeseen variations during deployment. In this paper, we propose a novel and holistic framework based on causality, named InPer, designed to enhance model generalization by incorporating causal intervention during training and causal perturbation during testing. Specifically, during the training phase, we employ entropy-based causal intervention (EnIn) to refine t",
    "link": "https://arxiv.org/abs/2408.03608",
    "context": "Title: InPer: Whole-Process Domain Generalization via Causal Intervention and Perturbation\nAbstract: arXiv:2408.03608v1 Announce Type: cross  Abstract: Despite the considerable advancements achieved by deep neural networks, their performance tends to degenerate when the test environment diverges from the training ones. Domain generalization (DG) solves this issue by learning representations independent of domain-related information, thus facilitating extrapolation to unseen environments. Existing approaches typically focus on formulating tailored training objectives to extract shared features from the source data. However, the disjointed training and testing procedures may compromise robustness, particularly in the face of unforeseen variations during deployment. In this paper, we propose a novel and holistic framework based on causality, named InPer, designed to enhance model generalization by incorporating causal intervention during training and causal perturbation during testing. Specifically, during the training phase, we employ entropy-based causal intervention (EnIn) to refine t",
    "path": "papers/24/08/2408.03608.json",
    "total_tokens": 327,
    "tldr": "该文章提出了一种名为InPer的框架，通过引入因果干预和测试时的因果扰动，旨在通过将因果理论应用于神经网络训练，以提升模型在未知环境中的表现和对外部变量的抵抗能力，从而解决深度学习模型在不同数据源之间推广问题。"
}