{
    "title": "Enhancing AI-based Generation of Software Exploits with Contextual Information",
    "abstract": "arXiv:2408.02402v1 Announce Type: cross  Abstract: This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability t",
    "link": "https://arxiv.org/abs/2408.02402",
    "context": "Title: Enhancing AI-based Generation of Software Exploits with Contextual Information\nAbstract: arXiv:2408.02402v1 Announce Type: cross  Abstract: This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability t",
    "path": "papers/24/08/2408.02402.json",
    "total_tokens": 354,
    "tldr": "该文章通过使用包含实际shellcodes的数据集，评估了神经机器翻译模型在从自然语言描述生成恶意软件代码方面的性能。研究发现，模型的表现因引入的上下文信息不同而有所差异，表明了适当上下文对提升模型性能的重要性。然而，过度增加上下文信息并未带来持续的改进，这表明在模型训练中存在一个最佳的上下文信息量。此外，模型还能识别并排除不必要的描述，从而更精确地生成代码。"
}