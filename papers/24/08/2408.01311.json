{
    "title": "TopoNAS: Boosting Search Efficiency of Gradient-based NAS via Topological Simplification",
    "abstract": "arXiv:2408.01311v1 Announce Type: new  Abstract: Improving search efficiency serves as one of the crucial objectives of Neural Architecture Search (NAS). However, many current approaches ignore the universality of the search strategy and fail to reduce the computational redundancy during the search process, especially in one-shot NAS architectures. Besides, current NAS methods show invalid reparameterization in non-linear search space, leading to poor efficiency in common search spaces like DARTS. In this paper, we propose TopoNAS, a model-agnostic approach for gradient-based one-shot NAS that significantly reduces searching time and memory usage by topological simplification of searchable paths. Firstly, we model the non-linearity in search spaces to reveal the parameterization difficulties. To improve the search efficiency, we present a topological simplification method and iteratively apply module-sharing strategies to simplify the topological structure of searchable paths. In addit",
    "link": "https://arxiv.org/abs/2408.01311",
    "context": "Title: TopoNAS: Boosting Search Efficiency of Gradient-based NAS via Topological Simplification\nAbstract: arXiv:2408.01311v1 Announce Type: new  Abstract: Improving search efficiency serves as one of the crucial objectives of Neural Architecture Search (NAS). However, many current approaches ignore the universality of the search strategy and fail to reduce the computational redundancy during the search process, especially in one-shot NAS architectures. Besides, current NAS methods show invalid reparameterization in non-linear search space, leading to poor efficiency in common search spaces like DARTS. In this paper, we propose TopoNAS, a model-agnostic approach for gradient-based one-shot NAS that significantly reduces searching time and memory usage by topological simplification of searchable paths. Firstly, we model the non-linearity in search spaces to reveal the parameterization difficulties. To improve the search efficiency, we present a topological simplification method and iteratively apply module-sharing strategies to simplify the topological structure of searchable paths. In addit",
    "path": "papers/24/08/2408.01311.json",
    "total_tokens": 614,
    "translated_title": "TopoNAS: 通过拓扑简化提高梯度NAS搜索效率",
    "translated_abstract": "arXiv:2408.01311v1 公告类型: 新闻  摘要: Neural Architecture Search (NAS)的搜索效率是目前的一个重要目标。然而，许多现有的方法忽视了搜索策略的通用性，并且在搜索过程中未能减少计算中的冗余，尤其是在一次性NAS架构中。此外，当前的NAS方法在像DARTS这样的常用搜索空间中显示出无效的重新参数化，导致搜索效率低下。在本论文中，我们提出TopoNAS，这是一种不依赖于模型的梯度一次性NAS方法，通过简化搜索路径的拓扑结构，可以显著减少搜索时间和内存使用。首先，我们用模型对搜索空间的非线性进行建模，以揭示参数化的困难。为了提高搜索效率，我们提出了一个拓扑简化方法，并迭代地应用模块共享策略来简化搜索路径的拓扑结构。在添加",
    "tldr": "TopoNAS通过优化搜索策略和简化路径结构，为神经架构搜索提供了一种高效的搜索方法。"
}