{
    "title": "Scalable Signal Temporal Logic Guided Reinforcement Learning via Value Function Space Optimization",
    "abstract": "arXiv:2408.01923v1 Announce Type: new  Abstract: The integration of reinforcement learning (RL) and formal methods has emerged as a promising framework for solving long-horizon planning problems. Conventional approaches typically involve abstraction of the state and action spaces and manually created labeling functions or predicates. However, the efficiency of these approaches deteriorates as the tasks become increasingly complex, which results in exponential growth in the size of labeling functions or predicates. To address these issues, we propose a scalable model-based RL framework, called VFSTL, which schedules pre-trained skills to follow unseen STL specifications without using hand-crafted predicates. Given a set of value functions obtained by goal-conditioned RL, we formulate an optimization problem to maximize the robustness value of Signal Temporal Logic (STL) defined specifications, which is computed using value functions as predicates. To further reduce the computation burde",
    "link": "https://arxiv.org/abs/2408.01923",
    "context": "Title: Scalable Signal Temporal Logic Guided Reinforcement Learning via Value Function Space Optimization\nAbstract: arXiv:2408.01923v1 Announce Type: new  Abstract: The integration of reinforcement learning (RL) and formal methods has emerged as a promising framework for solving long-horizon planning problems. Conventional approaches typically involve abstraction of the state and action spaces and manually created labeling functions or predicates. However, the efficiency of these approaches deteriorates as the tasks become increasingly complex, which results in exponential growth in the size of labeling functions or predicates. To address these issues, we propose a scalable model-based RL framework, called VFSTL, which schedules pre-trained skills to follow unseen STL specifications without using hand-crafted predicates. Given a set of value functions obtained by goal-conditioned RL, we formulate an optimization problem to maximize the robustness value of Signal Temporal Logic (STL) defined specifications, which is computed using value functions as predicates. To further reduce the computation burde",
    "path": "papers/24/08/2408.01923.json",
    "total_tokens": 357,
    "tldr": "该文章提出了一种名为VFSTL的模型驱动强化学习框架，该框架通过优化价值函数空间来提高Signal Temporal Logic（STL）定义的规格的鲁棒性价值，而无需使用手编制的谓词。此方法通过考虑预先训练的技能并根据未见的STL规格对其进行优先级安排，有效地解决了长时间计划问题。"
}