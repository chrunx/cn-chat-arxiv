{
    "title": "Scalable Signal Temporal Logic Guided Reinforcement Learning via Value Function Space Optimization",
    "abstract": "arXiv:2408.01923v1 Announce Type: new  Abstract: The integration of reinforcement learning (RL) and formal methods has emerged as a promising framework for solving long-horizon planning problems. Conventional approaches typically involve abstraction of the state and action spaces and manually created labeling functions or predicates. However, the efficiency of these approaches deteriorates as the tasks become increasingly complex, which results in exponential growth in the size of labeling functions or predicates. To address these issues, we propose a scalable model-based RL framework, called VFSTL, which schedules pre-trained skills to follow unseen STL specifications without using hand-crafted predicates. Given a set of value functions obtained by goal-conditioned RL, we formulate an optimization problem to maximize the robustness value of Signal Temporal Logic (STL) defined specifications, which is computed using value functions as predicates. To further reduce the computation burde",
    "link": "https://arxiv.org/abs/2408.01923",
    "context": "Title: Scalable Signal Temporal Logic Guided Reinforcement Learning via Value Function Space Optimization\nAbstract: arXiv:2408.01923v1 Announce Type: new  Abstract: The integration of reinforcement learning (RL) and formal methods has emerged as a promising framework for solving long-horizon planning problems. Conventional approaches typically involve abstraction of the state and action spaces and manually created labeling functions or predicates. However, the efficiency of these approaches deteriorates as the tasks become increasingly complex, which results in exponential growth in the size of labeling functions or predicates. To address these issues, we propose a scalable model-based RL framework, called VFSTL, which schedules pre-trained skills to follow unseen STL specifications without using hand-crafted predicates. Given a set of value functions obtained by goal-conditioned RL, we formulate an optimization problem to maximize the robustness value of Signal Temporal Logic (STL) defined specifications, which is computed using value functions as predicates. To further reduce the computation burde",
    "path": "papers/24/08/2408.01923.json",
    "total_tokens": 823,
    "translated_title": "基于信号时空逻辑的可扩展强化学习范式：价值函数空间优化方法",
    "translated_abstract": "arXiv:2408.01923v1 公告类型：新发布 摘要：强化学习（RL）与形式方法相结合的方法已经显示出解决长期计划问题的新思路。传统的方法通常涉及到状态和动作空间的抽象以及手动创建的标注函数或谓词。然而，随着任务变得越来越复杂，这些方法的效率会下降，导致所需的手动谓词数量呈指数增长。为了解决这些问题，我们提出了一种可扩展的模型-基础RL框架，称为VFSTL，该框架能够无需使用手动创建的谓词，通过调度预训练的技能来遵循未知的STL规范。",
    "tldr": "本文提出了一种新的可扩展强化学习框架VFSTL，通过优化价值函数来提高信号时空逻辑（STL）规范的满足度，无需手动谓词，并且能够处理复杂任务的长期规划问题。"
}