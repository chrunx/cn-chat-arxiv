{
    "title": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories",
    "abstract": "arXiv:2408.03519v1 Announce Type: cross  Abstract: With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes the evaluation poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mu",
    "link": "https://arxiv.org/abs/2408.03519",
    "context": "Title: RepoMasterEval: Evaluating Code Completion via Real-World Repositories\nAbstract: arXiv:2408.03519v1 Announce Type: cross  Abstract: With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes the evaluation poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mu",
    "path": "papers/24/08/2408.03519.json",
    "total_tokens": 335,
    "tldr": "该文章提出RepoMasterEval，一个基于真实世界Python和TypeScript仓库的代码完成模型评估基准。每个数据点都是通过从源代码文件中加密一个代码段（真值），并用现有的测试套件来改善模型生成的代码测试的准确性。"
}