{
    "title": "FE-Adapter: Adapting Image-based Emotion Classifiers to Videos",
    "abstract": "arXiv:2408.02421v1 Announce Type: new  Abstract: Utilizing large pre-trained models for specific tasks has yielded impressive results. However, fully fine-tuning these increasingly large models is becoming prohibitively resource-intensive. This has led to a focus on more parameter-efficient transfer learning, primarily within the same modality. But this approach has limitations, particularly in video understanding where suitable pre-trained models are less common. Addressing this, our study introduces a novel cross-modality transfer learning approach from images to videos, which we call parameter-efficient image-to-video transfer learning. We present the Facial-Emotion Adapter (FE-Adapter), designed for efficient fine-tuning in video tasks. This adapter allows pre-trained image models, which traditionally lack temporal processing capabilities, to analyze dynamic video content efficiently. Notably, it uses about 15 times fewer parameters than previous methods, while improving accuracy. ",
    "link": "https://arxiv.org/abs/2408.02421",
    "context": "Title: FE-Adapter: Adapting Image-based Emotion Classifiers to Videos\nAbstract: arXiv:2408.02421v1 Announce Type: new  Abstract: Utilizing large pre-trained models for specific tasks has yielded impressive results. However, fully fine-tuning these increasingly large models is becoming prohibitively resource-intensive. This has led to a focus on more parameter-efficient transfer learning, primarily within the same modality. But this approach has limitations, particularly in video understanding where suitable pre-trained models are less common. Addressing this, our study introduces a novel cross-modality transfer learning approach from images to videos, which we call parameter-efficient image-to-video transfer learning. We present the Facial-Emotion Adapter (FE-Adapter), designed for efficient fine-tuning in video tasks. This adapter allows pre-trained image models, which traditionally lack temporal processing capabilities, to analyze dynamic video content efficiently. Notably, it uses about 15 times fewer parameters than previous methods, while improving accuracy. ",
    "path": "papers/24/08/2408.02421.json",
    "total_tokens": 334,
    "tldr": "该文章提出了一种名为FE-Adapter的模型，它采用了一种创新的跨模态转移学习方法，将源自图像的情感分类器高效地适配到视频中，显著降低了所需的参数数量，同时提高了准确性。"
}