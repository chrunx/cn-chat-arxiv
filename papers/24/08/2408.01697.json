{
    "title": "Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization",
    "abstract": "arXiv:2408.01697v1 Announce Type: cross  Abstract: Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enha",
    "link": "https://arxiv.org/abs/2408.01697",
    "context": "Title: Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization\nAbstract: arXiv:2408.01697v1 Announce Type: cross  Abstract: Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enha",
    "path": "papers/24/08/2408.01697.json",
    "total_tokens": 815,
    "translated_title": "不变图学习与信息瓶颈用于过分布外泛化",
    "translated_abstract": "arXiv:2408.01697v1 公告类型：交叉摘要：图过分布外（OOD）泛化仍然是图学习中的主要挑战，因为图神经网络（GNNs）在分布偏移情况下常常遭受严重的表现度下降。不变学习，旨在提取跨多种分布的恒定特征，在OOD泛化问题上最近取得了一定成功。尽管不变学习在欧几里得数据（例如图像）上的OOD问题中取得了一些成功，但对于图数据的研究仍然受到了图数据复杂性的限制。现有的一些研究，如数据增强或因果干预，在处理图的过程中经常破坏了不变性，或者由于缺乏对因果部分的监督信号而存在可靠性问题。本工作中，我们提出了一个称为基于信息瓶颈理论的不变图学习（InfoIGL）的框架，旨在提取图的恒定特征并增强其分布外泛化能力。我们的框架通过信息瓶颈理论在数据与特征之间建立了有效的信息流，确保了不变性的提取和最优特征表示的学习。通过在多个图数据集上的实验，我们证明了InfoIGL能够有效地提高GNN在OOD数据上的性能，同时保持了对原始分布数据的良好泛化能力。 This framework not only ensures the effective extraction of invariant features but also optimizes the feature representations for graph learning, which is crucial for OOD generalization. Multiple experiments on various graph datasets demonstrate that InfoIGL significantly enhances the performance of GNNs in OOD tasks while maintaining good generalization abilities for the original distribution data.",
    "tldr": "该论文提出了一种基于信息瓶颈理论的不变图学习方法，旨在提高图神经网络在分布外数据上的性能。通过实验证明，该方法能有效地提取不变特征，优化图学习中的特征表示，同时保持对原分布数据的良好泛化能力。"
}