{
    "title": "Self-Taught Evaluators",
    "abstract": "arXiv:2408.02666v1 Announce Type: cross  Abstract: Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM jud",
    "link": "https://arxiv.org/abs/2408.02666",
    "context": "Title: Self-Taught Evaluators\nAbstract: arXiv:2408.02666v1 Announce Type: cross  Abstract: Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM jud",
    "path": "papers/24/08/2408.02666.json",
    "total_tokens": 443,
    "tldr": "该文章提出了一种不依赖人类注释，仅使用合成数据训练改进模型评价者的方法。通过对未标注的指令进行迭代式的自我改进，该方法能够生成对比性的模型输出，并通过训练LLM作为评判者来生成推理轨迹和最终判断结果。在无任何偏好标注数据的情况下，通过这一过程改进的“自我学习评估者”能够显著提高一个强大语言模型（如Llama3-70B-Instruct）在RewardBench上的表现，从75.4提升至88.3（使用多数投票时可达88.7），这超过了常用语言模型评价者的性能。"
}