{
    "title": "Self-Taught Evaluators",
    "abstract": "arXiv:2408.02666v1 Announce Type: cross  Abstract: Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM jud",
    "link": "https://arxiv.org/abs/2408.02666",
    "context": "Title: Self-Taught Evaluators\nAbstract: arXiv:2408.02666v1 Announce Type: cross  Abstract: Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM jud",
    "path": "papers/24/08/2408.02666.json",
    "total_tokens": 760,
    "translated_title": "自教裁判",
    "translated_abstract": "arXiv:2408.02666v1 公告类型：交叉 摘要：基于模型的评估是成功模型开发的核心——作为训练的奖励模型，以及作为人类评估的替代品。为了训练这样的裁判，传统的做法是收集大量人类对模型响应的偏好判断，这既耗时又耗力，因为随着时间的推移，数据也会变得过时。在这个工作中，我们提出了一种旨在不使用人类注释改善裁判的策略，仅使用合成训练数据。从未标注的指令开始，我们的迭代自改进方案生成对比性的模型输出，并训练一个基于LLM的裁判，以便生成推理轨迹和最终判断，在每个新迭代中使用改进的预测进行重复训练。我们的自教裁判不需要任何标注的偏好数据，就可以将一个强大的LLM（Llama3-70B-Instruct）的性能从75.4提高到88.3（加入多数投票，则为88.7）在RewardBench上。这超过了常用LLM评判工具的表现，有潜力节省成本并应对模型不断进步所带来的挑战。",
    "tldr": "本文提出了一种使用合成数据自教方法，无需人类注释，大幅度提升了Llama3-70B-Instruct模型的性能，并在RewardBench上取得了显著的改进。"
}