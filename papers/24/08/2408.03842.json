{
    "title": "Bi-Level Spatial and Channel-aware Transformer for Learned Image Compression",
    "abstract": "arXiv:2408.03842v1 Announce Type: new  Abstract: Recent advancements in learned image compression (LIC) methods have demonstrated superior performance over traditional hand-crafted codecs. These learning-based methods often employ convolutional neural networks (CNNs) or Transformer-based architectures. However, these nonlinear approaches frequently overlook the frequency characteristics of images, which limits their compression efficiency. To address this issue, we propose a novel Transformer-based image compression method that enhances the transformation stage by considering frequency components within the feature map. Our method integrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB), where a spatial-based branch independently handles high and low frequencies at the attention layer, and a Channel-aware Self-Attention (CaSA) module captures information across channels, significantly improving compression performance. Additionally, we introduce a Mixed Local-Glob",
    "link": "https://arxiv.org/abs/2408.03842",
    "context": "Title: Bi-Level Spatial and Channel-aware Transformer for Learned Image Compression\nAbstract: arXiv:2408.03842v1 Announce Type: new  Abstract: Recent advancements in learned image compression (LIC) methods have demonstrated superior performance over traditional hand-crafted codecs. These learning-based methods often employ convolutional neural networks (CNNs) or Transformer-based architectures. However, these nonlinear approaches frequently overlook the frequency characteristics of images, which limits their compression efficiency. To address this issue, we propose a novel Transformer-based image compression method that enhances the transformation stage by considering frequency components within the feature map. Our method integrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB), where a spatial-based branch independently handles high and low frequencies at the attention layer, and a Channel-aware Self-Attention (CaSA) module captures information across channels, significantly improving compression performance. Additionally, we introduce a Mixed Local-Glob",
    "path": "papers/24/08/2408.03842.json",
    "total_tokens": 347,
    "tldr": "该文章提出了一种新型的双层级时空与通道感知变压器网络，用于学习的图像压缩，它通过引入一种新型的时空-通道注意力变压器块（HSCATB）和通道感知自注意力（CaSA）模块，能够在处理图像时更加敏感地关注频率信息，同时在通道间进行信息整合，显著提高了图像压缩的性能。"
}