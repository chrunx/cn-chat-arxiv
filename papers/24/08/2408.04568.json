{
    "title": "Learning Fine-Grained Grounded Citations for Attributed Large Language Models",
    "abstract": "arXiv:2408.04568v1 Announce Type: cross  Abstract: Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. Attributed LLMs, which augment generated text with in-line citations, have shown potential in mitigating hallucinations and improving verifiability. However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning. Furthermore, the practice of citing only coarse document identifiers makes it challenging for users to perform fine-grained verification. In this work, we introduce FRONT, a training framework designed to teach LLMs to generate Fine-Grained Grounded Citations. By grounding model outputs in fine-grained supporting quotes, these quotes guide the generation of grounded and consistent responses, not only improving citation quality but also facilitating fine-grained verification. Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in gener",
    "link": "https://arxiv.org/abs/2408.04568",
    "context": "Title: Learning Fine-Grained Grounded Citations for Attributed Large Language Models\nAbstract: arXiv:2408.04568v1 Announce Type: cross  Abstract: Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. Attributed LLMs, which augment generated text with in-line citations, have shown potential in mitigating hallucinations and improving verifiability. However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning. Furthermore, the practice of citing only coarse document identifiers makes it challenging for users to perform fine-grained verification. In this work, we introduce FRONT, a training framework designed to teach LLMs to generate Fine-Grained Grounded Citations. By grounding model outputs in fine-grained supporting quotes, these quotes guide the generation of grounded and consistent responses, not only improving citation quality but also facilitating fine-grained verification. Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in gener",
    "path": "papers/24/08/2408.04568.json",
    "total_tokens": 320,
    "tldr": "该文章提出了一种名为FRONT的训练框架，旨在教导大型语言模型生成具有精细粒度支撑引用的引用，以提升生成内容的可靠性并促进精细验证。"
}