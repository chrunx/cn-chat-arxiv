{
    "title": "Understanding Deep Learning via Notions of Rank",
    "abstract": "arXiv:2408.02111v1 Announce Type: cross  Abstract: Despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. This thesis puts forth notions of rank as key for developing a theory of deep learning, focusing on the fundamental aspects of generalization and expressiveness. In particular, we establish that gradient-based training can induce an implicit regularization towards low rank for several neural network architectures, and demonstrate empirically that this phenomenon may facilitate an explanation of generalization over natural data (e.g., audio, images, and text). Then, we characterize the ability of graph neural networks to model interactions via a notion of rank, which is commonly used for quantifying entanglement in quantum physics. A central tool underlying these results is a connection between neural networks and tensor factorizations. Practical implications of our theory for designing explicit regularization schemes and data p",
    "link": "https://arxiv.org/abs/2408.02111",
    "context": "Title: Understanding Deep Learning via Notions of Rank\nAbstract: arXiv:2408.02111v1 Announce Type: cross  Abstract: Despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. This thesis puts forth notions of rank as key for developing a theory of deep learning, focusing on the fundamental aspects of generalization and expressiveness. In particular, we establish that gradient-based training can induce an implicit regularization towards low rank for several neural network architectures, and demonstrate empirically that this phenomenon may facilitate an explanation of generalization over natural data (e.g., audio, images, and text). Then, we characterize the ability of graph neural networks to model interactions via a notion of rank, which is commonly used for quantifying entanglement in quantum physics. A central tool underlying these results is a connection between neural networks and tensor factorizations. Practical implications of our theory for designing explicit regularization schemes and data p",
    "path": "papers/24/08/2408.02111.json",
    "total_tokens": 359,
    "tldr": "该文章提出了一种通过“秩”概念理解深度学习的理论框架，揭示了梯度下降训练在多个神经网络架构中倾向于导致低秩的隐式正则化现象，并对自然数据的泛化能力提出了可能的解释。此外，研究还利用秩的概念分析了图神经网络在模拟交互作用方面的能力，并通过神经网络与张量分解之间的联系作为主要工具，为设计具体的正则化策略和优化数据集预处理提供了指导。"
}