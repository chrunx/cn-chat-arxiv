{
    "title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation",
    "abstract": "arXiv:2408.00863v1 Announce Type: cross  Abstract: The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive",
    "link": "https://arxiv.org/abs/2408.00863",
    "context": "Title: UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation\nAbstract: arXiv:2408.00863v1 Announce Type: cross  Abstract: The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive",
    "path": "papers/24/08/2408.00863.json",
    "total_tokens": 699,
    "translated_title": "UniMoT：具有离散令牌表示的统一分子-文本语言模型",
    "translated_abstract": "arXiv:2408.00863v1 Announce Type: cross  摘要：大型语言模型（LLMs）在各种任务中的显著成功推动研究社区扩展其能力到分子应用。然而，大多数分子LLMs使用基于适配器的架构，这些架构不平等地对待分子和文本模态，缺乏对分子模态的监督信号。为了解决这些问题，我们介绍了UniMoT，一个统一分子-文本LLM，采用基于tokenizer的架构，将分子令牌扩展到LLM的词汇中。具体来说，我们引入了一个基于向量量化（Vector Quantization）的tokenizer，其采用Q-Former来弥合分子与文本模态之间的差距。该tokenizer将分子转换为具有因果依赖关系的分子令牌序列，封装了分子和文本的高层信息。装备了这个tokenizer，UniMoT可以将分子和文本模态统一到共享令牌表示和自回归模型中，从而在分子和文本之间实现真正的集成。",
    "tldr": "UniMoT是一种统一的分子-文本语言模型，通过使用基于向量量化的tokenizer，它在自回归模型中将分子转换成序列的分子令牌，实现分子与文本的高效集成。",
    "en_tdlr": "UniMoT is a unified molecular-text language model that unifies molecule and text modalities under a shared token representation and an autoregressive model by converting molecules into molecule tokens with causal dependency through a vector quantization-driven tokenizer, effectively integrating the two domains."
}