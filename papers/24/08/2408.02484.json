{
    "title": "Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection",
    "abstract": "arXiv:2408.02484v1 Announce Type: new  Abstract: Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier topic due to its capability to detect HOIs beyond a predefined set of categories. This task entails not only identifying the interactiveness of human-object pairs and localizing them but also recognizing both seen and unseen interaction categories. In this paper, we introduce a novel framework for zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP. This approach enhances the generalization of large foundation models, such as CLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning methods, we propose learning decoupled vision and language prompts for interactiveness-aware visual feature extraction and generalizable interaction classification, respectively. Specifically, we integrate prior knowledge of different granularity into conditional vision prompts, including an input-conditioned instance prior and a global spatia",
    "link": "https://arxiv.org/abs/2408.02484",
    "context": "Title: Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection\nAbstract: arXiv:2408.02484v1 Announce Type: new  Abstract: Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier topic due to its capability to detect HOIs beyond a predefined set of categories. This task entails not only identifying the interactiveness of human-object pairs and localizing them but also recognizing both seen and unseen interaction categories. In this paper, we introduce a novel framework for zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP. This approach enhances the generalization of large foundation models, such as CLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning methods, we propose learning decoupled vision and language prompts for interactiveness-aware visual feature extraction and generalizable interaction classification, respectively. Specifically, we integrate prior knowledge of different granularity into conditional vision prompts, including an input-conditioned instance prior and a global spatia",
    "path": "papers/24/08/2408.02484.json",
    "total_tokens": 401,
    "tldr": "该文章提出了一种名为CMMP的零 shot 人类对象交互（HOI）检测框架，该框架利用条件多模态提示来增强大型基础模型如CLIP在HOI检测中的泛化能力。这种方法通过学习独立的视觉和语言提示来提高泛化性，其中视觉提示用于交互性感知下的视觉特征提取，而语言提示用于可泛化的互动分类。此外，该文章还提出学习不同层次的前置知识，将其融入条件视觉提示中，从而提高对不可见交互类别的预测精度和泛化能力。"
}