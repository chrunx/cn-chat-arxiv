{
    "title": "Language Model Can Listen While Speaking",
    "abstract": "arXiv:2408.02622v1 Announce Type: cross  Abstract: Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio",
    "link": "https://arxiv.org/abs/2408.02622",
    "context": "Title: Language Model Can Listen While Speaking\nAbstract: arXiv:2408.02622v1 Announce Type: cross  Abstract: Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio",
    "path": "papers/24/08/2408.02622.json",
    "total_tokens": 365,
    "tldr": "该文章提出了一种新型的对话系统模型，称为“听而说的语言模型（LSLM）”，该模型能够在与人类实时交互的过程中既能说话又能听。这种模型设计采用了端到端的系统，配备了独立的语音生成和解码器和实时音频收听机制，能够实现真正的实时交互能力，提高了对话的自然性和流畅性。"
}