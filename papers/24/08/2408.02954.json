{
    "title": "WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal Deepfake Detection",
    "abstract": "arXiv:2408.02954v1 Announce Type: new  Abstract: All current benchmarks for multimodal deepfake detection manipulate entire frames using various generation techniques, resulting in oversaturated detection accuracies exceeding 94% at the video-level classification. However, these benchmarks struggle to detect dynamic deepfake attacks with challenging frame-by-frame alterations presented in real-world scenarios. To address this limitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed at identifying manipulated segments within both video and audio, providing insight into the origins of deepfakes. Furthermore, we propose novel evaluation metrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to assess the robustness of deepfake detection models. Evaluating state-of-the-art models against diverse deepfake benchmarks, particularly FakeMix, demonstrates the effectiveness of our approach comprehensively. Specifically, while achieving an Average Preci",
    "link": "https://arxiv.org/abs/2408.02954",
    "context": "Title: WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal Deepfake Detection\nAbstract: arXiv:2408.02954v1 Announce Type: new  Abstract: All current benchmarks for multimodal deepfake detection manipulate entire frames using various generation techniques, resulting in oversaturated detection accuracies exceeding 94% at the video-level classification. However, these benchmarks struggle to detect dynamic deepfake attacks with challenging frame-by-frame alterations presented in real-world scenarios. To address this limitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed at identifying manipulated segments within both video and audio, providing insight into the origins of deepfakes. Furthermore, we propose novel evaluation metrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to assess the robustness of deepfake detection models. Evaluating state-of-the-art models against diverse deepfake benchmarks, particularly FakeMix, demonstrates the effectiveness of our approach comprehensively. Specifically, while achieving an Average Preci",
    "path": "papers/24/08/2408.02954.json",
    "total_tokens": 373,
    "tldr": "该文章提出了一个名为FakeMix的评估基准，旨在分析和检测视频和音频中的片段级深伪数据，并引入了Temporal Accuracy（TA）和Frame-wise Discrimination Metric（FDM）等新型评估指标，以改善对深伪检测模型的动态检测性能，从而填补现有视频级分类方法的不足，并在测试不同深伪攻击时展现出更好地效果。"
}