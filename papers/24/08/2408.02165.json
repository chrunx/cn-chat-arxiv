{
    "title": "SelfBC: Self Behavior Cloning for Offline Reinforcement Learning",
    "abstract": "arXiv:2408.02165v1 Announce Type: cross  Abstract: Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain d",
    "link": "https://arxiv.org/abs/2408.02165",
    "context": "Title: SelfBC: Self Behavior Cloning for Offline Reinforcement Learning\nAbstract: arXiv:2408.02165v1 Announce Type: cross  Abstract: Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain d",
    "path": "papers/24/08/2408.02165.json",
    "total_tokens": 319,
    "tldr": "该文章提出的SelfBC技术通过动态政策约束机制，结合指数移动平均技术和离线强化学习，实现了对保守行为策略的突破，有助于提高非保守策略的性能，同时有效防止了策略崩溃问题，为离线强化学习提供了一种新的学习和改进策略的方法。"
}