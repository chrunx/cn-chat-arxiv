{
    "title": "SelfBC: Self Behavior Cloning for Offline Reinforcement Learning",
    "abstract": "arXiv:2408.02165v1 Announce Type: cross  Abstract: Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain d",
    "link": "https://arxiv.org/abs/2408.02165",
    "context": "Title: SelfBC: Self Behavior Cloning for Offline Reinforcement Learning\nAbstract: arXiv:2408.02165v1 Announce Type: cross  Abstract: Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain d",
    "path": "papers/24/08/2408.02165.json",
    "total_tokens": 649,
    "translated_title": "SelfBC: 自我行为克隆在离线强化学习中的应用",
    "translated_abstract": "arXiv:2408.02165v1 公告类型：交叉  摘要：在离线强化学习中，策略约束方法使用额外的正则化技术来限制学习策略与离线数据集之间的差异。然而，这些方法往往导致过于保守的策略，模仿行为策略，从而限制了它们的性能。我们研究了这一局限性，并将这一局限性归因于传统约束的静态性质。在本文中，我们提出了一种新的动态策略约束，它限制了以前学习策略生成指数平滑平均samples的学习策略。通过将这种自我约束机制集成到off-policy方法中，我们的方法在离线环境中促进了非保守策略的学习，同时避免了策略崩溃。理论结果表明，我们的方法导致了一个几乎单调改进的参考策略。在D4RL MuJoCo领域的广泛实验中，我们证明了SelfBC的有效性。",
    "tldr": "本文提出了SelfBC方法，它通过集成自我约束机制到off-policy方法中，在离线强化学习中学习非保守策略，避免了策略崩溃，并且理论和实验结果证明了其有效性。"
}