{
    "title": "An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding",
    "abstract": "arXiv:2408.01120v1 Announce Type: new  Abstract: Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual ",
    "link": "https://arxiv.org/abs/2408.01120",
    "context": "Title: An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding\nAbstract: arXiv:2408.01120v1 Announce Type: new  Abstract: Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual ",
    "path": "papers/24/08/2408.01120.json",
    "total_tokens": 722,
    "translated_title": "高效有效的Transformer解码器基多任务视觉定位框架",
    "translated_abstract": "到目前为止，大多数先进的视觉定位技术依赖于Transformer来融合视觉语言特征。但这种基于Transformer的方法存在一个重大缺陷：Transformer编码器中的自我注意机制导致的计算成本呈二次方增长，尤其是在处理高分辨率图像或者长文本上下文时。这种计算成本的二次增长限制了视觉定位技术在更复杂的场景中的应用，例如对话推理切片，它需要长篇的语言表达。本文提出了一种基于Transformer解码器的多任务视觉定位（EEVG）框架，以解决这一问题，该框架在语言和视觉方面都降低了成本。在语言方面，我们使用Transformer解码器来融合视觉和语言特征，其中语言特征被输入作为记忆，而视觉特征则是沿着编码器逐层传递。这种设计大大减少了结构化的语言特征与图像特征之间一系列的自我注意操作的计算代价。此外，我们还提出了有针对性的训练策略和可重用组件，如双注意力模块，它可以在训练过程中有效融合语言信息和视觉表示，并且在推理阶段能更快地提供泛化能力。我们证明，EEVG比先前的方法在对话语境下的视觉理解任务上取得了显著的性能提升，并且在处理具体任务，如图像标签上下文时，也展现了更好的通用性和效率。",
    "tldr": "本文提出了一种基于Transformer解码器的高效多任务视觉定位框架，该框架在保持高效的同时也能有效融合视觉和语言信息，尤其是在对话语境下的视觉理解任务中取得显著提升。"
}