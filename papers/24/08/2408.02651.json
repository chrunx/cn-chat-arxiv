{
    "title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?",
    "abstract": "arXiv:2408.02651v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs. Yet, the potential for generating harmful content through these models seems to persist. This paper explores the concept of jailbreaking LLMs-reversing their alignment through adversarial triggers. Previous methods, such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts, have had limited success on black-box models due to their requirements for model access and for producing a low variety of manually crafted prompts, making them susceptible to being blocked. This paper introduces a novel approach using reinforcement learning to optimize adversarial triggers, requiring only inference API ",
    "link": "https://arxiv.org/abs/2408.02651",
    "context": "Title: Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?\nAbstract: arXiv:2408.02651v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs. Yet, the potential for generating harmful content through these models seems to persist. This paper explores the concept of jailbreaking LLMs-reversing their alignment through adversarial triggers. Previous methods, such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts, have had limited success on black-box models due to their requirements for model access and for producing a low variety of manually crafted prompts, making them susceptible to being blocked. This paper introduces a novel approach using reinforcement learning to optimize adversarial triggers, requiring only inference API ",
    "path": "papers/24/08/2408.02651.json",
    "total_tokens": 312,
    "tldr": "该文章提出了一种使用强化学习优化对抗性触发器的创新方法，旨在通过仅需模型API接口访问来打破大型语言模型的一致性，从而揭示隐藏在这些模型中的潜在风险。"
}