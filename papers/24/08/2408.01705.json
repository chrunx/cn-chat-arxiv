{
    "title": "Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers",
    "abstract": "arXiv:2408.01705v1 Announce Type: cross  Abstract: With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on \\emph{sample-wise} transfer attacks and propose a novel attack method termed \\emph{Downstream Transfer Attack (DTA)}. For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of t",
    "link": "https://arxiv.org/abs/2408.01705",
    "context": "Title: Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers\nAbstract: arXiv:2408.01705v1 Announce Type: cross  Abstract: With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on \\emph{sample-wise} transfer attacks and propose a novel attack method termed \\emph{Downstream Transfer Attack (DTA)}. For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of t",
    "path": "papers/24/08/2408.01705.json",
    "total_tokens": 805,
    "translated_title": "下游转移攻击：预训练视觉变换器模型的下游任务攻击",
    "translated_abstract": "arXiv:2408.01705v1 公告类型：交叉  摘要：随着视觉变换器（ViT）和自监督学习（SSL）技术的发展，预训练的大型ViT已成为计算机视觉应用的新型基础模型。然而，研究表明，就像卷积神经网络（CNN）一样，ViT也容易受到 adversarial 攻击，这种攻击可以通过输入的小干扰欺骗模型做出错误的预测。本文研究了从预训练的ViT模型到下游任务的转移易感性。我们专注于 \\emph{样本-wise} 转移攻击，并提出了一种名为 \\emph{Downstream Transfer Attack (DTA)}  的攻击方法。对于给定的测试图像，DTA利用预训练的ViT模型来制作 adversarial 示例，然后将该 adversarial 示例应用于攻击一个在下游数据集上微调过的模型版本。在攻击过程中，DTA识别并利用模型中最脆弱的层，从而实现攻击的个性化定制。我们证明了DTA 能够在多个下游任务上有效地转移攻击，包括目标检测、图像分类和语义分割等。实验结果表明，DTA 与传统的梯度基于 adversar 攻击相比，能够提高攻击的准确性并减少所需的 adversarial 示例数量。此工作的进一步研究可能会揭示基于 ViT 的模型在新的应用场景中的安全性和鲁棒性问题，并促进安全模型的设计和部署。",
    "tldr": "本文提出了一种名为 \"Downstream Transfer Attack (DTA) 的创新方法，该方法利用预训练的视觉变换器模型的缺陷，转移到下游任务上进行有针对性的攻击。通过个性化的攻击策略，DTA能够在多个计算机视觉任务中有效运作。"
}