{
    "title": "Unlocking Exocentric Video-Language Data for Egocentric Video Representation Learning",
    "abstract": "arXiv:2408.03567v1 Announce Type: new  Abstract: We present EMBED (Egocentric Models Built with Exocentric Data), a method designed to transform exocentric video-language data for egocentric video representation learning. Large-scale exocentric data covers diverse activities with significant potential for egocentric learning, but inherent disparities between egocentric and exocentric data pose challenges in utilizing one view for the other seamlessly. Egocentric videos predominantly feature close-up hand-object interactions, whereas exocentric videos offer a broader perspective on human activities. Additionally, narratives in egocentric datasets are typically more action-centric and closely linked with the visual content, in contrast to the narrative styles found in exocentric datasets. To address these challenges, we employ a data transformation framework to adapt exocentric data for egocentric training, focusing on identifying specific video clips that emphasize hand-object interacti",
    "link": "https://arxiv.org/abs/2408.03567",
    "context": "Title: Unlocking Exocentric Video-Language Data for Egocentric Video Representation Learning\nAbstract: arXiv:2408.03567v1 Announce Type: new  Abstract: We present EMBED (Egocentric Models Built with Exocentric Data), a method designed to transform exocentric video-language data for egocentric video representation learning. Large-scale exocentric data covers diverse activities with significant potential for egocentric learning, but inherent disparities between egocentric and exocentric data pose challenges in utilizing one view for the other seamlessly. Egocentric videos predominantly feature close-up hand-object interactions, whereas exocentric videos offer a broader perspective on human activities. Additionally, narratives in egocentric datasets are typically more action-centric and closely linked with the visual content, in contrast to the narrative styles found in exocentric datasets. To address these challenges, we employ a data transformation framework to adapt exocentric data for egocentric training, focusing on identifying specific video clips that emphasize hand-object interacti",
    "path": "papers/24/08/2408.03567.json",
    "total_tokens": 376,
    "tldr": "该文章提出EMBED方法，通过独特的视频-语言数据变换框架实现对以第三人称视角为主的视频内容进行优化，使其更适合于第一人称视角视频的模型学习，尤其是在处理人手动作图像方面。通过这种创新的转换技术，文章为第一人称视频学习提供了更多的数据支持，促进了手部动作识别的研究进展，并且增强了模型的泛化能力。"
}