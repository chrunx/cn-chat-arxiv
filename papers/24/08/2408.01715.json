{
    "title": "Joint Universal Adversarial Perturbations with Interpretations",
    "abstract": "arXiv:2408.01715v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able t",
    "link": "https://arxiv.org/abs/2408.01715",
    "context": "Title: Joint Universal Adversarial Perturbations with Interpretations\nAbstract: arXiv:2408.01715v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able t",
    "path": "papers/24/08/2408.01715.json",
    "total_tokens": 303,
    "tldr": "该文章研究了能够泛化到所有输入样本的攻击性扰动，并提出了一种新方法来同时生成攻击性扰动和其解释性图。"
}