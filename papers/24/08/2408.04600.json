{
    "title": "Improving Network Interpretability via Explanation Consistency Evaluation",
    "abstract": "arXiv:2408.04600v1 Announce Type: new  Abstract: While deep neural networks have achieved remarkable performance, they tend to lack transparency in prediction. The pursuit of greater interpretability in neural networks often results in a degradation of their original performance. Some works strive to improve both interpretability and performance, but they primarily depend on meticulously imposed conditions. In this paper, we propose a simple yet effective framework that acquires more explainable activation heatmaps and simultaneously increase the model performance, without the need for any extra supervision. Specifically, our concise framework introduces a new metric, i.e., explanation consistency, to reweight the training samples adaptively in model learning. The explanation consistency metric is utilized to measure the similarity between the model's visual explanations of the original samples and those of semantic-preserved adversarial samples, whose background regions are perturbed ",
    "link": "https://arxiv.org/abs/2408.04600",
    "context": "Title: Improving Network Interpretability via Explanation Consistency Evaluation\nAbstract: arXiv:2408.04600v1 Announce Type: new  Abstract: While deep neural networks have achieved remarkable performance, they tend to lack transparency in prediction. The pursuit of greater interpretability in neural networks often results in a degradation of their original performance. Some works strive to improve both interpretability and performance, but they primarily depend on meticulously imposed conditions. In this paper, we propose a simple yet effective framework that acquires more explainable activation heatmaps and simultaneously increase the model performance, without the need for any extra supervision. Specifically, our concise framework introduces a new metric, i.e., explanation consistency, to reweight the training samples adaptively in model learning. The explanation consistency metric is utilized to measure the similarity between the model's visual explanations of the original samples and those of semantic-preserved adversarial samples, whose background regions are perturbed ",
    "path": "papers/24/08/2408.04600.json",
    "total_tokens": 335,
    "tldr": "该文章提出了一种简单的框架，通过引入一种称为\"解释一致性\"的新度量，能够在不需额外监督的情况下，获得更具解释性的激活热图并同时提高模型性能。通过重新加权训练样本，该框架确保了模型对原始样本的解释与对抗样本的解释相似，从而提高了模型的透明度和表现。"
}