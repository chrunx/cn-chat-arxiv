{
    "title": "StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation",
    "abstract": "arXiv:2408.01343v1 Announce Type: new  Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter t",
    "link": "https://arxiv.org/abs/2408.01343",
    "context": "Title: StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation\nAbstract: arXiv:2408.01343v1 Announce Type: new  Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter t",
    "path": "papers/24/08/2408.01343.json",
    "total_tokens": 686,
    "translated_title": "StitchFusion: 一种融合任何视觉模态以提高多模态语义分割准确性的方法",
    "translated_abstract": "arXiv:2408.01343v1 公告类型：新  摘要：多模态语义分割在复杂场景中显著提高了分割的准确性。然而，现有的方法往往包含专门的特征融合模块，这些模块特地为特定的模态设计，因此限制了输入的灵活性和增加了训练参数的数量。为了解决这些挑战，我们提出了一种简洁且有效的大模融合框架StitchFusion，该框架直接将大规模预训练模型作为编码器和特征融合器。这种方法在编码过程中实现模融合，通过共享多模态视觉信息，从而促进了信息流在编码过程中的双向传输。通过引入多模态特征融合模块，我们的框架能够更好地处理多模态和多尺度特征融合问题，并能够适应任何视觉模态的输入。特别是，我们的框架通过共享多模态视觉信息在编码过程中实现模融合。为了增强模态间的信息交流，我们引入了一个多方向适配器模块（MultiAdapter），以在编码过程中实现跨模态信息传输。通过利用MultiAdapter，我们的方法实现了多模态特征的有效融合，并在多个公共数据集上取得了优于现有方法的性能。",
    "tldr": "本文提出了一种新的多模态特征融合框架StitchFusion，它能够将任何视觉模态有效融合，提高多模态语义分割的准确性。"
}