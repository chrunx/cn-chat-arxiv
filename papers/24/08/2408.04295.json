{
    "title": "Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal Policy Optimization",
    "abstract": "arXiv:2408.04295v1 Announce Type: cross  Abstract: Multi-agent proximal policy optimization (MAPPO) has recently demonstrated state-of-the-art performance on challenging multi-agent reinforcement learning tasks. However, MAPPO still struggles with the credit assignment problem, wherein the sheer difficulty in ascribing credit to individual agents' actions scales poorly with team size. In this paper, we propose a multi-agent reinforcement learning algorithm that adapts recent developments in credit assignment to improve upon MAPPO. Our approach leverages partial reward decoupling (PRD), which uses a learned attention mechanism to estimate which of a particular agent's teammates are relevant to its learning updates. We use this estimate to dynamically decompose large groups of agents into smaller, more manageable subgroups. We empirically demonstrate that our approach, PRD-MAPPO, decouples agents from teammates that do not influence their expected future reward, thereby streamlining cred",
    "link": "https://arxiv.org/abs/2408.04295",
    "context": "Title: Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal Policy Optimization\nAbstract: arXiv:2408.04295v1 Announce Type: cross  Abstract: Multi-agent proximal policy optimization (MAPPO) has recently demonstrated state-of-the-art performance on challenging multi-agent reinforcement learning tasks. However, MAPPO still struggles with the credit assignment problem, wherein the sheer difficulty in ascribing credit to individual agents' actions scales poorly with team size. In this paper, we propose a multi-agent reinforcement learning algorithm that adapts recent developments in credit assignment to improve upon MAPPO. Our approach leverages partial reward decoupling (PRD), which uses a learned attention mechanism to estimate which of a particular agent's teammates are relevant to its learning updates. We use this estimate to dynamically decompose large groups of agents into smaller, more manageable subgroups. We empirically demonstrate that our approach, PRD-MAPPO, decouples agents from teammates that do not influence their expected future reward, thereby streamlining cred",
    "path": "papers/24/08/2408.04295.json",
    "total_tokens": 334,
    "tldr": "该文章提出了一种名为PRD-MAPPO的算法，它通过利用部分奖励解耦机制和注意力机制，有效解决了多 agent 强化学习中的信用分配问题，从而提高了MAPPO算法的性能。"
}