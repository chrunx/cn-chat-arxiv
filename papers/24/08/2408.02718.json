{
    "title": "MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models",
    "abstract": "arXiv:2408.02718v1 Announce Type: new  Abstract: The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Thro",
    "link": "https://arxiv.org/abs/2408.02718",
    "context": "Title: MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models\nAbstract: arXiv:2408.02718v1 Announce Type: new  Abstract: The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Thro",
    "path": "papers/24/08/2408.02718.json",
    "total_tokens": 406,
    "tldr": "该文章开发了一种名为MMIU的基准测试，旨在评估大型视觉语言模型的多模态多图像理解能力，并通过7种多图像关系、52个任务和11,000多个精心策划的多选择问题，提供了迄今为止最全面的benchmark，揭示了包括顶级模型在内的多图像理解挑战，并且在多图像理解和空间理解任务上表现不佳。"
}