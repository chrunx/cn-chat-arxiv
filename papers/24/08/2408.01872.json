{
    "title": "Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples",
    "abstract": "arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex",
    "link": "https://arxiv.org/abs/2408.01872",
    "context": "Title: Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples\nAbstract: arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex",
    "path": "papers/24/08/2408.01872.json",
    "total_tokens": 359,
    "tldr": "该文章提出了一种使用内分布数据作为正例的可靠 semi-supervised 对比学习方法，该方法解决了现有方法在类分布不匹配情况下的性能下降问题，其中未标记数据中存在 out-of-distribution (OOD) 数据。通过对比学习方法，文章最大化内分布数据的信息利用，并通过可变系数的时间表有效地聚合相应的负样。"
}