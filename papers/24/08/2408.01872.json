{
    "title": "Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples",
    "abstract": "arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex",
    "link": "https://arxiv.org/abs/2408.01872",
    "context": "Title: Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples\nAbstract: arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex",
    "path": "papers/24/08/2408.01872.json",
    "total_tokens": 712,
    "translated_title": "使用与正样本相同的内分布数据实现安全的半监督对比学习",
    "translated_abstract": "arXiv:2408.01872v1 公告类型：跨领域 摘要：当只有少量的标签可用时，半监督学习方法在解决许多实际问题方面显示出良好的结果。现有方法假设标注和未标注数据的类分布相等；然而，当未标注数据中存在出分布（OOD）数据时，它们的表现会显著下降。以往的安全半监督学习研究已经通过基于标注数据的策略有效降低了出分布数据的负面影响。然而，即使这些研究能够有效地筛选出不必要的出分布数据，他们也可能会失去数据之间共享的基本信息，而不管其类别如何。为此，我们提出了将自监督对比学习方法应用于充分利用大量未标注数据的策略。我们还提出了一个带有系数调度比例的对比损失函数，以聚合作为锚定的标注负样本，从而确保所有数据在对比学习过程中的代表性，避免因过滤出分布数据而损失基本信息。",
    "tldr": ""
}