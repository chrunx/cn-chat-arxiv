{
    "title": "Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders",
    "abstract": "arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us",
    "link": "https://arxiv.org/abs/2408.02245",
    "context": "Title: Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders\nAbstract: arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us",
    "path": "papers/24/08/2408.02245.json",
    "total_tokens": 787,
    "translated_title": "基于课程学习的多模态对比掩码自编码器预训练方法",
    "translated_abstract": "arXiv:2408.02245v1 公告类型: 新  翻译摘要: 本文提出了一种在课程学习(CL)范式下用于图像理解任务的新的预训练方法，该方法利用了RGB-D技术。该方法利用了多模态对比掩码自编码器以及去噪技术。最近的方法要么使用掩码自编码（例如，MultiMAE），要么使用对比学习（例如，Pri3D），或者在单一的对比掩码自编码器架构中结合这两种方法，例如CMAE和CAV-MAE。然而，单一的对比掩码自编码器都不能适用于RGB-D数据集。为了提高这些方法的表现和效率，我们提出了基于CL的新预训练策略。具体来说，在第一阶段，我们使用对比学习方法来学习跨模态表示。在第二阶段，我们使用第一阶段获得的权重初始化模态特定的编码器，然后使用掩码自编码和去噪/噪声预测继续预训练模型。在第三阶段，我们利用监督学习进一步微调模型，并结合主动学习策略来选择最有信息量的样本。最后，我们在RGB-D数据集上进行了一系列实验以验证所提出方法的稳健性和有效性。",
    "tldr": "本文提出了一种结合课程学习、多模态对比掩码自编码器和去噪技术的新预训练方法，专门用于改进图像理解的性能，特别是在RGB-D数据集上。",
    "en_tdlr": "This paper proposes a new pre-training method specifically designed for image understanding tasks under the Curriculum Learning paradigm, leveraging RGB-D data. It combines multi-modal contrastive masked autoencoding, denoising techniques, and a novel strategy based on curriculum learning, showing robustness and effectiveness in experiments on RGB-D datasets."
}