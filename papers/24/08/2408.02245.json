{
    "title": "Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders",
    "abstract": "arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us",
    "link": "https://arxiv.org/abs/2408.02245",
    "context": "Title: Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders\nAbstract: arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us",
    "path": "papers/24/08/2408.02245.json",
    "total_tokens": 432,
    "tldr": "该文章提出了一种基于Curriculum Learning的新型预训练方法，该方法结合了多模态对比masked autoencoder和去噪技术，旨在提升RGB-D数据集上的图像理解任务性能。这种方法首先在第一个阶段通过对比学习来学习交叉模态表示，然后在第二个阶段利用第一个阶段的权重初始化特定的模态编码器，并使用masked autoencoder和去噪技术进一步预训练模型。这种策略旨在通过不同的难度级别和数据实例来逐渐指导模型的学习，从而提高模型的理解和泛化能力。"
}