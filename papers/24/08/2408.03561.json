{
    "title": "MPC-Minimized Secure LLM Inference",
    "abstract": "arXiv:2408.03561v1 Announce Type: cross  Abstract: Many inference services based on large language models (LLMs) pose a privacy concern, either revealing user prompts to the service or the proprietary weights to the user. Secure inference offers a solution to this problem through secure multi-party computation (MPC), however, it is still impractical for modern LLM workload due to the large overhead imposed by MPC. To address this overhead, we propose Marill, a framework that adapts LLM fine-tuning to minimize MPC usage during secure inference. Marill introduces high-level architectural changes during fine-tuning that significantly reduce the number of expensive operations needed within MPC during inference, by removing some and relocating others outside MPC without compromising security. As a result, Marill-generated models are more efficient across all secure inference protocols and our approach complements MPC-friendly approximations for such operations. Compared to standard fine-tun",
    "link": "https://arxiv.org/abs/2408.03561",
    "context": "Title: MPC-Minimized Secure LLM Inference\nAbstract: arXiv:2408.03561v1 Announce Type: cross  Abstract: Many inference services based on large language models (LLMs) pose a privacy concern, either revealing user prompts to the service or the proprietary weights to the user. Secure inference offers a solution to this problem through secure multi-party computation (MPC), however, it is still impractical for modern LLM workload due to the large overhead imposed by MPC. To address this overhead, we propose Marill, a framework that adapts LLM fine-tuning to minimize MPC usage during secure inference. Marill introduces high-level architectural changes during fine-tuning that significantly reduce the number of expensive operations needed within MPC during inference, by removing some and relocating others outside MPC without compromising security. As a result, Marill-generated models are more efficient across all secure inference protocols and our approach complements MPC-friendly approximations for such operations. Compared to standard fine-tun",
    "path": "papers/24/08/2408.03561.json",
    "total_tokens": 384,
    "tldr": "\"该文章提出了一种名为Marill的框架，通过在模型微调过程中引入结构性的变化，减少了在安全推理过程中使用安全多方计算（MPC）时所需的高成本操作。这不仅减少了MPC的计算负担，而且还扩大了MPC优化的应用范围。此外，该框架能够在不牺牲安全性的情况下，优化现有LLM模型的推理性能，对于希望保护用户隐私和模型知识产权的推理服务提供商来说，这是一个重要的贡献。\""
}