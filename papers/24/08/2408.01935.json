{
    "title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference",
    "abstract": "arXiv:2408.01935v1 Announce Type: cross  Abstract: Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four",
    "link": "https://arxiv.org/abs/2408.01935",
    "context": "Title: Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference\nAbstract: arXiv:2408.01935v1 Announce Type: cross  Abstract: Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four",
    "path": "papers/24/08/2408.01935.json",
    "total_tokens": 728,
    "translated_title": "论文标题: 定义和评估在自然语言推理中被应用的语言模型的决策和复合风险",
    "translated_abstract": "论文摘要: 尽管像ChatGPT这样的大型语言模型(LLMs)表现出色，但它们也被认为存在重要的风险。其中一种风险来自于模板的信心错误放置，无论是过于自信还是缺乏自信，这在模型对语义推理的判断中尤为明显。虽然前者已被广泛研究，但后者缺乏研究，导致了对基于信心错误放置的模型全面风险的理解存在不对称性。本研究通过定义两种风险类型（决策风险和复合风险），并提出了一个实验框架，包括两级推理架构和相应的度量指标来测量在分类和生成性LLMs中的风险。第一级依赖于一个决策规则，它决定了底层语言模型是否应该避免做出推理。如果模型不回避，第二级(它将应用于模型回避的情况)就是模型的推理。在四个不同的场景中进行的详细实验展示了该框架的有效性，并揭示了LLMs在复杂推理和频繁错误报告方面的局限。此外，我们的工作为开发更可靠的语言模型，并在使用这些模型进行自动推理时减少事故风险提供了宝贵的见解。",
    "tldr": "本文定义了语言模型用于自然语言推理时的决策和复合风险，并通过实验框架评估了这两种风险，揭示了LLMs在复杂推理和频繁错误报告方面的局限性，为提高语言模型的可靠性提供了洞察。"
}