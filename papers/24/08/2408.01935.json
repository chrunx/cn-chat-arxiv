{
    "title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference",
    "abstract": "arXiv:2408.01935v1 Announce Type: cross  Abstract: Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four",
    "link": "https://arxiv.org/abs/2408.01935",
    "context": "Title: Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference\nAbstract: arXiv:2408.01935v1 Announce Type: cross  Abstract: Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four",
    "path": "papers/24/08/2408.01935.json",
    "total_tokens": 361,
    "tldr": "该文章创新性地提出了决策风险和复合风险的概念，用以全面评估语言模型在自然语言推断任务中的风险，特别是在处理过度自信和缺乏自信的情形。通过构建一个两级推理架构，并设计了相应的评估指标，该研究为理解大型语言模型在做出决策时可能存在的风险提供了新的洞见。"
}