{
    "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning",
    "abstract": "arXiv:2408.03029v1 Announce Type: cross  Abstract: Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experiences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implementation in high-dimensional continuous state spaces. This method provides a non-pa",
    "link": "https://arxiv.org/abs/2408.03029",
    "context": "Title: Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning\nAbstract: arXiv:2408.03029v1 Announce Type: cross  Abstract: Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experiences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implementation in high-dimensional continuous state spaces. This method provides a non-pa",
    "path": "papers/24/08/2408.03029.json",
    "total_tokens": 378,
    "tldr": "该文章提出了一种新型的自适应增强学习中稀疏奖励问题的解决方案，通过结合历史经验中的成功率来构造密集且信息量大的奖励信号。该方法使用从Beta分布中采样的成功率，这些分布随着数据的积累而逐渐变得可靠。在初始阶段，自适应成功率鼓励探索，随着数据的积累，逐渐变为鼓励利用的好方法。通过结合核密度估计（KDE）与随机傅里叶特征（RFF），该方法在处理高维连续状态空间时具有高效的计算效率。"
}