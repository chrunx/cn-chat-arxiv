{
    "title": "Label Augmentation for Neural Networks Robustness",
    "abstract": "arXiv:2408.01977v1 Announce Type: new  Abstract: Out-of-distribution generalization can be categorized into two types: common perturbations arising from natural variations in the real world and adversarial perturbations that are intentionally crafted to deceive neural networks. While deep neural networks excel in accuracy under the assumption of identical distributions between training and test data, they often encounter out-of-distribution scenarios resulting in a significant decline in accuracy. Data augmentation methods can effectively enhance robustness against common corruptions, but they typically fall short in improving robustness against adversarial perturbations. In this study, we develop Label Augmentation (LA), which enhances robustness against both common and intentional perturbations and improves uncertainty estimation. Our findings indicate a Clean error rate improvement of up to 23.29% when employing LA in comparisons to the baseline. Additionally, it enhances robustness",
    "link": "https://arxiv.org/abs/2408.01977",
    "context": "Title: Label Augmentation for Neural Networks Robustness\nAbstract: arXiv:2408.01977v1 Announce Type: new  Abstract: Out-of-distribution generalization can be categorized into two types: common perturbations arising from natural variations in the real world and adversarial perturbations that are intentionally crafted to deceive neural networks. While deep neural networks excel in accuracy under the assumption of identical distributions between training and test data, they often encounter out-of-distribution scenarios resulting in a significant decline in accuracy. Data augmentation methods can effectively enhance robustness against common corruptions, but they typically fall short in improving robustness against adversarial perturbations. In this study, we develop Label Augmentation (LA), which enhances robustness against both common and intentional perturbations and improves uncertainty estimation. Our findings indicate a Clean error rate improvement of up to 23.29% when employing LA in comparisons to the baseline. Additionally, it enhances robustness",
    "path": "papers/24/08/2408.01977.json",
    "total_tokens": 303,
    "tldr": "该文章提出了Label Augmentation（LA）方法，通过增强神经网络对抗样本和常见错误的一致性预测能力，为网络提供了一种对抗性训练策略，有效提高了网络的鲁棒性，降低了误判率。"
}