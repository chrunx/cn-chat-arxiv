{
    "title": "VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy Prediction",
    "abstract": "arXiv:2408.03551v1 Announce Type: cross  Abstract: Monocular 3D semantic occupancy prediction is becoming important in robot vision due to the compactness of using a single RGB camera. However, existing methods often do not adequately account for camera perspective geometry, resulting in information imbalance along the depth range of the image. To address this issue, we propose a vanishing point (VP) guided monocular 3D semantic occupancy prediction framework named VPOcc. Our framework consists of three novel modules utilizing VP. First, in the VPZoomer module, we initially utilize VP in feature extraction to achieve information balanced feature extraction across the scene by generating a zoom-in image based on VP. Second, we perform perspective geometry-aware feature aggregation by sampling points towards VP using a VP-guided cross-attention (VPCA) module. Finally, we create an information-balanced feature volume by effectively fusing original and zoom-in voxel feature volumes with a ",
    "link": "https://arxiv.org/abs/2408.03551",
    "context": "Title: VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy Prediction\nAbstract: arXiv:2408.03551v1 Announce Type: cross  Abstract: Monocular 3D semantic occupancy prediction is becoming important in robot vision due to the compactness of using a single RGB camera. However, existing methods often do not adequately account for camera perspective geometry, resulting in information imbalance along the depth range of the image. To address this issue, we propose a vanishing point (VP) guided monocular 3D semantic occupancy prediction framework named VPOcc. Our framework consists of three novel modules utilizing VP. First, in the VPZoomer module, we initially utilize VP in feature extraction to achieve information balanced feature extraction across the scene by generating a zoom-in image based on VP. Second, we perform perspective geometry-aware feature aggregation by sampling points towards VP using a VP-guided cross-attention (VPCA) module. Finally, we create an information-balanced feature volume by effectively fusing original and zoom-in voxel feature volumes with a ",
    "path": "papers/24/08/2408.03551.json",
    "total_tokens": 412,
    "tldr": "该文章提出了一种名为VPOcc的框架，该框架利用vanishing point（VP）对单摄像头3D语义占据预测进行了改进。其框架包括三个新的模块：VPZoomer用于实现视角几何信息平衡的特征提取，VPCA模块用于进行视角几何敏感的特征聚合，以及一个将原始和放大后的voxel特征融合的模块，以创建信息平衡的特征体积。通过这些创新模块，VPOcc能够更准确地预测场景的3D语义占据，从而提高了机器人视觉中单摄像头系统的性能。"
}