{
    "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
    "abstract": "arXiv:2408.02900v1 Announce Type: new  Abstract: This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert model",
    "link": "https://arxiv.org/abs/2408.02900",
    "context": "Title: MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine\nAbstract: arXiv:2408.02900v1 Announce Type: new  Abstract: This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert model",
    "path": "papers/24/08/2408.02900.json",
    "total_tokens": 479,
    "tldr": "该文章介绍了MedTrinity-25M，这是一个涵盖了2500万张图像的医学多模态大型数据集，其包含10种不同的模态，且被标注了超过65种疾病的多层次信息。数据集中的标注不仅包括全球性的文本信息，如疾病/病变类型、模态、特定区域描述，还包括对感兴趣区域（ROIs）的详细本地标注，包括边界框和分割掩码。与那些仅限于图像-文本对的数据集相比，我们开发了一种自动化的管线，能够在不依赖任何文本描述的情况下，生成多层次的视觉和文本标注（以图像-ROI-描述三元组的形式）。此外，数据来自于90多个不同的源头，已经过预处理和基于领域专家的模型进行定位。"
}