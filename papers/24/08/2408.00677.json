{
    "title": "Scaling Backwards: Minimal Synthetic Pre-training?",
    "abstract": "arXiv:2408.00677v2 Announce Type: replace  Abstract: Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we inve",
    "link": "https://arxiv.org/abs/2408.00677",
    "context": "Title: Scaling Backwards: Minimal Synthetic Pre-training?\nAbstract: arXiv:2408.00677v2 Announce Type: replace  Abstract: Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we inve",
    "path": "papers/24/08/2408.00677.json",
    "total_tokens": 442,
    "tldr": "该文章提出了一个使用单一分形图形和轻微变形的数据集进行最小化合成预训练的方法，以实现与ImageNet-1k等大型真实世界图像集类似的性能。通过这种方法，文章证明了即使在合成图像数据极其有限的情况下，预训练仍然能够取得显著的效果，并且在图像识别任务上能够与使用大规模真实图像集的预训练竞争。此外，文章发现即使这些合成图像对于人类来说几乎不可区分，但对于预训练模型来说，这些差异是至关重要的。同时，文章还探索了构建假想类别所需的一个关键参数，并证明了其对于获取强大的预训练性能的重要性。最终，这项研究为如何在资源受限的情况下进行高效预训练提供了新的视角和方法。"
}