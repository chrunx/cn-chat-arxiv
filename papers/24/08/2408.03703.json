{
    "title": "CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications",
    "abstract": "arXiv:2408.03703v1 Announce Type: new  Abstract: Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer's powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we construct a novel additive similarity function following this paradigm and present an efficient implementation named Convolutional Additive Token Mixer (CATM). This simplification leads to a significant reduc",
    "link": "https://arxiv.org/abs/2408.03703",
    "context": "Title: CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications\nAbstract: arXiv:2408.03703v1 Announce Type: new  Abstract: Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer's powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we construct a novel additive similarity function following this paradigm and present an efficient implementation named Convolutional Additive Token Mixer (CATM). This simplification leads to a significant reduc",
    "path": "papers/24/08/2408.03703.json",
    "total_tokens": 332,
    "tldr": "该文章提出CAS-ViT: Convolutional Additive Self-attention Vision Transformers，通过构建高效的信息交互机制，改进了ViT在全球背景信息获取方面的效率，适用于资源受限的移动应用场景。"
}