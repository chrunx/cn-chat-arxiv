{
    "title": "Learning to Learn without Forgetting using Attention",
    "abstract": "arXiv:2408.03219v1 Announce Type: cross  Abstract: Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchma",
    "link": "https://arxiv.org/abs/2408.03219",
    "context": "Title: Learning to Learn without Forgetting using Attention\nAbstract: arXiv:2408.03219v1 Announce Type: cross  Abstract: Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchma",
    "path": "papers/24/08/2408.03219.json",
    "total_tokens": 296,
    "tldr": "该文章提出了一种基于注意力机制的元学习方法，以避免在持续学习过程中忘记先前学到的知识，并通过使优化器元学习能够有效地更新模型参数，从而加速新任务的学"
}