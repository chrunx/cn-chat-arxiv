{
    "title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation",
    "abstract": "arXiv:2408.02976v1 Announce Type: cross  Abstract: Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in",
    "link": "https://arxiv.org/abs/2408.02976",
    "context": "Title: Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation\nAbstract: arXiv:2408.02976v1 Announce Type: cross  Abstract: Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in",
    "path": "papers/24/08/2408.02976.json",
    "total_tokens": 319,
    "tldr": "该文章提出了一种使用强化学习方法（EmpRL）进行同情性响应生成的框架，它通过有效的同情度奖励函数和最大化期望奖励来生成更加同情性的文本响应，并利用了预训练的语言模型来提高响应的生成能力。"
}