{
    "title": "Joint Learning of Emotions in Music and Generalized Sounds",
    "abstract": "arXiv:2408.02009v1 Announce Type: cross  Abstract: In this study, we aim to determine if generalized sounds and music can share a common emotional space, improving predictions of emotion in terms of arousal and valence. We propose the use of multiple datasets as a multi-domain learning technique. Our approach involves creating a common space encompassing features that characterize both generalized sounds and music, as they can evoke emotions in a similar manner. To achieve this, we utilized two publicly available datasets, namely IADS-E and PMEmo, following a standardized experimental protocol. We employed a wide variety of features that capture diverse aspects of the audio structure including key parameters of spectrum, energy, and voicing. Subsequently, we performed joint learning on the common feature space, leveraging heterogeneous model architectures. Interestingly, this synergistic scheme outperforms the state-of-the-art in both sound and music emotion prediction. The code enabli",
    "link": "https://arxiv.org/abs/2408.02009",
    "context": "Title: Joint Learning of Emotions in Music and Generalized Sounds\nAbstract: arXiv:2408.02009v1 Announce Type: cross  Abstract: In this study, we aim to determine if generalized sounds and music can share a common emotional space, improving predictions of emotion in terms of arousal and valence. We propose the use of multiple datasets as a multi-domain learning technique. Our approach involves creating a common space encompassing features that characterize both generalized sounds and music, as they can evoke emotions in a similar manner. To achieve this, we utilized two publicly available datasets, namely IADS-E and PMEmo, following a standardized experimental protocol. We employed a wide variety of features that capture diverse aspects of the audio structure including key parameters of spectrum, energy, and voicing. Subsequently, we performed joint learning on the common feature space, leveraging heterogeneous model architectures. Interestingly, this synergistic scheme outperforms the state-of-the-art in both sound and music emotion prediction. The code enabli",
    "path": "papers/24/08/2408.02009.json",
    "total_tokens": 387,
    "tldr": "该文章提出了一种多域学习方法，通过创建一个共同空间来概括并融合多样化的声音和音乐的情绪特征，并利用两个公开数据集进行实证研究。在标准化的实验协议下，研究者使用多种特征来描绘声音结构，并在相同的特征空间中培训了异构模型。这项工作以显著优势超越了现有音乐和声音情绪预测技术的记录。通过这一创新式的跨域学习和模型集成方法，作者致力于开发一种更为准确的跨媒体情绪分析算法。"
}