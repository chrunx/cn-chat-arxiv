{
    "title": "Distribution-Level Memory Recall for Continual Learning: Preserving Knowledge and Avoiding Confusion",
    "abstract": "arXiv:2408.02695v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn new data without forgetting previously learned knowledge. The key to achieving this goal is to avoid confusion at the feature level, i.e., avoiding confusion within old tasks and between new and old tasks. Previous prototype-based CL methods generate pseudo features for old knowledge replay by adding Gaussian noise to the centroids of old classes. However, the distribution in the feature space exhibits anisotropy during the incremental process, which prevents the pseudo features from faithfully reproducing the distribution of old knowledge in the feature space, leading to confusion in classification boundaries within old tasks. To address this issue, we propose the Distribution-Level Memory Recall (DMR) method, which uses a Gaussian mixture model to precisely fit the feature distribution of old knowledge at the distribution level and generate pseudo features in",
    "link": "https://arxiv.org/abs/2408.02695",
    "context": "Title: Distribution-Level Memory Recall for Continual Learning: Preserving Knowledge and Avoiding Confusion\nAbstract: arXiv:2408.02695v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn new data without forgetting previously learned knowledge. The key to achieving this goal is to avoid confusion at the feature level, i.e., avoiding confusion within old tasks and between new and old tasks. Previous prototype-based CL methods generate pseudo features for old knowledge replay by adding Gaussian noise to the centroids of old classes. However, the distribution in the feature space exhibits anisotropy during the incremental process, which prevents the pseudo features from faithfully reproducing the distribution of old knowledge in the feature space, leading to confusion in classification boundaries within old tasks. To address this issue, we propose the Distribution-Level Memory Recall (DMR) method, which uses a Gaussian mixture model to precisely fit the feature distribution of old knowledge at the distribution level and generate pseudo features in",
    "path": "papers/24/08/2408.02695.json",
    "total_tokens": 333,
    "tldr": "该文章提出了一种Distribution-Level Memory Recall（DMR）方法，通过精确地使用高斯混合模型在分布水平上拟合旧知识的特征分布，并生成在旧任务分类边界上能够避免混淆的特征。"
}