{
    "title": "Infusing Environmental Captions for Long-Form Video Language Grounding",
    "abstract": "arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.",
    "link": "https://arxiv.org/abs/2408.02336",
    "context": "Title: Infusing Environmental Captions for Long-Form Video Language Grounding\nAbstract: arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.",
    "path": "papers/24/08/2408.02336.json",
    "total_tokens": 610,
    "translated_title": "论文标题翻译成中文：本文提出了一种在长视频语言对准中注入环境描述的新方法",
    "translated_abstract": "论文摘要翻译成中文：本文致力于解决长期视频-语言对准（VLG）问题。给定一个长视频和一个自然语言查询，模型应该能够准确定位到回答查询的时间点。人类可以轻松解决VLG任务，即使视频很长，他们也可以通过丰富的经验知识过滤掉不相关的时刻。与人类不同，现有的VLG方法经常在学习到的小规模数据集表面模式上失败，即使正确的时间点是在无关的帧中。为了克服这个问题，我们提出了一种名为EI-VLG的方法，该方法利用多模态大型语言模型（MLLM）提供的丰富文本信息作为人类经验的替代品，帮助我们有效排除无关帧。本文通过在具有挑战性的EgoNLQ数据集上的大量实验验证了所提出方法的效率。",
    "tldr": "引入与长视频对齐的丰富环境描述信息，有效排除无关内容，提高了视频语言对准任务的精确度。",
    "en_tdlr": "Proposed approach enhances the accuracy of temporally grounding natural language queries with richer environmental caption information in long videos, effectively filtering out irrelevant content."
}