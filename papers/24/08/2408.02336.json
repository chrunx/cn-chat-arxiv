{
    "title": "Infusing Environmental Captions for Long-Form Video Language Grounding",
    "abstract": "arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.",
    "link": "https://arxiv.org/abs/2408.02336",
    "context": "Title: Infusing Environmental Captions for Long-Form Video Language Grounding\nAbstract: arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.",
    "path": "papers/24/08/2408.02336.json",
    "total_tokens": 350,
    "tldr": "该文章提出了一种名为EI-VLG的长期视频语言地面（VLG）方法，该方法通过利用多模态大型语言模型提供的大量文本信息来模拟人类经验，有效排除无关视频片段，增强了视频-语言共同表征的能力，解决了长视频语言地面问题中忽略非相关信息的问题。"
}