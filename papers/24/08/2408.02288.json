{
    "title": "Spin glass model of in-context learning",
    "abstract": "arXiv:2408.02288v1 Announce Type: cross  Abstract: Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention, and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and most importantly why an unseen function can be predicted by providing only a prompt yet without training. Our theory reveals that for single instance learning, increasing the task diversity leads to the emergence of the in-context learning, by allowing the Boltzmann distribution to co",
    "link": "https://arxiv.org/abs/2408.02288",
    "context": "Title: Spin glass model of in-context learning\nAbstract: arXiv:2408.02288v1 Announce Type: cross  Abstract: Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention, and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and most importantly why an unseen function can be predicted by providing only a prompt yet without training. Our theory reveals that for single instance learning, increasing the task diversity leads to the emergence of the in-context learning, by allowing the Boltzmann distribution to co",
    "path": "papers/24/08/2408.02288.json",
    "total_tokens": 326,
    "tldr": "该文章提出将大型语言模型中的“Contextual Learning”现象与物理学的“Spin Glass”模型相结合，建立了语言模型中的参数相互作用与模型结构的量子相变，揭示了为什么只需提供提示而无需重新训练，模型能够预测未见过的函数。"
}