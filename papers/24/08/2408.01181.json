{
    "title": "VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling",
    "abstract": "arXiv:2408.01181v1 Announce Type: new  Abstract: VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP's pr",
    "link": "https://arxiv.org/abs/2408.01181",
    "context": "Title: VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling\nAbstract: arXiv:2408.01181v1 Announce Type: new  Abstract: VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP's pr",
    "path": "papers/24/08/2408.01181.json",
    "total_tokens": 716,
    "translated_title": "VAR-CLIP: 使用视觉自回归建模的文本到图像生成器",
    "translated_abstract": "arXiv:2408.01181v1 公告类型：新消息  摘要：VAR是一种新的范式，利用“下一级预测”而不是“下一个 token 预测”。这种创新的转变能够使自回归（AR）变换器快速学习视觉分布并实现稳健的泛化能力。然而，原始的VAR模型只能在类条件合成下使用，它完全依赖于文本描述作为指导。在这篇论文中，我们介绍了一种名为VAR-CLIP的新型文本到图像模型，它将自回归视觉技术与CLIP的 capabilities 结合起来。VAR-CLIP框架将描述编码为文本嵌入，这些嵌入随后作为图像生成的文本条件使用。为了在像ImageNet这样的大型数据集上进行训练，我们构建了一个相当大的图像-文本数据集，利用了BLIP2。此外，我们还探讨了CLIP中单词定位的重要性，以用于描述指导的目的。广泛的实验确认了VAR-CLIP的卓越性能，能够生成高质量的图像，并且能够更好地理解和生成描述数据集中的具体图像内容。",
    "tldr": "VAR-CLIP是一个结合了自回归技术和CLIP能力的文本到图像生成器，它能够生成高质量的图像，并对具体图像内容进行更精确的理解和生成描述。"
}