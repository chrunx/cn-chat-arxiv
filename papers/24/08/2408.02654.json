{
    "title": "On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization",
    "abstract": "arXiv:2408.02654v1 Announce Type: cross  Abstract: The effectiveness of training neural networks directly impacts computational costs, resource allocation, and model development timelines in machine learning applications. An optimizer's ability to train the model adequately (in terms of trained model performance) depends on the model's initial weights. Model weight initialization schemes use pseudorandom number generators (PRNGs) as a source of randomness.   We investigate whether substituting PRNGs for low-discrepancy quasirandom number generators (QRNGs) -- namely Sobol' sequences -- as a source of randomness for initializers can improve model performance. We examine Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random Normal",
    "link": "https://arxiv.org/abs/2408.02654",
    "context": "Title: On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization\nAbstract: arXiv:2408.02654v1 Announce Type: cross  Abstract: The effectiveness of training neural networks directly impacts computational costs, resource allocation, and model development timelines in machine learning applications. An optimizer's ability to train the model adequately (in terms of trained model performance) depends on the model's initial weights. Model weight initialization schemes use pseudorandom number generators (PRNGs) as a source of randomness.   We investigate whether substituting PRNGs for low-discrepancy quasirandom number generators (QRNGs) -- namely Sobol' sequences -- as a source of randomness for initializers can improve model performance. We examine Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random Normal",
    "path": "papers/24/08/2408.02654.json",
    "total_tokens": 381,
    "tldr": "该文章通过使用低偏差伪随机数序列在机器学习中的模型初始化中，对标准正则化器和计算库中的常见初始化方法（如Glorot、He、Lecun）进行改进，从而提高了神经网络模型在训练过程中的性能表现。这项研究为优化模型的初始条件从而简化训练过程提供了新的思路和方法。"
}