{
    "title": "Learning Rate-Free Reinforcement Learning: A Case for Model Selection with Non-Stationary Objectives",
    "abstract": "arXiv:2408.04046v1 Announce Type: cross  Abstract: The performance of reinforcement learning (RL) algorithms is sensitive to the choice of hyperparameters, with the learning rate being particularly influential. RL algorithms fail to reach convergence or demand an extensive number of samples when the learning rate is not optimally set. In this work, we show that model selection can help to improve the failure modes of RL that are due to suboptimal choices of learning rate. We present a model selection framework for Learning Rate-Free Reinforcement Learning that employs model selection methods to select the optimal learning rate on the fly. This approach of adaptive learning rate tuning neither depends on the underlying RL algorithm nor the optimizer and solely uses the reward feedback to select the learning rate; hence, the framework can input any RL algorithm and produce a learning rate-free version of it. We conduct experiments for policy optimization methods and evaluate various mode",
    "link": "https://arxiv.org/abs/2408.04046",
    "context": "Title: Learning Rate-Free Reinforcement Learning: A Case for Model Selection with Non-Stationary Objectives\nAbstract: arXiv:2408.04046v1 Announce Type: cross  Abstract: The performance of reinforcement learning (RL) algorithms is sensitive to the choice of hyperparameters, with the learning rate being particularly influential. RL algorithms fail to reach convergence or demand an extensive number of samples when the learning rate is not optimally set. In this work, we show that model selection can help to improve the failure modes of RL that are due to suboptimal choices of learning rate. We present a model selection framework for Learning Rate-Free Reinforcement Learning that employs model selection methods to select the optimal learning rate on the fly. This approach of adaptive learning rate tuning neither depends on the underlying RL algorithm nor the optimizer and solely uses the reward feedback to select the learning rate; hence, the framework can input any RL algorithm and produce a learning rate-free version of it. We conduct experiments for policy optimization methods and evaluate various mode",
    "path": "papers/24/08/2408.04046.json",
    "total_tokens": 327,
    "tldr": "该文章提出了一种无需学习率的强化学习模型选择方法，该方法能够在无需调整RL算法和优化器的前提下，仅通过奖励反馈实现自适应学习率的选择，有望减少学习率选择不当对RL算法性能的影响。"
}