{
    "title": "NOLO: Navigate Only Look Once",
    "abstract": "arXiv:2408.01384v1 Announce Type: new  Abstract: The in-context learning ability of Transformer models has brought new possibilities to visual navigation. In this paper, we focus on the video navigation setting, where an in-context navigation policy needs to be learned purely from videos in an offline manner, without access to the actual environment. For this setting, we propose Navigate Only Look Once (NOLO), a method for learning a navigation policy that possesses the in-context ability and adapts to new scenes by taking corresponding context videos as input without finetuning or re-training. To enable learning from videos, we first propose a pseudo action labeling procedure using optical flow to recover the action label from egocentric videos. Then, offline reinforcement learning is applied to learn the navigation policy. Through extensive experiments on different scenes, we show that our algorithm outperforms baselines by a large margin, which demonstrates the in-context learning a",
    "link": "https://arxiv.org/abs/2408.01384",
    "context": "Title: NOLO: Navigate Only Look Once\nAbstract: arXiv:2408.01384v1 Announce Type: new  Abstract: The in-context learning ability of Transformer models has brought new possibilities to visual navigation. In this paper, we focus on the video navigation setting, where an in-context navigation policy needs to be learned purely from videos in an offline manner, without access to the actual environment. For this setting, we propose Navigate Only Look Once (NOLO), a method for learning a navigation policy that possesses the in-context ability and adapts to new scenes by taking corresponding context videos as input without finetuning or re-training. To enable learning from videos, we first propose a pseudo action labeling procedure using optical flow to recover the action label from egocentric videos. Then, offline reinforcement learning is applied to learn the navigation policy. Through extensive experiments on different scenes, we show that our algorithm outperforms baselines by a large margin, which demonstrates the in-context learning a",
    "path": "papers/24/08/2408.01384.json",
    "total_tokens": 632,
    "translated_title": "NOLO：一次只看导航",
    "translated_abstract": "arXiv:2408.01384v1 宣布类型：新 摘要：Transformer模型中的上下文学习能力为视觉导航带来了新的可能性。在本文中，我们专注于视频导航设置，其中一个人工智能导航策略需要在没有访问实际环境的情况下从视频中纯粹学习，而不是访问实际的资源。对于这种设置，我们认为Navigate Only Look Once（NOLO）是一个方法，用于学习一个具有上下文学习能力的导航策略，它可以通过输入相应的背景视频来适应新的场景，而无需微调或重新训练。为了实现从视频学习，我们首先提出了一种伪动作标注程序，使用光流法从第一人称视频中恢复动作标签。然后，我们使用离线强化学习来学习导航策略。通过在不同场景上的广泛实验，我们表明我们的算法在基线上的表现有了显著的提升，这证明了在无需重新训练或微调的情况下，仅通过观察新场景的视频就能学习到高效的导航策略。",
    "tldr": "NOLO方法通过使用光流和离线强化学习，实现了无需实际环境数据即可自我训练视频导航策略的创新。"
}