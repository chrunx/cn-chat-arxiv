{
    "title": "ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer",
    "abstract": "arXiv:2408.03284v1 Announce Type: new  Abstract: Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers. While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts. In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information. The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer. By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training. Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing prope",
    "link": "https://arxiv.org/abs/2408.03284",
    "context": "Title: ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer\nAbstract: arXiv:2408.03284v1 Announce Type: new  Abstract: Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers. While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts. In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information. The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer. By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training. Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing prope",
    "path": "papers/24/08/2408.03284.json",
    "total_tokens": 357,
    "tldr": "该文章提出了ReSyncer框架，这是一个统一的框架，能够高效地将音频和视频中的面部信息同步。通过重构风格生成器的设计，并将其与3D面部动态预测相结合，文章实现了高保真的音频至视频面部同步，并且该框架支持多种有吸引力的特性。"
}