{
    "title": "Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages",
    "abstract": "arXiv:2408.02044v1 Announce Type: cross  Abstract: The aspect-based sentiment analysis (ABSA) is a standard NLP task with numerous approaches and benchmarks, where large language models (LLM) represent the current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data in underrepresented languages. On such narrow tasks, small tuned language models can often outperform universal large ones, providing available and cheap solutions.   We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for classification of sentiment towards Russia and Ukraine in the context of the ongoing military conflict. The training/testing dataset was obtained from the academic API from Twitter/X during 2023, narrowed to the languages of the V4 countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their performance under a variety of settings including translations, sentiment targets, in-context learning and more, using GPT4 as a reference model. We document several inte",
    "link": "https://arxiv.org/abs/2408.02044",
    "context": "Title: Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages\nAbstract: arXiv:2408.02044v1 Announce Type: cross  Abstract: The aspect-based sentiment analysis (ABSA) is a standard NLP task with numerous approaches and benchmarks, where large language models (LLM) represent the current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data in underrepresented languages. On such narrow tasks, small tuned language models can often outperform universal large ones, providing available and cheap solutions.   We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for classification of sentiment towards Russia and Ukraine in the context of the ongoing military conflict. The training/testing dataset was obtained from the academic API from Twitter/X during 2023, narrowed to the languages of the V4 countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their performance under a variety of settings including translations, sentiment targets, in-context learning and more, using GPT4 as a reference model. We document several inte",
    "path": "papers/24/08/2408.02044.json",
    "total_tokens": 478,
    "tldr": "该文章研究展示了在Twitter/X平台上的多语言文本中，针对东欧V4国家的语言（即捷克语、斯洛伐克语、波兰语和匈牙利语）进行细粒度情感分析时，采用微调的多语言语言模型在理解和分析用户情感表达方面的有效性。通过实验比较了几种著名语言模型的性能，包括BERT、BERTweet、Llama2和Mistral，以及GPT-4作为参考模型，揭示了特定任务上小规模微调模型可能优于通用大型模型的趋势。实验结果表明，针对特定情感分析任务进行微调的语言模型能够提供更为准确和高效的预测，这对于处理在特定背景下（如冲突时期）的复杂语言现象尤为重要。"
}