{
    "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
    "abstract": "arXiv:2408.04631v1 Announce Type: new  Abstract: We present Puppet-Master, an interactive video generative model that can serve as a motion prior for part-level dynamics. At test time, given a single image and a sparse set of motion trajectories (i.e., drags), Puppet-Master can synthesize a video depicting realistic part-level motion faithful to the given drag interactions. This is achieved by fine-tuning a large-scale pre-trained video diffusion model, for which we propose a new conditioning architecture to inject the dragging control effectively. More importantly, we introduce the all-to-first attention mechanism, a drop-in replacement for the widely adopted spatial attention modules, which significantly improves generation quality by addressing the appearance and background issues in existing models. Unlike other motion-conditioned video generators that are trained on in-the-wild videos and mostly move an entire object, Puppet-Master is learned from Objaverse-Animation-HQ, a new dat",
    "link": "https://arxiv.org/abs/2408.04631",
    "context": "Title: Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics\nAbstract: arXiv:2408.04631v1 Announce Type: new  Abstract: We present Puppet-Master, an interactive video generative model that can serve as a motion prior for part-level dynamics. At test time, given a single image and a sparse set of motion trajectories (i.e., drags), Puppet-Master can synthesize a video depicting realistic part-level motion faithful to the given drag interactions. This is achieved by fine-tuning a large-scale pre-trained video diffusion model, for which we propose a new conditioning architecture to inject the dragging control effectively. More importantly, we introduce the all-to-first attention mechanism, a drop-in replacement for the widely adopted spatial attention modules, which significantly improves generation quality by addressing the appearance and background issues in existing models. Unlike other motion-conditioned video generators that are trained on in-the-wild videos and mostly move an entire object, Puppet-Master is learned from Objaverse-Animation-HQ, a new dat",
    "path": "papers/24/08/2408.04631.json",
    "total_tokens": 501,
    "tldr": "该文章开发了一种名为Puppet-Master的交互式视频生成模型，该模型能够作为部分动态的先验知识。在测试阶段，仅凭一张图像和部分运动轨迹（即拖动操作），该模型能够生成一个视频，其中描绘的现实主义部分动态忠实地反映了给定的拖动操作。这种能力是通过对大型预训练视频扩散模型进行微调实现的，并且提出了一个新式的条件化架构，以有效注入拖动控制。此外，作者还引入了一种全部到第一次的注意力机制，这是一种空间注意力模块的替代方案，它通过解决现有模型中出现的视觉外观和背景问题，显著提高了生成质量。与训练于自然场景视频且主要移动整个物体的其他运动条件视频生成器不同，Puppet-Master是在新构建的Objaverse-Animation-HQ数据集上进行训练的，该数据集专门用于动作和动画的高质量视频数据。"
}