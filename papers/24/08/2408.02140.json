{
    "title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces",
    "abstract": "arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of",
    "link": "https://arxiv.org/abs/2408.02140",
    "context": "Title: VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces\nAbstract: arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of",
    "path": "papers/24/08/2408.02140.json",
    "total_tokens": 404,
    "tldr": "该文章提出了一个使用SHAP（SHapley Additive exPlanations）增强的GAN生成方法，能够在高维空间中高效提取黑箱模型的解释性，并且对视频分类的模型平均提升效果高达26.11%，并在一些挑战性数据集上实现了最大33.36%的提升，证明了该方法在理解和改进黑箱模型方面的巨大潜力。"
}