{
    "title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces",
    "abstract": "arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of",
    "link": "https://arxiv.org/abs/2408.02140",
    "context": "Title: VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces\nAbstract: arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of",
    "path": "papers/24/08/2408.02140.json",
    "total_tokens": 716,
    "translated_title": "VidModEx: 一种可解释且高效的黑色盒子模型提取方法，适用于高维空间",
    "translated_abstract": "arXiv:2408.02140v1 宣布类型：新  摘要：在黑箱模型提取领域，依赖于软标签或替代数据集的传统方法在扩展到高维输入空间和处理大量相关联的类别的复杂性方面遇到了挑战。在这项工作中，我们介绍了一种新的方法，该方法利用SHAP（SHapley Additive exPlanations）来增强合成数据生成。SHAP量化了每个输入特征对受害者模型的输出所做的个体贡献，这有助于优化一个基于能量的生成对抗网络以达到一个理想的输出。这种方法显著提升了性能，在图像分类模型上的准确度提升了16.45%，并且将这种方法扩展到了视频分类模型，在UCF11、UCF101、Kinetics 400、Kinetics 600和Something-Something V2等难以挑战的数据集上，平均提升了26.11%，最高提升了33.36%。我们进一步展示了这种方法的有效性和其实际实用性。",
    "tldr": "VidModEx方法使用SHAP增强了合成数据生成，提高了图像和视频分类模型的性能，在多个高维数据集上取得了显著的提升。",
    "en_tdlr": "Our VidModEx method, which enhances synthetic data generation through SHAP, significantly improves the accuracy of both image and video classification models, offering notable enhancements across various high-dimensional datasets."
}