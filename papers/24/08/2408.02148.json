{
    "title": "Environment Complexity and Nash Equilibria in a Sequential Social Dilemma",
    "abstract": "arXiv:2408.02148v1 Announce Type: cross  Abstract: Multi-agent reinforcement learning (MARL) methods, while effective in zero-sum or positive-sum games, often yield suboptimal outcomes in general-sum games where cooperation is essential for achieving globally optimal outcomes. Matrix game social dilemmas, which abstract key aspects of general-sum interactions, such as cooperation, risk, and trust, fail to model the temporal and spatial dynamics characteristic of real-world scenarios. In response, our study extends matrix game social dilemmas into more complex, higher-dimensional MARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma to more closely match the decision-space of a one-shot matrix game while also introducing variable environment complexity. Our findings indicate that as complexity increases, MARL agents trained in these environments converge to suboptimal strategies, consistent with the risk-dominant Nash equilibria strategies found in matrix games",
    "link": "https://arxiv.org/abs/2408.02148",
    "context": "Title: Environment Complexity and Nash Equilibria in a Sequential Social Dilemma\nAbstract: arXiv:2408.02148v1 Announce Type: cross  Abstract: Multi-agent reinforcement learning (MARL) methods, while effective in zero-sum or positive-sum games, often yield suboptimal outcomes in general-sum games where cooperation is essential for achieving globally optimal outcomes. Matrix game social dilemmas, which abstract key aspects of general-sum interactions, such as cooperation, risk, and trust, fail to model the temporal and spatial dynamics characteristic of real-world scenarios. In response, our study extends matrix game social dilemmas into more complex, higher-dimensional MARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma to more closely match the decision-space of a one-shot matrix game while also introducing variable environment complexity. Our findings indicate that as complexity increases, MARL agents trained in these environments converge to suboptimal strategies, consistent with the risk-dominant Nash equilibria strategies found in matrix games",
    "path": "papers/24/08/2408.02148.json",
    "total_tokens": 363,
    "tldr": "该文章创新性地将矩阵游戏社交困境扩展到多agent强化学习（MARL）环境中，并在其中加入了环境复杂性，从而更好地模拟真实世界的动态决策问题。研究结果表明，随着环境的复杂性增加，MARL策略趋向于收敛到风险主导的纳什均衡，这为解决一般性游戏中的合作问题提供了新的视角。"
}