{
    "title": "GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation",
    "abstract": "arXiv:2402.15745v2 Announce Type: replace-cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs ha",
    "link": "https://arxiv.org/abs/2402.15745",
    "context": "Title: GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation\nAbstract: arXiv:2402.15745v2 Announce Type: replace-cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs ha",
    "path": "papers/24/02/2402.15745.json",
    "total_tokens": 445,
    "tldr": "该文章提出GAOKAO-MM，一个基于中文高考的全新多模态评估基准，它要求模型展现出对图片、图表、函数图和其他10种类型图片的正确理解和分析能力，同时要求模型具有推理和处理高级认知任务的能力。在GAOKAO-MM基准中，10个大型视觉语言模型（LVLMs）表现不佳，它们的正确率都低于50%。这表明现有的LVLMs在处理复杂多模态任务方面仍有较大的提升空间。"
}