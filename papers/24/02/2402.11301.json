{
    "title": "ReViT: Enhancing Vision Transformers Feature Diversity with Attention Residual Connections",
    "abstract": "arXiv:2402.11301v2 Announce Type: replace  Abstract: Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on th",
    "link": "https://arxiv.org/abs/2402.11301",
    "context": "Title: ReViT: Enhancing Vision Transformers Feature Diversity with Attention Residual Connections\nAbstract: arXiv:2402.11301v2 Announce Type: replace  Abstract: Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on th",
    "path": "papers/24/02/2402.11301.json",
    "total_tokens": 322,
    "tldr": "该文章提出了一种名为ReViT的改进型Vision Transformer架构，通过引入注意力残差连接来增强模型在更深层级的特征多样性，从而提升模型的准确性和鲁棒性。"
}