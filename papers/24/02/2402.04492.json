{
    "title": "ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation",
    "abstract": "arXiv:2402.04492v2 Announce Type: replace  Abstract: This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to cha",
    "link": "https://arxiv.org/abs/2402.04492",
    "context": "Title: ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation\nAbstract: arXiv:2402.04492v2 Announce Type: replace  Abstract: This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to cha",
    "path": "papers/24/02/2402.04492.json",
    "total_tokens": 486,
    "tldr": "该文章引入了ColorSwap数据集，旨在通过评估和改善 multimodal 模型在不同对象与颜色匹配方面的性能来推动这一领域的研究发展。通过自动化生成题目和图像以及人类后续的校对，这一数据集包含了2,000组图片和句子，每组包含原始的图片和句子以及颜色对调的配对，从而为模型在颜色感知和理解上的准确性和灵活性提供了挑战。即便使用了诸如 GPT-4V 和 LLaVA 等较先进的模型，在处理这类问题时模型的表现也只能达到72%和42%，而且通过更高级的提示技术可能会进一步提升。此外，对比性模型诸如 CLIP 和 SigLIP 在处理这类任务方面的表现也得到了研究。"
}