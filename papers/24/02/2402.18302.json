{
    "title": "EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving",
    "abstract": "arXiv:2402.18302v2 Announce Type: replace-cross  Abstract: This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from ",
    "link": "https://arxiv.org/abs/2402.18302",
    "context": "Title: EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving\nAbstract: arXiv:2402.18302v2 Announce Type: replace-cross  Abstract: This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from ",
    "path": "papers/24/02/2402.18302.json",
    "total_tokens": 383,
    "tldr": "该文章提出了一种名为EchoTrack的端到端框架，用于在自动驾驶场景中的音频指代多对象跟踪（AR-MOT）。EchoTrack通过双向频率域跨注意力融合模块（Bi-FCFM）实现了音频和视频特征的双流端到端融合，解决了传统音频视频融合方法和文本依赖的多对象跟踪在应用中的局限性。"
}