{
    "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
    "abstract": "arXiv:2402.08831v2 Announce Type: replace-cross  Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-",
    "link": "https://arxiv.org/abs/2402.08831",
    "context": "Title: eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data\nAbstract: arXiv:2402.08831v2 Announce Type: replace-cross  Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-",
    "path": "papers/24/02/2402.08831.json",
    "total_tokens": 519,
    "tldr": "研究成果 I - 科学汇 [arXiv:2311.12324v1]\nTitle: Orthogonal Learning for Multi-Task and Domain Generalization\nAbstract: 本文提出了一种新的策略，通过引入正交约束优化多任务和学习过程中的域泛化问题。这种方法能在多个不同领域的数据集上取得更好的泛化能力，适用于复杂任务的解决。工作结果表明，正交约束能够促进模型在复杂多变的数据集上保持更好的性能。我们还在多个公开的数据集上进行了实验，实验结果表明，基于正交学习的模型在跨域迁移学习任务上均有显著的优势。\n\n该文章构建了一种新的正交学习策略，通过引入正交约束优化多任务和学习过程中的域泛化问题，从而在多个不同领域的数据集上取得更好的泛化能力。实验表明，该方法在复杂多变的数据集上保持良好的性能，并在跨域迁移学习任务上表现出了显著的优势。"
}