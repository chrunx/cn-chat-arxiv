{
    "title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition",
    "abstract": "arXiv:2407.15706v5 Announce Type: replace  Abstract: Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with",
    "link": "https://arxiv.org/abs/2407.15706",
    "context": "Title: Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition\nAbstract: arXiv:2407.15706v5 Announce Type: replace  Abstract: Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with",
    "path": "papers/24/07/2407.15706.json",
    "total_tokens": 374,
    "tldr": "该文章提出了一种名为多模态共学习（MMCL）的框架，用于在训练阶段通过结合多种模态特征进行共学习，并在推理阶段只使用简洁的骨骼信息。这种方法整合了大型语言模型来辅助骨骼动作识别的训练，以充分利用不同模态特征之间的互补性，同时保持了系统的效率。通过这种方式，该研究旨在提高骨骼动作识别的性能，并减少对其他详细身体信息的依赖。"
}