{
    "title": "Self-supervised visual learning from interactions with objects",
    "abstract": "arXiv:2407.06704v2 Announce Type: replace  Abstract: Self-supervised learning (SSL) has revolutionized visual representation learning, but has not achieved the robustness of human vision. A reason for this could be that SSL does not leverage all the data available to humans during learning. When learning about an object, humans often purposefully turn or move around objects and research suggests that these interactions can substantially enhance their learning. Here we explore whether such object-related actions can boost SSL. For this, we extract the actions performed to change from one ego-centric view of an object to another in four video datasets. We then introduce a new loss function to learn visual and action embeddings by aligning the performed action with the representations of two images extracted from the same clip. This permits the performed actions to structure the latent visual representation. Our experiments show that our method consistently outperforms previous methods on",
    "link": "https://arxiv.org/abs/2407.06704",
    "context": "Title: Self-supervised visual learning from interactions with objects\nAbstract: arXiv:2407.06704v2 Announce Type: replace  Abstract: Self-supervised learning (SSL) has revolutionized visual representation learning, but has not achieved the robustness of human vision. A reason for this could be that SSL does not leverage all the data available to humans during learning. When learning about an object, humans often purposefully turn or move around objects and research suggests that these interactions can substantially enhance their learning. Here we explore whether such object-related actions can boost SSL. For this, we extract the actions performed to change from one ego-centric view of an object to another in four video datasets. We then introduce a new loss function to learn visual and action embeddings by aligning the performed action with the representations of two images extracted from the same clip. This permits the performed actions to structure the latent visual representation. Our experiments show that our method consistently outperforms previous methods on",
    "path": "papers/24/07/2407.06704.json",
    "total_tokens": 379,
    "tldr": "该文章提出了一种新的自我监督学习方法，通过模拟人类观察物体时所进行的动作，即改变观察角度的动作，来增强视觉表示学习的鲁棒性。这种方法通过提取视频数据集中的动作信息，并开发了一个新的损失函数来学习视觉和动作嵌入，使得动作与两个图像的表示进行对齐，从而将动作有意安排进隐含的视觉表示中。实验结果表明，该方法在多个视频数据集上均显著优于现有方法，表明了动作信息在自我监督学习中的积极作用。"
}