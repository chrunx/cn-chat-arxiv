{
    "title": "MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image Relational Association Capabilities in Large Visual Language Models",
    "abstract": "arXiv:2407.17379v2 Announce Type: replace  Abstract: Given the remarkable success that large visual language models (LVLMs) have achieved in image perception tasks, the endeavor to make LVLMs perceive the world like humans is drawing increasing attention. Current multi-modal benchmarks primarily focus on facts or specific topic-related knowledge contained within individual images. However, they often overlook the associative relations between multiple images, which require the identification and analysis of similarities among entities or content present in different images. Therefore, we propose the multi-image relation association task and a meticulously curated Multi-granularity Multi-image Relational Association (MMRA) benchmark, comprising 1,024 samples. In order to systematically and comprehensively evaluate current LVLMs, we establish an associational relation system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent) at two granularity levels (i.e., image and ",
    "link": "https://arxiv.org/abs/2407.17379",
    "context": "Title: MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image Relational Association Capabilities in Large Visual Language Models\nAbstract: arXiv:2407.17379v2 Announce Type: replace  Abstract: Given the remarkable success that large visual language models (LVLMs) have achieved in image perception tasks, the endeavor to make LVLMs perceive the world like humans is drawing increasing attention. Current multi-modal benchmarks primarily focus on facts or specific topic-related knowledge contained within individual images. However, they often overlook the associative relations between multiple images, which require the identification and analysis of similarities among entities or content present in different images. Therefore, we propose the multi-image relation association task and a meticulously curated Multi-granularity Multi-image Relational Association (MMRA) benchmark, comprising 1,024 samples. In order to systematically and comprehensively evaluate current LVLMs, we establish an associational relation system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent) at two granularity levels (i.e., image and ",
    "path": "papers/24/07/2407.17379.json",
    "total_tokens": 398,
    "tldr": "该文章提出MMRA (Multi-Granularity and Multi-Image Relational Association) 基准，用于评估大型视觉语言模型在多维度和多图像关系关联方面的能力。通过精心编制的1024个样本，文章推动了图像间关联关系的任务研究，通过11个不同层次的子任务，如“使用相似性”和“子事件”，对当前的视觉语言模型进行全面的评估。"
}