{
    "title": "Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets",
    "abstract": "arXiv:2407.19394v3 Announce Type: replace  Abstract: The Vision Transformer (ViT) leverages the Transformer's encoder to capture global information by dividing images into patches and achieves superior performance across various computer vision tasks. However, the self-attention mechanism of ViT captures the global context from the outset, overlooking the inherent relationships between neighboring pixels in images or videos. Transformers mainly focus on global information while ignoring the fine-grained local details. Consequently, ViT lacks inductive bias during image or video dataset training. In contrast, convolutional neural networks (CNNs), with their reliance on local filters, possess an inherent inductive bias, making them more efficient and quicker to converge than ViT with less data. In this paper, we present a lightweight Depth-Wise Convolution module as a shortcut in ViT models, bypassing entire Transformer blocks to ensure the models capture both local and global informatio",
    "link": "https://arxiv.org/abs/2407.19394",
    "context": "Title: Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets\nAbstract: arXiv:2407.19394v3 Announce Type: replace  Abstract: The Vision Transformer (ViT) leverages the Transformer's encoder to capture global information by dividing images into patches and achieves superior performance across various computer vision tasks. However, the self-attention mechanism of ViT captures the global context from the outset, overlooking the inherent relationships between neighboring pixels in images or videos. Transformers mainly focus on global information while ignoring the fine-grained local details. Consequently, ViT lacks inductive bias during image or video dataset training. In contrast, convolutional neural networks (CNNs), with their reliance on local filters, possess an inherent inductive bias, making them more efficient and quicker to converge than ViT with less data. In this paper, we present a lightweight Depth-Wise Convolution module as a shortcut in ViT models, bypassing entire Transformer blocks to ensure the models capture both local and global informatio",
    "path": "papers/24/07/2407.19394.json",
    "total_tokens": 699,
    "translated_title": "论文标题：在小型数据集上进行有效训练的视觉变换器的深度卷积",
    "translated_abstract": "arXiv:2407.19394v3 公告类型：替换 摘要：视觉变换器（ViT）通过将图像划分为图块并使用Transformer的编码器来捕获全局信息，在各种计算机视觉任务中取得了优异的表现。然而，ViT的自注意力机制从一开始就捕捉全局上下文，忽略了图像或视频中相邻像素之间固有的关系。变换器主要注重全局信息而忽视了图像或视频数据的精细局部细节。因此，在仅用少量数据对图像或视频数据集进行训练时，ViT缺乏归纳偏见。相比之下，卷积神经网络（CNNs）依靠局部滤波器，具有固有的归纳偏见，使其在训练时的效率和收敛速度比ViT更快，并且需要的数据更少。本文提出了一种轻量级的深度卷积模块，作为ViT模型中的捷径，绕过整个Transformer块，以确保模型能够捕捉到局部和全局信息。",
    "tldr": "该论文提出了一种轻量级的深度卷积模块，将其集成到ViT模型中，以提高在小型数据集上的训练效率，同时捕捉图像的局部和全局信息，从而增强了模型的归纳偏差，提高了训练效率和泛化能力。"
}