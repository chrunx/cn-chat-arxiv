{
    "title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models",
    "abstract": "arXiv:2407.00569v4 Announce Type: replace  Abstract: Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not ",
    "link": "https://arxiv.org/abs/2407.00569",
    "context": "Title: Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models\nAbstract: arXiv:2407.00569v4 Announce Type: replace  Abstract: Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not ",
    "path": "papers/24/07/2407.00569.json",
    "total_tokens": 372,
    "tldr": "该文章提出了一种名为MMHalSnowball的框架，用于评估大型视觉-语言模型在面对之前生成的幻觉时是否会受到误导。实验表明，在遇到幻觉相关的查询时，开源的视觉-语言模型性能下降了至少31%，表明这些模型倾向于接受生成的幻觉并作出错误的陈述。"
}