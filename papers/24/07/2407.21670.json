{
    "title": "Universal Approximation Theory: Foundations for Parallelism in Neural Networks",
    "abstract": "arXiv:2407.21670v2 Announce Type: replace-cross  Abstract: Neural networks are increasingly evolving towards training large models with big data, a method that has demonstrated superior performance across many tasks. However, this approach introduces an urgent problem: current deep learning models are predominantly serial, meaning that as the number of network layers increases, so do the training and inference times. This is unacceptable if deep learning is to continue advancing. Therefore, this paper proposes a deep learning parallelization strategy based on the Universal Approximation Theorem (UAT). From this foundation, we designed a parallel network called Para-Former to test our theory. Unlike traditional serial models, the inference time of Para-Former does not increase with the number of layers, significantly accelerating the inference speed of multi-layer networks. Experimental results validate the effectiveness of this network.",
    "link": "https://arxiv.org/abs/2407.21670",
    "context": "Title: Universal Approximation Theory: Foundations for Parallelism in Neural Networks\nAbstract: arXiv:2407.21670v2 Announce Type: replace-cross  Abstract: Neural networks are increasingly evolving towards training large models with big data, a method that has demonstrated superior performance across many tasks. However, this approach introduces an urgent problem: current deep learning models are predominantly serial, meaning that as the number of network layers increases, so do the training and inference times. This is unacceptable if deep learning is to continue advancing. Therefore, this paper proposes a deep learning parallelization strategy based on the Universal Approximation Theorem (UAT). From this foundation, we designed a parallel network called Para-Former to test our theory. Unlike traditional serial models, the inference time of Para-Former does not increase with the number of layers, significantly accelerating the inference speed of multi-layer networks. Experimental results validate the effectiveness of this network.",
    "path": "papers/24/07/2407.21670.json",
    "total_tokens": 317,
    "tldr": "该文章提出了一种基于通用逼近理论的深度学习并行化策略，设计了名为Para-Former的并行网络架构，能够在不增加网络层数情况下加速多层网络的推理速度，验证了该网络的有效性。"
}