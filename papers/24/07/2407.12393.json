{
    "title": "PersLLM: A Personified Training Approach for Large Language Models",
    "abstract": "arXiv:2407.12393v4 Announce Type: replace-cross  Abstract: Large language models exhibit aspects of human-level intelligence that catalyze their application as human-like agents in domains such as social simulations, human-machine interactions, and collaborative multi-agent systems. However, the absence of distinct personalities, such as displaying ingratiating behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs utility in practical applications. Addressing this, the development of personality traits in LLMs emerges as a crucial area of research to unlock their latent potential. Existing methods to personify LLMs generally involve strategies like employing stylized training data for instruction tuning or using prompt engineering to simulate different personalities. These methods only capture superficial linguistic styles instead of the core of personalities and are therefore not stable. In this study, we propose PersLLM, integrating psychology-grounded princi",
    "link": "https://arxiv.org/abs/2407.12393",
    "context": "Title: PersLLM: A Personified Training Approach for Large Language Models\nAbstract: arXiv:2407.12393v4 Announce Type: replace-cross  Abstract: Large language models exhibit aspects of human-level intelligence that catalyze their application as human-like agents in domains such as social simulations, human-machine interactions, and collaborative multi-agent systems. However, the absence of distinct personalities, such as displaying ingratiating behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs utility in practical applications. Addressing this, the development of personality traits in LLMs emerges as a crucial area of research to unlock their latent potential. Existing methods to personify LLMs generally involve strategies like employing stylized training data for instruction tuning or using prompt engineering to simulate different personalities. These methods only capture superficial linguistic styles instead of the core of personalities and are therefore not stable. In this study, we propose PersLLM, integrating psychology-grounded princi",
    "path": "papers/24/07/2407.12393.json",
    "total_tokens": 329,
    "tldr": "关键句：该文章提出了一种名为PersLLM的训练方法，它为大型语言模型赋予了心理学意义上的性格特征，从而能够呈现出更加多样化和真实的交互行为，这使得LLMs在社交模拟、人机交互以及多 agent系统等应用领域更具实用性。"
}