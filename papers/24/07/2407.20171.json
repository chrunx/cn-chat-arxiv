{
    "title": "Diffusion Feedback Helps CLIP See Better",
    "abstract": "arXiv:2407.20171v2 Announce Type: replace  Abstract: Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedb",
    "link": "https://arxiv.org/abs/2407.20171",
    "context": "Title: Diffusion Feedback Helps CLIP See Better\nAbstract: arXiv:2407.20171v2 Announce Type: replace  Abstract: Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedb",
    "path": "papers/24/07/2407.20171.json",
    "total_tokens": 364,
    "tldr": "该文章提出了一种通过自我监督的扩散过程训练CLIP模型以克服其视觉短板的简单方法。通过引入DIVA（DIffusion model as a Visual Assistant for CLIP），本文展示了使用扩散模型作为CLIP的视觉助手，显著提升了CLIP在视觉感知方面的性能。"
}