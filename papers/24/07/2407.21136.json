{
    "title": "Adding Multimodal Controls to Whole-body Human Motion Generation",
    "abstract": "arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-",
    "link": "https://arxiv.org/abs/2407.21136",
    "context": "Title: Adding Multimodal Controls to Whole-body Human Motion Generation\nAbstract: arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-",
    "path": "papers/24/07/2407.21136.json",
    "total_tokens": 605,
    "translated_title": "在整个人体运动生成中添加多模态控制",
    "translated_abstract": "arXiv:2407.21136v2 新闻类型：替换 摘要：整个人体多模态运动生成，通过文本，语音或音乐控制，具有多种应用，包括视频生成和角色动画。然而，使用统一模型来实现不同条件模式的各种生成任务有两个主要挑战：在不同生成场景之间运动分布会发生偏移，并且对于具有不同粒度的混合条件进行优化是一个复杂的问题。此外，现有数据集中不一致的运动格式进一步阻碍了有效多模态运动生成。在此论文中，我们提出ControlMM，一个统一框架，用于在多模态运动生成的可插拔方式中控制整个人体运动。为了有效地学习和在不同的运动分布之间转移运动知识，我们提出了ControlMM-Attn，用于并行建模静态和动态的人类拓扑图。为了处理具有不同粒度的条件，ControlMM employs a coarse-grained",
    "tldr": "我们提出了一种名为ControlMM的框架，它使用统一的模型来同时控制整个身体的运动生成，并处理不同的条件模式，包括文本、语音和音乐。ControlMM解决了运动分布在不同生成任务中的偏移问题，同时通过使用一种粗粒度的方法来处理不同粒度的条件。"
}