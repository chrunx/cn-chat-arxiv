{
    "title": "Adding Multimodal Controls to Whole-body Human Motion Generation",
    "abstract": "arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-",
    "link": "https://arxiv.org/abs/2407.21136",
    "context": "Title: Adding Multimodal Controls to Whole-body Human Motion Generation\nAbstract: arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-",
    "path": "papers/24/07/2407.21136.json",
    "total_tokens": 374,
    "tldr": "该文章创新性地提出了一个名为ControlMM的统一框架，用于控制整个人体动作的多种模式生成，该框架能够通过文本、语音或音乐等多种输入方式进行控制。文章还介绍了一种名为ControlMM-Attn的新技术，用于同时处理人体的静态和动态拓扑图，从而能够在不同类型的输入条件之间有效学习和转移动作知识。此外，文章还提出了解决输入条件差异化的问题，确保了在多种复杂条件下的优化处理，为视频生成和人动画设计提供了新的可能性。"
}