{
    "title": "A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image Fusion",
    "abstract": "arXiv:2407.06159v2 Announce Type: replace  Abstract: Multi-modality image fusion aims at fusing specific-modality and shared-modality information from two source images. To tackle the problem of insufficient feature extraction and lack of semantic awareness for complex scenes, this paper focuses on how to model correlation-driven decomposing features and reason high-level graph representation by efficiently extracting complementary features and multi-guided feature aggregation. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. The transformer with Multi-Dconv Transposed Attention and Local-enhanced Feed Forward network is used to extract shallow features after the depthwise convolution. In the three parallel branches encoder, Cross Attention and Invertible Block (CAI) enables to extract local features and preserve high-frequency texture details. Base feature extraction module (BFE) with residual connections can capture",
    "link": "https://arxiv.org/abs/2407.06159",
    "context": "Title: A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image Fusion\nAbstract: arXiv:2407.06159v2 Announce Type: replace  Abstract: Multi-modality image fusion aims at fusing specific-modality and shared-modality information from two source images. To tackle the problem of insufficient feature extraction and lack of semantic awareness for complex scenes, this paper focuses on how to model correlation-driven decomposing features and reason high-level graph representation by efficiently extracting complementary features and multi-guided feature aggregation. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. The transformer with Multi-Dconv Transposed Attention and Local-enhanced Feed Forward network is used to extract shallow features after the depthwise convolution. In the three parallel branches encoder, Cross Attention and Invertible Block (CAI) enables to extract local features and preserve high-frequency texture details. Base feature extraction module (BFE) with residual connections can capture",
    "path": "papers/24/07/2407.06159.json",
    "total_tokens": 331,
    "tldr": "该文章提出了一个具有多重引导的神经网络，该网络对于融合远红外和可见光图像具有语义感知能力，通过优化特征提取和多引导特征融合，使得模型能够更有效地处理复杂场景下的信息融合问题。"
}