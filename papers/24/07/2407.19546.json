{
    "title": "XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training",
    "abstract": "arXiv:2407.19546v2 Announce Type: replace  Abstract: Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modelling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes a XLIP (Masked modelling for medical Language-Image Pre-training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modelling (AttMIM) and entity-driven masked language modelling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interacti",
    "link": "https://arxiv.org/abs/2407.19546",
    "context": "Title: XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training\nAbstract: arXiv:2407.19546v2 Announce Type: replace  Abstract: Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modelling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes a XLIP (Masked modelling for medical Language-Image Pre-training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modelling (AttMIM) and entity-driven masked language modelling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interacti",
    "path": "papers/24/07/2407.19546.json",
    "total_tokens": 676,
    "translated_title": "XLIP: 跨模态注意屏蔽建模方法用于医疗语言图像预训练",
    "translated_abstract": "arXiv:2407.19546v2 公告类型：替换 摘要：医学领域的视觉语言预训练（VLP）使用对比学习在图像文本对上，实现跨任务的有效转移。然而，当应用于医疗领域时，基于遮罩建模策略的当前VLP方法面临两个挑战。首先，由于医学数据稀缺，当前模型难以准确重建病理性特征。其次，大多数方法只采用图像文本对或仅图像数据，未能利用两者之间的数据组合。为此，本文提出XLIP（医疗语言图像预训练的遮罩建模方法）框架，通过未配对数据增强病理学习和特征学习。首先，我们引入了注意遮罩图像建模（AttMIM）和方法驱动的遮罩语言建模模块（EntMLM），这些模块通过多模态特征交互学习重建病理性视觉和文本 tokens。",
    "tldr": "本文提出XLIP框架，通过未配对数据增强病理学习和特征学习，引入了注意遮罩图像建模和目标驱动的遮罩语言建模模块，有效提高了病理特征的准确重建。"
}