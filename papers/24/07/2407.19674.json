{
    "title": "Advancing Prompt Learning through an External Layer",
    "abstract": "arXiv:2407.19674v3 Announce Type: replace  Abstract: Prompt learning represents a promising method for adapting pre-trained vision-language models (VLMs) to various downstream tasks by learning a set of text embeddings. One challenge inherent to these methods is the poor generalization performance due to the invalidity of the learned text embeddings for unseen tasks. A straightforward approach to bridge this gap is to freeze the text embeddings in prompts, which results in a lack of capacity to adapt VLMs for downstream tasks. To address this dilemma, we propose a paradigm called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a textual external layer and learnable visual embeddings for adapting VLMs to downstream tasks. The learnable external layer is built upon valid embeddings of pre-trained CLIP. This design considers the balance of learning capabilities between the two branches. To align the textual and visual features, we propose a novel two-pronged approach",
    "link": "https://arxiv.org/abs/2407.19674",
    "context": "Title: Advancing Prompt Learning through an External Layer\nAbstract: arXiv:2407.19674v3 Announce Type: replace  Abstract: Prompt learning represents a promising method for adapting pre-trained vision-language models (VLMs) to various downstream tasks by learning a set of text embeddings. One challenge inherent to these methods is the poor generalization performance due to the invalidity of the learned text embeddings for unseen tasks. A straightforward approach to bridge this gap is to freeze the text embeddings in prompts, which results in a lack of capacity to adapt VLMs for downstream tasks. To address this dilemma, we propose a paradigm called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a textual external layer and learnable visual embeddings for adapting VLMs to downstream tasks. The learnable external layer is built upon valid embeddings of pre-trained CLIP. This design considers the balance of learning capabilities between the two branches. To align the textual and visual features, we propose a novel two-pronged approach",
    "path": "papers/24/07/2407.19674.json",
    "total_tokens": 405,
    "tldr": "该文章提出了一种名为EnPrompt的框架，通过在VLMs（预训练视觉-语言模型）中集成文本外部层（EnLa），以增加模型的泛化能力。EnLa包含了预训练CLIP的有效文本嵌入和可学习的视觉嵌入，旨在为下游任务提供更好的适应性。通过这种方式，文章旨在平衡视觉和文本分支的学习能力，并通过一种独特的协同学习机制来增强它们之间的交互作用。这种设计允许模型在保持灵活性的同时，能够学习到更加有效的任务特异性文本嵌入，从而提升在各种视觉相关任务上的性能。"
}