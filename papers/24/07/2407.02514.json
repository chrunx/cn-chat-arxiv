{
    "title": "LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations",
    "abstract": "arXiv:2407.02514v3 Announce Type: replace-cross  Abstract: In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. Although recent works have started to employ formal languages as an intermediate representation for reasoning tasks, they often face challenges in accurately generating and refining these formal specifications to ensure correctness. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and other contemporary techniques across natural language reasoning tasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.",
    "link": "https://arxiv.org/abs/2407.02514",
    "context": "Title: LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations\nAbstract: arXiv:2407.02514v3 Announce Type: replace-cross  Abstract: In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. Although recent works have started to employ formal languages as an intermediate representation for reasoning tasks, they often face challenges in accurately generating and refining these formal specifications to ensure correctness. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and other contemporary techniques across natural language reasoning tasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.",
    "path": "papers/24/07/2407.02514.json",
    "total_tokens": 404,
    "tldr": "该文章提出了Logic-LM++，这是对Logic-LM 的一项改进，它在生成和精炼形式化表达方面解决了大型语言模型（LLMs）的局限性，通过使用LLM进行成对比较的能力来评估所建议的改进。Logic-LM++在三个自然语言推理数据集上显著优于Logic-LM和其他当前方法：FOLIO、ProofWriter和AR-LSAT，在标准提示、链式想法提示以及Logic-LM上的平均改进率分别为18.5%、12.3%和5%。"
}