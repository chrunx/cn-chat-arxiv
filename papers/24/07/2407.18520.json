{
    "title": "Text-Region Matching for Multi-Label Image Recognition with Missing Labels",
    "abstract": "arXiv:2407.18520v2 Announce Type: replace  Abstract: Recently, large-scale visual language pre-trained (VLP) models have demonstrated impressive performance across various downstream tasks. Motivated by these advancements, pioneering efforts have emerged in multi-label image recognition with missing labels, leveraging VLP prompt-tuning technology. However, they usually cannot match text and vision features well, due to complicated semantics gaps and missing labels in a multi-label image. To tackle this challenge, we propose \\textbf{T}ext-\\textbf{R}egion \\textbf{M}atching for optimizing \\textbf{M}ulti-\\textbf{L}abel prompt tuning, namely TRM-ML, a novel method for enhancing meaningful cross-modal matching. Compared to existing methods, we advocate exploring the information of category-aware regions rather than the entire image or pixels, which contributes to bridging the semantic gap between textual and visual representations in a one-to-one matching manner. Concurrently, we further int",
    "link": "https://arxiv.org/abs/2407.18520",
    "context": "Title: Text-Region Matching for Multi-Label Image Recognition with Missing Labels\nAbstract: arXiv:2407.18520v2 Announce Type: replace  Abstract: Recently, large-scale visual language pre-trained (VLP) models have demonstrated impressive performance across various downstream tasks. Motivated by these advancements, pioneering efforts have emerged in multi-label image recognition with missing labels, leveraging VLP prompt-tuning technology. However, they usually cannot match text and vision features well, due to complicated semantics gaps and missing labels in a multi-label image. To tackle this challenge, we propose \\textbf{T}ext-\\textbf{R}egion \\textbf{M}atching for optimizing \\textbf{M}ulti-\\textbf{L}abel prompt tuning, namely TRM-ML, a novel method for enhancing meaningful cross-modal matching. Compared to existing methods, we advocate exploring the information of category-aware regions rather than the entire image or pixels, which contributes to bridging the semantic gap between textual and visual representations in a one-to-one matching manner. Concurrently, we further int",
    "path": "papers/24/07/2407.18520.json",
    "total_tokens": 368,
    "tldr": "该文章提出了一种名为TRM-ML的新方法，该方法通过优化多标签提示 tuning，在多标签图像中有效地匹配文本和视觉特征，特别是通过探索类别特定区域的额外信息，而不是整个图像或像素信息，这有助于在一种一对一匹配方式中弥合文本和视觉表示之间的语义差距。"
}