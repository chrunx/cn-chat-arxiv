{
    "title": "DiffX: Guide Your Layout to Cross-Modal Generative Modeling",
    "abstract": "arXiv:2407.15488v3 Announce Type: replace  Abstract: Diffusion models have made significant strides in language-driven and layout-driven image generation. However, most diffusion models are limited to visible RGB image generation. In fact, human perception of the world is enriched by diverse viewpoints, such as chromatic contrast, thermal illumination, and depth information. In this paper, we introduce a novel diffusion model for general layout-guided cross-modal generation, called DiffX. Notably, DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space. Moreover, we introduce the Joint-Modality Embedder (JME) to enhance interaction between layout and text conditions by incorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is employed for long caption embedding for user instruction. To facilitate the user-instructed generative training, we construct the cro",
    "link": "https://arxiv.org/abs/2407.15488",
    "context": "Title: DiffX: Guide Your Layout to Cross-Modal Generative Modeling\nAbstract: arXiv:2407.15488v3 Announce Type: replace  Abstract: Diffusion models have made significant strides in language-driven and layout-driven image generation. However, most diffusion models are limited to visible RGB image generation. In fact, human perception of the world is enriched by diverse viewpoints, such as chromatic contrast, thermal illumination, and depth information. In this paper, we introduce a novel diffusion model for general layout-guided cross-modal generation, called DiffX. Notably, DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space. Moreover, we introduce the Joint-Modality Embedder (JME) to enhance interaction between layout and text conditions by incorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is employed for long caption embedding for user instruction. To facilitate the user-instructed generative training, we construct the cro",
    "path": "papers/24/07/2407.15488.json",
    "total_tokens": 364,
    "tldr": "该文章提出了一种名为DiffX的扩散模型，这是一种用于跨模态生成的布局指导模型。DiffX模型在共享 latent 空间中执行扩散和去噪过程，并通过引入具有门控注意力机制的Joint-Modality Embedder（JME）和用于长描述嵌入的Long-CLIP，有效地增强了布局和文本条件的交互。"
}