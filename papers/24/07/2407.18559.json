{
    "title": "VSSD: Vision Mamba with Non-Causal State Space Duality",
    "abstract": "arXiv:2407.18559v2 Announce Type: replace  Abstract: Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous t",
    "link": "https://arxiv.org/abs/2407.18559",
    "context": "Title: VSSD: Vision Mamba with Non-Causal State Space Duality\nAbstract: arXiv:2407.18559v2 Announce Type: replace  Abstract: Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous t",
    "path": "papers/24/07/2407.18559.json",
    "total_tokens": 360,
    "tldr": "仅反馈「该文章提出了一个名为Vision Mamba的模型，该模型使用了非因果状态空间对偶性（State Space Duality），相比于传统的因果状态空间模型，VSSD在处理非因果视觉任务时，通过调整隐藏状态与token之间的权重关系，降低了计算复杂性并提升了模型的表现力。」"
}