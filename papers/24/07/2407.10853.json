{
    "title": "An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases",
    "abstract": "arXiv:2407.10853v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can exhibit bias in a variety of ways. Such biases can create or exacerbate unfair outcomes for certain groups within a protected attribute, including, but not limited to sex, race, sexual orientation, or age. This paper aims to provide a technical guide for practitioners to assess bias and fairness risks in LLM use cases. The main contribution of this work is a decision framework that allows practitioners to determine which metrics to use for a specific LLM use case. To achieve this, this study categorizes LLM bias and fairness risks, maps those risks to a taxonomy of LLM use cases, and then formally defines various metrics to assess each type of risk. As part of this work, several new bias and fairness metrics are introduced, including innovative counterfactual metrics as well as metrics based on stereotype classifiers. Instead of focusing solely on the model itself, the sensitivity of both prompt",
    "link": "https://arxiv.org/abs/2407.10853",
    "context": "Title: An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases\nAbstract: arXiv:2407.10853v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can exhibit bias in a variety of ways. Such biases can create or exacerbate unfair outcomes for certain groups within a protected attribute, including, but not limited to sex, race, sexual orientation, or age. This paper aims to provide a technical guide for practitioners to assess bias and fairness risks in LLM use cases. The main contribution of this work is a decision framework that allows practitioners to determine which metrics to use for a specific LLM use case. To achieve this, this study categorizes LLM bias and fairness risks, maps those risks to a taxonomy of LLM use cases, and then formally defines various metrics to assess each type of risk. As part of this work, several new bias and fairness metrics are introduced, including innovative counterfactual metrics as well as metrics based on stereotype classifiers. Instead of focusing solely on the model itself, the sensitivity of both prompt",
    "path": "papers/24/07/2407.10853.json",
    "total_tokens": 397,
    "tldr": "该文章提出了一种评估大型语言模型使用案例中偏见和公平性的行动导向框架，并通过对模型的偏见和公平性风险的分类、使用案例的税务分类以及各种风险评估指标的正式定义，帮助实践者确定适用于特定模型使用案例的指标。此外，该研究开发了几个新的偏见和公平性指标，包括创新的对抗性指标和基于刻板印象检测器的指标，从而不仅关注模型的内部属性，还关注问题和上下文的敏感性。"
}