{
    "title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts",
    "abstract": "arXiv:2407.21770v2 Announce Type: replace  Abstract: We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7x overall, with 2.6x for text and 5.2x for image processing compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3x overall FLOPs savings (3x for text",
    "link": "https://arxiv.org/abs/2407.21770",
    "context": "Title: MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts\nAbstract: arXiv:2407.21770v2 Announce Type: replace  Abstract: We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7x overall, with 2.6x for text and 5.2x for image processing compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3x overall FLOPs savings (3x for text",
    "path": "papers/24/07/2407.21770.json",
    "total_tokens": 377,
    "tldr": "该文章提出MoMa架构，一种创新的模式感知专家混合模型，用于处理任意序列的混合模式语言模型早期融合预训练。MoMa通过将专家模块分为专属处理特定模式组别的模式感知专家，显著提高了预训练的效率和性能。"
}