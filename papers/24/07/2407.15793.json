{
    "title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning",
    "abstract": "arXiv:2407.15793v2 Announce Type: replace  Abstract: With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improvin",
    "link": "https://arxiv.org/abs/2407.15793",
    "context": "Title: CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning\nAbstract: arXiv:2407.15793v2 Announce Type: replace  Abstract: With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improvin",
    "path": "papers/24/07/2407.15793.json",
    "total_tokens": 371,
    "tldr": "该文章提出了一种名为 CLIP with Generative Latent Replay 的强有力基线方法，用于解决增量学习中的遗忘问题。该方法通过利用生成式回放技术，同时在保持模型零样本能力的同时，在截然不同的任务域中适应模型。通过在多个任务域上的广泛实验，文章展示了该方法在适应新任务方面的有效性，同时显著提升了 CL 基准测试中的零样本能力。"
}