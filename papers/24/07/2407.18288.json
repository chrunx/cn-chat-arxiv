{
    "title": "Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT",
    "abstract": "arXiv:2407.18288v2 Announce Type: replace  Abstract: Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements ",
    "link": "https://arxiv.org/abs/2407.18288",
    "context": "Title: Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT\nAbstract: arXiv:2407.18288v2 Announce Type: replace  Abstract: Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements ",
    "path": "papers/24/07/2407.18288.json",
    "total_tokens": 699,
    "translated_title": "通过知识蒸馏利用基础模型在多目标跟踪中的创新：将DINOv2特征蒸馏到FairMOT",
    "translated_abstract": "arXiv:2407.18288v2 公告类型：替换  摘要：多目标跟踪（MOT）是一种计算机视觉任务，已经在许多领域得到了应用。MOT的一些常见局限性包括对象外观的变化、遮挡或拥挤的场景。为了应对这些挑战，机器学习方法被大量采用，并利用了大量的数据集、复杂的模型和大量的计算资源。由于实际限制，不一定总能获得上述资源。然而，随着知名人工智能公司最近发布的预训练的基础模型，这些模型已经在使用最先进的方法对庞大的数据集进行了训练。这项工作尝试通过知识蒸馏来利用这样一个基础模型，即DINOv2。提出的策略利用了一种老师-学生架构，其中DINOv2作为老师，FairMOT的后端HRNetv2 W18作为学生。结果表明，尽管提出的策略显示出改进，但仍需进一步研究以验证其在实际应用中的有效性。",
    "tldr": "本文通过知识蒸馏利用了DINOv2基础模型，将其特征蒸馏到FairMOT中，虽然取得了一定的改进，但仍需要更多研究来验证其实际应用的有效性。"
}