{
    "title": "Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services",
    "abstract": "arXiv:2407.00110v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of HPC clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of LLM models on HPC syste",
    "link": "https://arxiv.org/abs/2407.00110",
    "context": "Title: Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services\nAbstract: arXiv:2407.00110v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of HPC clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of LLM models on HPC syste",
    "path": "papers/24/07/2407.00110.json",
    "total_tokens": 639,
    "translated_title": "Chat AI: 一种无缝支持Slurm的HPC服务解决方案",
    "translated_abstract": "arXiv:2407.00110v2 公告类型：替换交叉文摘：大型语言模型（LLM）的广泛应用已促使人们迫切需要一个高效、安全、私密的托管基础设施，这不仅允许研究人员运行开源或自定义的精调LLM，而且确保用户的数据不会在没有用户同意的情况下被存储。虽然配备了先进GPU的高性能计算（HPC）系统非常适合训练LLM，但它们的批次调度范式并不是为了支持实时AI应用的托管而设计的。另一方面，云系统非常适合提供Web服务，但通常缺乏访问HPC集群的计算能力，尤其是昂贵且稀缺的高端GPU，这些GPU对于获得最佳推理性能是必需的。我们提出了一个架构，并在云VM上运行了一个Web服务，该服务具有安全访问基于HPC系统的可扩展后端的能力，该后端运行着多种LLM模型。此外，该架构实现了安全的数据访问，用户数据不会被未经授权的访问。",
    "tldr": "该论文提出了一种架构，能够在HPC集群上运行实时AI应用的托管和云VM服务，同时保证用户数据的隐私和安全，避免了存储用户数据而不经用户同意的情况。"
}