{
    "title": "OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation",
    "abstract": "arXiv:2407.02371v2 Announce Type: replace  Abstract: Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V g",
    "link": "https://arxiv.org/abs/2407.02371",
    "context": "Title: OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\nAbstract: arXiv:2407.02371v2 Announce Type: replace  Abstract: Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V g",
    "path": "papers/24/07/2407.02371.json",
    "total_tokens": 716,
    "translated_title": "OpenVid-1M 大型高质量文本到视频生成数据集",
    "translated_abstract": "arXiv:2407.02371v2 公告类型：替换  翻译摘要：文本到视频（T2V）生成最近因大型多模态模型Sora的兴起而引起了极大关注。然而，T2V生成仍然面临两个重要挑战：1）缺乏精确开源的高质量数据集。先前流行的视频数据集，如WebVid-10M和Panda-70M，要么质量低，要么太大，超出了大多数研究机构的能力范围。因此，对于T2V生成来说，收集精确的高质量文本视频对是一个具有挑战性的但至关重要的任务。2）忽略了充分利用文本信息。最近的一些T2V方法专注于视觉变换器，使用一个简单的交注意力模块来生成视频，这种方法未能彻底从文本提示中提取出语义信息。为了解决这些问题，我们介绍了OpenVid-1M，一个精确的高质量数据集，它拥有丰富的描述性文本说法。这个开放式场景的数据集包含了超过一百万个文本视频对，为T2V的研发提供了支持。",
    "tldr": "OpenVid-1M 是一个精确的高质量数据集，拥有大量的文本视频对，旨在支持文本到视频生成的研究。"
}