{
    "title": "A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model",
    "abstract": "arXiv:2407.15362v2 Announce Type: replace-cross  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\\&E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPa",
    "link": "https://arxiv.org/abs/2407.15362",
    "context": "Title: A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model\nAbstract: arXiv:2407.15362v2 Announce Type: replace-cross  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\\&E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPa",
    "path": "papers/24/07/2407.15362.json",
    "total_tokens": 742,
    "translated_title": "多模态知识增强的全切片病理学基础模型",
    "translated_abstract": "arXiv:2407.15362v2 宣布类型：替换交叉  翻译：在任务无关的基础模型领域取得了显著的进步，该模型提高了多种下游临床任务的性能。尽管性能显著，但仍存在一些挑战。首先，先前的工作要么依赖于仅视觉数据，要么依赖于视觉-描述数据，而忽略了对于多种临床应用至关重要的病理报告和基因表达谱。其次，目前病理FM的进展主要集中在切片级，其中切片级预训练的有限上下文未能捕捉全切片模式。我们 curated 了包含最大数量的 H\\&E 诊断全切片图像和相关病理报告和 RNA-Seq 数据的多元模态数据集，总共得到了 26,169 个来自 10,275 名患者和 32 种癌症类型的 slide-level 模态对。为了利用这些数据进行癌症病理学的研究，我们提出了一个全新的网络结构，该结构可以同时学习全切片图像的高级纹理特征和病理报告中的文本知识。通过这种多模态的预训练，我们的模型能够更好地理解病理切片的全局特征，并且在多种下游任务上取得了优于单一模态预训练模型的性能。",
    "tldr": "文章提出了一种全新的网络结构，通过整合病理切片图像、病理报告和基因表达谱的多模态数据，训练出一个能够理解病理切片全局特征的模型，并在多种下游任务上取得了优于单一模态模型的性能。"
}