{
    "title": "A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model",
    "abstract": "arXiv:2407.15362v2 Announce Type: replace  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\\&E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we",
    "link": "https://arxiv.org/abs/2407.15362",
    "context": "Title: A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model\nAbstract: arXiv:2407.15362v2 Announce Type: replace  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\\&E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we",
    "path": "papers/24/07/2407.15362.json",
    "total_tokens": 384,
    "tldr": "该文章创新地构建了包含病理报告和基因表达数据的最大规模多模态病理学数据集，通过集成病理诊断全切片图像和H&E染色切片图像，实现了在32种癌症类型中的10,275名患者中的高效病理报告生成。这种多模态训练提高了病理图像的诊断准确性，并对病理报告和基因表达数据的综合利用为个性化医学和精准治疗提供了新的视角。"
}