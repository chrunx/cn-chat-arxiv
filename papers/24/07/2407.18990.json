{
    "title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications",
    "abstract": "arXiv:2407.18990v2 Announce Type: replace-cross  Abstract: Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance on downstream tasks. However, choosing the appropriate setting of tuning hyperparameters (HPs) is a labor-intensive and computationally expensive process. Here, we provide recommended HP configurations for practical use-cases that represent a better starting point for practitioners, when considering two SOTA LLMs and two commonly used tuning methods. We describe Coverage-based Search (CBS), a process for ranking HP configurations based on an offline extensive grid search, such that the top ranked configurations collectively provide a practical robust recommendation for a wide range of datasets and domains. We focus our experiments on Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a total of > 10,000 tuning experiments. Our results suggest that, in general, Llama-3-8B and LoRA should be preferred, when possib",
    "link": "https://arxiv.org/abs/2407.18990",
    "context": "Title: Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications\nAbstract: arXiv:2407.18990v2 Announce Type: replace-cross  Abstract: Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance on downstream tasks. However, choosing the appropriate setting of tuning hyperparameters (HPs) is a labor-intensive and computationally expensive process. Here, we provide recommended HP configurations for practical use-cases that represent a better starting point for practitioners, when considering two SOTA LLMs and two commonly used tuning methods. We describe Coverage-based Search (CBS), a process for ranking HP configurations based on an offline extensive grid search, such that the top ranked configurations collectively provide a practical robust recommendation for a wide range of datasets and domains. We focus our experiments on Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a total of > 10,000 tuning experiments. Our results suggest that, in general, Llama-3-8B and LoRA should be preferred, when possib",
    "path": "papers/24/07/2407.18990.json",
    "total_tokens": 391,
    "tldr": "该文章在大型语言模型的超参数调优对真实世界应用的影响方面进行了深入的实证研究，提供了针对两个顶级LLMs（Llama-3-8B和Mistral-7B）和两种常见调优方法的实用超参数推荐配置，为实践者提供了更好的一开始起点，进而提高了他们的性能。"
}