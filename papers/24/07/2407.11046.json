{
    "title": "A Survey on LoRA of Large Language Models",
    "abstract": "arXiv:2407.11046v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation~(LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this",
    "link": "https://arxiv.org/abs/2407.11046",
    "context": "Title: A Survey on LoRA of Large Language Models\nAbstract: arXiv:2407.11046v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation~(LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this",
    "path": "papers/24/07/2407.11046.json",
    "total_tokens": 710,
    "translated_title": "LoRA 大规模语言模型低秩适应性综述",
    "translated_abstract": "arXiv:2407.11046v2 公告类型: 替换交叉 摘要: 低秩适应性(LoRA)，这是一种在下游任务上通过更新密集型神经网络层与可插拔的低秩矩阵来进行的最有效的参数精简微调范式。此外，它还具有在跨任务泛化以及在确保隐私方面的显着优势。因此，LoRA最近受到了大量的关注，与它相关的工作数量呈指数级增长。对LoRA的当前进展进行全面概述变得尤为必要。本文从以下几个方面对LoRA的发展进行分类和综述：(1) 下游适应改进变体，这些变体提高了LoRA在下游任务上的表现；(2) 跨任务泛化方法，这些方法混合了多个LoRA插件以实现跨任务泛化；(3) 效率改进方法，这些方法提高了LoRA的计算效率；(4) 数据隐私保护方法，这些方法在联邦学习中使用LoRA；(5) 应用。此外，这篇综述还讨论了LoRA研究的未来方向和对未来工作的期望。",
    "tldr": "这篇综述详细介绍了LoRA技术，它通过更新低秩矩阵来提高大规模语言模型的性能，并探讨了其在提升计算效率和保护数据隐私方面的应用。"
}