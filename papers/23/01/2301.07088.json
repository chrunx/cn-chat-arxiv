{
    "title": "Vision Learners Meet Web Image-Text Pairs",
    "abstract": "arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from",
    "link": "https://arxiv.org/abs/2301.07088",
    "context": "Title: Vision Learners Meet Web Image-Text Pairs\nAbstract: arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from",
    "path": "papers/23/01/2301.07088.json",
    "total_tokens": 672,
    "translated_title": "视觉学习者与网络图像文本对",
    "translated_abstract": "arXiv:2301.07088v3 公告类型：替换 摘要： 许多自监督学习方法在一组精心编排的图像Net-1K数据集上进行预训练。在本工作中，考虑到网络数据出色的可扩展性，我们考虑在由网络来源的图像文本配对数据上进行自监督预训练。首先，我们在类似配置的设置中，对代表性的自监督预训练方法在大型网络数据上的表现进行了基准测试。我们比较了一系列方法，包括使用掩码训练目标的单模态方法，以及使用图像文本对比性训练的多模态方法。我们观察到，现有的多模态方法在视觉转移学习任务上并没有超过单模态方法的表现。我们推导出一个信息论观点来解释这些基准结果，该观点为设计一种新型视觉学习者提供了洞察。受此洞察的启发，我们介绍了一种新的视觉表示预训练方法MUlti-modal Generator（MUG），它从",
    "tldr": "新型视觉学习者MUG通过自监督学习，在网络图像文本配对数据上进行预训练，旨在改进视觉表示。"
}