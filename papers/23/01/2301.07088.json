{
    "title": "Vision Learners Meet Web Image-Text Pairs",
    "abstract": "arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from",
    "link": "https://arxiv.org/abs/2301.07088",
    "context": "Title: Vision Learners Meet Web Image-Text Pairs\nAbstract: arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from",
    "path": "papers/23/01/2301.07088.json",
    "total_tokens": 361,
    "tldr": "该文章提出了一种新的视觉表示预训练方法MUlti-modal Generator（MUG），它能够从Web图像文本对中学习，并通过分析现有多模态方法在视觉迁移学习任务上的性能，推导出一个信息论视角的解释，为设计新的视觉学习者提供了指导。"
}