{
    "title": "Large AI Model Empowered Multimodal Semantic Communications",
    "abstract": "arXiv:2309.01249v2 Announce Type: replace  Abstract: Multimodal signals, including text, audio, image, and video, can be integrated into Semantic Communication (SC) systems to provide an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal distortion during transmission. Recent advancements in large AI models, particularly in the Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for addressing these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery",
    "link": "https://arxiv.org/abs/2309.01249",
    "context": "Title: Large AI Model Empowered Multimodal Semantic Communications\nAbstract: arXiv:2309.01249v2 Announce Type: replace  Abstract: Multimodal signals, including text, audio, image, and video, can be integrated into Semantic Communication (SC) systems to provide an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal distortion during transmission. Recent advancements in large AI models, particularly in the Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for addressing these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery",
    "path": "papers/23/09/2309.01249.json",
    "total_tokens": 743,
    "translated_title": "利用大型AI模型 empowered multimodal semantic communications",
    "translated_abstract": "arXiv:2309.01249v2 公告类型：替换 摘要：在语义通信（SC）系统中集成多模态信号，如文本、音频、图像和视频，可以提供具有低延迟和高质量的沉浸式体验，这在语义层次上。然而，多模态SC也存在一些挑战，包括数据异质性、语义模糊性和信号在传输过程中的失真。在大型AI模型，特别是多模态语言模型（MLM）和大型语言模型（LLM）方面最近的进展，为解决这些问题提供了潜在的解决方案。因此，我们提出了基于大型AI模型的多模态SC（LAM-MSC）框架，其中我们首先介绍了多模态对齐（MMA），它使用MLM来允许在多模态和单模态数据之间进行转换，同时保持语义一致性。然后，我们提出了一个个性化的LLM-based Knowledge Base（LKB），允许用户执行个性化的语义提取或恢复。",
    "tldr": "本文提出了一种基于大型AI模型的大型AI模型-强化型多模态语义通信（LAM-MSC）框架，该框架利用多模态语言模型（MLM）和大型语言模型（LLM）进行多模态数据转换，以保持语义一致性，并进行个性化语义提取或恢复。"
}