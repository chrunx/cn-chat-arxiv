{
    "title": "Consistency-guided Prompt Learning for Vision-Language Models",
    "abstract": "arXiv:2306.01195v4 Announce Type: replace  Abstract: We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and outp",
    "link": "https://arxiv.org/abs/2306.01195",
    "context": "Title: Consistency-guided Prompt Learning for Vision-Language Models\nAbstract: arXiv:2306.01195v4 Announce Type: replace  Abstract: We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and outp",
    "path": "papers/23/06/2306.01195.json",
    "total_tokens": 352,
    "tldr": "该文章提出了一种名为CoPrompt的新的训练方法，该方法通过在下游任务上强制训练模型与预训练模型的一致性，从而提高了大型基础模型的泛化能力。通过在训练样本的两个随机变体上强制一致性以及结合了模型微调和提示学习的两种技术，CoPrompt提升了性能并提高了调参的灵活性。"
}