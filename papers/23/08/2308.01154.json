{
    "title": "Arithmetic with Language Models: from Memorization to Computation",
    "abstract": "arXiv:2308.01154v4 Announce Type: replace  Abstract: A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input to",
    "link": "https://arxiv.org/abs/2308.01154",
    "context": "Title: Arithmetic with Language Models: from Memorization to Computation\nAbstract: arXiv:2308.01154v4 Announce Type: replace  Abstract: A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input to",
    "path": "papers/23/08/2308.01154.json",
    "total_tokens": 617,
    "translated_title": "语言模型中的算术：从记忆到计算",
    "translated_abstract": "arXiv:2308.01154v4 公告类型：替换 摘要：对最近的大型语言模型的涌现计算和问题求解能力的更好理解对进一步改进它们并拓宽其应用范围至关重要。本文研究了一种语言模型，它在训练中用来预测下一个词，是如何执行算术计算的，这种计算能够超出训练数据范围。二进制加法和乘法构成了一个很好的测试平台，因为它们只需要非常小的词汇量，并且在输入/输出 discontinuities方面表现出相关性，使得对新的数据运行平滑的输入插值无效。我们成功地训练了一个轻量级语言模型，让它学会了这些任务，并进行了多项实验，以研究该模型扩展的能力和内部信息处理。我们的发现支持这样的假设：语言模型在工作时作为一个编码-回归-解码机，其中计算在值空间中发生，一旦输入被编码，语言模型就可以对算术计算进行预测和解码。",
    "tldr": "本文研究了大型语言模型的算术计算能力，发现它们能够通过一种“编码-回归-解码”机制进行泛化计算。"
}