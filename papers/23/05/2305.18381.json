{
    "title": "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation",
    "abstract": "arXiv:2305.18381v4 Announce Type: replace-cross  Abstract: Data-efficient learning has garnered significant attention, especially given the current trend of large multi-modal models. Recently, dataset distillation has become an effective approach by synthesizing data samples that are essential for network training. However, it remains to be explored which samples are essential for the dataset distillation process itself. In this work, we study the data efficiency and selection for the dataset distillation task. By re-formulating the dynamics of distillation, we provide insight into the inherent redundancy in the real dataset, both theoretically and empirically. We propose to use the empirical loss value as a static data pruning criterion. To further compensate for the variation of the data value in training, we find the most contributing samples based on their causal effects on the distillation. The proposed selection strategy can efficiently exploit the training dataset, outperform th",
    "link": "https://arxiv.org/abs/2305.18381",
    "context": "Title: Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation\nAbstract: arXiv:2305.18381v4 Announce Type: replace-cross  Abstract: Data-efficient learning has garnered significant attention, especially given the current trend of large multi-modal models. Recently, dataset distillation has become an effective approach by synthesizing data samples that are essential for network training. However, it remains to be explored which samples are essential for the dataset distillation process itself. In this work, we study the data efficiency and selection for the dataset distillation task. By re-formulating the dynamics of distillation, we provide insight into the inherent redundancy in the real dataset, both theoretically and empirically. We propose to use the empirical loss value as a static data pruning criterion. To further compensate for the variation of the data value in training, we find the most contributing samples based on their causal effects on the distillation. The proposed selection strategy can efficiently exploit the training dataset, outperform th",
    "path": "papers/23/05/2305.18381.json",
    "total_tokens": 351,
    "tldr": "该文章通过重新形式化蒸馏过程的动力学，提供了对真实数据集内在冗余的理论和实证洞察。文章提出了一种基于经验损失值的数据静态裁剪标准，并进一步根据数据对蒸馏贡献的因果效应，找到最能影响蒸馏过程的数据样本，从而有效利用训练数据集，超越了现有技术。"
}