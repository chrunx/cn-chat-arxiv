{
    "title": "LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
    "abstract": "arXiv:2305.18403v5 Announce Type: replace-cross  Abstract: Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather tha",
    "link": "https://arxiv.org/abs/2305.18403",
    "context": "Title: LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning\nAbstract: arXiv:2305.18403v5 Announce Type: replace-cross  Abstract: Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather tha",
    "path": "papers/23/05/2305.18403.json",
    "total_tokens": 379,
    "tldr": "该文章提出LoRAPrune框架，实现了对大型语言模型在下游任务上的低秩参数高效精调过程中的结构化压缩，通过利用LoRA指导的评残准则，该方法在无需原始预训练权重梯度的情况下，实现了高精度压缩和高内存效率。"
}