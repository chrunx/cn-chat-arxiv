{
    "title": "Markov Decision Processes under External Temporal Processes",
    "abstract": "arXiv:2305.16056v2 Announce Type: replace-cross  Abstract: Most reinforcement learning algorithms treat the context under which they operate as a stationary, isolated, and undisturbed environment. However, in real world applications, environments constantly change due to a variety of external events. To address this problem, we study Markov Decision Processes (MDP) under the influence of an external temporal process. We formalize this notion and discuss conditions under which the problem becomes tractable with suitable solutions. We propose a policy iteration algorithm to solve this problem and theoretically analyze its performance. We derive results on the sample complexity of the algorithm and study its dependency on the extent of non-stationarity of the environment. We then conduct experiments to illustrate our results in a classic control environment.",
    "link": "https://arxiv.org/abs/2305.16056",
    "context": "Title: Markov Decision Processes under External Temporal Processes\nAbstract: arXiv:2305.16056v2 Announce Type: replace-cross  Abstract: Most reinforcement learning algorithms treat the context under which they operate as a stationary, isolated, and undisturbed environment. However, in real world applications, environments constantly change due to a variety of external events. To address this problem, we study Markov Decision Processes (MDP) under the influence of an external temporal process. We formalize this notion and discuss conditions under which the problem becomes tractable with suitable solutions. We propose a policy iteration algorithm to solve this problem and theoretically analyze its performance. We derive results on the sample complexity of the algorithm and study its dependency on the extent of non-stationarity of the environment. We then conduct experiments to illustrate our results in a classic control environment.",
    "path": "papers/23/05/2305.16056.json",
    "total_tokens": 571,
    "translated_title": "受外部时间过程影响的马尔可夫决策过程",
    "translated_abstract": "arXiv:2305.16056v2 公告类型：替换交叉  翻译摘要：大多数强化学习算法都将其运作的环境视为一个稳定的、隔离的和不受干扰的环境。然而，在现实世界的应用中，环境由于各种外部事件不断变化。为了解决这个问题，我们研究了在受到外部时间过程影响下的马尔可夫决策过程（MDP）。我们formalize了这一概念，并讨论了在哪些条件下问题可以得到合适的解决。我们提出了一种策略迭代算法来解决这个问题，并对它的性能进行了理论分析。我们导出了算法的样本复杂性的结果，并研究了它与环境非稳定性程度的关系。然后，我们进行了实验，以在经典的控制环境中说明我们的结果。",
    "tldr": "这篇论文研究了在不断变化的外部时间过程影响下的马尔可夫决策过程，并提出了一种策略迭代算法来处理这个问题，同时分析了解决方案的样本复杂性和性能。"
}