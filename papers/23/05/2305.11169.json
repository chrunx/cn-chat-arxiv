{
    "title": "Emergent Representations of Program Semantics in Language Models Trained on Programs",
    "abstract": "arXiv:2305.11169v3 Announce Type: replace-cross  Abstract: We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We antic",
    "link": "https://arxiv.org/abs/2305.11169",
    "context": "Title: Emergent Representations of Program Semantics in Language Models Trained on Programs\nAbstract: arXiv:2305.11169v3 Announce Type: replace-cross  Abstract: We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We antic",
    "path": "papers/23/05/2305.11169.json",
    "total_tokens": 587,
    "translated_title": "程序语言模型中涌现的程序语义表示",
    "translated_abstract": "我们表明，虽然只是在训练中预测下一个字符，代码语言模型（LMs）也能学会代表程序的正式语义。具体来说，我们为一种专门用于在二维网格世界中导航的域特定语言训练一个Transformer模型。文档库中的每个程序都伴随着一个格盘世界状态的（部分）规范，形式为几个输入输出格盘世界状态。尽管没有提供任何其他归纳偏差，我们发现一个探测分类器能够从训练过程中提取越来越准确的程序语义表示，这些表示隐藏在语言模型的隐藏状态中，这表明语言模型在未经正式训练的情况下，学会了程序意义上的解释能力。我们还开发了一种新颖的干预性基线，它使我们能够区分由语言模型代表的内容和通过探索器学习的内容。我们旨在通过这种方式进APA官方客服",
    "tldr": "即使未曾直接训练过，AI模型也在阅读代码的过程中学习到了程序语义表示"
}