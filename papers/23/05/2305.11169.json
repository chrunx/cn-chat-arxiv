{
    "title": "Emergent Representations of Program Semantics in Language Models Trained on Programs",
    "abstract": "arXiv:2305.11169v3 Announce Type: replace-cross  Abstract: We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We antic",
    "link": "https://arxiv.org/abs/2305.11169",
    "context": "Title: Emergent Representations of Program Semantics in Language Models Trained on Programs\nAbstract: arXiv:2305.11169v3 Announce Type: replace-cross  Abstract: We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We antic",
    "path": "papers/23/05/2305.11169.json",
    "total_tokens": 346,
    "tldr": "该文章表明，即使语言模型仅通过预测下一个 token 进行训练，但通过对特定代码输出训练，它们也能够学习到程序的正式语义表示。例如，以程序导航环境的行为作为输入，语言模型能够学习表示程序执行过程中未观测到的中间状态。"
}