{
    "title": "Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos",
    "abstract": "arXiv:2305.03713v3 Announce Type: replace  Abstract: Modern avatar generators allow anyone to synthesize photorealistic real-time talking avatars, ushering in a new era of avatar-based human communication, such as with immersive AR/VR interactions or videoconferencing with limited bandwidths. Their safe adoption, however, requires a mechanism to verify if the rendered avatar is trustworthy: does it use the appearance of an individual without their consent? We term this task avatar fingerprinting. To tackle it, we first introduce a large-scale dataset of real and synthetic videos of people interacting on a video call, where the synthetic videos are generated using the facial appearance of one person and the expressions of another. We verify the identity driving the expressions in a synthetic video, by learning motion signatures that are independent of the facial appearance shown. Our solution, the first in this space, achieves an average AUC of 0.85. Critical to its practical use, it al",
    "link": "https://arxiv.org/abs/2305.03713",
    "context": "Title: Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos\nAbstract: arXiv:2305.03713v3 Announce Type: replace  Abstract: Modern avatar generators allow anyone to synthesize photorealistic real-time talking avatars, ushering in a new era of avatar-based human communication, such as with immersive AR/VR interactions or videoconferencing with limited bandwidths. Their safe adoption, however, requires a mechanism to verify if the rendered avatar is trustworthy: does it use the appearance of an individual without their consent? We term this task avatar fingerprinting. To tackle it, we first introduce a large-scale dataset of real and synthetic videos of people interacting on a video call, where the synthetic videos are generated using the facial appearance of one person and the expressions of another. We verify the identity driving the expressions in a synthetic video, by learning motion signatures that are independent of the facial appearance shown. Our solution, the first in this space, achieves an average AUC of 0.85. Critical to its practical use, it al",
    "path": "papers/23/05/2305.03713.json",
    "total_tokens": 342,
    "tldr": "该文章提出了一个名为'Avatar Fingerprinting'的新方法，用于验证合成实时对话头像视频的安全性和版权，通过学习独立于面部外观的运动模式，区分真实和合成视频，提高了用户的信任度。"
}