{
    "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",
    "abstract": "arXiv:2310.01469v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from \\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that \\emph{hallucination may be another view of adversarial examples}, and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formaliz",
    "link": "https://arxiv.org/abs/2310.01469",
    "context": "Title: LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples\nAbstract: arXiv:2310.01469v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from \\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that \\emph{hallucination may be another view of adversarial examples}, and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formaliz",
    "path": "papers/23/10/2310.01469.json",
    "total_tokens": 346,
    "tldr": "该文章揭示了大型语言模型（LLMs）在应对某些随机输入时可能会产生看似合理的错误输出，这表明LLMs可能在某种程度上可以被操控以生成特定的错误信息。"
}