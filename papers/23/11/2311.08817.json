{
    "title": "MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy",
    "abstract": "arXiv:2311.08817v2 Announce Type: replace-cross  Abstract: It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior work has attributed this behavior to either a fundamental and unavoidable inadequacy of modes in probabilistic models or weaknesses in language modeling. Contrastingly, we argue that degenerate modes can even occur in the absence of any modeling error, due to contamination of the training data. Specifically, we argue that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate. We therefore propose to apply MAP decoding to the model's true conditional distribution where the conditioning variable explicitly avoids specific degenerate behavior. Using exact search, we empirically verify that the length-conditional modes of",
    "link": "https://arxiv.org/abs/2311.08817",
    "context": "Title: MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy\nAbstract: arXiv:2311.08817v2 Announce Type: replace-cross  Abstract: It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior work has attributed this behavior to either a fundamental and unavoidable inadequacy of modes in probabilistic models or weaknesses in language modeling. Contrastingly, we argue that degenerate modes can even occur in the absence of any modeling error, due to contamination of the training data. Specifically, we argue that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate. We therefore propose to apply MAP decoding to the model's true conditional distribution where the conditioning variable explicitly avoids specific degenerate behavior. Using exact search, we empirically verify that the length-conditional modes of",
    "path": "papers/23/11/2311.08817.json",
    "total_tokens": 339,
    "tldr": "该文章提出了一种新的方法来解决自然语言生成模型中的模式问题，通过在条件变量中排除特定退化行为，有效地避免了过度简化的输出，从而提高了模型的多样性与真实感。"
}