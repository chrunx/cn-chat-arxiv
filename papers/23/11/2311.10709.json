{
    "title": "Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning",
    "abstract": "arXiv:2311.10709v2 Announce Type: replace  Abstract: We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.",
    "link": "https://arxiv.org/abs/2311.10709",
    "context": "Title: Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning\nAbstract: arXiv:2311.10709v2 Announce Type: replace  Abstract: We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.",
    "path": "papers/23/11/2311.10709.json",
    "total_tokens": 523,
    "tldr": "该文章提出了一种名为Emu Video的文本到视频生成模型，通过显式图像条件化将生成过程分为两步：首先根据文本生成图像，然后根据文本和生成的图像生成视频。文章识别出关键的设计决策，包括对扩散过程的噪声调度调整和多阶段训练方法，这些方法使得模型可以直接生成高质量和高分辨率视频，而不需要像先前工作那样依赖深层模型 cascades。在人类评估中，与所有先前的工作相比，Emu Video生成的视频在质量上被 strongly preferred，分别为 81% 对谷歌的Imagen Video，90% 对英伟达的PYOCO，以及 96% 对 Meta的Make-A-Video。我们的模型在商用解决方案，如RunwayML的Gen2和Pika Labs方面表现出色。此外，由于我们的分解方法自然地适用于根据用户文本提示动画化图像，因此我们的生成物在用户之间受到更广泛的选择，其为 96% 对先前的工作。"
}