{
    "title": "Exploiting Novel GPT-4 APIs",
    "abstract": "arXiv:2312.14302v2 Announce Type: replace-cross  Abstract: Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose \"gray-box\" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed b",
    "link": "https://arxiv.org/abs/2312.14302",
    "context": "Title: Exploiting Novel GPT-4 APIs\nAbstract: arXiv:2312.14302v2 Announce Type: replace-cross  Abstract: Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose \"gray-box\" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed b",
    "path": "papers/23/12/2312.14302.json",
    "total_tokens": 691,
    "translated_title": "利用新型 GPT-4 API",
    "translated_abstract": "天文学术：2312.14302v2 公告类型：替换交叉 摘要：语言模型攻击通常假设两种极端的威胁模型：完全开放访问模型的权重，或者是仅限于文本生成API的封闭访问。然而，现实世界的API往往比仅仅文本生成要灵活得多：这些API暴露的\"灰盒\"访问导致了新的威胁途径。为了探索这一点，我们红队对GPT-4 APIs暴露的新功能进行了三向研究：微调、函数调用和知识检索。我们发现，即使是微调一个模型也只有15个有害示例或100个良性示例就可以移除GPT-4的核心保护措施，使得可以产生一系列有害输出。此外，我们还发现GPT-4助理愿意透露函数调用模式，并且可以被导向执行任意函数调用。最后，我们还发现知识检索可以被通过注入指令到检索文档中进行劫持。这些漏洞表明，任何对API功能的新增都可能导致安全风险，尤其是对于这些新API提供的额外能力和权限，必须被仔细审视和管理。",
    "tldr": "本研究对新型 GPT-4 APIs 的新功能进行了利用，发现即使是微调模型也可以移除其核心保护措施，并且能够进行函数调用和知识检索的劫持。"
}