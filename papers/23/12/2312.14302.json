{
    "title": "Exploiting Novel GPT-4 APIs",
    "abstract": "arXiv:2312.14302v2 Announce Type: replace-cross  Abstract: Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose \"gray-box\" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed b",
    "link": "https://arxiv.org/abs/2312.14302",
    "context": "Title: Exploiting Novel GPT-4 APIs\nAbstract: arXiv:2312.14302v2 Announce Type: replace-cross  Abstract: Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose \"gray-box\" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed b",
    "path": "papers/23/12/2312.14302.json",
    "total_tokens": 352,
    "tldr": "该文章发现并展示了GPT-4 APIs中三个新功能的潜在威胁，包括在极少数恶意示例下改变模型的行为、能够执行任意函数调用、以及操纵知识检索机制，表明了真实世界API的功能扩展可能引入的新风险。"
}