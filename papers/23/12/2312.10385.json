{
    "title": "Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning",
    "abstract": "arXiv:2312.10385v4 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''",
    "link": "https://arxiv.org/abs/2312.10385",
    "context": "Title: Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning\nAbstract: arXiv:2312.10385v4 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''",
    "path": "papers/23/12/2312.10385.json",
    "total_tokens": 400,
    "tldr": "该文章提供了一种名为“模仿优秀避免失败：一种安全的强化学习逐步方法”的创新策略，它旨在确保强化学习中的策略能够安全地进行，同时最大化预期收益。该策略通过不断改进的策略生成“好”和“坏”的轨迹并对其进行模仿和避免，从而不对轨迹成本约束进行修改。此外，通过一个在线决策器，该策略能够根据学习过程中的奖励阈值和总体成本约束，对轨迹进行分类。这种策略的实施不需要对成本约束进行任何近似处理，且能够平衡风险和奖励，有效地提高了学习策略的安全性。"
}