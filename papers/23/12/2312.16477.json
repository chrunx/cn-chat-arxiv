{
    "title": "Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding",
    "abstract": "arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e",
    "link": "https://arxiv.org/abs/2312.16477",
    "context": "Title: Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding\nAbstract: arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e",
    "path": "papers/23/12/2312.16477.json",
    "total_tokens": 720,
    "translated_title": "组多视图自编码器用于空间编码的3D形状分析",
    "translated_abstract": "arXiv:2312.16477v3 公告类型：替换  摘要：近年来，基于视图的3D形状识别方法的结果已经饱和，并且由于参数尺寸巨大，性能优异的模型无法部署在内存受限的设备上。为了解决这个问题，我们为这一领域引入了一种基于知识蒸馏的压缩方法，这种方法在尽可能保留模型性能的同时显著减少了参数的数量。具体来说，为了提高小型模型的能力，我们设计了一个高性能的大型模型，称为组多视图视觉Transformer（GMViT）。在GMViT中，视图级别的ViT首先建立了视图级别特征之间的关系。此外，为了捕获更深层次的特征，我们对视图级别特征进行了分组模块的处理，使其提升到了组级别特征。最后，组级别ViT将组级别特征整合成完整的、结构良好的3D形状描述符。值得注意的是，在这两个ViT中，我们都引入了空间自编码器来进一步增强特征表示。我们的压缩策略叫做DeCoV,它可以在图像分类任务上达到比当前最佳压缩技术更好的性能。在论文中，我们详细说明了如何使用知识蒸馏来实现这一愿景，并讨论了DeCoV在另一方面的影响可能给我们带来的便利。",
    "tldr": "本文提出了一种基于知识蒸馏的压缩方法，用于减少3D形状识别中视图级别方法的参数数量，同时保持性能。通过设计一种名为GMViT的高性能大型模型，以及引入空间自编码器增强特征表示，该方法在一个名为DeCoV的策略下实现了在图像分类任务上的较好性能。"
}