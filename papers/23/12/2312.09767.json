{
    "title": "DreamTalk: When Emotional Talking Head Generation Meets Diffusion Probabilistic Models",
    "abstract": "arXiv:2312.09767v2 Announce Type: replace  Abstract: Emotional talking head generation has attracted growing attention. Previous methods, which are mainly GAN-based, still struggle to consistently produce satisfactory results across diverse emotions and cannot conveniently specify personalized emotions. In this work, we leverage powerful diffusion models to address the issue and propose DreamTalk, a framework that employs meticulous design to unlock the potential of diffusion models in generating emotional talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network can consistently synthesize high-quality audio-driven face motions across diverse emotions. To enhance lip-motion accuracy and emotional fullness, we introduce a style-aware lip expert that can guide lip-sync while preserving emotion intensity. To more conveniently specify personalized emotions, a diff",
    "link": "https://arxiv.org/abs/2312.09767",
    "context": "Title: DreamTalk: When Emotional Talking Head Generation Meets Diffusion Probabilistic Models\nAbstract: arXiv:2312.09767v2 Announce Type: replace  Abstract: Emotional talking head generation has attracted growing attention. Previous methods, which are mainly GAN-based, still struggle to consistently produce satisfactory results across diverse emotions and cannot conveniently specify personalized emotions. In this work, we leverage powerful diffusion models to address the issue and propose DreamTalk, a framework that employs meticulous design to unlock the potential of diffusion models in generating emotional talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network can consistently synthesize high-quality audio-driven face motions across diverse emotions. To enhance lip-motion accuracy and emotional fullness, we introduce a style-aware lip expert that can guide lip-sync while preserving emotion intensity. To more conveniently specify personalized emotions, a diff",
    "path": "papers/23/12/2312.09767.json",
    "total_tokens": 354,
    "tldr": "该文章提出了DreamTalk框架，利用强大的扩散模型在生成情绪化的说话头像时实现了突破。它通过精心设计的组件，如 denoising 网络、风格指导的唇部专家和一个风格预测器，能够一致地生成高质量的情感驱动的面部动作，并且能够通过个性化的情绪指定来增强 lip-sync 过程的精确度和情感表达的丰富性。"
}