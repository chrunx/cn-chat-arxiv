{
    "title": "ASR: Attention-alike Structural Re-parameterization",
    "abstract": "arXiv:2304.06345v3 Announce Type: replace  Abstract: The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, and multi-branch convolution. However, the widely used attention modules which drastically slow inference speed cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. ",
    "link": "https://arxiv.org/abs/2304.06345",
    "context": "Title: ASR: Attention-alike Structural Re-parameterization\nAbstract: arXiv:2304.06345v3 Announce Type: replace  Abstract: The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, and multi-branch convolution. However, the widely used attention modules which drastically slow inference speed cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. ",
    "path": "papers/23/04/2304.06345.json",
    "total_tokens": 305,
    "tldr": "该文章提出的注意力相似结构再参数化技术，通过允许不同网络架构之间变换的等效参数变换，能够在不增加训练成本的情况下，为工业和实际应用中性能提高提供可能。"
}