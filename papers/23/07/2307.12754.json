{
    "title": "Nonparametric Linear Feature Learning in Regression Through Regularisation",
    "abstract": "arXiv:2307.12754v4 Announce Type: replace-cross  Abstract: Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for joint linear feature learning and non-parametric function estimation, aimed at more effectively leveraging hidden features for learning. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By using alternative minimisation, w",
    "link": "https://arxiv.org/abs/2307.12754",
    "context": "Title: Nonparametric Linear Feature Learning in Regression Through Regularisation\nAbstract: arXiv:2307.12754v4 Announce Type: replace-cross  Abstract: Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for joint linear feature learning and non-parametric function estimation, aimed at more effectively leveraging hidden features for learning. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By using alternative minimisation, w",
    "path": "papers/23/07/2307.12754.json",
    "total_tokens": 374,
    "tldr": "该文章提出了一种新的方法，用于联合线性特征学习和非参数函数估计，旨在更有效地利用隐藏特征进行学习。该方法通过最小化经验风险并加入对函数导数的惩罚，确保了估计的变异性。通过利用 Hermite 多项式的正交性和旋转不变性，该文章介绍了一个新的估计器，名为 RegFeaL。通过交替最小化，该估计器能够在多索引模型中有效地学习隐藏特征，并增强预测、计算和解释能力。"
}