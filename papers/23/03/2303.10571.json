{
    "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft",
    "abstract": "arXiv:2303.10571v2 Announce Type: replace-cross  Abstract: One of the essential missions in the AI research community is to build an autonomous embodied agent that can achieve high-level performance across a wide spectrum of tasks. However, acquiring or manually designing rewards for all open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-ended tasks. Simply utilizing the similarity between the video snippet and the language prompt is not RL-friendly since standard VLMs may only capture the similarity at a coarse level. To achieve RL-friendliness, we incorporate the task completion degree into the VLM training objective, as this information can assist agents in distinguishing the importance between different states. Moreover, we provide neat YouTube datasets based on the l",
    "link": "https://arxiv.org/abs/2303.10571",
    "context": "Title: Reinforcement Learning Friendly Vision-Language Model for Minecraft\nAbstract: arXiv:2303.10571v2 Announce Type: replace-cross  Abstract: One of the essential missions in the AI research community is to build an autonomous embodied agent that can achieve high-level performance across a wide spectrum of tasks. However, acquiring or manually designing rewards for all open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-ended tasks. Simply utilizing the similarity between the video snippet and the language prompt is not RL-friendly since standard VLMs may only capture the similarity at a coarse level. To achieve RL-friendliness, we incorporate the task completion degree into the VLM training objective, as this information can assist agents in distinguishing the importance between different states. Moreover, we provide neat YouTube datasets based on the l",
    "path": "papers/23/03/2303.10571.json",
    "total_tokens": 384,
    "tldr": "该文章提出了一种名为CLIP4MC的跨模态对比学习框架，旨在训练一个对强化学习友好的视觉语言模型（VLM），该模型可以作为开放式任务的内在奖励函数。通过在VLM训练目标中融入任务完成度的信息，以帮助代理区分不同状态的重要性，从而实现对RL的友好性。同时，文章还提供了一个干净的YouTube数据集，用于基于语言指令的测试，验证了模型的实用性和有效性。"
}