{
    "title": "Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need",
    "abstract": "arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM ",
    "link": "https://arxiv.org/abs/2303.07338",
    "context": "Title: Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need\nAbstract: arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM ",
    "path": "papers/23/03/2303.07338.json",
    "total_tokens": 736,
    "translated_title": "重新审视预训练模型下的类增量学习：通用性和适应性就是你所需的一切",
    "translated_abstract": "arXiv:2303.07338v2 公告类型：替换交叉  摘要：类增量学习（CIL）旨在不断获得知识的同时不会忘记学过的旧知识。传统的CIL模型是从零开始训练的，以便随着数据的发展不断获得知识。最近，预训练技术已经取得了显著的进展，使得大量的预训练模型（PTMs）可用于CIL。与传统方法相反，PTMs提供了一种易于转移的通用的嵌入表示，可以为CIL所用。在这项工作中，我们对以PTMs为基础的CIL进行了重新审视，并认为CIL的核心因素在于模型的更新能力和知识的转移能力。1）首先，我们揭示了冻结的PTM能够为CIL提供通用的嵌入表示。令人惊讶的是，一个简单的基线（SimpleCIL），它不断地将PTM的类器设置为原型特征，可以在不进行下游任务训练的情况下击败最先进的算法。2）由于预训练数据集和下游数据集之间的分布差距，PTM的通用嵌入可能不适合立即用于CIL。为了克服这一障碍，我们需要一套适应性和通用性的训练策略，以确保模型的有效适应性和知识的高效利用。",
    "tldr": "类增量学习的研究表明，利用预训练模型和简单更新策略可以取得很好的性能，无需对下游任务进行额外训练。",
    "en_tdlr": "In class-incremental learning, the study shows that by utilizing pre-trained models and a simple updating strategy, good performance can be achieved without the need for additional training on downstream tasks."
}