{
    "title": "Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need",
    "abstract": "arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM ",
    "link": "https://arxiv.org/abs/2303.07338",
    "context": "Title: Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need\nAbstract: arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM ",
    "path": "papers/23/03/2303.07338.json",
    "total_tokens": 415,
    "tldr": "该文章推荐使用预训练模型进行增量学习的思考，并强调适应性和知识转移在增量学习中的核心作用。通过实验表明，即使不针对特定任务进行训练，预训练模型的通用特征表示也能通过简单的原型特征方法在增量学习中取得比当前最佳方法更高的性能。文章作者认为，通过预训练模型的这种通用化和适配性，研究者可以避免从零开始训练模型，从而实现更有效的知识传播和任务适应性。"
}