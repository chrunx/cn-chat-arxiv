# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs](https://arxiv.org/abs/2408.01417) | "我们的研究揭示了多模态语言模型在对话过程中缺乏自适应和形成即兴约定的能力，这些能力是沟通效率提高的关键。" |
| [^2] | [Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2408.01372) | 本文提出了一种新的基于深度可分离卷积操作的空间光谱形态鳗鱼模型，该模型能够生成并增强空间光谱 token，从而提高超光谱图像分类的效率和性能。 |
| [^3] | [EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization](https://arxiv.org/abs/2408.01370) | 本文提出了一种结合事件和惯性传感器数据的多模态跟踪方法，通过使用基于窗口的非线性优化算法，实现了在动态和照明条件变化下的精确事件相机自我运动估计，增强了跟踪器的性能。 |
| [^4] | [Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation](https://arxiv.org/abs/2408.01366) | 本文提出了一种基于任务阶段动态调整多感官优先级的机器人操纵方法，使得机器人能够像人类一样灵活地在不同的任务阶段使用不同的感官信息。 |
| [^5] | [Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation](https://arxiv.org/abs/2408.01363) | 这项研究评估了基于视觉语言模型的自动相关性判断在图像文本检索评价中的潜力，发现GPT-4V在相关性判断方面表现优于CLIP和其他模型，但CLIPScore指标在量化评价中更为优越。 |
| [^6] | [Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2408.01356) | 该研究提出了一种新的平衡残差蒸馏框架（BRD-CIL），通过动态扩展网络结构和平衡的伪标签学习，有效地提高了3D点云分类式增量语义分割的性能。 |
| [^7] | [PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval](https://arxiv.org/abs/2408.01349) | 本文提出PC²框架，融合了伪分类和伪描述的方法，用于解决跨模态检索中的噪声对应学习问题。通过对描述进行分类，并为每个错配对生成伪描述，来提供更具有信息和直观的监督，提高学习效率和模型性能。 |
| [^8] | [StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation](https://arxiv.org/abs/2408.01343) | 本文提出了一种新的多模态特征融合框架StitchFusion，它能够将任何视觉模态有效融合，提高多模态语义分割的准确性。 |
| [^9] | [A Backbone for Long-Horizon Robot Task Understanding](https://arxiv.org/abs/2408.01334) | 该研究提出了一种基于Therblig的框架，通过将高级机器人任务分解为基本配置，并结合基础模型，提高了机器人任务的理解和泛化能力。 |
| [^10] | [TopoNAS: Boosting Search Efficiency of Gradient-based NAS via Topological Simplification](https://arxiv.org/abs/2408.01311) | TopoNAS通过优化搜索策略和简化路径结构，为神经架构搜索提供了一种高效的搜索方法。 |
| [^11] | [TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling](https://arxiv.org/abs/2408.01291) | TexGen是一个用于3D纹理生成的框架，它通过文本描述和预训练的文本到图像扩散模型来生成和组装纹理。这种方法通过多视图采样和重采样技术减少了视图差异，并在保持纹理细节的同时进行了噪声估算和输入指导。 |
| [^12] | [Deep Learning based Visually Rich Document Content Understanding: A Survey](https://arxiv.org/abs/2408.01287) | 本文综述了基于深度学习的丰富视觉文档内容理解框架，分析了现有方法、数据集，并指出了当前面临的主要挑战和未来的研究方向。 |
| [^13] | [Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework](https://arxiv.org/abs/2408.01284) | 本文提出了一种新的通用框架，利用分布外检测技术来解决音频视觉泛化零样本学习中的域发生问题。通过整合生成对抗网络和嵌入方法，该框架提高了对未知类别的准确分类能力。同时，还介绍了一种新的训练和评估策略，以优化模型性能。 |
| [^14] | [Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition Low-Light Image Enhancement](https://arxiv.org/abs/2408.01276) | Wave-Mamba利用小波变换和无损下采样的优势，提出了一种针对超高清低光照增强的新方法，该方法能够有效提升图像的清晰度和对比度，即使在噪声较大的环境中也能保持良好的图像质量。 |
| [^15] | [A General Framework to Boost 3D GS Initialization for Text-to-3D Generation by Lexical Richness](https://arxiv.org/abs/2408.01269) | 本文提出了一种框架，通过提升文本描述中的词汇信息，改善了基于3D Gaussians的文本到3D生成方法的初始化阶段。 |
| [^16] | [S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from a Single Sketch](https://arxiv.org/abs/2408.01218) | 本文提出了一种名为S2TD-Face的方法，可以仅从单幅草图中高保真地重建具有详细几何结构和可控纹理的3D面孔，适用于多种应用程序。 |
| [^17] | [A Weakly Supervised and Globally Explainable Learning Framework for Brain Tumor Segmentation](https://arxiv.org/abs/2408.01191) | 这篇论文提出了一个不需要像素级标注且具有可解释性的新框架，用于快速准确的大脑肿瘤分割。它通过生成保留身份特征但改变类别属性的新样本，有效地分离了类别相关和类别无关的特征。此外，该框架使用拓扑数据分析来创建一个全局可解释的流形，从而为每个需要分割的异常样本生成一个正常样本。 |
| [^18] | [VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling](https://arxiv.org/abs/2408.01181) | VAR-CLIP是一个结合了自回归技术和CLIP能力的文本到图像生成器，它能够生成高质量的图像，并对具体图像内容进行更精确的理解和生成描述。 |
| [^19] | [Rethinking Pre-trained Feature Extractor Selection in Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2408.01167) | 本文研究了在无须进行切片标签注释的情况下，通过多实例学习对整张切片图像进行分类的预训练特征提取器的选择问题。研究对比了多种不同类型的预训练特征提取器，并针对TCGA-NSCLC和Camelyon16两个公共数据集进行了广泛的实验。 |
| [^20] | [PreMix: Boosting Multiple Instance Learning in Digital Histopathology through Pre-training with Intra-Batch Slide Mixing](https://arxiv.org/abs/2408.01162) | PreMix通过预训练内批量切片混合方法提高数字组织病理学的多实例学习效率和准确性，即使在数据量相对较小的任务中也表现出良好的泛化能力。 |
| [^21] | [Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition](https://arxiv.org/abs/2408.01139) | 该研究提出了一种无模型的全局解释性方法，用于理解和评估图像模型在全球扰动下的鲁棒性。通过分析受扰自然图像的谱信噪比随频率的指数下降趋势，揭示了低频信号在模型鲁棒性中的正面作用，并发现高频率信号的贡献与模型的鲁棒性度量负相关。这些发现有助于设计更加鲁棒的模型结构。 |
| [^22] | [PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting Network](https://arxiv.org/abs/2408.01137) | 论文提出了一种名为PGNeXt的全新单阶段框架，该框架通过结合Transformer和CNN网络提取特征，解决了高分辨率显著对象检测中的采样深度与感受野大小之间的矛盾，有效提高了检测的准确性。 |
| [^23] | [IG-SLAM: Instant Gaussian SLAM](https://arxiv.org/abs/2408.01126) | IG-SLAM是一种仅使用RGB的密集SLAM系统，它结合了健壮的密集SLAM跟踪方法和高斯斑点技术，以构建环境的三维地图。通过准确的姿态和密集深度，系统能够优化地图，并采用有效的衰减策略来提高收敛性和运行速度。 |
| [^24] | [An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding](https://arxiv.org/abs/2408.01120) | 本文提出了一种基于Transformer解码器的高效多任务视觉定位框架，该框架在保持高效的同时也能有效融合视觉和语言信息，尤其是在对话语境下的视觉理解任务中取得显著提升。 |
| [^25] | [Prototypical Partial Optimal Transport for Universal Domain Adaptation](https://arxiv.org/abs/2408.01089) | 本文为减轻更大领域的差异提出了一个名为m-PPOT的方法，该方法通过重置权重和优化损失函数提高了泛化能力，并在多种跨领域适配任务上取得了显著效果。 |
| [^26] | [PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote Physiological Measurement](https://arxiv.org/abs/2408.01077) | 本文提出了一种名为PhysMamba的模型，它结合了Mamba-2模型的状态和双流架构来增强rPPG信号在对抗噪声环境下的鲁棒性，并通过跨注意力状态空间对偶模块提升了特征互补性。 |
| [^27] | [Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning](https://arxiv.org/abs/2408.01076) | 本文提出了一种使用预训练文本编码器语义知识进行持续学习的策略，通过在任务之间和任务内部整合语义引导，提高了模型对新数据的可迁移学习能力，并在实验中取得了优于传统方法的保持效果。 |
| [^28] | [Amodal Segmentation for Laparoscopic Surgery Video Instruments](https://arxiv.org/abs/2408.01067) | 本文提出了一种跨模态分割技术，可以识别出腹腔镜手术中器械的完整遮挡部分，提高了手术质量和安全性。 |
| [^29] | [Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model](https://arxiv.org/abs/2408.01044) | 本文提出了一种利用像素级监督的视线对象预测方法，通过与VFM的集成，显著提升了视线对象检测和分割的准确性。 |
| [^30] | [Privacy-Preserving Split Learning with Vision Transformers using Patch-Wise Random and Noisy CutMix](https://arxiv.org/abs/2408.01040) | 本文提出了一种全新的隐私保护分成学习框架，通过在随机选择的颠簸数据块之间进行高斯噪声混合，提高了训练数据的隐私保护水平，同时保持了分类任务的准确性。 |
| [^31] | [Structure from Motion-based Motion Estimation and 3D Reconstruction of Unknown Shaped Space Debris](https://arxiv.org/abs/2408.01035) | 本研究提出了一种基于运动结构的算法，能够利用有限的资源和对未知形状的空间碎片进行运动估计，并同时输出物体的不明形状和相关摄像机的相对姿态轨迹，这些信息用于精确估计目标的运动状态。 |
| [^32] | [PINNs for Medical Image Analysis: A Survey](https://arxiv.org/abs/2408.01026) | 论文要点：本文综述了将物理信息集成到机器学习框架中的医学图像分析方法，探讨了用于MIA的物理知识、建模方式以及这些方法在实际图像分析任务中的应用。 |
| [^33] | [EIUP: A Training-Free Approach to Erase Non-Compliant Concepts Conditioned on Implicit Unsafe Prompts](https://arxiv.org/abs/2408.01014) | EIUP提出了一种新的训练无需方法，通过隐式不安全提示来消除非合规概念，解决了文本到图像扩散模型可能产生的安全问题。 |
| [^34] | [Extracting Object Heights From LiDAR & Aerial Imagery](https://arxiv.org/abs/2408.00967) | 本文提供了一种提取激光雷达和航空影像中物体高度的过程方法，并讨论了其方法在未来激光雷达和影像处理中的应用前景。 |
| [^35] | [PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking Services](https://arxiv.org/abs/2408.00950) | 本研究提出了一种名为PrivateGaze的新方法，该方法能够在不损害视线估计性能的情况下，有效地保护用户在黑盒式移动眼动跟踪服务中的隐私。 |
| [^36] | [A dual-task mutual learning framework for predicting post-thrombectomy cerebral hemorrhage](https://arxiv.org/abs/2408.00940) | 本文提出了一种双任务互惠学习框架，用于仅基于患者的初始CT扫描预测血栓切除术后的脑出血。该框架结合多模态数据的互补信息，并通过自监督学习机制提高预测准确性，具有重要的临床意义。 |
| [^37] | [CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression](https://arxiv.org/abs/2408.00938) | 本研究提出了一种基于临床知识改进的扩散模型，用于更准确地预测特发性肺纤维化（IPF）的进展。 |
| [^38] | [Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)](https://arxiv.org/abs/2408.00932) | 本论文提出使用视觉语言模型来从卫星图像中自动标注城市环境中的各种特征，以此减少对大规模人工标注的需求，并提高了对城市环境下奇特特征描述的质量。 |
| [^39] | [Temporal Evolution of Knee Osteoarthritis: A Diffusion-based Morphing Model for X-ray Medical Image Synthesis](https://arxiv.org/abs/2408.00891) | 该研究提出了一种基于扩散的模型，用于合成患者膝关节骨关节炎疾病进程中的不同阶段的X射线图像，能够帮助医学监测和研究。 |
| [^40] | [Medical SAM 2: Segment medical images as video via Segment Anything Model 2](https://arxiv.org/abs/2408.00874) | 医学 SAM 2 是利用 SAM 2 框架对医学图像进行分割的先进模型，能够将医学图像视为视频，不仅适用于3D医学图像，还能自动在所有后续图像中分割相同的对象，不考虑图像之间的时序关系，性能相较于现有模型有显著提升。 |
| [^41] | [Fuzzy Logic Approach For Visual Analysis Of Websites With K-means Clustering-based Color Extraction](https://arxiv.org/abs/2408.00774) | 本文引入了一种新颖的基于K-means聚类方法，有效预测中文摘要中每个词的颜色，即使存在语义不明确的问题，但仍能准确预测颜色偏好。 |
| [^42] | [Hybrid Deep Learning Framework for Enhanced Melanoma Detection](https://arxiv.org/abs/2408.00772) | 本文提出了一种结合U-Net分割和EfficientNet分类的混合深度学习框架，显著提高了黑色素瘤检测的准确率。 |
| [^43] | [Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves](https://arxiv.org/abs/2408.00768) | 本文提出了一种结合光流、深度学习和空间填满曲线的框架，实现了对车辆前向摄像头捕获的视频数据中交通事件的实时、高效检测。该框架有助于为驾驶员或自动驾驶车辆提供实时反馈，识别前方道路潜在的威胁或突发事件，提高驾驶情况感知，并可能提高安全性。 |
| [^44] | [Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving](https://arxiv.org/abs/2408.00374) | V2INet 提出了一个创新的端到端训练框架，用于结合多角度信息进行轨迹预测，以克服单一视角的局限性，提高校正后的多模态轨迹预测的性能。 |
| [^45] | [XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training](https://arxiv.org/abs/2407.19546) | 本文提出XLIP框架，通过未配对数据增强病理学习和特征学习，引入了注意遮罩图像建模和目标驱动的遮罩语言建模模块，有效提高了病理特征的准确重建。 |
| [^46] | [Domain Adaptive Lung Nodule Detection in X-ray Image](https://arxiv.org/abs/2407.19397) | 本文提出了一种结合自训练和对比学习的方法，用于跨域适应性肺部结节检测，通过改进结节表示和捕获域不变特征来解决数据分布差异的问题。 |
| [^47] | [Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets](https://arxiv.org/abs/2407.19394) | 该论文提出了一种轻量级的深度卷积模块，将其集成到ViT模型中，以提高在小型数据集上的训练效率，同时捕捉图像的局部和全局信息，从而增强了模型的归纳偏差，提高了训练效率和泛化能力。 |
| [^48] | [CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging](https://arxiv.org/abs/2407.11652) | 本文提出了一种名为“跨客户端变异性自适应联邦学习”（CCVA-FL）的方法，通过生成合成医学图像解决医疗图像数据中的跨客户端变异问题，从而提高联邦学习的性能。 |
| [^49] | [Rethinking Learned Image Compression: Context is All You Need](https://arxiv.org/abs/2407.11590) | 本研究表明，通过对上下文模型和解码器的优化，学习了的图像压缩（LIC）的方法可以大幅提高PSNR性能，并在与VVC的比较中取得了14.39%的改进。 |
| [^50] | [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](https://arxiv.org/abs/2407.02371) | OpenVid-1M 是一个精确的高质量数据集，拥有大量的文本视频对，旨在支持文本到视频生成的研究。 |
| [^51] | [Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising](https://arxiv.org/abs/2406.08782) | 该论文提出了一种混合空间-谱去噪网络，利用CNN和Transformer的特征设计了一种新的双路径网络，有效捕捉局部和非局部空间细节，同时减少了计算复杂性。 |
| [^52] | [Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation](https://arxiv.org/abs/2406.07867) | 我们介绍了一个新型的面向面对面对话的口语对话模型，该模型能够处理音频-视觉言语输入并产生相应的回应。这是创建不依赖中间文本虚拟助手的第一步。我们新引入了MultiDialog，这是首个大规模的多模态口语对话语料库，含约340小时的9,000多个对话的平行音频-视觉记录，这些记录是基于开放域对话数据集TopicalChat录制的。 |
| [^53] | [Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance](https://arxiv.org/abs/2406.04551) | 该研究提出了一种称为上下文化Vendi分数指引的方法，旨在提高生成图像的地理多样性，使特定地区的图像表现与现实世界相符。 |
| [^54] | [L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration](https://arxiv.org/abs/2406.03298) | 这篇论文提出了一个名为 L-PR 的框架，该框架利用激光雷达 fiducial标记对多视图点云进行注册，并解决了低重叠情况下的注册问题。 |
| [^55] | [MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection](https://arxiv.org/abs/2404.18849) | 该研究探索了一种共享的视觉编码器如何在单个模型中有效地整合RGB和IR数据，以提高对象检测的性能，同时减少内存占用，适用于自动驾驶和监控等实际应用。 |
| [^56] | [SPIdepth: Strengthened Pose Information for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2404.12501) | SPIdepth通过强化姿势网络，在自监督单目深度估计中实现了对场景细节的高级捕捉和提升。 |
| [^57] | [Generalization Gap in Data Augmentation: Insights from Illumination](https://arxiv.org/abs/2404.07514) | 本研究发现，尽管数据增强能够提高模型在模拟光照条件下的泛化能力，但其性能在真实光照下并未显著提升，表明在自然光照条件下训练的模型可能具有更强的泛化能力。 |
| [^58] | [SAM-guided Graph Cut for 3D Instance Segmentation](https://arxiv.org/abs/2312.08372) | 本文提出了一种新的3D实例分割方法，该方法结合了3D几何信息与多视图图像信息，并通过2D实例分割提升到3D的方式提高了分割的性能和鲁棒性。 |
| [^59] | [The Importance of Downstream Networks in Digital Pathology Foundation Models](https://arxiv.org/abs/2311.17804) | 本文研究揭示了特征提取器对下游网络聚合模型配置的敏感性，指出传统的特征提取器模型评估可能无法准确反映其性能，并强调了在选择基础模型时需对聚合模型的影响予以考虑。 |
| [^60] | [Applications of Spiking Neural Networks in Visual Place Recognition](https://arxiv.org/abs/2311.13186) | 我们的研究提出了模块化脉冲神经网络在视觉位置识别中的应用，包括紧凑的SNN模块、多个代表相同地点的SNN组合，以及对连续图像序列匹配技术的研究，这些技术为机器人视觉系统的精确和低功率视觉位置识别提供了新的框架。 |
| [^61] | [SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios](https://arxiv.org/abs/2309.04421) | 我们提出了一个使用虚拟3D模型的全新框架，该框架使用Unreal Engine生成现实的手势动作，通过节省时间和努力，为驾驶情景中的动态手势生成提供了更高效的方法。 |
| [^62] | [PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans](https://arxiv.org/abs/2308.13651) | 本文提出了一种新颖的方法，即利用图像比较器对输入图像与最可能的K类最近的邻图像进行比较，并根据比较结果对预训练分类器的置信度进行加权，从而显著提升了细粒度图像分类任务中人工智能和人类的预测准确性。 |
| [^63] | [Accurate and Efficient Event-based Semantic Segmentation Using Adaptive Spiking Encoder-Decoder Network](https://arxiv.org/abs/2304.11857) | 本文提出了一种高效的自适应脉冲编码解码网络，能够在基于事件的语义分割任务中利用自适应阈值提高稀疏事件的表示能力，从而在动态事件流中实现准确的分割效果。 |
| [^64] | [Vision Transformers: From Semantic Segmentation to Dense Prediction](https://arxiv.org/abs/2207.09339) | 本研究不仅为密集预测领域提供了一个强大且有力的基线，而且还证明了在没有预训练的情况下，使用ViT进行任务的可行性。在各种密集预测任务上的进一步实验还表明，ViT不仅在常规分割任务中与其他模型竞争，而且还能很好地泛化到其他现实世界应用中，如基于精细生物医学图像分析的新医学诊断脚本。 |

# 详细

[^1]: "少说话，多互动：在多模态LLM中评估上下文对话适应性"

    Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs

    [https://arxiv.org/abs/2408.01417](https://arxiv.org/abs/2408.01417)

    "我们的研究揭示了多模态语言模型在对话过程中缺乏自适应和形成即兴约定的能力，这些能力是沟通效率提高的关键。"

    

    "在本文中，我们研究了人类如何在互动过程中自发地使用越来越高效的言语，通过调整和形成即兴约定。这种现象已经在参考游戏中得到了广泛的研究，显示出人类语言的一些特性，这些特性超出了传达意图的范围。至今尚未探讨的是，多模态大型语言模型(MLLM)是否会像人类一样在互动中提高沟通效率，以及它们可能采用哪些机制来实现这一目标。我们介绍了一个自动框架ICCA，用于评估MLLM在互动中的对话适应性作为一种内在行为。我们评估了几种最先进的多模态语言模型，并观察到尽管它们可能理解他们的对话者的言语变得越来越高效，但它们并不能像人类那样在互动过程中自发地使自己的语言变得更加高效。后者能力只能在某些模型(例如GPT-4)中通过被动的提示方式激发出来。这表明了这种自然语言交互的特性还没有完全被当前的模型所掌握。"

    arXiv:2408.01417v1 Announce Type: cross  Abstract: Humans spontaneously use increasingly efficient language as interactions progress, by adapting and forming ad-hoc conventions. This phenomenon has been studied extensively using reference games, showing properties of human language that go beyond relaying intents. It remains unexplored whether multimodal large language models (MLLMs) similarly increase communication efficiency during interactions, and what mechanisms they may adopt for this purpose. We introduce ICCA, an automated framework to evaluate such conversational adaptation as an in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and observe that while they may understand the increasingly efficient language of their interlocutor, they do not spontaneously make their own language more efficient over time. This latter ability can only be elicited in some models (e.g., GPT-4) with heavy-handed prompting. This shows that this property of linguistic interacti
    
[^2]: 基于空间光谱形态学的鳗鱼模型用于 hyperspectral 图像分类

    Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification

    [https://arxiv.org/abs/2408.01372](https://arxiv.org/abs/2408.01372)

    本文提出了一种新的基于深度可分离卷积操作的空间光谱形态鳗鱼模型，该模型能够生成并增强空间光谱 token，从而提高超光谱图像分类的效率和性能。

    

    最近几年，因为它们的自我注意力机制，Transformer 在超光谱图像分类（HSIC）领域中引起了广泛的关注，并提供了强大的分类性能。然而，这些模型在计算效率方面面临重大挑战，因为它们的复杂性与序列长度成平方增加。Mamba 架构利用状态空间模型提供了一个比Transformer更高效的替代方案。本文介绍了一种新的模型——空间光谱形态鳗鱼（MorpMamba）模型。在MorpMamba模型中，一个 token 生成模块首先将超光谱图像（HSI）的补丁转换为空间光谱 token。这些 token 随后由一个形态学块处理，形态学块使用深度可分离卷积操作计算结构信息和形状信息。一个特征增强模块进一步增强提取的信息，该模块根据计算的空间和平行通道的 token 调整它们的空间和平行通道。

    arXiv:2408.01372v1 Announce Type: new  Abstract: In recent years, Transformers have garnered significant attention for Hyperspectral Image Classification (HSIC) due to their self-attention mechanism, which provides strong classification performance. However, these models face major challenges in computational efficiency, as their complexity increases quadratically with the sequence length. The Mamba architecture, leveraging a State Space Model, offers a more efficient alternative to Transformers. This paper introduces the Spatial-Spectral Morphological Mamba (MorpMamba) model. In the MorpMamba model, a token generation module first converts the Hyperspectral Image (HSI) patch into spatial-spectral tokens. These tokens are then processed by a morphology block, which computes structural and shape information using depthwise separable convolutional operations. The extracted information is enhanced in a feature enhancement module that adjusts the spatial and spectral tokens based on the ce
    
[^3]: EVIT: 基于事件的事件-惯性半密集地图跟踪方法使用窗限非线性优化

    EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization

    [https://arxiv.org/abs/2408.01370](https://arxiv.org/abs/2408.01370)

    本文提出了一种结合事件和惯性传感器数据的多模态跟踪方法，通过使用基于窗口的非线性优化算法，实现了在动态和照明条件变化下的精确事件相机自我运动估计，增强了跟踪器的性能。

    

    arXiv:2408.01370v1 公告类型：交叉  摘要：事件摄像头是一种特殊的视觉外感官器，它对亮度变化做出反应，而不是整合绝对图像强度。由于这种设计，传感器在动态和照明条件恶劣的情况下显示出很强的性能。尽管事件驱动的同步跟踪和映射仍然是一个挑战性的问题，但最近的一些工作已经指出，传感器在基于先验创建的传统传感器地图的跟踪方面的适用性。通过使用跨模态注册范式，相机的大范围照明动态条件下的自我运动可以在准确创建的先验地图上得到跟踪。本文延续了最近介绍的事件驱动几何半密集跟踪范式，并提出了添加惯性信号以增强估计的鲁棒性。更具体地说，添加的信号提供了有关位置和速度的重要线索，这样可以增强对事件相机的精确自我运动估计。此外，结合了事件和惯性测量的多模态信息进一步增强了跟踪器的性能。

    arXiv:2408.01370v1 Announce Type: cross  Abstract: Event cameras are an interesting visual exteroceptive sensor that reacts to brightness changes rather than integrating absolute image intensities. Owing to this design, the sensor exhibits strong performance in situations of challenging dynamics and illumination conditions. While event-based simultaneous tracking and mapping remains a challenging problem, a number of recent works have pointed out the sensor's suitability for prior map-based tracking. By making use of cross-modal registration paradigms, the camera's ego-motion can be tracked across a large spectrum of illumination and dynamics conditions on top of accurate maps that have been created a priori by more traditional sensors. The present paper follows up on a recently introduced event-based geometric semi-dense tracking paradigm, and proposes the addition of inertial signals in order to robustify the estimation. More specifically, the added signals provide strong cues for po
    
[^4]: 基于分数的舞台引导动态多感官融合方法用于机器手操纵

    Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation

    [https://arxiv.org/abs/2408.01366](https://arxiv.org/abs/2408.01366)

    本文提出了一种基于任务阶段动态调整多感官优先级的机器人操纵方法，使得机器人能够像人类一样灵活地在不同的任务阶段使用不同的感官信息。

    

    人类具有在交互环境中灵活地交替不同感觉的惊人天赋。想象一个厨师熟练地根据配料的变化和控制温度，根据颜色、声音和气味的颜色，无缝地导航通过复杂烹饪过程的每一个阶段。这种能力是基于对任务阶段的深刻理解，因为实现每个阶段的子目标是可能需要使用不同的感官。为了赋予机器人类似的能力，我们将在模仿学习过程中结合任务阶段，以相应的指导动态多感官融合。我们提出了MS-Bot，一种基于舞台的动态多感官融合方法，该方法具有从粗到精的阶段理解，根据当前预测阶段的精细状态动态调整模态的优先级。我们训练了一个配备的机器人系统，该系统成功地利用了我们提出的方法在多感官信息融合过程中适应了不同的任务阶段。

    arXiv:2408.01366v1 Announce Type: cross  Abstract: Humans possess a remarkable talent for flexibly alternating to different senses when interacting with the environment. Picture a chef skillfully gauging the timing of ingredient additions and controlling the heat according to the colors, sounds, and aromas, seamlessly navigating through every stage of the complex cooking process. This ability is founded upon a thorough comprehension of task stages, as achieving the sub-goal within each stage can necessitate the utilization of different senses. In order to endow robots with similar ability, we incorporate the task stages divided by sub-goals into the imitation learning process to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a stage-guided dynamic multi-sensory fusion method with coarse-to-fine stage understanding, which dynamically adjusts the priority of modalities based on the fine-grained state within the predicted current stage. We train a robot system equipped
    
[^5]: 使用视觉语言模型对图像文本检索评价自动相关性判断的研究

    Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation

    [https://arxiv.org/abs/2408.01363](https://arxiv.org/abs/2408.01363)

    这项研究评估了基于视觉语言模型的自动相关性判断在图像文本检索评价中的潜力，发现GPT-4V在相关性判断方面表现优于CLIP和其他模型，但CLIPScore指标在量化评价中更为优越。

    

    arXiv:2408.01363v1 Announce Type: 交叉 摘要: 视觉语言模型（VLMs）在各种应用中表现出色，但它们在相关性判断方面的潜力尚不明确。本文在针对多媒体内容创建设定的大规模“non-determined”检索任务中，评估了VLMs，包括CLIP、LLaVA和GPT-4V的相关性估计能力。初步实验表明：(1)当与人类的相关性判断相比较时，无论是开源还是闭源的视觉指导性调整的大型语言模型（LLMs），LLaVA和GPT-4V都能达到显著的 Kendall’s τ ∼ 0.4。这一分数超过了CLIPScore指标。(2)虽然CLIPScore更受青睐，但LLMs对基于CLIP的检索系统的偏见较小。(3)与其他模型相比，GPT-4V的分数分布与人类判断更为吻合，实现了Cohen’s κ值在0.08左右，这超过了CLIPScore的评估，其中GPT-4V的评估分数分布与人类判断更为接近，表现出较好的相关性判断能力。

    arXiv:2408.01363v1 Announce Type: cross  Abstract: Vision--Language Models (VLMs) have demonstrated success across diverse applications, yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs, including CLIP, LLaVA, and GPT-4V, within a large-scale \textit{ad hoc} retrieval task tailored for multimedia content creation in a zero-shot fashion. Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V, encompassing open-source and closed-source visual-instruction-tuned Large Language Models (LLMs), achieve notable Kendall's $\tau \sim 0.4$ when compared to human relevance judgments, surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems. (3) GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's $\kappa$ value of around 0.08, which outperforms CLIPScore at approx
    
[^6]: 平衡残差蒸馏学习方法在3D点云分类式增量语义分割中的应用

    Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation

    [https://arxiv.org/abs/2408.01356](https://arxiv.org/abs/2408.01356)

    该研究提出了一种新的平衡残差蒸馏框架（BRD-CIL），通过动态扩展网络结构和平衡的伪标签学习，有效地提高了3D点云分类式增量语义分割的性能。

    

    arXiv:2408.01356v1 公告类型：新发布  摘要：由于能够在不断增加的新类别中有效处理信息的涌入，并且不会忘记旧知识造成的灾难性遗忘，因此分类式增量学习（CIL）取得了成功。目前的研究尚未考虑目前研究中CIL的性能突破，有效地细化来自基础模型的知识和平衡新的学习是一个问题。在我们的工作中，我们探索了CIL的潜力，并提出了一个基于平衡残差蒸馏框架（BRD-CIL）的新方法，以推动CIL的性能到一个新的更高水平。特别是，BRD-CIL设计了一种残差蒸馏学习策略，它可以通过生成指导矩阵来动态扩展网络结构，捕捉基础和目标模型之间残差的指导矩阵，有效地细化过去的知识。此外，BRD-CIL设计了一种平衡的伪标签学习策略，通过生成指导矩阵来帮助学习算法对类别进行精确定位，同时确保新旧知识之间的平衡。我们通过大量的实验证实了BRD-CIL的有效性，它能够有效地克服CIL中遇到的各种挑战，并显著提高3D点云语义分割的性能。

    arXiv:2408.01356v1 Announce Type: new  Abstract: Class-incremental learning (CIL) thrives due to its success in processing the influx of information by learning from continuously added new classes while preventing catastrophic forgetting about the old ones. It is essential for the performance breakthrough of CIL to effectively refine past knowledge from the base model and balance it with new learning. However, such an issue has not yet been considered in current research. In this work, we explore the potential of CIL from these perspectives and propose a novel balanced residual distillation framework (BRD-CIL) to push the performance bar of CIL to a new higher level. Specifically, BRD-CIL designs a residual distillation learning strategy, which can dynamically expand the network structure to capture the residuals between the base and target models, effectively refining the past knowledge. Furthermore, BRD-CIL designs a balanced pseudo-label learning strategy by generating a guidance ma
    
[^7]: PC²：基于伪分类的伪描述生成方法在跨模态检索中的噪声对应学习

    PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval

    [https://arxiv.org/abs/2408.01349](https://arxiv.org/abs/2408.01349)

    本文提出PC²框架，融合了伪分类和伪描述的方法，用于解决跨模态检索中的噪声对应学习问题。通过对描述进行分类，并为每个错配对生成伪描述，来提供更具有信息和直观的监督，提高学习效率和模型性能。

    

    在这篇论文中，我们探讨了跨模态检索中不同模态数据的无缝集成问题，尤其是对于噪声对应学习(NCL)所带来的复杂性。这种噪声通常源于数据对的错配，这是与传统有噪声标签问题相比的一个显著障碍。我们提出了一个基于伪分类的伪描述生成方法(PC²)框架来应对这一挑战。PC²提供了一个三重策略：首先，我们建立了一个辅助的“伪分类”任务，将描述解释为分类标签，通过非对比机制引导模型学习图像-文本语义相似性。其次，与现有的基于边际的技术不同，我们利用PC²的伪分类能力，生成伪描述，为每个错配对提供更具有信息和直观的监督。最后，我们通过伪描述的振荡来进一步增加损失函数的复杂性，以提升学习效率和模型性能。通过在多个具有挑战性的零样本和少样本检索任务上进行广泛的实验验证，我们的方法优于最先进的同类方法。

    arXiv:2408.01349v1 Announce Type: cross  Abstract: In the realm of cross-modal retrieval, seamlessly integrating diverse modalities within multimedia remains a formidable challenge, especially given the complexities introduced by noisy correspondence learning (NCL). Such noise often stems from mismatched data pairs, which is a significant obstacle distinct from traditional noisy labels. This paper introduces Pseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address this challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an auxiliary "pseudo-classification" task that interprets captions as categorical labels, steering the model to learn image-text semantic similarity through a non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques, capitalizing on PC$^2$'s pseudo-classification capability, we generate pseudo-captions to provide more informative and tangible supervision for each mismatched pair. Thirdly, the oscillation of pse
    
[^8]: StitchFusion: 一种融合任何视觉模态以提高多模态语义分割准确性的方法

    StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation

    [https://arxiv.org/abs/2408.01343](https://arxiv.org/abs/2408.01343)

    本文提出了一种新的多模态特征融合框架StitchFusion，它能够将任何视觉模态有效融合，提高多模态语义分割的准确性。

    

    arXiv:2408.01343v1 公告类型：新  摘要：多模态语义分割在复杂场景中显著提高了分割的准确性。然而，现有的方法往往包含专门的特征融合模块，这些模块特地为特定的模态设计，因此限制了输入的灵活性和增加了训练参数的数量。为了解决这些挑战，我们提出了一种简洁且有效的大模融合框架StitchFusion，该框架直接将大规模预训练模型作为编码器和特征融合器。这种方法在编码过程中实现模融合，通过共享多模态视觉信息，从而促进了信息流在编码过程中的双向传输。通过引入多模态特征融合模块，我们的框架能够更好地处理多模态和多尺度特征融合问题，并能够适应任何视觉模态的输入。特别是，我们的框架通过共享多模态视觉信息在编码过程中实现模融合。为了增强模态间的信息交流，我们引入了一个多方向适配器模块（MultiAdapter），以在编码过程中实现跨模态信息传输。通过利用MultiAdapter，我们的方法实现了多模态特征的有效融合，并在多个公共数据集上取得了优于现有方法的性能。

    arXiv:2408.01343v1 Announce Type: new  Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter t
    
[^9]: 机器人长期任务理解的骨干框架

    A Backbone for Long-Horizon Robot Task Understanding

    [https://arxiv.org/abs/2408.01334](https://arxiv.org/abs/2408.01334)

    该研究提出了一种基于Therblig的框架，通过将高级机器人任务分解为基本配置，并结合基础模型，提高了机器人任务的理解和泛化能力。

    

    arXiv:2408.01334v1 公告类型: 新  翻译摘要: 端到端机器人学习，特别是在长期任务领域，常常导致不可预测的结果和不良的泛化。为了解决这些挑战，我们提出了一种基于Therblig的框架，即TBBF (Therblig-based Backbone Framework)，来提高机器人任务理解的能力和转移性。该框架将高级机器人任务分解为基本机器人配置，使用therbligs（基本动作元素）作为支撑，并与当前的基础模型相结合，以提高任务理解的效果。该方法包括两个阶段：离线训练和在线测试。在离线训练阶段，我们开发了Meta-RGate SynerFusion (MGSF)网络来准确地分割各种任务的therbligs。在线测试阶段，在收集一个新的任务演示之后，我们的MGSF网络提取高阶知识，并通过ActionREG（动作注册）将其编码成图像。此外，我们还开发了一种有效的Meta-Learner，它可以从单个任务的表现中提取知识并泛化到不同任务中，从而提高了模型的泛化能力。这种综合方法通过在多个复杂的实际任务中进行验证，展示了提升机器人任务理解性能的有效性。

    arXiv:2408.01334v1 Announce Type: new  Abstract: End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Additionally
    
[^10]: TopoNAS: 通过拓扑简化提高梯度NAS搜索效率

    TopoNAS: Boosting Search Efficiency of Gradient-based NAS via Topological Simplification

    [https://arxiv.org/abs/2408.01311](https://arxiv.org/abs/2408.01311)

    TopoNAS通过优化搜索策略和简化路径结构，为神经架构搜索提供了一种高效的搜索方法。

    

    arXiv:2408.01311v1 公告类型: 新闻  摘要: Neural Architecture Search (NAS)的搜索效率是目前的一个重要目标。然而，许多现有的方法忽视了搜索策略的通用性，并且在搜索过程中未能减少计算中的冗余，尤其是在一次性NAS架构中。此外，当前的NAS方法在像DARTS这样的常用搜索空间中显示出无效的重新参数化，导致搜索效率低下。在本论文中，我们提出TopoNAS，这是一种不依赖于模型的梯度一次性NAS方法，通过简化搜索路径的拓扑结构，可以显著减少搜索时间和内存使用。首先，我们用模型对搜索空间的非线性进行建模，以揭示参数化的困难。为了提高搜索效率，我们提出了一个拓扑简化方法，并迭代地应用模块共享策略来简化搜索路径的拓扑结构。在添加

    arXiv:2408.01311v1 Announce Type: new  Abstract: Improving search efficiency serves as one of the crucial objectives of Neural Architecture Search (NAS). However, many current approaches ignore the universality of the search strategy and fail to reduce the computational redundancy during the search process, especially in one-shot NAS architectures. Besides, current NAS methods show invalid reparameterization in non-linear search space, leading to poor efficiency in common search spaces like DARTS. In this paper, we propose TopoNAS, a model-agnostic approach for gradient-based one-shot NAS that significantly reduces searching time and memory usage by topological simplification of searchable paths. Firstly, we model the non-linearity in search spaces to reveal the parameterization difficulties. To improve the search efficiency, we present a topological simplification method and iteratively apply module-sharing strategies to simplify the topological structure of searchable paths. In addit
    
[^11]: TexGen：具有多视图采样和重采样的文本指导3D纹理生成

    TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling

    [https://arxiv.org/abs/2408.01291](https://arxiv.org/abs/2408.01291)

    TexGen是一个用于3D纹理生成的框架，它通过文本描述和预训练的文本到图像扩散模型来生成和组装纹理。这种方法通过多视图采样和重采样技术减少了视图差异，并在保持纹理细节的同时进行了噪声估算和输入指导。

    

    给定一个3D网格，我们的目的是生成与任意文本描述相对应的3D纹理。当前从采样视图生成的纹理往往导致突出的接缝或过度的平滑处理。为了解决这些问题，我们提出了一种名为TexGen的新的多视图采样和重采样框架，用于利用预训练的文本到图像扩散模型来进行纹理生成。首先，我们在RGB空间保持一张纹理图，该图参数化于去噪步骤，并在扩散模型的每次采样步骤后更新，以逐渐减少视图差异。我们利用了一个注意力指导的多视图采样策略来在视图中传播外观信息。为了保留纹理细节，我们开发了一种噪声重采样技术，它帮助估计噪声，生成对后续去噪步骤有指导性的输入，而这些步骤是由文本提示和当前纹理状态定制的。随后，一种自聚焦金字塔结构被集成到纹理重采样过程中，用于捕获多尺度细节。最后，通过调节纹理的不同特征，纹理的平滑性和细节程度可根据文本描述进行调整。通过在多个基准数据集上的实验，我们展示了TexGen能够高效稳定地在多姿态和多光照条件下生成高质量的纹理，而其多尺度细节捕获和自聚焦机制能够避免在纹理对齐过程中出现显著的接缝和模糊。

    arXiv:2408.01291v1 Announce Type: new  Abstract: Given a 3D mesh, we aim to synthesize 3D textures that correspond to arbitrary textual descriptions. Current methods for generating and assembling textures from sampled views often result in prominent seams or excessive smoothing. To tackle these issues, we present TexGen, a novel multi-view sampling and resampling framework for texture generation leveraging a pre-trained text-to-image diffusion model. For view consistent sampling, first of all we maintain a texture map in RGB space that is parameterized by the denoising step and updated after each sampling step of the diffusion model to progressively reduce the view discrepancy. An attention-guided multi-view sampling strategy is exploited to broadcast the appearance information across views. To preserve texture details, we develop a noise resampling technique that aids in the estimation of noise, generating inputs for subsequent denoising steps, as directed by the text prompt and curre
    
[^12]: 基于深度学习的丰富视觉文档内容理解综述

    Deep Learning based Visually Rich Document Content Understanding: A Survey

    [https://arxiv.org/abs/2408.01287](https://arxiv.org/abs/2408.01287)

    本文综述了基于深度学习的丰富视觉文档内容理解框架，分析了现有方法、数据集，并指出了当前面临的主要挑战和未来的研究方向。

    

    arXiv:2408.01287v1 公告类型：跨学科 Abstract: 丰富视觉文档（VRD）在学术、金融、医疗和营销等领域因其多模式信息内容而至关重要。传统的从VRD中提取信息的方法依赖于专家知识和人工劳动，这使得它们既昂贵又低效。深度学习的发展为这个过程带来了革命，它介绍了一系列模型，这些模型利用多模式信息包括视觉、文本和布局，以及预训练任务来发展全面的文档表示。这些模型在各种下游任务上实现了先进的性能，显著提高了从VRD提取信息的工作效率和准确性。鉴于对丰富视觉文档理解（VRDU）的需求增长和快速发展，本文提供了基于深度学习的VRDU框架的全面回顾。我们系统地调查和分析了现有的方法和基准数据集，讨论了当前面临的主要挑战和未来的研究方向，旨在为该领域的研究提供一个全面的视角，并促进新颖方法的发展。

    arXiv:2408.01287v1 Announce Type: cross  Abstract: Visually Rich Documents (VRDs) are essential in academia, finance, medical fields, and marketing due to their multimodal information content. Traditional methods for extracting information from VRDs depend on expert knowledge and manual labor, making them costly and inefficient. The advent of deep learning has revolutionized this process, introducing models that leverage multimodal information vision, text, and layout along with pretraining tasks to develop comprehensive document representations. These models have achieved state-of-the-art performance across various downstream tasks, significantly enhancing the efficiency and accuracy of information extraction from VRDs. In response to the growing demands and rapid developments in Visually Rich Document Understanding (VRDU), this paper provides a comprehensive review of deep learning-based VRDU frameworks. We systematically survey and analyze existing methods and benchmark datasets, ca
    
[^13]: 音频视觉泛化零样本学习中的分布外检测：一个通用的框架

    Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework

    [https://arxiv.org/abs/2408.01284](https://arxiv.org/abs/2408.01284)

    本文提出了一种新的通用框架，利用分布外检测技术来解决音频视觉泛化零样本学习中的域发生问题。通过整合生成对抗网络和嵌入方法，该框架提高了对未知类别的准确分类能力。同时，还介绍了一种新的训练和评估策略，以优化模型性能。

    

    本文研究了音频视觉泛化零样本学习（GZSL）中的影响。GZSL任务要求对既有训练数据又有测试数据的样本进行准确分类。在多模式输入中，同时包含视觉和声学特征使其成为了一个极具挑战性的任务。虽然现有的研究大多集中在嵌入方法和生成方法上，但由于训练不稳定性和生成方法之间的差距，我们对将这两种方法整合到一个框架中持乐观态度。我们提出了一种新的通用框架，利用了分布外（OOD）检测技术，旨在利用这些方法的优势同时缓解它们各自的缺点。我们首先使用生成对抗网络来模拟未见过的特征，并在这个过程中训练一个OOD检测器。这是通用框架的一种创新方法，旨在解决泛化零样本分类中出现的域发生问题。我们的实采纳生成对抗网络和嵌入方法，通过整合在一起来提高训练数据的质量和多样性。此外，我们还介绍了一种新的训练和评估策略，以有效地评估模型对未见过的类别的性能。

    arXiv:2408.01284v1 Announce Type: cross  Abstract: Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. However, generative training is difficult and unstable, while embedding-based methods often encounter domain shift problem. Thus, we find it promising to integrate both methods into a unified framework to leverage their advantages while mitigating their respective disadvantages. Our study introduces a general framework employing out-of-distribution (OOD) detection, aiming to harness the strengths of both approaches. We first employ generative adversarial networks to synthesize unseen features, enabling the training of an OOD detector alongside cla
    
[^14]: Wave-Mamba：适用于超高清低光照增强的波尔状态空间模型

    Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition Low-Light Image Enhancement

    [https://arxiv.org/abs/2408.01276](https://arxiv.org/abs/2408.01276)

    Wave-Mamba利用小波变换和无损下采样的优势，提出了一种针对超高清低光照增强的新方法，该方法能够有效提升图像的清晰度和对比度，即使在噪声较大的环境中也能保持良好的图像质量。

    

    超高清（UHD）技术因其卓越的视觉效果而受到广泛关注，但它也对低光照图像增强（LLIE）技术提出了新的挑战。UHD图像固有的高计算复杂性使得现有的UHD LLIE方法在降低计算成本时采用了高倍率下采样，这又导致了信息的损失。小波变换不仅允许无损下采样，而且可以将图像内容与噪声分离。它使状态空间模型（SSM）在不受噪声影响的情况下建模长序列，从而充分利用了SSM的长序列建模能力。在此基础上，我们提出了一种名为Wave-Mamba的全新方法，该方法基于小波域的两个关键洞察：1）图像中的绝大多数内容信息存在于低频成分中，高频成分中的较少；2）SSMs可以避免在高噪声条件下建模带来的问题，因为小波变换会分离噪声。基于这些洞察，Wave-Mamba能够实现对超高清低光照图像的高效和高质量增强，同时保留图像的细节和对比度，并在噪声水平较高的情况下仍然保持清晰度。我们实现了Wave-Mamba，并通过实验验证了其在UHD LLIE方面的有效性和优越性。

    arXiv:2408.01276v1 Announce Type: new  Abstract: Ultra-high-definition (UHD) technology has attracted widespread attention due to its exceptional visual quality, but it also poses new challenges for low-light image enhancement (LLIE) techniques. UHD images inherently possess high computational complexity, leading existing UHD LLIE methods to employ high-magnification downsampling to reduce computational costs, which in turn results in information loss. The wavelet transform not only allows downsampling without loss of information, but also separates the image content from the noise. It enables state space models (SSMs) to avoid being affected by noise when modeling long sequences, thus making full use of the long-sequence modeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel approach based on two pivotal insights derived from the wavelet domain: 1) most of the content information of an image exists in the low-frequency component, less in the high-frequency componen
    
[^15]: 文本到3D内容创建的通用框架，通过词汇丰富性提高3D GS初始化

    A General Framework to Boost 3D GS Initialization for Text-to-3D Generation by Lexical Richness

    [https://arxiv.org/abs/2408.01269](https://arxiv.org/abs/2408.01269)

    本文提出了一种框架，通过提升文本描述中的词汇信息，改善了基于3D Gaussians的文本到3D生成方法的初始化阶段。

    

    arXiv:2408.01269v1 公告类型：新  翻译摘要：文本到3D内容创建最近受到很大关注，尤其是在3D Gaussians Splatting普及的情况下。一般来说，基于GS的方法包括两个关键阶段：初始化和渲染优化。为了达到初始化，现有的工作直接应用随机球体初始化或者3D扩散模型，比如Point-E，来获取初始形状。然而，这样的策略存在着两个根本性的挑战：1）即使经过训练，最终形状仍然与初始形状相似；2）对于简单文本，如“一只狗”，形状可以被产生，但对于词汇丰富的文本，如“一只狗坐在飞机的顶部”，则不能。为了解决这些问题，本文提出了一种新的通用框架，利用词汇丰富性提高文本到3D生成的3D GS初始化。我们的关键想法是将3D Gaussians聚集到空间均匀的voxels中来表示复杂的形状，同时确保了复杂形状的表达能力，并为更丰富描述的语言提供了详细的3D表示。

    arXiv:2408.01269v1 Announce Type: new  Abstract: Text-to-3D content creation has recently received much attention, especially with the prevalence of 3D Gaussians Splatting. In general, GS-based methods comprise two key stages: initialization and rendering optimization. To achieve initialization, existing works directly apply random sphere initialization or 3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such strategies suffer from two critical yet challenging problems: 1) the final shapes are still similar to the initial ones even after training; 2) shapes can be produced only from simple texts, e.g., "a dog", not for lexically richer texts, e.g., "a dog is sitting on the top of the airplane". To address these problems, this paper proposes a novel general framework to boost the 3D GS Initialization for text-to-3D generation upon the lexical richness. Our key idea is to aggregate 3D Gaussians into spatially uniform voxels to represent complex shapes while enab
    
[^16]: S2TD-Face: 从单幅草图重建具有可控纹理的详细3D面孔

    S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from a Single Sketch

    [https://arxiv.org/abs/2408.01218](https://arxiv.org/abs/2408.01218)

    本文提出了一种名为S2TD-Face的方法，可以仅从单幅草图中高保真地重建具有详细几何结构和可控纹理的3D面孔，适用于多种应用程序。

    

    本文提出了一种从草图重建可控纹理和详细3D面孔的新方法，名为S2TD-Face。S2TD-Face引入了一种两阶段的几何重建框架，可以直接从输入草图中重建详细的几何结构。为了保持几何结构与草图精细笔触的一致性，我们提出了一种新的草图至几何损失，即 \textbf{descriptive text}，用以指导网络在几何重建过程中的笔触控制。此外，为了保留草图中纹理的丰富性，S2TD-Face还设计了一个从草图纹理生成3D空间内纹理映射的方法，使得重建的3D面孔既具有细节丰富的几何结构，也覆盖了纹理细节。通过实验验证，S2TD-Face可以应用于多种场景，如动画制作、3D虚拟形象创建、艺术设计、失踪人员搜索等。

    arXiv:2408.01218v1 Announce Type: new  Abstract: 3D textured face reconstruction from sketches applicable in many scenarios such as animation, 3D avatars, artistic design, missing people search, etc., is a highly promising but underdeveloped research topic. On the one hand, the stylistic diversity of sketches leads to existing sketch-to-3D-face methods only being able to handle pose-limited and realistically shaded sketches. On the other hand, texture plays a vital role in representing facial appearance, yet sketches lack this information, necessitating additional texture control in the reconstruction process. This paper proposes a novel method for reconstructing controllable textured and detailed 3D faces from sketches, named S2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework that directly reconstructs detailed geometry from the input sketch. To keep geometry consistent with the delicate strokes of the sketch, we propose a novel sketch-to-geometry loss that 
    
[^17]: 论文标题: 一个弱监督和全局可解释的大脑肿瘤分割学习框架

    A Weakly Supervised and Globally Explainable Learning Framework for Brain Tumor Segmentation

    [https://arxiv.org/abs/2408.01191](https://arxiv.org/abs/2408.01191)

    这篇论文提出了一个不需要像素级标注且具有可解释性的新框架，用于快速准确的大脑肿瘤分割。它通过生成保留身份特征但改变类别属性的新样本，有效地分离了类别相关和类别无关的特征。此外，该框架使用拓扑数据分析来创建一个全局可解释的流形，从而为每个需要分割的异常样本生成一个正常样本。

    

    arXiv:2408.01191v1 新闻类型: 新更新 摘要: 基于机器的大脑肿瘤分割可以帮助医生做出更好的诊断。然而，大脑肿瘤的复杂结构以及昂贵的像素级标注给自动肿瘤分割带来了挑战。在这篇论文中，我们提出了一种反事实生成框架，该框架不仅在不需要像素级标注的情况下达到了异常的大脑肿瘤分割性能，而且还提供了可解释性。我们的框架有效地区分了样本的类别相关特征和类别无关特征，通过嵌入不同的类别相关特征，生成新的样本以保留身份特征并更改类别属性。我们对提取出的类别相关特征进行拓扑数据分析，获取一个全局可解释的流形，对于要分割的每个异常样本，都可以有效地生成一个具有规则路径指导的正常样本，该路径设计旨在在不同类型的样本中表达不同类别的类相关特征。

    arXiv:2408.01191v1 Announce Type: new  Abstract: Machine-based brain tumor segmentation can help doctors make better diagnoses. However, the complex structure of brain tumors and expensive pixel-level annotations present challenges for automatic tumor segmentation. In this paper, we propose a counterfactual generation framework that not only achieves exceptional brain tumor segmentation performance without the need for pixel-level annotations, but also provides explainability. Our framework effectively separates class-related features from class-unrelated features of the samples, and generate new samples that preserve identity features while altering class attributes by embedding different class-related features. We perform topological data analysis on the extracted class-related features and obtain a globally explainable manifold, and for each abnormal sample to be segmented, a meaningful normal sample could be effectively generated with the guidance of the rule-based paths designed w
    
[^18]: VAR-CLIP: 使用视觉自回归建模的文本到图像生成器

    VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling

    [https://arxiv.org/abs/2408.01181](https://arxiv.org/abs/2408.01181)

    VAR-CLIP是一个结合了自回归技术和CLIP能力的文本到图像生成器，它能够生成高质量的图像，并对具体图像内容进行更精确的理解和生成描述。

    

    arXiv:2408.01181v1 公告类型：新消息  摘要：VAR是一种新的范式，利用“下一级预测”而不是“下一个 token 预测”。这种创新的转变能够使自回归（AR）变换器快速学习视觉分布并实现稳健的泛化能力。然而，原始的VAR模型只能在类条件合成下使用，它完全依赖于文本描述作为指导。在这篇论文中，我们介绍了一种名为VAR-CLIP的新型文本到图像模型，它将自回归视觉技术与CLIP的 capabilities 结合起来。VAR-CLIP框架将描述编码为文本嵌入，这些嵌入随后作为图像生成的文本条件使用。为了在像ImageNet这样的大型数据集上进行训练，我们构建了一个相当大的图像-文本数据集，利用了BLIP2。此外，我们还探讨了CLIP中单词定位的重要性，以用于描述指导的目的。广泛的实验确认了VAR-CLIP的卓越性能，能够生成高质量的图像，并且能够更好地理解和生成描述数据集中的具体图像内容。

    arXiv:2408.01181v1 Announce Type: new  Abstract: VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP's pr
    
[^19]: 重思面向多实例学习在整张切片图像分类中的预训练特征提取器选择

    Rethinking Pre-trained Feature Extractor Selection in Multiple Instance Learning for Whole Slide Image Classification

    [https://arxiv.org/abs/2408.01167](https://arxiv.org/abs/2408.01167)

    本文研究了在无须进行切片标签注释的情况下，通过多实例学习对整张切片图像进行分类的预训练特征提取器的选择问题。研究对比了多种不同类型的预训练特征提取器，并针对TCGA-NSCLC和Camelyon16两个公共数据集进行了广泛的实验。

    

    arXiv:2408.01167v1 通告类型: 新摘要: 对于在没有需要切片标签注释的情况下对百万像素整张切片图像 (WSIs) 进行分类，多实例学习 (MIL) 已成为一个受到青睐的方法。当前的多实例学习研究热潮集中在基于嵌入的学习方法上，该方法涉及到使用预先在 ImageNet-1K 上训练的 ResNet50 监督模型提取切片块的特征向量。这些特征向量然后被馈送到一个多实例聚合器以对整个切片进行级别预测。尽管有前导研究建议增强在 ImageNet-1K 上预训练的热门 ResNet50 模型的性能，但仍然缺乏对选择最佳特征提取器以最大化 WSI 性能的明确指导。本研究旨在通过评估基于四项最先进的 MIL 模型在两个公共 WSIs 数据集 (TCGA-NSCLC 和 Camelyon16) 上的三维度特征提取器来填补这一空白：预训练数据集、骨干模型和预训练方法。在四个 SOTA MIL 模型上进行了广泛的实验。这些特征向量然后被馈送到一个多实例聚合器以对整个切片进行级别预测。尽管有前主导研究建议增强在 ImageNet-1K 上预训练的热门 ResNet50 模型的性能，但仍然缺乏对选择最佳特征提取器以最大化 WSI 性能的明确指导。本研究旨在通过评估基于四项最先进的 MIL 模型在两个公共 WSIs 数据集 (TCGA-NSCLC 和 Camelyon16) 上的三维度特征提取器来填补这一空白：预训练数据集、骨干模型和预训练方法。在四个 SOTA MIL 模型上进行了广泛的实验。这些特征向量然后被馈送到一个多实例聚合器以对整个切片进行级别预测。尽管有前主导研究建议增强在 ImageNet-1K 上预训练的热门 ResNet50 模型的性能，但仍然缺乏对选择最佳特征提取器以最大化 WSI 性能的明确指导。本研究旨在通过评估基于四项最先进的 MIL 模型在两个公共 WSIs 数据集 (TCGA-NSCLC 和 Camelyon16) 上的三维度特征提取器来填补这一空白：预训练数据集、骨干模型和预训练方法。在四个 SOTA MIL 模型上进行了广泛的实验。

    arXiv:2408.01167v1 Announce Type: new  Abstract: Multiple instance learning (MIL) has become a preferred method for classifying gigapixel whole slide images (WSIs), without requiring patch label annotation. The focus of the current MIL research stream is on the embedding-based MIL approach, which involves extracting feature vectors from patches using a pre-trained feature extractor. These feature vectors are then fed into an MIL aggregator for slide-level prediction. Despite prior research suggestions on enhancing the most commonly used ResNet50 supervised model pre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting the optimal feature extractor to maximize WSI performance. This study aims at addressing this gap by examining MIL feature extractors across three dimensions: pre-training dataset, backbone model, and pre-training method. Extensive experiments were carried out on the two public WSI datasets (TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The
    
[^20]: PreMix:通过预训练内批量切片混合提高数字组织病理学的多实例学习

    PreMix: Boosting Multiple Instance Learning in Digital Histopathology through Pre-training with Intra-Batch Slide Mixing

    [https://arxiv.org/abs/2408.01162](https://arxiv.org/abs/2408.01162)

    PreMix通过预训练内批量切片混合方法提高数字组织病理学的多实例学习效率和准确性，即使在数据量相对较小的任务中也表现出良好的泛化能力。

    

    arXiv:2408.01162v1 公告类型: 新  摘要: 对以高分辨率扫描仪获得的组织学切片数字化表示的整张切片图像(WSIs)进行分类，面临着与细致且耗时的精细粒度标注相关的重大挑战。尽管弱监督的多实例学习(MIL)已经显示出作为一种有吸引力的方法，但现有的MIL方法在从零开始在特征提取过程中训练MIL特征聚合器的能力上受到了限制。这往往需要在大规模未标注WSIs上从头开始训练MIL特征聚合器，从而阻碍效率和准确性。PreMix通过内批量切片混合方法扩展了MIL框架的预训练。具体来说，PreMix在预训练期间结合了Barlow Twins Slide Mixing，提高了处理不同WSIs大小的能力，并最大限度地利用了未标注WSIs的价值。结合高分辨率采样和动态随机投影，PreMix学习到的特征能够较好地区分病理和正常细胞，即使在数据量相对较小的任务中也能表现出良好的泛化能力。实验表明，与当前最先进的MIL方法相比，PreMix在数字组织病理学数据集上的性能得到了提升。

    arXiv:2408.01162v1 Announce Type: new  Abstract: The classification of gigapixel-sized whole slide images (WSIs), digital representations of histological slides obtained via a high-resolution scanner, faces significant challenges associated with the meticulous and time-consuming nature of fine-grained labeling. While weakly-supervised multiple instance learning (MIL) has emerged as a promising approach, current MIL methods are constrained by their limited ability to leverage the wealth of information embedded within unlabeled WSIs. This limitation often necessitates training MIL feature aggregators from scratch after the feature extraction process, hindering efficiency and accuracy. PreMix extends the general MIL framework by pre-training the MIL aggregator with an intra-batch slide mixing approach. Specifically, PreMix incorporates Barlow Twins Slide Mixing during pre-training, enhancing its ability to handle diverse WSI sizes and maximizing the utility of unlabeled WSIs. Combined wit
    
[^21]: 使用定理谱重要性分解解释图像模型的全局扰动鲁棒性

    Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition

    [https://arxiv.org/abs/2408.01139](https://arxiv.org/abs/2408.01139)

    该研究提出了一种无模型的全局解释性方法，用于理解和评估图像模型在全球扰动下的鲁棒性。通过分析受扰自然图像的谱信噪比随频率的指数下降趋势，揭示了低频信号在模型鲁棒性中的正面作用，并发现高频率信号的贡献与模型的鲁棒性度量负相关。这些发现有助于设计更加鲁棒的模型结构。

    

    arXiv:2408.01139v1 公告类型: 交叉 摘要: 扰动鲁棒性评估了模型对各种扰动的脆弱性，包括数据污染和 adversarial攻击。理解扰动鲁棒性的机制对于全局解释性至关重要。我们提出了一种无模型的全局机制解释性方法，用于解释图像模型的扰动鲁棒性。这项研究受到两个关键因素的启发。首先，以前的全球解释性工作与鲁棒性基准（例如平均污染错误mCE）同时进行，并不是为了直接解释图像模型中扰动鲁棒性的工作机制。其次，我们注意到，受扰自然图像的谱信噪比（SNR）随频率指数下降。这种幂律类似的下降表明：低频信号通常比高频信号更鲁棒——然而，高分类精度并不能保证模型的鲁棒性。我们还进一步洞察到，模型的鲁棒性度量-mCE和高频信号的贡献有负相关性，这意味着在高频信号较小的图像区域中，即使存在高噪声水平，模型的鲁棒性通常也很高。这些发现揭示了高频率信号在模型鲁棒性中的负面作用，并为模型结构的设计提供了上下文。例如，即使对于轻度扰动，具有良好鲁棒性的模型也倾向于在低频信号更大的空间区域中保持更高的SNR值。最后，我们在多个图像模型上进行了广泛实验，展示了该方法的有效性和洞察力。总的来说，我们的工作扩展了对图像模型鲁棒性的全球解释性理解和评估方法，有助于推动后续的模型设计、理解和优化工作。下方是该论文的英文标题和摘要，请注意，对于以下的tldr和en_tldr部分，我会总结出一个中文和英文版的概要：

    arXiv:2408.01139v1 Announce Type: cross  Abstract: Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals -- yet high classification accuracy can not be ac
    
[^22]: PGNeXt：基于金字塔嫁接网络的超高分辨率显著对象检测

    PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting Network

    [https://arxiv.org/abs/2408.01137](https://arxiv.org/abs/2408.01137)

    论文提出了一种名为PGNeXt的全新单阶段框架，该框架通过结合Transformer和CNN网络提取特征，解决了高分辨率显著对象检测中的采样深度与感受野大小之间的矛盾，有效提高了检测的准确性。

    

    我们提出了一种先进的研究，专注于更高难度的超高分辨率显著对象检测（UHRSD），从数据集和网络框架两个角度进行考量。为了弥补过去高分辨率显著对象检测（HRSOD）数据集的空缺，我们细心收集了一个大规模的高分辨率显著对象检测数据集，即UHRSD数据集，包含来自复杂现实世界场景的5,920张图片，分辨率达到了4K-8K级别。所有的图片均进行了精细的像素级别标注，远远超出了先前低分辨率SOD数据集的水平。为了解决过去方法中采样深度与感受野大小之间的矛盾，我们提出了一种针对HR-SOD任务的全新单阶段框架，采用了金字塔嫁接机制。总的来说，该框架采用了基于Transformer和CNN的骨干网络独立提取不同分辨率图像的特征，然后将这些特征从Transformer分支嫁接到CNN分支。设计了一种基于注意力的跨模型嫁接模块（CMGM），通过学习不同模型的强关联愿意，克服传统方法的局限性。

    arXiv:2408.01137v1 Announce Type: new  Abstract: We present an advanced study on more challenging high-resolution salient object detection (HRSOD) from both dataset and network framework perspectives. To compensate for the lack of HRSOD dataset, we thoughtfully collect a large-scale high resolution salient object detection dataset, called UHRSD, containing 5,920 images from real-world complex scenarios at 4K-8K resolutions. All the images are finely annotated in pixel-level, far exceeding previous low-resolution SOD datasets. Aiming at overcoming the contradiction between the sampling depth and the receptive field size in the past methods, we propose a novel one-stage framework for HR-SOD task using pyramid grafting mechanism. In general, transformer-based and CNN-based backbones are adopted to extract features from different resolution images independently and then these features are grafted from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) i
    
[^23]: IG-SLAM: 即时高斯SLAM

    IG-SLAM: Instant Gaussian SLAM

    [https://arxiv.org/abs/2408.01126](https://arxiv.org/abs/2408.01126)

    IG-SLAM是一种仅使用RGB的密集SLAM系统，它结合了健壮的密集SLAM跟踪方法和高斯斑点技术，以构建环境的三维地图。通过准确的姿态和密集深度，系统能够优化地图，并采用有效的衰减策略来提高收敛性和运行速度。

    

    arXiv:2408.01126v1 宣言类型: 交叉 Abstract: 3D高斯斑点最近被证明是SLAM系统中场景表示的替代方案，与神经隐式表示相比，它显示了令人鼓舞的结果。然而，现有的方法要么缺乏用于指导映射过程的密集深度图，要么在考虑环境大小时没有详尽的训练设计。为了解决这些缺点，我们提出了IG-SLAM，一个仅使用RGB的密集SLAM系统，它采用了健壮的密集SLAM跟踪方法，并将它们与高斯斑点相结合。使用跟踪提供的精确姿态和密集深度，环境的三维地图被构建起来。此外，我们还利用深度不确定性来优化地图，以提高3D重建的质量。我们在地图优化中使用的衰减策略提高了收敛性，并允许系统在一进程中以10帧每秒的速度运行。我们证明了在我们的实验中对与现有技术先进的纯RGB SLAM系统表现出竞争性性能，同时实现了更快的运行速度。我们展示了在...

    arXiv:2408.01126v1 Announce Type: cross  Abstract: 3D Gaussian Splatting has recently shown promising results as an alternative scene representation in SLAM systems to neural implicit representations. However, current methods either lack dense depth maps to supervise the mapping process or detailed training designs that consider the scale of the environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only SLAM system that employs robust Dense-SLAM methods for tracking and combines them with Gaussian Splatting. A 3D map of the environment is constructed using accurate pose and dense depth provided by tracking. Additionally, we utilize depth uncertainty in map optimization to improve 3D reconstruction. Our decay strategy in map optimization enhances convergence and allows the system to run at 10 fps in a single process. We demonstrate competitive performance with state-of-the-art RGB-only SLAM systems while achieving faster operation speeds. We present our experiments on
    
[^24]: 高效有效的Transformer解码器基多任务视觉定位框架

    An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding

    [https://arxiv.org/abs/2408.01120](https://arxiv.org/abs/2408.01120)

    本文提出了一种基于Transformer解码器的高效多任务视觉定位框架，该框架在保持高效的同时也能有效融合视觉和语言信息，尤其是在对话语境下的视觉理解任务中取得显著提升。

    

    到目前为止，大多数先进的视觉定位技术依赖于Transformer来融合视觉语言特征。但这种基于Transformer的方法存在一个重大缺陷：Transformer编码器中的自我注意机制导致的计算成本呈二次方增长，尤其是在处理高分辨率图像或者长文本上下文时。这种计算成本的二次增长限制了视觉定位技术在更复杂的场景中的应用，例如对话推理切片，它需要长篇的语言表达。本文提出了一种基于Transformer解码器的多任务视觉定位（EEVG）框架，以解决这一问题，该框架在语言和视觉方面都降低了成本。在语言方面，我们使用Transformer解码器来融合视觉和语言特征，其中语言特征被输入作为记忆，而视觉特征则是沿着编码器逐层传递。这种设计大大减少了结构化的语言特征与图像特征之间一系列的自我注意操作的计算代价。此外，我们还提出了有针对性的训练策略和可重用组件，如双注意力模块，它可以在训练过程中有效融合语言信息和视觉表示，并且在推理阶段能更快地提供泛化能力。我们证明，EEVG比先前的方法在对话语境下的视觉理解任务上取得了显著的性能提升，并且在处理具体任务，如图像标签上下文时，也展现了更好的通用性和效率。

    arXiv:2408.01120v1 Announce Type: new  Abstract: Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual 
    
[^25]: 半优化的典型部分最优运输理论在跨领域适配中的普遍应用

    Prototypical Partial Optimal Transport for Universal Domain Adaptation

    [https://arxiv.org/abs/2408.01089](https://arxiv.org/abs/2408.01089)

    本文为减轻更大领域的差异提出了一个名为m-PPOT的方法，该方法通过重置权重和优化损失函数提高了泛化能力，并在多种跨领域适配任务上取得了显著效果。

    

    跨领域适配（UniDA）旨在从带标签的源领域向无标签的目标领域传输知识，无需两者具有相同的标签集。领域和类别偏移的存在使得任务具有挑战性，并要求我们对两者中的样本进行区分，这些样本具有在领域中的标签，但同时在目标领域中却没有标签。在论文中，我们考虑到分布匹配问题，我们只需要对分布进行部分对齐。提出了一种名为mini-batch Prototypical Partial Optimal Transport（m-PPOT）的全新方法，旨在为UniDA进行部分分布对齐。在训练阶段，除了最小化m-PPOT之外，我们还在运输计划中重置源原型和目标样本的权重，并设计了重置熵损失和重置权重来改进学习效果。同时，我们还提出了一种用于衡量模型稳健性的uniform converage criterion，并定义了基于该准则的损失函数，从而加强了模型在泛化能力上的改进。最后，通过在多种跨领域适配任务上的实验，证明了我们的方法在减轻领域差异和提高预测性能方面的有效性。

    arXiv:2408.01089v1 Announce Type: new  Abstract: Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain without requiring the same label sets of both domains. The existence of domain and category shift makes the task challenging and requires us to distinguish "known" samples (i.e., samples whose labels exist in both domains) and "unknown" samples (i.e., samples whose labels exist in only one domain) in both domains before reducing the domain gap. In this paper, we consider the problem from the point of view of distribution matching which we only need to align two distributions partially. A novel approach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is proposed to conduct partial distribution alignment for UniDA. In training phase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT to reweight source prototypes and target samples, and design reweighted entropy loss and reweigh
    
[^26]: PhysMamba: 利用双流跨注意力SSD进行远程生理测量

    PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote Physiological Measurement

    [https://arxiv.org/abs/2408.01077](https://arxiv.org/abs/2408.01077)

    本文提出了一种名为PhysMamba的模型，它结合了Mamba-2模型的状态和双流架构来增强rPPG信号在对抗噪声环境下的鲁棒性，并通过跨注意力状态空间对偶模块提升了特征互补性。

    

    arXiv:2408.01077v1 宣布类型：新  摘要：远程光电容积脉搏图（rPPG）是一种从面部视频中提取生理信号的非接触技术，用于情绪监测、医疗援助和反面部欺骗等应用。与受控的实验室环境相比，真实世界环境中经常存在运动干扰和噪声，这影响现有方法的表现。为了解决这个问题，我们提出了PhysMamba，一个基于Mamba的时间频谱交互双流模型。PhysMamba集成了最先进的Mamba-2模型，并采用了双流架构来学习多样化的rPPG特征，从而在噪声条件下提高鲁棒性。此外，我们还设计了跨注意力状态空间对偶（CASSD）模块，以改善两个流间的信息交换和特征互补性。我们使用PURE、UBFC-rPPG和MMPD验证了PhysMamba。实验结果表明，PhysMamba在各种场景中实现了最先进的表现。

    arXiv:2408.01077v1 Announce Type: new  Abstract: Remote Photoplethysmography (rPPG) is a non-contact technique for extracting physiological signals from facial videos, used in applications like emotion monitoring, medical assistance, and anti-face spoofing. Unlike controlled laboratory settings, real-world environments often contain motion artifacts and noise, affecting the performance of existing methods. To address this, we propose PhysMamba, a dual-stream time-frequency interactive model based on Mamba. PhysMamba integrates the state-of-the-art Mamba-2 model and employs a dual-stream architecture to learn diverse rPPG features, enhancing robustness in noisy conditions. Additionally, we designed the Cross-Attention State Space Duality (CASSD) module to improve information exchange and feature complementarity between the two streams. We validated PhysMamba using PURE, UBFC-rPPG and MMPD. Experimental results show that PhysMamba achieves state-of-the-art performance across various scen
    
[^27]: 使用预训练文本编码器语义知识进行持续学习

    Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning

    [https://arxiv.org/abs/2408.01076](https://arxiv.org/abs/2408.01076)

    本文提出了一种使用预训练文本编码器语义知识进行持续学习的策略，通过在任务之间和任务内部整合语义引导，提高了模型对新数据的可迁移学习能力，并在实验中取得了优于传统方法的保持效果。

    

    arXiv:2408.01076v1 新闻类型：新报告类型 摘要：深度神经网络（DNNs）在固定数据集上表现出色，但在现实世界中不断变化的场景中学习新数据时却遇到困难。持续学习这一挑战旨在允许模型在新知识学习的同时保留以前学到的知识。现有的方法主要依赖于视觉特征，经常忽视图像标签信息中的丰富语义信息。图像标签中的语义信息提供了与先前获得的分类知识相关的宝贵信息。因此，在实际应用中有效地利用这些信息应该是有益的。为了解决这个问题，我们提出了一种通过使用文本嵌入捕捉语义相似性的方法，该方法能够在任务之间和任务内部整合语义引导。我们从CLIP模型开始，使用“Semantically-guided Representation Learning（SG-RL）”模块进行软分配，在任务T的监督下得分最好的文本embedding，该嵌入与新的任务T'中检索到的类别相关的图片embedding相似。我们对一个多任务学习网络进行实验，改网络在分割数据集上进行训练，并在迁移到流行的CIFAR-100数据集上时能够进行可迁移的学习。实验结果表明，与没有语义指导的持续学习传统方法相比，我们的方法具有更好的保持能力。

    arXiv:2408.01076v1 Announce Type: new  Abstract: Deep neural networks (DNNs) excel on fixed datasets but struggle with incremental and shifting data in real-world scenarios. Continual learning addresses this challenge by allowing models to learn from new data while retaining previously learned knowledge. Existing methods mainly rely on visual features, often neglecting the rich semantic information encoded in text. The semantic knowledge available in the label information of the images, offers important semantic information that can be related with previously acquired knowledge of semantic classes. Consequently, effectively leveraging this information throughout continual learning is expected to be beneficial. To address this, we propose integrating semantic guidance within and across tasks by capturing semantic similarity using text embeddings. We start from a pre-trained CLIP model, employ the \emph{Semantically-guided Representation Learning (SG-RL)} module for a soft-assignment tow
    
[^28]: 跨模态分割法在腹腔镜手术视频中的应用

    Amodal Segmentation for Laparoscopic Surgery Video Instruments

    [https://arxiv.org/abs/2408.01067](https://arxiv.org/abs/2408.01067)

    本文提出了一种跨模态分割技术，可以识别出腹腔镜手术中器械的完整遮挡部分，提高了手术质量和安全性。

    

    arXiv:2408.01067v1 公告类型：新 Abstract: 分割手术器械对于提高手术质量并确保患者安全至关重要。像二值、语义和实例分割这样的传统方法都有共同的问题：它们不能处理那些被组织或其它器械遮挡的器械部分。精确预测这些遮挡部分的全貌可以在多个方面显著提高腹腔镜手术，包括在手术过程中提供关键的指导、协助分析潜在的手术错误以及为教育目的提供服务。在本文中，我们将跨模态分割技术引入到医疗领域中的手术器械识别领域。这项技术能够识别物体的可见和遮挡部分。为了实现这一点，我们引入了一个新的跨模态器具分割（AIS）数据集，该数据集通过重新注释每个器械的全貌，利用了2017年MICCAI EndoVis Robotic手术视频。

    arXiv:2408.01067v1 Announce Type: new  Abstract: Segmentation of surgical instruments is crucial for enhancing surgeon performance and ensuring patient safety. Conventional techniques such as binary, semantic, and instance segmentation share a common drawback: they do not accommodate the parts of instruments obscured by tissues or other instruments. Precisely predicting the full extent of these occluded instruments can significantly improve laparoscopic surgeries by providing critical guidance during operations and assisting in the analysis of potential surgical errors, as well as serving educational purposes. In this paper, we introduce Amodal Segmentation to the realm of surgical instruments in the medical field. This technique identifies both the visible and occluded parts of an object. To achieve this, we introduce a new Amoal Instruments Segmentation (AIS) dataset, which was developed by reannotating each instrument with its complete mask, utilizing the 2017 MICCAI EndoVis Robotic
    
[^29]: 通过视觉基础模型像素级监督提升视线对象预测

    Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model

    [https://arxiv.org/abs/2408.01044](https://arxiv.org/abs/2408.01044)

    本文提出了一种利用像素级监督的视线对象预测方法，通过与VFM的集成，显著提升了视线对象检测和分割的准确性。

    

    本文提出了一种新的挑战性任务——视线对象分割(GOS)，它旨在推断人类视线行为捕获的对象的像素级掩码。特别是，我们提出了一个视线对象检测和分割框架，该框架利用了来自VFM的像素级监督，这有助于减少由于对象靠得太近而产生的语义模糊性。通过这种方法，我们可以显著提高视线对象预测的准确性和效果。

    arXiv:2408.01044v1 Announce Type: new  Abstract: Gaze object prediction (GOP) aims to predict the category and location of the object that a human is looking at. Previous methods utilized box-level supervision to identify the object that a person is looking at, but struggled with semantic ambiguity, ie, a single box may contain several items since objects are close together. The Vision foundation model (VFM) has improved in object segmentation using box prompts, which can reduce confusion by more precisely locating objects, offering advantages for fine-grained prediction of gaze objects. This paper presents a more challenging gaze object segmentation (GOS) task, which involves inferring the pixel-level mask corresponding to the object captured by human gaze behavior. In particular, we propose that the pixel-level supervision provided by VFM can be integrated into gaze object prediction to mitigate semantic ambiguity. This leads to our gaze object detection and segmentation framework th
    
[^30]: 使用随机和噪杂切混分式 vision transformers 的隐私保护分成学习

    Privacy-Preserving Split Learning with Vision Transformers using Patch-Wise Random and Noisy CutMix

    [https://arxiv.org/abs/2408.01040](https://arxiv.org/abs/2408.01040)

    本文提出了一种全新的隐私保护分成学习框架，通过在随机选择的颠簸数据块之间进行高斯噪声混合，提高了训练数据的隐私保护水平，同时保持了分类任务的准确性。

    

    arXiv:2408.01040v1 公告类型: 交叉 摘要：在计算机视觉领域，vision transformer（ViT）由于其提高的准确性和鲁棒性逐渐超过了卷积神经网络（CNN）。然而，ViT的大模型尺寸和高样本复杂度使得在资源受限的边缘设备上训练变得困难。分成学习（SL）作为一种可行的解决方案，利用服务器端的资源来训练ViT，同时利用分布在设备上的私有数据。然而，SL在设备与服务器之间进行权重更新时需要额外的信息交换，这可能会暴露于各种训练数据隐私攻击中。为了在分类任务中减少数据泄露的风险，受CutMix正则化的启发，我们提出了一个全新的隐私保护SL框架，它向颠簸数据中注入高斯噪声，并在客户端的随机选择的颠簸数据块之间进行混合，被称为DP-CutMixSL。我们的分析显示，DP-CutMixSL是一种不同的方法，可以提高训练数据的隐私保护水平，同时保持分类任务的准确性。

    arXiv:2408.01040v1 Announce Type: cross  Abstract: In computer vision, the vision transformer (ViT) has increasingly superseded the convolutional neural network (CNN) for improved accuracy and robustness. However, ViT's large model sizes and high sample complexity make it difficult to train on resource-constrained edge devices. Split learning (SL) emerges as a viable solution, leveraging server-side resources to train ViTs while utilizing private data from distributed devices. However, SL requires additional information exchange for weight updates between the device and the server, which can be exposed to various attacks on private training data. To mitigate the risk of data breaches in classification tasks, inspired from the CutMix regularization, we propose a novel privacy-preserving SL framework that injects Gaussian noise into smashed data and mixes randomly chosen patches of smashed data across clients, coined DP-CutMixSL. Our analysis demonstrates that DP-CutMixSL is a differenti
    
[^31]: 基于运动结构的全动态估计和未知形状空间碎片的三维重建

    Structure from Motion-based Motion Estimation and 3D Reconstruction of Unknown Shaped Space Debris

    [https://arxiv.org/abs/2408.01035](https://arxiv.org/abs/2408.01035)

    本研究提出了一种基于运动结构的算法，能够利用有限的资源和对未知形状的空间碎片进行运动估计，并同时输出物体的不明形状和相关摄像机的相对姿态轨迹，这些信息用于精确估计目标的运动状态。

    

    arXiv:2408.01035v1 公告类型：新  摘要：随着近年来 spacecraft 发射数量的增加，空间碎片问题日益成为人类必须面对的关键问题。为了实现空间利用的可持续性，在轨的碎片清除任务可靠性是其面临的主要问题之一。为了提高在轨清除任务的成功率，目标的高精度动量估计至关重要。由于碎片失掉了姿态和轨道控制能力，以及形状因破碎不明，因此本文提出了一种基于运动结构的算法，它使用有限资源来执行不明形状空间碎片的运动估计，仅需要二维图像作为输入。该方法输出物体的不明形状和目标与摄像机之间的相对姿态轨迹，这些信息被用来估计目标的运动。该方法在现实图像数据上进行了定量的验证。

    arXiv:2408.01035v1 Announce Type: new  Abstract: With the boost in the number of spacecraft launches in the current decades, the space debris problem is daily becoming significantly crucial. For sustainable space utilization, the continuous removal of space debris is the most severe problem for humanity. To maximize the reliability of the debris capture mission in orbit, accurate motion estimation of the target is essential. Space debris has lost its attitude and orbit control capabilities, and its shape is unknown due to the break. This paper proposes the Structure from Motion-based algorithm to perform unknown shaped space debris motion estimation with limited resources, where only 2D images are required as input. The method then outputs the reconstructed shape of the unknown object and the relative pose trajectory between the target and the camera simultaneously, which are exploited to estimate the target's motion. The method is quantitatively validated with the realistic image data
    
[^32]: 论文标题：用于医学图像分析的物理信息网络：一个综述

    PINNs for Medical Image Analysis: A Survey

    [https://arxiv.org/abs/2408.01026](https://arxiv.org/abs/2408.01026)

    论文要点：本文综述了将物理信息集成到机器学习框架中的医学图像分析方法，探讨了用于MIA的物理知识、建模方式以及这些方法在实际图像分析任务中的应用。

    

    论文摘要：机器学习框架中物理信息的整合正在改变医学图像分析（MIA）。通过整合基本知识和指导物理定律，这些模型实现了增强的鲁棒性和可解释性。在这项工作中，我们对80多篇专注于医学图像分析（MIA）的物理信息方法进行了系统的文献综述。我们提出了一种统一的分类法，以调查用于MIA的物理知识以及用于建模的方式。我们将深入研究一系列图像分析任务，包括成像、生成、预测、逆成像（超分辨率和重建）、标定和图像分析（分割和分类）。对于每项任务，我们都详细检查了一个专注于医学图像分析的物理信息方法综述。

    arXiv:2408.01026v1 Announce Type: cross  Abstract: The incorporation of physical information in machine learning frameworks is transforming medical image analysis (MIA). By integrating fundamental knowledge and governing physical laws, these models achieve enhanced robustness and interpretability. In this work, we explore the utility of physics-informed approaches for MIA (PIMIA) tasks such as registration, generation, classification, and reconstruction. We present a systematic literature review of over 80 papers on physics-informed methods dedicated to MIA. We propose a unified taxonomy to investigate what physics knowledge and processes are modelled, how they are represented, and the strategies to incorporate them into MIA models. We delve deep into a wide range of image analysis tasks, from imaging, generation, prediction, inverse imaging (super-resolution and reconstruction), registration, and image analysis (segmentation and classification). For each task, we thoroughly examine an
    
[^33]: EIUP：一种基于隐式不安全提示的条件擦除非合规概念训练免费方法

    EIUP: A Training-Free Approach to Erase Non-Compliant Concepts Conditioned on Implicit Unsafe Prompts

    [https://arxiv.org/abs/2408.01014](https://arxiv.org/abs/2408.01014)

    EIUP提出了一种新的训练无需方法，通过隐式不安全提示来消除非合规概念，解决了文本到图像扩散模型可能产生的安全问题。

    

    arXiv:2408.01014v1 公告类型：新  翻译摘要：文本到图像扩散模型已经显示了学习广泛概念的能力。然而，值得注意的是，它们也可能产生不希望的输出，这引起了重大的安全问题。特别是，可能遇到的问题包括不适合工作（NSFW）的内容和潜在的样式版权违规。由于图像生成是根据文本来条件的，纯洁提示对于内容安全来说是一种简单有效的解决方案。类似于大型语言模型（LLM）所采取的方法，已经有一些努力旨在通过纯洁提示来控制生成安全的输出。然而，需要注意的是，即使进行了这些努力，非有毒文本仍然存在风险，可能导致生成非合规的图像，这种情况被称为隐式不安全提示。此外，一些现有工作通过调整模型来从模型权重中擦除不希望的概念。这种方法需要多个训练周期。我们的贡献点在于提出了一种新的训练无需方法EIUP，通过隐式不安全提示来消除非合规概念'''与原文中提到的性能问题进行了相应调整，以确保输出内容的准确性。

    arXiv:2408.01014v1 Announce Type: new  Abstract: Text-to-image diffusion models have shown the ability to learn a diverse range of concepts. However, it is worth noting that they may also generate undesirable outputs, consequently giving rise to significant security concerns. Specifically, issues such as Not Safe for Work (NSFW) content and potential violations of style copyright may be encountered. Since image generation is conditioned on text, prompt purification serves as a straightforward solution for content safety. Similar to the approach taken by LLM, some efforts have been made to control the generation of safe outputs by purifying prompts. However, it is also important to note that even with these efforts, non-toxic text still carries a risk of generating non-compliant images, which is referred to as implicit unsafe prompts. Furthermore, some existing works fine-tune the models to erase undesired concepts from model weights. This type of method necessitates multiple training i
    
[^34]: 论文标题: 从激光雷达和航空影像中提取物体高度

    Extracting Object Heights From LiDAR & Aerial Imagery

    [https://arxiv.org/abs/2408.00967](https://arxiv.org/abs/2408.00967)

    本文提供了一种提取激光雷达和航空影像中物体高度的过程方法，并讨论了其方法在未来激光雷达和影像处理中的应用前景。

    

    arXiv:2408.00967v1 公告类型: 更新  摘要: 本文介绍了一种提取激光雷达和航空影像中物体高度的过程方法。我们讨论了如何获取高度，以及激光雷达和影像处理的未来。最先进的对象分割技术使我们能够在没有深度学习背景的情况下获取物体高度。工程师将追踪世界数据在不同代际之间的进展，并重新处理它们。他们将使用像本文中讨论的那些较新的方法和一些较旧的、在这里也讨论的方法。最先进的方法正在超越分析，进入生成式人工智能。我们覆盖了过程方法和与语言模型一起执行的新方法。这些包括点云、影像和文本编码，允许人工智能具备空间意识。

    arXiv:2408.00967v1 Announce Type: new  Abstract: This work shows a procedural method for extracting object heights from LiDAR and aerial imagery. We discuss how to get heights and the future of LiDAR and imagery processing. SOTA object segmentation allows us to take get object heights with no deep learning background. Engineers will be keeping track of world data across generations and reprocessing them. They will be using older procedural methods like this paper and newer ones discussed here. SOTA methods are going beyond analysis and into generative AI. We cover both a procedural methodology and the newer ones performed with language models. These include point cloud, imagery and text encoding allowing for spatially aware AI.
    
[^35]: PrivateGaze：保护黑盒式移动眼动跟踪服务中用户隐私的方法

    PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking Services

    [https://arxiv.org/abs/2408.00950](https://arxiv.org/abs/2408.00950)

    本研究提出了一种名为PrivateGaze的新方法，该方法能够在不损害视线估计性能的情况下，有效地保护用户在黑盒式移动眼动跟踪服务中的隐私。

    

    arXiv:2408.00950v1 宣布类型：新 摘要：眼睛的视线包含有关人类注意力和认知过程的大量信息。这项能力使得基於视线跟踪的底层技术成为许多普及应用的关键推动者，并激发了视线估计服务的开发。确实，通过利用平板电脑和智能手机上无处不在的摄像头，用户可以轻松访问许多视线估计服务。在这些服务中，用户必须将他们的全脸图像提供给视线估算器，后者通常是黑匣子。这给用户带来了严重的隐私威胁，尤其是当一个恶意的服务提供商收集了大量面孔图像来分类敏感的用户属性时。在这次的工作中，我们提出了PrivateGaze，这是第一个能在黑盒式视线跟踪服务中有效保护用户隐私，同时又不 compromising视线估计性能的方法。特别是，我们提出了一种新的框架来培训一个能够生成高质量伪视线标记的生成模型，并通过这些标记来保护用户的隐私。与之前的保护隐私的技术相比，我们的方法能够在保持高准确性方面做得更好。

    arXiv:2408.00950v1 Announce Type: new  Abstract: Eye gaze contains rich information about human attention and cognitive processes. This capability makes the underlying technology, known as gaze tracking, a critical enabler for many ubiquitous applications and has triggered the development of easy-to-use gaze estimation services. Indeed, by utilizing the ubiquitous cameras on tablets and smartphones, users can readily access many gaze estimation services. In using these services, users must provide their full-face images to the gaze estimator, which is often a black box. This poses significant privacy threats to the users, especially when a malicious service provider gathers a large collection of face images to classify sensitive user attributes. In this work, we present PrivateGaze, the first approach that can effectively preserve users' privacy in black-box gaze tracking services without compromising gaze estimation performance. Specifically, we proposed a novel framework to train a p
    
[^36]: 双任务互惠学习框架用于预测血栓切除术后脑出血

    A dual-task mutual learning framework for predicting post-thrombectomy cerebral hemorrhage

    [https://arxiv.org/abs/2408.00940](https://arxiv.org/abs/2408.00940)

    本文提出了一种双任务互惠学习框架，用于仅基于患者的初始CT扫描预测血栓切除术后的脑出血。该框架结合多模态数据的互补信息，并通过自监督学习机制提高预测准确性，具有重要的临床意义。

    

    梗死性卒中有可能导致脑组织缺氧死亡。血栓切除术因其即时效用已成为治疗梗死性卒中的常用选择。然而，血栓切除术存在术后脑出血的风险。在临床上，术后0至72小时内的多次CT扫描用于监测脑出血。然而，这种方法增加了患者的辐射剂量，并可能导致脑出血检测延迟。为了解决这一难题，我们提出了一个仅使用患者初始CT扫描预测术后脑出血的预测框架。特别是，我们引入了一种双任务互惠学习框架，该框架以初始CT扫描为输入，同时估计后续CT扫描和预测标签，以预测术后脑出血的发生。我们提出的框架通过结合多模态数据的互补信息，以及通过自监督学习机制预测病变的发展，提高了预测的准确性。我们的模型不仅可以预测术后CT图像中的出血情况，还可以预测脑损伤程度和不稳定的可能性，这对于提高术后监测的精确性具有重要意义。

    arXiv:2408.00940v1 Announce Type: cross  Abstract: Ischemic stroke is a severe condition caused by the blockage of brain blood vessels, and can lead to the death of brain tissue due to oxygen deprivation. Thrombectomy has become a common treatment choice for ischemic stroke due to its immediate effectiveness. But, it carries the risk of postoperative cerebral hemorrhage. Clinically, multiple CT scans within 0-72 hours post-surgery are used to monitor for hemorrhage. However, this approach exposes radiation dose to patients, and may delay the detection of cerebral hemorrhage. To address this dilemma, we propose a novel prediction framework for measuring postoperative cerebral hemorrhage using only the patient's initial CT scan. Specifically, we introduce a dual-task mutual learning framework to takes the initial CT scan as input and simultaneously estimates both the follow-up CT scan and prognostic label to predict the occurrence of postoperative cerebral hemorrhage. Our proposed framew
    
[^37]: 这里是翻译过的论文标题

    CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression

    [https://arxiv.org/abs/2408.00938](https://arxiv.org/abs/2408.00938)

    本研究提出了一种基于临床知识改进的扩散模型，用于更准确地预测特发性肺纤维化（IPF）的进展。

    

    这里是翻译过的论文摘要

    arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
    
[^38]: 基于视觉语言模型的城市环境零样本标注研究

    Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)

    [https://arxiv.org/abs/2408.00932](https://arxiv.org/abs/2408.00932)

    本论文提出使用视觉语言模型来从卫星图像中自动标注城市环境中的各种特征，以此减少对大规模人工标注的需求，并提高了对城市环境下奇特特征描述的质量。

    

    arXiv:2408.00932v1  宣布类型：新  翻译摘要：平等的城市交通应用需要高质量的城市环境数字表示：不只是街道和人行道，还包括自行车道、标记和未标记的交叉口、栏杆坡道、障碍物、交通信号、路标、道路标记、坑洼等。在规模上直接检查和手动注释成本过高。传统的机器学习方法需要大量的手动注释训练数据才能达到良好的性能。在这篇论文中，我们将视觉语言模型视为从卫星图像中注释多样城市的特征的机制，减少对大量人工注释数据的需求。尽管这些模型在从人类视角拍摄的图像描述常见物体方面取得了显著成果，但它们的训练数据不太可能包含城市环境中奇特特征的强烈信号，而且它们在这些方面的性能可能有限。我们探索了如何利用视觉语言模型，结合多模态学习和领域特定的数据增强技术，提高其对城市环境特征的描述质量和支持。通过实证研究，本工作验证了该方法在实际问题中的有效性和潜力，并对未来的研究提出了展望。

    arXiv:2408.00932v1 Announce Type: new  Abstract: Equitable urban transportation applications require high-fidelity digital representations of the built environment: not just streets and sidewalks, but bike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions, traffic signals, signage, street markings, potholes, and more. Direct inspections and manual annotations are prohibitively expensive at scale. Conventional machine learning methods require substantial annotated training data for adequate performance. In this paper, we consider vision language models as a mechanism for annotating diverse urban features from satellite images, reducing the dependence on human annotation to produce large training sets. While these models have achieved impressive results in describing common objects in images captured from a human perspective, their training sets are less likely to include strong signals for esoteric features in the built environment, and their performance in these s
    
[^39]: 膝关节骨关节炎时间演化: 一种基于扩散的X射线医学图像合成形态学模型

    Temporal Evolution of Knee Osteoarthritis: A Diffusion-based Morphing Model for X-ray Medical Image Synthesis

    [https://arxiv.org/abs/2408.00891](https://arxiv.org/abs/2408.00891)

    该研究提出了一种基于扩散的模型，用于合成患者膝关节骨关节炎疾病进程中的不同阶段的X射线图像，能够帮助医学监测和研究。

    

    arXiv:2408.00891v1 公告类型: 交叉 摘要: 膝关节骨关节炎(KOA)是一种常见于老年人且严重影响其活动能力的肌肉骨骼疾病。在医疗领域，为了研究疾病的动态变化并对其进行统计监测，经常使用含有时间数据的图像。尽管基于深度学习的自然图像生成模型已经得到了广泛的研究，但用于合成膝X射线图像的模型却相对较少。在本工作中，我们提出了一种新型深层学习模型，旨在合成特定患者健康膝关节和严重KOA阶段之间的一系列X射线图像。在测试阶段，基于患者的健康膝关节X射线图像，我们提出的模型能够生成一条连续且有效的KOA X射线图像序列，序列中的图像表现出了不同的严重程度。具体来说，我们引入了一种基于扩散的形态学模型，通过修改 denoising diffusion probabilistic model 来实现。我们的方法整合了扩散 和生成对抗网络，从而为进一步的医学图像合成和分析提供了可能。实验验证了所提方法的有效性，并为疾病的动态监测提供了一种有力的工具。

    arXiv:2408.00891v1 Announce Type: cross  Abstract: Knee Osteoarthritis (KOA) is a common musculoskeletal disorder that significantly affects the mobility of older adults. In the medical domain, images containing temporal data are frequently utilized to study temporal dynamics and statistically monitor disease progression. While deep learning-based generative models for natural images have been widely researched, there are comparatively few methods available for synthesizing temporal knee X-rays. In this work, we introduce a novel deep-learning model designed to synthesize intermediate X-ray images between a specific patient's healthy knee and severe KOA stages. During the testing phase, based on a healthy knee X-ray, the proposed model can produce a continuous and effective sequence of KOA X-ray images with varying degrees of severity. Specifically, we introduce a Diffusion-based Morphing Model by modifying the Denoising Diffusion Probabilistic Model. Our approach integrates diffusion 
    
[^40]: 医学 SAM 2：通过 Segment Anything Model 2 对医学图像进行分割，将其视为视频

    Medical SAM 2: Segment medical images as video via Segment Anything Model 2

    [https://arxiv.org/abs/2408.00874](https://arxiv.org/abs/2408.00874)

    医学 SAM 2 是利用 SAM 2 框架对医学图像进行分割的先进模型，能够将医学图像视为视频，不仅适用于3D医学图像，还能自动在所有后续图像中分割相同的对象，不考虑图像之间的时序关系，性能相较于现有模型有显著提升。

    

    arXiv:2408.00874v1 公告类型：新增 摘要：在这篇论文中，我们介绍了医学 SAM 2（MedSAM-2），这是一个高级的分割模型，它利用 SAM 2框架来解决二维和三维医学图像分割任务。通过采用将医学图像视为视频的哲学，MedSAM-2不仅适用于三维医学图像，而且解锁了新的“一次性提示”分割能力。这允许用户只为一个或特定图像的目标对象提供一个提示，之后模型可以独立地在所有后续图像中自动分割相同的对象，而不考虑图像之间的时序关系。我们在多种医学成像模式中评估了 MedSAM-2，包括腹部器官、视网膜、脑肿瘤、甲状腺结节和皮肤病变，将其与现有技术在传统和互动分割设置中对齐。我们的发现表明，MedSAM-2不仅在性能上超过了现有模型，而且表现出在无需用户触发的情况下对单一目标对象进行多帧图像分割的能力。

    arXiv:2408.00874v1 Announce Type: new  Abstract: In this paper, we introduce Medical SAM 2 (MedSAM-2), an advanced segmentation model that utilizes the SAM 2 framework to address both 2D and 3D medical image segmentation tasks. By adopting the philosophy of taking medical images as videos, MedSAM-2 not only applies to 3D medical images but also unlocks new One-prompt Segmentation capability. That allows users to provide a prompt for just one or a specific image targeting an object, after which the model can autonomously segment the same type of object in all subsequent images, regardless of temporal relationships between the images. We evaluated MedSAM-2 across a variety of medical imaging modalities, including abdominal organs, optic discs, brain tumors, thyroid nodules, and skin lesions, comparing it against state-of-the-art models in both traditional and interactive segmentation settings. Our findings show that MedSAM-2 not only surpasses existing models in performance but also exhi
    
[^41]: 基于模糊逻辑的网站视觉分析方法：基于K-means聚类的中文摘要提取

    Fuzzy Logic Approach For Visual Analysis Of Websites With K-means Clustering-based Color Extraction

    [https://arxiv.org/abs/2408.00774](https://arxiv.org/abs/2408.00774)

    本文引入了一种新颖的基于K-means聚类方法，有效预测中文摘要中每个词的颜色，即使存在语义不明确的问题，但仍能准确预测颜色偏好。

    

    arXiv:2408.00774v1 公告类型：交叉 摘要：网站构成了互联网的基础，作为信息传播和访问数字资源的平台。它们允许用户参与广泛的内容和服务，提高互联网对所有人的实用性。网站的设计美学在提升用户体验方面起着至关重要的作用，而在全世界互联网用户数量日益增长的大背景下这一点尤为重要。本文探讨了网站设计美学的提高用户体验的重要性。它强调了用户在50毫秒内形成的“第一印象”对用户对网站吸引力和易用性的看法的重大影响。我们提出了一个基于颜色和谐度和字体流行度测量网站美学的创新方法，使用模糊逻辑预测美学偏好。我们对包含近200个热门和经常使用的网站的数据集进行了研究，并且我们使用了一种新颖的基于颜色提取的K-means聚类方法来有效预测中文摘要中每个词的颜色，尽管存在语义不明确的问题，但这种颜色预

    arXiv:2408.00774v1 Announce Type: cross  Abstract: Websites form the foundation of the Internet, serving as platforms for disseminating information and accessing digital resources. They allow users to engage with a wide range of content and services, enhancing the Internet's utility for all. The aesthetics of a website play a crucial role in its overall effectiveness and can significantly impact user experience, engagement, and satisfaction. This paper examines the importance of website design aesthetics in enhancing user experience, given the increasing number of internet users worldwide. It emphasizes the significant impact of first impressions, often formed within 50 milliseconds, on users' perceptions of a website's appeal and usability. We introduce a novel method for measuring website aesthetics based on color harmony and font popularity, using fuzzy logic to predict aesthetic preferences. We collected our own dataset, consisting of nearly 200 popular and frequently used website 
    
[^42]: 混合深度学习框架在增强黑色素瘤检测中的应用

    Hybrid Deep Learning Framework for Enhanced Melanoma Detection

    [https://arxiv.org/abs/2408.00772](https://arxiv.org/abs/2408.00772)

    本文提出了一种结合U-Net分割和EfficientNet分类的混合深度学习框架，显著提高了黑色素瘤检测的准确率。

    

    在癌症已经成为全球死亡的主要原因之一的情况下，迫切需要改进早期检测和治疗技术。在这篇论文中，我们提出了一种新型的、高效的黑色素瘤检测框架，该框架结合了U-Net分割模型和EfficientNet皮肤图像分类模型的独特优势。我们的研究主要目标是通过创新的融合方法提高黑色素瘤的准确性和效率。我们利用HAM10000数据集精心训练了U-Net模型，使其能够精确地分割出癌性区域。同时，我们使用ISIC 2020数据集对EfficientNet模型进行训练，优化了它对皮肤癌症的二元分类。我们的混合模型在ISIC 2020数据集上的表现显著提高，达到了惊人的99.01%的准确性。这一卓越的结果表明，我们的方法在性能上比现有的方法有了很大的提升。

    arXiv:2408.00772v1 Announce Type: cross  Abstract: Cancer is a leading cause of death worldwide, necessitating advancements in early detection and treatment technologies. In this paper, we present a novel and highly efficient melanoma detection framework that synergistically combines the strengths of U-Net for segmentation and EfficientNet for the classification of skin images. The primary objective of our study is to enhance the accuracy and efficiency of melanoma detection through an innovative hybrid approach. We utilized the HAM10000 dataset to meticulously train the U-Net model, enabling it to precisely segment cancerous regions. Concurrently, we employed the ISIC 2020 dataset to train the EfficientNet model, optimizing it for the binary classification of skin cancer. Our hybrid model demonstrates a significant improvement in performance, achieving a remarkable accuracy of 99.01% on the ISIC 2020 dataset. This exceptional result underscores the superiority of our approach compared
    
[^43]: 使用空间 filling curves 对比光流和深度学习实现计算高效的交通事件检测

    Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves

    [https://arxiv.org/abs/2408.00768](https://arxiv.org/abs/2408.00768)

    本文提出了一种结合光流、深度学习和空间填满曲线的框架，实现了对车辆前向摄像头捕获的视频数据中交通事件的实时、高效检测。该框架有助于为驾驶员或自动驾驶车辆提供实时反馈，识别前方道路潜在的威胁或突发事件，提高驾驶情况感知，并可能提高安全性。

    

    我们面临着在视频、雷达和激光雷达等交通数据中识别事件和收集数据的挑战，这对评价感知系统的性能至关重要。这类数据通常是无结构的、多模态的、时间序列的，且缺乏元数据或注释。本文比较了光流和深度学习在视频数据（来自车辆前向摄像头）中实现计算高效的交通事件检测的能力。我们的方法是利用车辆周围光流场的干扰来发现潜在的事件，而另一种方法是通过训练深度学习模型预测驾驶员的视线，以预测潜在事件的位置。我们将这些结果输送到空间填满曲线上，以降低维度并实现计算上的效率。我们通过标准的实验，针对在不同场景下检测事件的有效性，验证了本概念的效力。计算效率的评估取决于算法的执行时间，总内存消耗以及空间解构的时间。结果表明，本概念所展示的算法可以比传统的基于光流的方法更快地找到事件，并且能够在达到良好的检测精度时，更快地完成计算密集型操作。Our approach is designed to serve as an early warning or an auxiliary system that can provide real-time feedback to drivers or autonomous vehicles by identifying potential hazards or sudden events in the road ahead, improving situational awareness and potentially enhancing safety. In summary, this paper presents a novel framework for computationally efficient traffic event detection, which relies on optical flow, deep learning, and space-filling curves, offering a promising solution for the autonomous driving industry to achieve real-time event detection with minimal computational resources.

    arXiv:2408.00768v1 Announce Type: new  Abstract: Gathering data and identifying events in various traffic situations remains an essential challenge for the systematic evaluation of a perception system's performance. Analyzing large-scale, typically unstructured, multi-modal, time series data obtained from video, radar, and LiDAR is computationally demanding, particularly when meta-information or annotations are missing. We compare Optical Flow (OF) and Deep Learning (DL) to feed computationally efficient event detection via space-filling curves on video data from a forward-facing, in-vehicle camera. Our first approach leverages unexpected disturbances in the OF field from vehicle surroundings; the second approach is a DL model trained on human visual attention to predict a driver's gaze to spot potential event locations. We feed these results to a space-filling curve to reduce dimensionality and achieve computationally efficient event retrieval. We systematically evaluate our concept b
    
[^44]: 基于多视图数据融合的 conformal 轨迹预测在合作驾驶中的应用

    Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving

    [https://arxiv.org/abs/2408.00374](https://arxiv.org/abs/2408.00374)

    V2INet 提出了一个创新的端到端训练框架，用于结合多角度信息进行轨迹预测，以克服单一视角的局限性，提高校正后的多模态轨迹预测的性能。

    

    arXiv:2408.00374v2 Announce Type: replace-cross 摘要: 目前关于轨迹预测的研究主要依赖于车载传感器收集的数据。随着连接的快速发展，如车对车（V2V）和车对基础设施（V2I）通信，通过无线网络收集的有价值的信息变得可用。多视图信息的集成有潜力克服仅从单一视角收集数据的内在局限性，如遮挡和有限视野。在本工作中，我们介绍了 V2INet，一个新颖的轨迹预测框架，旨在通过扩展现有单一视图模型来建模多视图数据。与以前的方法不同，我们的模型支持端到端训练，增强了模型的灵活性和性能。此外，预测的多模态轨迹得到了校正，以：

    arXiv:2408.00374v2 Announce Type: replace-cross  Abstract: Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated 
    
[^45]: XLIP: 跨模态注意屏蔽建模方法用于医疗语言图像预训练

    XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training

    [https://arxiv.org/abs/2407.19546](https://arxiv.org/abs/2407.19546)

    本文提出XLIP框架，通过未配对数据增强病理学习和特征学习，引入了注意遮罩图像建模和目标驱动的遮罩语言建模模块，有效提高了病理特征的准确重建。

    

    arXiv:2407.19546v2 公告类型：替换 摘要：医学领域的视觉语言预训练（VLP）使用对比学习在图像文本对上，实现跨任务的有效转移。然而，当应用于医疗领域时，基于遮罩建模策略的当前VLP方法面临两个挑战。首先，由于医学数据稀缺，当前模型难以准确重建病理性特征。其次，大多数方法只采用图像文本对或仅图像数据，未能利用两者之间的数据组合。为此，本文提出XLIP（医疗语言图像预训练的遮罩建模方法）框架，通过未配对数据增强病理学习和特征学习。首先，我们引入了注意遮罩图像建模（AttMIM）和方法驱动的遮罩语言建模模块（EntMLM），这些模块通过多模态特征交互学习重建病理性视觉和文本 tokens。

    arXiv:2407.19546v2 Announce Type: replace  Abstract: Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modelling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes a XLIP (Masked modelling for medical Language-Image Pre-training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modelling (AttMIM) and entity-driven masked language modelling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interacti
    
[^46]: 跨域适应性肺部结节检测在X射线图像中的应用

    Domain Adaptive Lung Nodule Detection in X-ray Image

    [https://arxiv.org/abs/2407.19397](https://arxiv.org/abs/2407.19397)

    本文提出了一种结合自训练和对比学习的方法，用于跨域适应性肺部结节检测，通过改进结节表示和捕获域不变特征来解决数据分布差异的问题。

    

    arXiv:2407.19397v2 公告类型: 替换

    arXiv:2407.19397v2 Announce Type: replace  Abstract: Medical images from different healthcare centers exhibit varied data distributions, posing significant challenges for adapting lung nodule detection due to the domain shift between training and application phases. Traditional unsupervised domain adaptive detection methods often struggle with this shift, leading to suboptimal outcomes. To overcome these challenges, we introduce a novel domain adaptive approach for lung nodule detection that leverages mean teacher self-training and contrastive learning. First, we propose a hierarchical contrastive learning strategy to refine nodule representations and enhance the distinction between nodules and background. Second, we introduce a nodule-level domain-invariant feature learning (NDL) module to capture domain-invariant features through adversarial learning across different domains. Additionally, we propose a new annotated dataset of X-ray images to aid in advancing lung nodule detection re
    
[^47]: 论文标题：在小型数据集上进行有效训练的视觉变换器的深度卷积

    Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets

    [https://arxiv.org/abs/2407.19394](https://arxiv.org/abs/2407.19394)

    该论文提出了一种轻量级的深度卷积模块，将其集成到ViT模型中，以提高在小型数据集上的训练效率，同时捕捉图像的局部和全局信息，从而增强了模型的归纳偏差，提高了训练效率和泛化能力。

    

    arXiv:2407.19394v3 公告类型：替换 摘要：视觉变换器（ViT）通过将图像划分为图块并使用Transformer的编码器来捕获全局信息，在各种计算机视觉任务中取得了优异的表现。然而，ViT的自注意力机制从一开始就捕捉全局上下文，忽略了图像或视频中相邻像素之间固有的关系。变换器主要注重全局信息而忽视了图像或视频数据的精细局部细节。因此，在仅用少量数据对图像或视频数据集进行训练时，ViT缺乏归纳偏见。相比之下，卷积神经网络（CNNs）依靠局部滤波器，具有固有的归纳偏见，使其在训练时的效率和收敛速度比ViT更快，并且需要的数据更少。本文提出了一种轻量级的深度卷积模块，作为ViT模型中的捷径，绕过整个Transformer块，以确保模型能够捕捉到局部和全局信息。

    arXiv:2407.19394v3 Announce Type: replace  Abstract: The Vision Transformer (ViT) leverages the Transformer's encoder to capture global information by dividing images into patches and achieves superior performance across various computer vision tasks. However, the self-attention mechanism of ViT captures the global context from the outset, overlooking the inherent relationships between neighboring pixels in images or videos. Transformers mainly focus on global information while ignoring the fine-grained local details. Consequently, ViT lacks inductive bias during image or video dataset training. In contrast, convolutional neural networks (CNNs), with their reliance on local filters, possess an inherent inductive bias, making them more efficient and quicker to converge than ViT with less data. In this paper, we present a lightweight Depth-Wise Convolution module as a shortcut in ViT models, bypassing entire Transformer blocks to ensure the models capture both local and global informatio
    
[^48]: 跨客户端变异性自适应联邦学习方法 CCVA-FL：应用于基于医学影像的数据集

    CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging

    [https://arxiv.org/abs/2407.11652](https://arxiv.org/abs/2407.11652)

    本文提出了一种名为“跨客户端变异性自适应联邦学习”（CCVA-FL）的方法，通过生成合成医学图像解决医疗图像数据中的跨客户端变异问题，从而提高联邦学习的性能。

    

    arXiv:2407.11652v3 公告类型：替换

    arXiv:2407.11652v3 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
    
[^49]: 重新思考学习了图像压缩：所有的你所需要的都是上下文

    Rethinking Learned Image Compression: Context is All You Need

    [https://arxiv.org/abs/2407.11590](https://arxiv.org/abs/2407.11590)

    本研究表明，通过对上下文模型和解码器的优化，学习了的图像压缩（LIC）的方法可以大幅提高PSNR性能，并在与VVC的比较中取得了14.39%的改进。

    

    arXiv:2407.11590v3 公告类型：替换-交叉

    arXiv:2407.11590v3 Announce Type: replace-cross  Abstract: Since LIC has made rapid progress recently compared to traditional methods, this paper attempts to discuss the question about 'Where is the boundary of Learned Image Compression(LIC)?'. Thus this paper splits the above problem into two sub-problems:1)Where is the boundary of rate-distortion performance of PSNR? 2)How to further improve the compression gain and achieve the boundary? Therefore this paper analyzes the effectiveness of scaling parameters for encoder, decoder and context model, which are the three components of LIC. Then we conclude that scaling for LIC is to scale for context model and decoder within LIC. Extensive experiments demonstrate that overfitting can actually serve as an effective context. By optimizing the context, this paper further improves PSNR and achieves state-of-the-art performance, showing a performance gain of 14.39% with BD-RATE over VVC.
    
[^50]: OpenVid-1M 大型高质量文本到视频生成数据集

    OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation

    [https://arxiv.org/abs/2407.02371](https://arxiv.org/abs/2407.02371)

    OpenVid-1M 是一个精确的高质量数据集，拥有大量的文本视频对，旨在支持文本到视频生成的研究。

    

    arXiv:2407.02371v2 公告类型：替换  翻译摘要：文本到视频（T2V）生成最近因大型多模态模型Sora的兴起而引起了极大关注。然而，T2V生成仍然面临两个重要挑战：1）缺乏精确开源的高质量数据集。先前流行的视频数据集，如WebVid-10M和Panda-70M，要么质量低，要么太大，超出了大多数研究机构的能力范围。因此，对于T2V生成来说，收集精确的高质量文本视频对是一个具有挑战性的但至关重要的任务。2）忽略了充分利用文本信息。最近的一些T2V方法专注于视觉变换器，使用一个简单的交注意力模块来生成视频，这种方法未能彻底从文本提示中提取出语义信息。为了解决这些问题，我们介绍了OpenVid-1M，一个精确的高质量数据集，它拥有丰富的描述性文本说法。这个开放式场景的数据集包含了超过一百万个文本视频对，为T2V的研发提供了支持。

    arXiv:2407.02371v2 Announce Type: replace  Abstract: Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V g
    
[^51]: 混合空间-谱神经网络用于超光谱图像去噪

    Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising

    [https://arxiv.org/abs/2406.08782](https://arxiv.org/abs/2406.08782)

    该论文提出了一种混合空间-谱去噪网络，利用CNN和Transformer的特征设计了一种新的双路径网络，有效捕捉局部和非局部空间细节，同时减少了计算复杂性。

    

    arXiv:2406.08782v2 公告类型: 替换交叉  简介: 超光谱图像(HSI)去噪是HSI应用中的重要步骤。不幸的是，现有的基于Transformer的方法主要集中在非局部建模上，忽视了图像去噪中局部的 importance。此外，深度学习方法使用复杂的谱学习机制，这引入了巨大的计算成本。   为了解决这些问题，我们提出了一种混合空间-谱去噪网络(HSSD)，其中我们设计了一种新的混合双路径网络，灵感来自CNN和Transformer的特征，引导着捕捉局部和非局部空间细节的同时有效地抑制噪声。此外，为了减少计算复杂性，我们采用了简单的但有效的解耦策略，该策略将空间和谱通道的学习分开，其中参数很少的多层感知机被用来学习谱之间的全球关联。合成和现实派生张量去噪实验证明了我们方法的优越性，实现了比现有方法更高的峰值信噪比(PSNR)和更低的结构相似性指数(SSIM)。

    arXiv:2406.08782v2 Announce Type: replace-cross  Abstract: Hyperspectral image (HSI) denoising is an essential procedure for HSI applications. Unfortunately, the existing Transformer-based methods mainly focus on non-local modeling, neglecting the importance of locality in image denoising. Moreover, deep learning methods employ complex spectral learning mechanisms, thus introducing large computation costs.   To address these problems, we propose a hybrid spatial-spectral denoising network (HSSD), in which we design a novel hybrid dual-path network inspired by CNN and Transformer characteristics, leading to capturing both local and non-local spatial details while suppressing noise efficiently. Furthermore, to reduce computational complexity, we adopt a simple but effective decoupling strategy that disentangles the learning of space and spectral channels, where multilayer perception with few parameters is utilized to learn the global correlations among spectra. The synthetic and real exp
    
[^52]: 现实对话：面向面对面对话的口语对话模型

    Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation

    [https://arxiv.org/abs/2406.07867](https://arxiv.org/abs/2406.07867)

    我们介绍了一个新型的面向面对面对话的口语对话模型，该模型能够处理音频-视觉言语输入并产生相应的回应。这是创建不依赖中间文本虚拟助手的第一步。我们新引入了MultiDialog，这是首个大规模的多模态口语对话语料库，含约340小时的9,000多个对话的平行音频-视觉记录，这些记录是基于开放域对话数据集TopicalChat录制的。

    

    在本文中，我们介绍了一种新型的面向面对面对话的口语对话模型。该模型处理用户的音频-视觉言语输入，并产生音频-视觉言语作为回应，标志着朝着创建不依赖中间文本的虚拟助手迈出了第一步。为此，我们新引入了MultiDialog，这是首个大规模的多模态（即声音和视觉）口语对话语料库，含约340小时的9,000多个对话的平行音频-视觉记录，它们是基于开放域对话数据集TopicalChat录制的。MultiDialog包含了对话伙伴根据给定脚本进行角色扮演并进行情感标注的音频-视频对话记录，我们期待这些记录将为多模态合成研究开辟新的机会。我们的面向面对面对话的口语对话模型结合了经过文本预训练的大型语言模型，并将其适应到音频-视觉口语对话领域，通过将语音-文本联合预训练纳入模型之中。

    arXiv:2406.07867v2 Announce Type: replace  Abstract: In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. 
    
[^53]: 使用上下文化Vendi分数指引改进生成的图像的地理多样性

    Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance

    [https://arxiv.org/abs/2406.04551](https://arxiv.org/abs/2406.04551)

    该研究提出了一种称为上下文化Vendi分数指引的方法，旨在提高生成图像的地理多样性，使特定地区的图像表现与现实世界相符。

    

    arXiv:2406.04551v2 公告类型：替换

    arXiv:2406.04551v2 Announce Type: replace  Abstract: With the growing popularity of text-to-image generative models, there has been increasing focus on understanding their risks and biases. Recent work has found that state-of-the-art models struggle to depict everyday objects with the true diversity of the real world and have notable gaps between geographic regions. In this work, we aim to increase the diversity of generated images of common objects such that per-region variations are representative of the real world. We introduce an inference time intervention, contextualized Vendi Score Guidance (c-VSG), that guides the backwards steps of latent diffusion models to increase the diversity of a sample as compared to a "memory bank" of previously generated images while constraining the amount of variation within that of an exemplar set of real-world contextualizing images. We evaluate c-VSG with two geographically representative datasets and find that it substantially increases the dive
    
[^54]: L-PR: 利用激光雷达 fiducial标记进行点云注册的论文标题

    L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration

    [https://arxiv.org/abs/2406.03298](https://arxiv.org/abs/2406.03298)

    这篇论文提出了一个名为 L-PR 的框架，该框架利用激光雷达 fiducial标记对多视图点云进行注册，并解决了低重叠情况下的注册问题。

    

    L-PR: 利用激光雷达 fiducial标记进行低重叠多视图点云注册的论文摘要

    arXiv:2406.03298v2 Announce Type: replace-cross  Abstract: Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework dubbed L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consis
    
[^55]: MiPa：混合补丁红外-可见模式 agnostic 对象检测

    MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection

    [https://arxiv.org/abs/2404.18849](https://arxiv.org/abs/2404.18849)

    该研究探索了一种共享的视觉编码器如何在单个模型中有效地整合RGB和IR数据，以提高对象检测的性能，同时减少内存占用，适用于自动驾驶和监控等实际应用。

    

    arXiv:2404.18849v2 公告类型：替换  译文摘要：在实际场景中，使用多种模式，如可见（RGB）和红外（IR），可以大大提高预测任务，如对象检测（OD）的性能。多模态学习是一种常见的方法，利用这些模式，其中多个模态特定的编码器和融合模块用于提高性能。在本文中，我们探讨了一种不同的方式来利用RGB和IR模式，其中只有一个模式或另一个模式被单个共享视觉编码器观察到。这种现实的情况要求较小的内存占用，并且更适合依赖于RGB和IR数据的应用程序，如自动驾驶和监控。然而，在多个模式上训练单一编码器时，一种模式可能会主导另一种模式，产生不均衡的识别结果。本文研究了如何高效地利用RGB和IR模式，以训练一个基于transformer的OD视觉编码器，同时...

    arXiv:2404.18849v2 Announce Type: replace  Abstract: In real-world scenarios, using multiple modalities like visible (RGB) and infrared (IR) can greatly improve the performance of a predictive task such as object detection (OD). Multimodal learning is a common way to leverage these modalities, where multiple modality-specific encoders and a fusion module are used to improve performance. In this paper, we tackle a different way to employ RGB and IR modalities, where only one modality or the other is observed by a single shared vision encoder. This realistic setting requires a lower memory footprint and is more suitable for applications such as autonomous driving and surveillance, which commonly rely on RGB and IR data. However, when learning a single encoder on multiple modalities, one modality can dominate the other, producing uneven recognition results. This work investigates how to efficiently leverage RGB and IR modalities to train a common transformer-based OD vision encoder, while
    
[^56]: SPIdepth: 强化姿势信息用于自监督的单目深度估计

    SPIdepth: Strengthened Pose Information for Self-supervised Monocular Depth Estimation

    [https://arxiv.org/abs/2404.12501](https://arxiv.org/abs/2404.12501)

    SPIdepth通过强化姿势网络，在自监督单目深度估计中实现了对场景细节的高级捕捉和提升。

    

    arXiv:2404.12501v2 公告类型: 替换  翻译摘要: 自监督的单目深度估计由于其在自动驾驶和机器人学领域的应用而受到了广泛关注。虽然最近的方法已经在利用诸如自查询层(SQL)这样的技术从运动中推断深度方面取得了进展，但它们往往忽视了加强姿势信息潜力的重要性。在本论文中，我们提出了一种名为SPIdepth的新方法，该方法注重加强姿势网络以提高深度估计。SPIdepth建立在SQL基础之上，强调了姿势信息在捕捉精细场景结构中的重要性。通过加强姿势网络的能力，SPIdepth在场景理解和深度估计方面取得了显著的进步。在KITTI、Cityscapes和Make3D等基准数据集上的实验结果展示了SPIdepth的先进性能，在多个方面明显超过了先前的方法。特别是，SPIdepth在目标识别、实例分割和深度估计方面取得了卓越的成绩，它采用了自监督学习框架，大大提高了模型的鲁棒性和准确性。

    arXiv:2404.12501v2 Announce Type: replace  Abstract: Self-supervised monocular depth estimation has garnered considerable attention for its applications in autonomous driving and robotics. While recent methods have made strides in leveraging techniques like the Self Query Layer (SQL) to infer depth from motion, they often overlook the potential of strengthening pose information. In this paper, we introduce SPIdepth, a novel approach that prioritizes enhancing the pose network for improved depth estimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the importance of pose information in capturing fine-grained scene structures. By enhancing the pose network's capabilities, SPIdepth achieves remarkable advancements in scene understanding and depth estimation. Experimental results on benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's state-of-the-art performance, surpassing previous methods by significant margins. Specifically, SPIdepth tops the
    
[^57]: 数据增强中的泛化差距：光照输入的洞察

    Generalization Gap in Data Augmentation: Insights from Illumination

    [https://arxiv.org/abs/2404.07514](https://arxiv.org/abs/2404.07514)

    本研究发现，尽管数据增强能够提高模型在模拟光照条件下的泛化能力，但其性能在真实光照下并未显著提升，表明在自然光照条件下训练的模型可能具有更强的泛化能力。

    

    arXiv:2404.07514v2 通告类型：替换  翻译摘要：在计算机视觉领域，数据增强技术被广泛应用于通过深度学习方法丰富训练数据的特征复杂性。然而，关于模型的泛化能力，数据增强生成的模拟特征与自然视觉特征之间的差异尚未被完全揭示。本研究引入了“视觉表示变量”的概念，来定义任务中可能的视觉变量的联合分布。我们专注于视觉表示变量“光照”，通过模拟其分布的退化，考察数据增强技术在分类任务上如何提高模型性能。我们的目标是研究在数据增强数据上训练的模型与在真实世界光照条件下训练的模型之间的泛化差异。结果表明，在应用了各种数据增强方法之后，模型的泛化能力增强了，但泛化性能在光照条件下并没有得到显著提升。这表明，尽管数据增强能够提供丰富的训练数据，但在自然光照条件下训练的模型可能更具有泛化能力。我们进一步讨论了泛化差距的存在及其对模型性能的影响。

    arXiv:2404.07514v2 Announce Type: replace  Abstract: In the field of computer vision, data augmentation is widely used to enrich the feature complexity of training datasets with deep learning techniques. However, regarding the generalization capabilities of models, the difference in artificial features generated by data augmentation and natural visual features has not been fully revealed. This study introduces the concept of "visual representation variables" to define the possible visual variations in a task as a joint distribution of these variables. We focus on the visual representation variable "illumination", by simulating its distribution degradation and examining how data augmentation techniques enhance model performance on a classification task. Our goal is to investigate the differences in generalization between models trained with augmented data and those trained under real-world illumination conditions. Results indicate that after applying various data augmentation methods, m
    
[^58]: SAM-指导的图切割方法用于3D实例分割

    SAM-guided Graph Cut for 3D Instance Segmentation

    [https://arxiv.org/abs/2312.08372](https://arxiv.org/abs/2312.08372)

    本文提出了一种新的3D实例分割方法，该方法结合了3D几何信息与多视图图像信息，并通过2D实例分割提升到3D的方式提高了分割的性能和鲁棒性。

    

    本论文解决3D实例分割挑战，通过同时利用3D几何信息和多视图图像信息。许多先前的工作已经应用深度学习技术到3D点云中进行实例分割。然而，这些方法往往因为标注的3D点云数据的稀缺性和多样性低而无法在不同类型的场景中进行泛化。一些最近的工作试图在基于底部的框架中将2D实例分割提升到3D。在不同视角中2D实例分割的不一致性可以显著降低3D分割的效果。在本工作中，我们介绍了一种新的3D-to-2D查询框架，以有效利用2D分割模型进行3D实例分割。具体地，我们将场景预先分割成3D中的多个超点，将任务转化为图切割问题。超级点图是基于2D分割模型构建的，其中节点的分割质量通过两个角度进行评估：单个视图的局部优势和图像间的信息流。实验结果表明，我们的方法在3D实例分割方面表现出了优越的性能，能够泛化到不同的场景中，并且提高了分割的鲁棒性和准确性。

    arXiv:2312.08372v3 Announce Type: replace  Abstract: This paper addresses the challenge of 3D instance segmentation by simultaneously leveraging 3D geometric and multi-view image information. Many previous works have applied deep learning techniques to 3D point clouds for instance segmentation. However, these methods often failed to generalize to various types of scenes due to the scarcity and low-diversity of labeled 3D point cloud data. Some recent works have attempted to lift 2D instance segmentations to 3D within a bottom-up framework. The inconsistency in 2D instance segmentations among views can substantially degrade the performance of 3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to effectively exploit 2D segmentation models for 3D instance segmentation. Specifically, we pre-segment the scene into several superpoints in 3D, formulating the task into a graph cut problem. The superpoint graph is constructed based on 2D segmentation models, where node
    
[^59]: 下游网络在数字病理学基础模型中的重要性

    The Importance of Downstream Networks in Digital Pathology Foundation Models

    [https://arxiv.org/abs/2311.17804](https://arxiv.org/abs/2311.17804)

    本文研究揭示了特征提取器对下游网络聚合模型配置的敏感性，指出传统的特征提取器模型评估可能无法准确反映其性能，并强调了在选择基础模型时需对聚合模型的影响予以考虑。

    

    arXiv:2311.17804v3 宣布类型：替换摘要：数字病理学通过分析 gigapixel 整个切片图像（WSI）显著提高了疾病检测和病理医生的效率。在这个过程中，WSI首先被分成若干小块，应用一个特征提取器模型来获得特征向量，然后将这些向量传递给一个聚合模型，用于预测相应的 WSI 标签。随着表示学习技术的快速发展，出现了许多新的特征提取器模型，这些模型通常被称为基础模型。我们的研究揭示了特征提取器对聚合模型配置的敏感性，表明基于选择的配置可能会偏颇性能的可比性。通过考虑这种敏感性，我们可以结束对特征提取器模型选择的无视，并加深对聚合模型配置对模型性能影响的感受，最终达成对数字病理学基础模型更全面的理解。

    arXiv:2311.17804v3 Announce Type: replace  Abstract: Digital pathology has significantly advanced disease detection and pathologist efficiency through the analysis of gigapixel whole-slide images (WSI). In this process, WSIs are first divided into patches, for which a feature extractor model is applied to obtain feature vectors, which are subsequently processed by an aggregation model to predict the respective WSI label. With the rapid evolution of representation learning, numerous new feature extractor models, often termed foundational models, have emerged. Traditional evaluation methods rely on a static downstream aggregation model setup, encompassing a fixed architecture and hyperparameters, a practice we identify as potentially biasing the results. Our study uncovers a sensitivity of feature extractor models towards aggregation model configurations, indicating that performance comparability can be skewed based on the chosen configurations. By accounting for this sensitivity, we fin
    
[^60]: 《脉冲神经网络在视觉位置识别中的应用》

    Applications of Spiking Neural Networks in Visual Place Recognition

    [https://arxiv.org/abs/2311.13186](https://arxiv.org/abs/2311.13186)

    我们的研究提出了模块化脉冲神经网络在视觉位置识别中的应用，包括紧凑的SNN模块、多个代表相同地点的SNN组合，以及对连续图像序列匹配技术的研究，这些技术为机器人视觉系统的精确和低功率视觉位置识别提供了新的框架。

    

    arXiv:2311.13186v2 公告类型：替换交叉 摘要：在机器人学中，脉冲神经网络（SNNs）因其在大大未实现的潜力能源效率和低延迟，特别是在用于神经形态硬件时，获得了越来越多的认可。我们的论文强调了SNNs在视觉位置识别（VPR）方面的三项先进技术。首先，我们提出了《模块化脉冲神经网络》，其中每个SNN代表一组不重叠的地理不同地点，使网络在大型环境中可扩展。其次，我们提出了《模块化脉冲神经网络的集合》，其中多个网络代表同一地点，与单一网络模型的精度相比，准确性显著提高。我们的模块化SNN模块非常紧凑，仅包含1500个神经元和474k个突触，使其因为它们的体积小而非常适合组群。最后，我们研究了SNN基础VPR中序列匹配的作用，一种技术，使用连续图像来细化位置识别。我们分析了重复使用同一个SNN模块的能力，并设计了一种高效的权重更新策略，从而显著提高了输出的位置识别精度。通过这项研究，我们提出了一种新的框架，基于模块化SNN的集合和序列匹配，它可以在机器人视觉系统中实现高精度和低功率的视觉位置识别。

    arXiv:2311.13186v2 Announce Type: replace-cross  Abstract: In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for their largely-unrealized potential energy efficiency and low latency particularly when implemented on neuromorphic hardware. Our paper highlights three advancements for SNNs in Visual Place Recognition (VPR). Firstly, we propose Modular SNNs, where each SNN represents a set of non-overlapping geographically distinct places, enabling scalable networks for large environments. Secondly, we present Ensembles of Modular SNNs, where multiple networks represent the same place, significantly enhancing accuracy compared to single-network models. Each of our Modular SNN modules is compact, comprising only 1500 neurons and 474k synapses, making them ideally suited for ensembling due to their small size. Lastly, we investigate the role of sequence matching in SNN-based VPR, a technique where consecutive images are used to refine place recognition. We analyze the re
    
[^61]: 这里是一个针对动态手势生成的全新框架，用于驾驶情景

    SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios

    [https://arxiv.org/abs/2309.04421](https://arxiv.org/abs/2309.04421)

    我们提出了一个使用虚拟3D模型的全新框架，该框架使用Unreal Engine生成现实的手势动作，通过节省时间和努力，为驾驶情景中的动态手势生成提供了更高效的方法。

    

    arXiv:2309.04421v2 公告类型：替换 摘要：在汽车领域创建一个多样化的手势动作数据库可能会很艰难且耗时。为了解决这一挑战，我们提出了使用由虚拟3D模型生成的合成手势数据的构想。我们的框架使用虚幻引擎生成现实的手势动作，提供自定义选项并降低过度拟合的危险。我们还生成了多种变体，包括手势速度、表现力和手形，以提高泛化能力。此外，我们还模拟了不同位置的摄像头和类型的摄像头，如RGB、红外和深度摄像头，而不会增加获取这些摄像头的额外时间和成本。实验结果表明，我们提出的框架“SynthoGestures（https://github.com/amrgomaaelhady/SynthoGestures）”提高了手势识别精度，并可以代替或补充真实手势的数据集。通过节省时间，我们为驾驶情景中的动态手势生成提供了更高效的方法。

    arXiv:2309.04421v2 Announce Type: replace  Abstract: Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures (https://github.com/amrgomaaelhady/SynthoGestures), improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the
    
[^62]: PCNN: 概率分类最近邻解释提高人工智能和人类的细粒度图像分类准确性

    PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans

    [https://arxiv.org/abs/2308.13651](https://arxiv.org/abs/2308.13651)

    本文提出了一种新颖的方法，即利用图像比较器对输入图像与最可能的K类最近的邻图像进行比较，并根据比较结果对预训练分类器的置信度进行加权，从而显著提升了细粒度图像分类任务中人工智能和人类的预测准确性。

    

    arXiv:2308.13651v4 宣布类型：替换 抽象： 传统上，最近邻（NN）用于计算最终决定，例如在支持向量机或k-NN分类器中，以及为用户提供模型决策的解释。在本文中，我们展示了最近邻的一种新颖用途：改进冻结的预训练分类器C的预测。我们利用一个图像比较器S，其（1）将输入图像与最可能的K类最近的邻图像进行比较；并（2）使用S的输出分数对C的置信分数进行加权。我们的方法在CUB-200、Cars-196和Dogs-120上一致地提高了细粒度图像分类的准确性。此外，针对人类的研究发现，向我们显示概率分类最近邻（PCNN）降低了人们对人工智能的过度依赖，从而提高了他们对工作的决策准确性，而这项工作此前的工作只展示了最可能的1类例子。

    arXiv:2308.13651v4 Announce Type: replace  Abstract: Nearest neighbors (NN) are traditionally used to compute final decisions, e.g., in Support Vector Machines or k-NN classifiers, and to provide users with explanations for the model's decision. In this paper, we show a novel utility of nearest neighbors: To improve predictions of a frozen, pretrained classifier C. We leverage an image comparator S that (1) compares the input image with NN images from the top-K most probable classes; and (2) uses S' output scores to weight the confidence scores of C. Our method consistently improves fine-grained image classification accuracy on CUB-200, Cars-196, and Dogs-120. Also, a human study finds that showing lay users our probable-class nearest neighbors (PCNN) reduces over-reliance on AI, thus improving their decision accuracy over prior work which only shows only the top-1 class examples.
    
[^63]: 自适应脉冲编码解码网络：准确且高效的基于事件的语义分割

    Accurate and Efficient Event-based Semantic Segmentation Using Adaptive Spiking Encoder-Decoder Network

    [https://arxiv.org/abs/2304.11857](https://arxiv.org/abs/2304.11857)

    本文提出了一种高效的自适应脉冲编码解码网络，能够在基于事件的语义分割任务中利用自适应阈值提高稀疏事件的表示能力，从而在动态事件流中实现准确的分割效果。

    

    arXiv:2304.11857v3 公告类型：替换 摘要：脉冲神经网络（SNNs）因其低功耗的、事件驱动的计算和固有的时间动态特性而逐渐被认为是动态、异步信号处理的解决方案，特别是从基于事件的传感器的输入信号。尽管它们的潜力，SNNs在训练和架构设计上面临挑战，导致在基于事件的密集预测任务上的表现与人工神经网络（ANNs）相比有限。在本文中，我们为大规模基于事件的语义分割任务开发了一个高效的脉冲编码解码网络（SpikingEDN）。为了从动态事件流中提高学习效率，我们利用自适应阈值来提高网络精度、稀疏度和在流式推理中的鲁棒性。此外，我们还开发了一个双路径脉冲空间自适应调制模块，该模块专门针对增强稀疏事件和非模式输入的表示能力而设计，从而在少量事件下实现了高效的分割效果。

    arXiv:2304.11857v3 Announce Type: replace  Abstract: Spiking neural networks (SNNs), known for their low-power, event-driven computation and intrinsic temporal dynamics, are emerging as promising solutions for processing dynamic, asynchronous signals from event-based sensors. Despite their potential, SNNs face challenges in training and architectural design, resulting in limited performance in challenging event-based dense prediction tasks compared to artificial neural networks (ANNs). In this work, we develop an efficient spiking encoder-decoder network (SpikingEDN) for large-scale event-based semantic segmentation tasks. To enhance the learning efficiency from dynamic event streams, we harness the adaptive threshold which improves network accuracy, sparsity and robustness in streaming inference. Moreover, we develop a dual-path Spiking Spatially-Adaptive Modulation module, which is specifically tailored to enhance the representation of sparse events and multi-modal inputs, thereby co
    
[^64]: 视觉变换器：从语义分割到密集预测

    Vision Transformers: From Semantic Segmentation to Dense Prediction

    [https://arxiv.org/abs/2207.09339](https://arxiv.org/abs/2207.09339)

    本研究不仅为密集预测领域提供了一个强大且有力的基线，而且还证明了在没有预训练的情况下，使用ViT进行任务的可行性。在各种密集预测任务上的进一步实验还表明，ViT不仅在常规分割任务中与其他模型竞争，而且还能很好地泛化到其他现实世界应用中，如基于精细生物医学图像分析的新医学诊断脚本。

    

    arXiv:2207.09339v4 宣布类型：替换  摘要：在图像分类领域，视觉变换器（ViTs）的出现改变了视觉表示学习的方法论。特别是，ViTs在第层对所有图像块的全视场范围内学习视觉表示，而与卷积神经网络（CNNs）层间和其他替代方法的逐渐扩大视场相比。在这项工作中，我们首次探索了ViTs在全球上下文中对密集视觉预测（如语义分割）的能力。我们的动机是，通过在每一层学习全视场的全局上下文，ViTs可能捕获更强的长距离依赖信息，这对于密集预测任务至关重要。我们首先证明，将图像编码为序列块，一种没有局部卷积和分辨率降级的原始ViT，可以为语义分割提供更强的视觉表示。例如，我们的实顶实验结果表明，与传统的分割网络相比，ViT即使在没有预训练的情况下，在语义分割任务上也表现出与离散网络匹÷的能力。 This study's findings not only provide a robust and compelling baseline for the dense prediction domain, but also demonstrate the feasibility of using ViTs for the task in a completely unsupervised manner. Further experiments on various dense prediction tasks also show that ViTs are highly competitive and generalize well beyond the conventional segmentation tasks to other real-world applications, such as new medical diagnosis scripts based on fine-grained biomedical image analysis.通过进一步在各种密集预测任务上的实验，我们发现ViT在常规分割任务之外的许多实际应用中也非常有竞争力，并展示了良好的泛化能力，例如，基于精细生物医学图像分析的新医学诊断脚本。

    arXiv:2207.09339v4 Announce Type: replace  Abstract: The emergence of vision transformers (ViTs) in image classification has shifted the methodologies for visual representation learning. In particular, ViTs learn visual representation at full receptive field per layer across all the image patches, in comparison to the increasing receptive fields of CNNs across layers and other alternatives (e.g., large kernels and atrous convolution). In this work, for the first time we explore the global context learning potentials of ViTs for dense visual prediction (e.g., semantic segmentation). Our motivation is that through learning global context at full receptive field layer by layer, ViTs may capture stronger long-range dependency information, critical for dense prediction tasks. We first demonstrate that encoding an image as a sequence of patches, a vanilla ViT without local convolution and resolution reduction can yield stronger visual representation for semantic segmentation. For example, ou
    

