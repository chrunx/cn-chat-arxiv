# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics](https://arxiv.org/abs/2408.02672) | 本文提出了一种名为Latent-INR的框架，该框架能够生成具有区分性语义的隐式视频表示，同时保持强大的压缩能力，使其适用于各种下游任务。 |
| [^2] | [Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](https://arxiv.org/abs/2408.02657) | 本研究提出了Lumina-mGPT，一种多模态自回归模型，它通过预训练的解码器-编码器变体在庞大的文本-图像序列上进行训练，能够生成灵活和现实主义的图像，并且通过预训练模型的渐进式强化正则化，在无监督的情形下取得了与监督图像生成模型相当甚至更好的性能。 |
| [^3] | [LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba](https://arxiv.org/abs/2408.02615) | 该论文提出了一种新的线性时间高保真扩散模型LaMamba-Diff，它结合了自注意力机制和Mamba块来同时处理全局和局部上下文，有效提高了处理长序列输入的效率和模型性能。 |
| [^4] | [Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection](https://arxiv.org/abs/2408.02595) | 本文提出了一种新框架来检测多模态讽刺，该框架利用图文描述来捕捉和检测讽刺，并提高了模型处理图文不匹配的能力。 |
| [^5] | [HQOD: Harmonious Quantization for Object Detection](https://arxiv.org/abs/2408.02561) | 这里是中文总结出的一句话要点 |
| [^6] | [Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts](https://arxiv.org/abs/2408.02496) | 本文首次提出了自动评级不完全海马反转（IHI）的方法，该方法通过预测四个解剖学标准并将它们相加获得综合评分。尽管如此，研究中的挑战是如何克服模型对不同队列中特征空间的差异的敏感性。为此，作者提出了将不同队列的数据统一后进行训练，并在单独的队列上进行验证和测试的策略。实验结果表明，这种方法在保持较高评分准确性的同时，也加快了评估的速度和可行性，对于大规模研究和临床应用具有重要意义。 |
| [^7] | [HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions](https://arxiv.org/abs/2408.02494) | 本文提出了一种名为 HyperSpaceX 的技术，通过探索多径向超球面空间中的角向和径向特征，使用了一种新型的 DistArc 损失函数来提高分类任务的类间区分度。 |
| [^8] | [Attenuation-adjusted deep learning of pore defects in 2D radiographs of additive manufacturing powders](https://arxiv.org/abs/2408.02427) | 本文提出的深度学习模型能够在单张2D X射线图像中准确分割孔隙，以提高金属粉末添加剂制造的孔隙度分析效率。与基线模型相比，该方法实现了F1分数的显著提升。 |
| [^9] | [FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification](https://arxiv.org/abs/2408.02426) | FPT+是一种高效的转移学习方法，特别适用于高分辨率医学图像分类，能够在减少内存消耗的同时，从大型预训练模型中学习知识。 |
| [^10] | [Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models](https://arxiv.org/abs/2408.02408) | 论文提出 MCGF 多天气跨视角地理定位框架，通过使用去噪扩散模型动态适应未知天气条件，并采用联合优化方法提升跨视角地理定位的准确性。 |
| [^11] | [Tensorial template matching for fast cross-correlation with rotations and its application for tomography](https://arxiv.org/abs/2408.02398) | 本文通过张量模板匹配算法大幅提升了在三维图像中检测旋转物体的速度和精度。 |
| [^12] | [CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration](https://arxiv.org/abs/2408.02394) | 研究提出了一种通过强化学习和模仿学习初始化的CMR-Agent，它可以实现迭代图像到点云注册。该方法通过提出2D-3D混合状态表示来充分利用2D图像和3D点云的特征，旨在提高注册精度和解释能力。 |
| [^13] | [Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial Images](https://arxiv.org/abs/2408.02382) | 本研究提出了一种改进的跨伪监督框架，用于在稀疏标注的高分辨率卫星图像上训练精确的土地使用土地覆盖（LULC）分割模型，尤其适用于资源有限的繁忙城市环境。 |
| [^14] | [The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024](https://arxiv.org/abs/2408.02369) | 本文介绍了NPU-ASLP团队在CNVSRC 2024竞赛中对VSR系统的技术细节，包括数据处理、模型架构和实现效果，并获得了良好成绩。 |
| [^15] | [StoDIP: Efficient 3D MRF image reconstruction with deep image priors and stochastic iterations](https://arxiv.org/abs/2408.02367) | 该算法通过Deep Image Prior和随机迭代技术改进了3D MRF图像的准确重建，无需大量ground-truth数据即可进行高效的三维重建，克服了传统3D成像的挑战。 |
| [^16] | [Infusing Environmental Captions for Long-Form Video Language Grounding](https://arxiv.org/abs/2408.02336) | 引入与长视频对齐的丰富环境描述信息，有效排除无关内容，提高了视频语言对准任务的精确度。 |
| [^17] | [Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face Manipulation Detection and Localization](https://arxiv.org/abs/2408.02306) | 本研究提出了一种名为MoNFAP的新框架，该框架通过集成伪造意识统一预测和噪声混合模块增强了多脸操纵检测和定位的性能。 |
| [^18] | [Network Fission Ensembles for Low-Cost Self-Ensembles](https://arxiv.org/abs/2408.02301) | 论文提出了一种名为Network Fission Ensembles的低成本集成学习与推论方法，通过在单个网络中生成多个输出来实现集成，消除了多重模型带来的成本。 |
| [^19] | [Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation](https://arxiv.org/abs/2408.02297) | 此研究通过校准感知概率和不确定性改进了身体人工智能的搜索任务，特别是在不同类型的语义感知模型中取得了显著进展。 |
| [^20] | [Joint-Motion Mutual Learning for Pose Estimation in Videos](https://arxiv.org/abs/2408.02285) | 本文提出了一种新的视频姿态估计学习框架，通过结合原始关节信息和运动特征动态，有效提高了姿态估计的准确性。 |
| [^21] | [Cascading Refinement Video Denoising with Uncertainty Adaptivity](https://arxiv.org/abs/2408.02284) | 本文提出的级联精化视频去噪方法通过自适应不确定性升采样技术，成功实现了在CRVD数据集上的大幅度性能提升。通过创建不确定度图，该方法平均减少了25%的计算量。 |
| [^22] | [Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes](https://arxiv.org/abs/2408.02275) | 本系统利用几何代数与大型语言模型结合，实现了3D场景中对象精确重新定位，无需专业训练数据，仅通过自然语言指令即可操作。 |
| [^23] | [COM Kitchens: An Unedited Overhead-view Video Dataset as a Vision-Language Benchmark](https://arxiv.org/abs/2408.02272) | 本研究提出了一项名为COM厨房的数据集，它是由智能手机拍摄的未经编辑的上方视角烹饪视频组成，旨在帮助深度学习模型在视觉和语言社区中对程序视频进行理解和分析。 |
| [^24] | [VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking](https://arxiv.org/abs/2408.02263) | VoxelTrack是一个新型3D对象跟踪框架，通过体元化和稀疏卷积块有效建模了精确的3D空间信息，实现了对跟踪对象位置预测的准确性。 |
| [^25] | [Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders](https://arxiv.org/abs/2408.02245) | 本文提出了一种结合课程学习、多模态对比掩码自编码器和去噪技术的新预训练方法，专门用于改进图像理解的性能，特别是在RGB-D数据集上。 |
| [^26] | [ProCreate, Don\'t Reproduce! Propulsive Energy Diffusion for Creative Generation](https://arxiv.org/abs/2408.02226) | 本研究提出 ProCreate 策略，旨在提高差异化图像生成模型的样本多样性和创造力，并防止训练数据的重复。它通过在生成过程中推动生成的图像远离参考图像，以实现这一目标。该策略在处理八个不同类别少样本创意生成数据集时表现出色，并且在大规模训练文本提示评估中也显示出了防止再现训练数据的能力。 |
| [^27] | [More Than Positive and Negative: Communicating Fine Granularity in Medical Diagnosis](https://arxiv.org/abs/2408.02214) | 研究提出了一种新的医学图像基准，以精细粒度学习为目标，证明了AI模型能够超越现有知识界限，捕捉并传递更详细的疾病状态信息，为医学图像的AI诊断提供了新的视角。 |
| [^28] | [ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning](https://arxiv.org/abs/2408.02210) | ExoViP是一种新方法，通过验证模块的辅助，能够改善视觉语言编程方案中规划与执行错误的问题，确保任务的正确执行和决策过程的设计。 |
| [^29] | [PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance](https://arxiv.org/abs/2408.02157) | PanoFree是一种无需调参的全景图像生成方法，通过迭代变形和上色技术，无需微调即可解决多视角图形生成中的累积误差问题。 |
| [^30] | [VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces](https://arxiv.org/abs/2408.02140) | VidModEx方法使用SHAP增强了合成数据生成，提高了图像和视频分类模型的性能，在多个高维数据集上取得了显著的提升。 |
| [^31] | [RICA^2: Rubric-Informed, Calibrated Assessment of Actions](https://arxiv.org/abs/2408.02138) | RICA^2模型是一种深层概率模型，它通过整合评分大纲并量化预测不确定性，在行动质量评估方面取得了新纪录，特别是在FineDiving、MTL-AQA和JIGSAWS基准测试中表现优秀。 |
| [^32] | [A First Look at Chebyshev-Sobolev Series for Digital Ink](https://arxiv.org/abs/2408.02135) | 本文探讨了使用切比雪夫-索布列夫级数对数字墨的初步研究，发现了这种表示方法可能在某些方面优于拉格朗日-索布列夫级数对数字墨表示。 |
| [^33] | [View-consistent Object Removal in Radiance Fields](https://arxiv.org/abs/2408.02100) | 在这里是中文总结出的一句话要点 |
| [^34] | [Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models](https://arxiv.org/abs/2408.02085) | 本研究综述了评估与选择用于语言模型指令训练的数据方法的现有文献，揭示了不同评估方法的实际应用及未来研究的可能性，旨在为最优的数据驱动训练提供有价值的见解和策略。 |
| [^35] | [Improving Neural Surface Reconstruction with Feature Priors from Multi-View Image](https://arxiv.org/abs/2408.02079) | 本研究旨在通过探究七种预假视觉任务的多视图特征先验来改进神经表面重建，并利用迭代训练显著提升了重建质量。 |
| [^36] | [LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake Generation](https://arxiv.org/abs/2408.02078) | LDFaceNet是使用指导潜扩散模型进行面部交换的视频生成方法，能够生成高质量Deepfake视频。 |
| [^37] | [Case-based reasoning approach for diagnostic screening of children with developmental delays](https://arxiv.org/abs/2408.02073) | 本文提出了一种基于案例推理的儿童发展迟缓诊断筛查方法，旨在早发现、早干预，减少医疗和社会成本，并提高治疗效果。 |
| [^38] | [Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image Generation](https://arxiv.org/abs/2408.02054) | 该研究提出了一个经过调优的NLP模型，它能够实时推荐用于生成高质量图像的最优去噪步骤数，显著节约了计算资源。 |
| [^39] | [3D Single-object Tracking in Point Clouds with High Temporal Variation](https://arxiv.org/abs/2408.02049) | 我们提出了HVTrack框架，用于处理点云中具有高时序变化的三维单对象跟踪问题，通过相对姿态感知记忆模块、基扩张特征交叉注意力模块和上下文点引导自注意力模块来解决挑战。 |
| [^40] | [Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2408.02039) | 本文提出了一种名为 Pixel-Level Domain Adaptation (PLDA) 的全新方法，旨在通过对抗训练的方式，使模型学习到在物体不同部分之间具有一致性的像素级特征，从而在从图像标签进行弱监督语义分割的背景下，生成更清晰、更精确的伪 mask 信息。 |
| [^41] | [LEGO: Self-Supervised Representation Learning for Scene Text Images](https://arxiv.org/abs/2408.02036) | 我们提出了一种名为LEGO的自监督方法，用于学习场景文本图像的表示，通过创造性任务捕捉复杂结构，提升文本识别能力，适用于多种深度学习机器视觉任务。 |
| [^42] | [Enhancing Human Action Recognition and Violence Detection Through Deep Learning Audiovisual Fusion](https://arxiv.org/abs/2408.02033) | 本文提出了一种基于深度学习视听融合的混合融合技术，显著提高了在公共场所的人体动作识别和暴力检测的准确性。 |
| [^43] | [Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease](https://arxiv.org/abs/2408.02018) | 本文通过使用条件变分AutoEncoder，构建了一个能在个体基础上进行MRI时间序列预测的模型，从而提高了阿尔茨海默病诊断的特异性。 |
| [^44] | [Decision Support System to triage of liver trauma](https://arxiv.org/abs/2408.02012) | 本文提出了一种决策支持系统，用于高效地评估和优先处理肝脏创伤病例，以减少治疗成本和继发并发症的风险。 |
| [^45] | [What Happens Without Background? Constructing Foreground-Only Data for Fine-Grained Tasks](https://arxiv.org/abs/2408.01998) | 该研究提出了一种通过SAM和Detic技术创建仅包含前景数据的工程化管道，以促进微细分任务中忽视背景噪声影响的科学研究。 |
| [^46] | [SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning](https://arxiv.org/abs/2408.01970) | SR-CIS通过结合小模型快速推理和慢速决策的大模型，并通过CA-OAD机制实现高效协作，提供了一种新的内存与推理解耦的机制，以解决当前深度学习模型在面对人类记忆和学习机制时的挑战。 |
| [^47] | [Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI](https://arxiv.org/abs/2408.01959) | 研究揭示了43个CLIP视觉语言模型在面部印象偏见方面的学习情况，证明了社会一致性会中介模型反映的人类偏见程度，并且在数据集大规模训练下，模型更倾向于准确形成那些依赖于不可见属性的印象。 |
| [^48] | [EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning](https://arxiv.org/abs/2408.01953) | 我们的EqvAfford框架通过确保点级affordance学习的SE(3)等变性，为机器人操作任务提供优异的性能和泛化能力，无论物体姿态如何。 |
| [^49] | [Generalized Maximum Likelihood Estimation for Perspective-n-Point Problem](https://arxiv.org/abs/2408.01945) | 该研究提出了一个广义最大似然估计的PnP求解器，提高了对噪声数据的鲁棒性，并在多种姿势估计场景中提供了更准确的参数估计。 |
| [^50] | [RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under Continuous Representation](https://arxiv.org/abs/2408.01944) | 本文提出了一种新的自适应多尺度扩散采样策略，提高了扩散磁共振成像（dMRI）参数估计的计算效率和模型对未知测试样品的泛化能力，特别是在测试扩散方向与训练方向不一致的情况下。 |
| [^51] | [A Survey and Evaluation of Adversarial Attacks for Object Detection](https://arxiv.org/abs/2408.01934) | 本文全面调查了专门针对对象检测的多种对抗攻击方法，提供了对现有攻击评价指标的回顾，并对多种开源攻击方法进行了系统性的评估。 |
| [^52] | [Advancing H&E-to-IHC Stain Translation in Breast Cancer: A Multi-Magnification and Attention-Based Approach](https://arxiv.org/abs/2408.01929) | 本文提出了一种新的模型，该模型结合了注意力机制和多倍率信息处理，旨在解决从H&E染色切片合成IHC-HER2切片时遇到的挑战，特别是处理多倍率病理图像和在翻译过程中关注重要信息的问题。 |
| [^53] | [CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in Biomedical Imagery](https://arxiv.org/abs/2408.01897) | 本文提出了一种基于YOLOv8架构的CAF-YOLO方法，用于多尺度病变检测，通过引入ACFM和SFM模块，提高了对生物医学数据中异常细胞和肺结节等细小结构的检测精度。 |
| [^54] | [FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and Fisheye Neural Radiance Fields](https://arxiv.org/abs/2408.01878) | This study introduces FBINeRF, a novel technique integrating feature-based recurrent neural networks with adaptive GRUs for improved camera pose adjustment in capturing 3D scenes, especially for fisheye cameras, offering faster convergence and higher reconstruction quality relative to existing methods. |
| [^55] | [Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?](https://arxiv.org/abs/2408.01877) | 本研究通过在实体地面代理和空中代理之间实施基于视觉语言模型的生成式通信，显著提高了其在模拟环境中的零样本目标导航能力，同时减少了盲目探索的时间。 |
| [^56] | [Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples](https://arxiv.org/abs/2408.01872) |  |
| [^57] | [Supervised Image Translation from Visible to Infrared Domain for Object Detection](https://arxiv.org/abs/2408.01843) | 本文提出了一种由可见域到红外域的监督图像翻译方法，通过将问题描述为图像翻译并采用两阶段训练策略，改进了物体检测等下游任务的准确性。通过集成生成对抗网络和物体检测模型，该方法学习了一种既能保留可见图像的结构细节又能保持红外图像纹理的转换过程。生成的图像被用于训练标准物体检测框架，实现了显著的准确性提升。 |
| [^58] | [A Deep CNN Model for Ringing Effect Attenuation of Vibroseis Data](https://arxiv.org/abs/2408.01831) | 本文提出了一种使用深度卷积神经网络的新方法，直接通过训练模型端到端地减少Vibroseis数据的串联效应，并通过跳过连接保持数据的细节，从而有效降低了串联效应。 |
| [^59] | [GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer](https://arxiv.org/abs/2408.01826) | GLDiTalker是一种新方法，通过引入动作先验和随机性提高了基于语音的3D面部动画生成模型的多样性。 |
| [^60] | [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800) | MiniCPM-V是一款能实现在手机上运行的强大且高效的多模态大型语言模型，它在性能上超越了GPT-4V-1106，Gemini Pro和Claudrat's Pluribus，并且大大减少了能耗和成本，可以为移动应用提供高质量的语言理解和生成能力，同时也保护了用户隐私和数据安全。 |
| [^61] | [NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation and Classification](https://arxiv.org/abs/2408.01797) | 我们提出了一种轻量级且高效的核实例分割和分类模型νLite，与SOTA模型CellViT相当，但参数和GFlops显著减少，为病理学提供了一种高效而强大的解决方案。 |
| [^62] | [Comparison of Embedded Spaces for Deep Learning Classification](https://arxiv.org/abs/2408.01767) | 本文对比了不同嵌入空间技术以设计用于深度学习分类的嵌入空间，提供了两种和三维嵌入技术的展示案例。 |
| [^63] | [MultiFuser: Multimodal Fusion Transformer for Enhanced Driver Action Recognition](https://arxiv.org/abs/2408.01766) | MultiFuser是一个多模态融合变换器，用于增强驾驶员行为识别，通过事件分解模块和模态合成器集成多种摄像头数据来提升表示力。 |
| [^64] | [Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification](https://arxiv.org/abs/2408.01752) | 本次研究专注于建立轻量级CNN模型，如ShuffleNet、MobileNetV2和EfficientNet-B0，用于实现对水稻叶片病害的高效和准确识别。通过结合局部绝对差异操作、空间金字塔池化和改进的全连接层，该模型能够显著提高病害识别的准确率。 |
| [^65] | [Domain penalisation for improved Out-of-Distribution Generalisation](https://arxiv.org/abs/2408.01746) | 本文提出了一种域惩罚框架，用于平衡多源域数据集中对象检测的训练过程，解决了域泛化问题，并在GWHD 2021数据集上取得了提升。 |
| [^66] | [Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation](https://arxiv.org/abs/2408.01732) | 我们提出了一个两阶段扩散模型，使用音频同步面部特征点，并在此基础上生成高质量、同步且时间一致的说话人头像视频，解决了当前模型在图像质量和唇形同步方面的不足。 |
| [^67] | [A Novel Evaluation Framework for Image2Text Generation](https://arxiv.org/abs/2408.01723) | 该论文提出了一种基于现代大型语言模型的新框架，用于评估自动生成的图像描述质量，该框架结合了人类评价和模型性能度量，旨在提供更准确和可靠的评估结果。 |
| [^68] | [Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the Benefits and Computational Costs of Loop Closing](https://arxiv.org/abs/2408.01716) | 本文对多种视觉-惯性SLAM系统进行了农业机器人场景下的性能评估，重点分析了闭环闭合对定位精度和计算成本的影响。 |
| [^69] | [A General Ambiguity Model for Binary Edge Images with Edge Tracing and its Implementation](https://arxiv.org/abs/2408.01712) | 该研究提出了一款适用于二值边缘图像模糊结构的一般模型，结合了边缘追踪技术，并解释了算法以实现后续任务的处理优化。 |
| [^70] | [AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation](https://arxiv.org/abs/2408.01708) | AVESFormer是一个高效的音频视觉变换器模型，实现了实时、高效和轻量级的AVS任务。 |
| [^71] | [Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics](https://arxiv.org/abs/2408.01701) | 本文提出了一种名为信号-SGN的脉冲图卷积网络，该网络结合了图卷积特性和脉冲神经网络特性，旨在有效的处理骨骼动作序列的时间维度。信号-SGN能够通过在单个帧上进行图卷积，并在脉冲神经网络中捕获帧间的时序关系，以及在频率空间处理时空频率动态，以实现动作识别的高效和精确。 |
| [^72] | [Bayesian Active Learning for Semantic Segmentation](https://arxiv.org/abs/2408.01694) | 我们提出了一个贝叶斯主动学习框架，通过平衡熵来优化对稀疏像素级标注的数据集的语义分割模型的训练，仅使用很少的像素标注即可达到监督训练水平。 |
| [^73] | [A Comparative Analysis of CNN-based Deep Learning Models for Landslide Detection](https://arxiv.org/abs/2408.01692) | 这篇论文比较了四种传统语义分割模型（U-Net、LinkNet、PSPNet和FPN）和ResNet50 backbone encoder的应用，通过实验评估证明所提出的模型可以有效检测滑坡并提高检测精度。 |
| [^74] | [IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection](https://arxiv.org/abs/2408.01690) | 本研究引入了一个新的数据集IDNet，旨在促进隐私保护的欺诈检测技术的进步。 |
| [^75] | [Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization](https://arxiv.org/abs/2408.01689) | 本文提出了一种使用ε控制因子调节图像到图像生成模型去学习程度的方法，旨在平衡去学习和模型性能，保护用户数据安全。 |
| [^76] | [SiamMo: Siamese Motion-Centric 3D Object Tracking](https://arxiv.org/abs/2408.01688) | SiamMo是一种新的3D物体跟踪方法，它基于Siamese运动中心范式，通过Siamese特征提取和Spatio-Temporal Feature Aggregation模块改善了跟踪性能，并使用了Box-aware Feature Encoding模块来编码物体框相关特征。 |
| [^77] | [iControl3D: An Interactive System for Controllable 3D Scene Generation](https://arxiv.org/abs/2408.01678) | iControl3D是一个使用户能够精确控制并生成可定制3D场景的交互式系统。它使用3D网格作为中间代理，通过迭代合并2D扩散生成的图像来创建连贯统一的3D场景表示。 |
| [^78] | [HIVE: HIerarchical Volume Encoding for Neural Implicit Surface Reconstruction](https://arxiv.org/abs/2408.01677) | HIVE方法通过引入层次化体积编码和稀疏结构，实现了对三维形状的高精度重建，并显著提高了细节表现。 |
| [^79] | [Multiple Contexts and Frequencies Aggregation Network forDeepfake Detection](https://arxiv.org/abs/2408.01668) | 本文提出了一种名为 MkfaNet 的网络，通过自适应地聚合空间和频率域的特征，提升了深度假造检测的准确性和鲁棒性。 |
| [^80] | [SAT3D: Image-driven Semantic Attribute Transfer in 3D](https://arxiv.org/abs/2408.01664) | 这项研究提出了一种新的方法，可以基于参考图像在3D环境中实现更准确的语义属性转移，解决了以往方法在语义属性编辑方面的局限性。 |
| [^81] | [Deep Patch Visual SLAM](https://arxiv.org/abs/2408.01654) | 我们提出了一种名为“深度补丁视觉SLAM”的新方法，它能在单GPU上实现高质量的单目视觉SLAM，即使在大型数据集上也能保持接近真实时间的帧率，同时内存消耗仅为现有系统的很小一部分。 |
| [^82] | [MCPDepth: Omnidirectional Depth Estimation via Stereo Matching from Multi-Cylindrical Panoramas](https://arxiv.org/abs/2408.01653) | MCPDepth是一种使用圆柱全景的立体匹配方法来估计全景深度的新框架，它在不同视图的深度图融合中表现出优异的性能。这项技术特别设计为简化对嵌入式设备的部署，并显著提高了深度估计的准确性。 |
| [^83] | [Zero-Shot Surgical Tool Segmentation in Monocular Video Using Segment Anything Model 2](https://arxiv.org/abs/2408.01648) | 最新一代的图像和视频分割基础模型Segmen Anything Model 2在零样本手术工具分割方面取得了显著进展，尤其是在手术视频中，它能够适应不同手术类型和工具长度的复杂背景，并且性能得到了良好验证。 |
| [^84] | [Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation](https://arxiv.org/abs/2408.01640) | 本文提出了一种利用消费型车辆上的GNSS数据和摄像头图像进行道路图自动创建的方法，该方法不仅提高了创建道路图的准确性，还大大减少了所需的工作量和成本。 |
| [^85] | [JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2408.01627) | 本文介绍了一种名为JambaTalk的3D说话头像生成方法，该方法基于混合Transformer-Mamba语言模型，旨在通过提高动作多样性实现更快的速度，并在与现有最先进模型的比较中取得了出色成绩。 |
| [^86] | [MedUHIP: Towards Human-In-the-Loop Medical Segmentation](https://arxiv.org/abs/2408.01620) | MedUHIP是一个结合了不确定性感知模型与人类参与循环的医疗分割新方法，旨在从具有不确定性的医疗图像中实现定量分割，并在临床实践中通过与医生的互动不断改进分割结果。 |
| [^87] | [Deep Learning Meets OBIA: Tasks, Challenges, Strategies, and Perspectives](https://arxiv.org/abs/2408.01607) | 本文详细阐述了深度学习在遥感领域中的OBIA任务、面临的挑战以及应对策略，并提出了未来的研究方向。 |
| [^88] | [Self-Supervised Depth Estimation Based on Camera Models](https://arxiv.org/abs/2408.01565) | 该论文提出了一种基于摄像机模型的新方法，利用摄像机内部结构的信息来提高深度估计的质量，无需额外的数据和成本。 |
| [^89] | [Accelerating Domain-Aware Electron Microscopy Analysis Using Deep Learning Models with Synthetic Data and Image-Wide Confidence Scoring](https://arxiv.org/abs/2408.01558) | 使用合成数据和全局置信度评分的深度学习模型，本研究有效提高了电子显微镜分析的领域特定性，减少了人类标注的依赖，并提升了分析的效率和可靠性。 |
| [^90] | [Enhanced Knee Kinematics: Leveraging Deep Learning and Morphing Algorithms for 3D Implant Modeling](https://arxiv.org/abs/2408.01557) | 论文提出了一种利用机器学习和变形技术自动且精确重建术前膝关节植入模型的方法，极大地方便了手术规划和生物力学评估。 |
| [^91] | [Multi-task SAR Image Processing via GAN-based Unsupervised Manipulation](https://arxiv.org/abs/2408.01553) | 本文提出了一种名为GUE的SAR图像处理框架，利用GAN在无监督的情况下处理图像，解决了语义方向在GAN潜在空间的分离和标记数据缺失的问题。 |
| [^92] | [Trainable Pointwise Decoder Module for Point Cloud Segmentation](https://arxiv.org/abs/2408.01548) | 本文提出的一个自训练解码器模块旨在改善点云分割，尤其是在处理低密度点云时。它能够学习适应性点值转换，预测因投影而丢失的点类别，从而提高性能。 |
| [^93] | [Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics](https://arxiv.org/abs/2408.01541) | 本研究全面评估了25种防御策略，对其在对抗攻击下的表现进行了测试，并分析了对单个防御措施的差异。 |
| [^94] | [SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts](https://arxiv.org/abs/2408.01537) | SceneMotion模型通过使用一个新的潜在上下文模块将局部agent-centric嵌入转换为全景级预测，在Waymo Open Interaction Prediction Challenge中表现出色。 |
| [^95] | [Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization](https://arxiv.org/abs/2408.01532) | 本文提出了一种基于RNN的音频-视觉深度伪造检测和定位的新框架，通过跨模态注意力和上下文信息的学习，提高检测准确性并定位伪造图像的不自然区域。 |
| [^96] | [Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps](https://arxiv.org/abs/2408.01471) | 我们的研究提出了一种新的混合制图框架，该框架通过利用实时地标数据和标准定义地图，实现了高效的在线高清地图生成。 |
| [^97] | [Estimating Environmental Cost Throughout Model's Adaptive Life Cycle](https://arxiv.org/abs/2408.01446) | 本文提出了一种预测指数PreIndex，用来估计在模型适应性生命周期的再培训过程中，从当前数据分布到新数据分布的变化中，所涉及的环境成本（如碳排放和能源消耗）。 |
| [^98] | [A New Clustering-based View Planning Method for Building Inspection with Drone](https://arxiv.org/abs/2408.01435) | 本文提出了一种基于聚类的两步计算方法，使用谱聚类、局部潜在场方法和超遗传算法来为无人机建筑检查找到近似最优视角。 |
| [^99] | [VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance](https://arxiv.org/abs/2408.01432) | VLG-CBM是一种新的框架，旨在通过引入视觉语言指导机制，提高概念瓶颈模型的解释性和一致性，并解决概念预测与输入图像不一致的问题。 |
| [^100] | [Transferable Adversarial Facial Images for Privacy Protection](https://arxiv.org/abs/2408.01428) | 本文提出了一种新的面部隐私保护方案，能在保持高视觉质量的同时提高转移能力。通过直接塑造整个面部空间，而不是仅利用妆容信息等一种特征来整合对抗性噪声，该方案能够在黑盒场景中生成自然且高度可转移的对抗性面部图像。 |
| [^101] | [Siamese Transformer Networks for Few-shot Image Classification](https://arxiv.org/abs/2408.01427) | 本研究提出了基于Siamese Transformer的网络，通过提取全局和局部特征，结合了Euclidean distance measure，在少样本图像分类任务中取得了显著提升。 |
| [^102] | [Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs](https://arxiv.org/abs/2408.01355) | 这里是中文总结出的一句话要点 |
| [^103] | [CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression](https://arxiv.org/abs/2408.00938) | 本研究提出了一种基于临床知识改进的扩散模型，用于更准确地预测特发性肺纤维化（IPF）的进展。 |
| [^104] | [Scaling Backwards: Minimal Synthetic Pre-training?](https://arxiv.org/abs/2408.00677) | 论文发现即使在最基本的合成图像上进行预训练也能达到与大型数据集类似的性能，证明了预训练即使在假想情境下也有效。 |
| [^105] | [MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation](https://arxiv.org/abs/2407.21640) | MSA²Net 是一种新的深度分割框架，它通过设计快捷的跳转连接，解决了医学图像分割中对本地和全局特征进行有效融合的问题，以处理结构变化，促进特征的动态加权和组合。 |
| [^106] | [Adding Multimodal Controls to Whole-body Human Motion Generation](https://arxiv.org/abs/2407.21136) | 我们提出了一种名为ControlMM的框架，它使用统一的模型来同时控制整个身体的运动生成，并处理不同的条件模式，包括文本、语音和音乐。ControlMM解决了运动分布在不同生成任务中的偏移问题，同时通过使用一种粗粒度的方法来处理不同粒度的条件。 |
| [^107] | [EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from bi-planar X-ray images](https://arxiv.org/abs/2407.20937) | EAR网络通过自编码器架构的改进，加强了边缘信息的感知并提高了椎体形状的重建精度。 |
| [^108] | [Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation](https://arxiv.org/abs/2407.20391) | 本研究提出了一种稳健的度量方法，用于评估相机姿态估计的准确性，该方法旨在识别和纠正姿态估计中的误差，通过构建一个可靠的评估框架，帮助提高图像序列中相机位置和方向的精确度。 |
| [^109] | [Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images](https://arxiv.org/abs/2407.19719) | 该论文提出了一种使用多模态大型语言模型和街景图像来评估城市安全的新方法，旨在自动化大规模的安全评估过程，提高效率和准确性。 |
| [^110] | [VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary](https://arxiv.org/abs/2407.19524) | 我们提出了一种通用的文本到图像模型偏激消除框架VersusDebias，使用小语言模型和生成对手机制来减少偏激想象的影响，并通过提示工程生成无偏的提示。 |
| [^111] | [Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers](https://arxiv.org/abs/2407.18534) | 本文提出的RPD方法通过蒸馏机制让3D网络从预训练的2D视觉网络中学习关系先验知识，提高了其在跨域点分类任务上的性能。 |
| [^112] | [MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos](https://arxiv.org/abs/2407.18289) | 模型MARINE采用了高效的动作识别技术，成功地识破了动物视频中罕见捕食者-猎物的交互行为，在多个数据集上表现优异，显示出其在动物行为分析平台中的潜在应用价值。 |
| [^113] | [Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT](https://arxiv.org/abs/2407.18288) | 本文通过知识蒸馏利用了DINOv2基础模型，将其特征蒸馏到FairMOT中，虽然取得了一定的改进，但仍需要更多研究来验证其实际应用的有效性。 |
| [^114] | [Spatiotemporal Graph Guided Multi-modal Network for Livestreaming Product Retrieval](https://arxiv.org/abs/2407.16248) | 该研究提出了一种名为SGMN的模型，利用文本指导注意力机制和视频空间时间信息，有效地捕捉并识别直播中的产品特征，为未来的实时产品识别与推荐提供支持。 |
| [^115] | [Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual Transformers](https://arxiv.org/abs/2407.13200) | 该研究提出了一种自适应PointFormer（APF），通过微调2D视觉模型直接处理3D点云，从而无需将点云映射到图像上。这种方法可以有效学习低维空间中的点云表示，同时显著提高现有方法性能。 |
| [^116] | [HPC: Hierarchical Progressive Coding Framework for Volumetric Video](https://arxiv.org/abs/2407.09026) | 本文提出了一种使用单一模型的分层渐进式体积视频编码框架HPC，可以实现灵活的比特率调整，以适应各种网络和设备能力。 |
| [^117] | [The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective](https://arxiv.org/abs/2407.08583) | 论文讨论了多模态大型语言模型与数据协同发展的关系，表明两者在模型性能和数据质量提升中起到相互促进的作用。 |
| [^118] | [Exploiting Scale-Variant Attention for Segmenting Small Medical Objects](https://arxiv.org/abs/2407.07720) | 本文提出了一种名为SvANet的新网络，旨在利用可变注意力机制提高在小医疗图片中识别小区域的目标效率。此网络能更精确地分割图片，并且能应用于实际医疗诊断中，从而提高疾病的早期识别率和治疗成功率。 |
| [^119] | [Beyond Aesthetics: Cultural Competence in Text-to-Image Models](https://arxiv.org/abs/2407.06863) | 该研究提出了一种评估文本到图像模型文化能力的框架，并建立了一个覆盖多个国家、多个文化的文化基准测试，以评估模型在文化意识和多样性方面的表现。 |
| [^120] | [ViG-Bias: Visually Grounded Bias Discovery and Mitigation](https://arxiv.org/abs/2407.01996) | ViG-Bias是一种新方法，用于在视觉模型中自动发现和缓解潜藏的偏差，通过大型视觉语言模型提取跨模态嵌入，即使在属性未知的场景中也能识别出系统失败的模式。 |
| [^121] | [Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/abs/2406.19905) | 本文提出的方法旨在解决在大视觉语言模型中使用混合专家模式时出现的项目标梯度冲突问题，通过使用专门的学习项和初步的部署标签来优化模型性能。 |
| [^122] | [InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection](https://arxiv.org/abs/2406.16464) | InterCLIP-MEP 框架使用交互式 CLIP 和增强的记忆预测器改进了对社交媒体上多模态 sarcasm 的检测效果。 |
| [^123] | [CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition](https://arxiv.org/abs/2406.11340) | CM2-Net提出了一种创新的不断学习新模态特征表示的多模态映射网络，能够在多种模态数据上提高驾驶员行为识别的精度。 |
| [^124] | [Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval](https://arxiv.org/abs/2406.10107) | 本研究提出了一种名为ANNEAL的主动学习方法，用于在遥感图像检索中高效地学习深度度量学习模型。该方法通过结合不确定性与多样性的评价标准，创造了一个小但信息丰富的训练集，显著提升了检索性能。 |
| [^125] | [VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model](https://arxiv.org/abs/2406.01059) | 本文提出了一个基于多模态大型语言模型的图像无界描摹框架，能够根据用户需求定制结果。引入了一种特殊的交叉注意力模块，促进了中心和周边图像区域的交互。 |
| [^126] | [SE3D: A Framework For Saliency Method Evaluation In 3D Imaging](https://arxiv.org/abs/2405.14584) | 我们提出了SE3D框架，用于评估三维成像中的注意力方法，并针对3D CNN进行了基准测试和评价指标的改进。 |
| [^127] | [Reduced storage direct tensor ring decomposition for convolutional neural networks compression](https://arxiv.org/abs/2405.10802) | 本文提出了一种基于减少存储直接张量环分解的新型CNNs压缩方法，该方法在保持高分类准确性的同时，实现了参数和计算复杂度的大幅降低，并在CIFAR-10和ImageNet数据集上得到了验证。 |
| [^128] | [When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX](https://arxiv.org/abs/2405.01661) | CoReX是一种新的基于概念和关系的解释器，可以帮助更深入理解模型预测的内在机制，并提高模型在生物医学图像分析等领域的透明性和可解释性。 |
| [^129] | [Large Multi-modality Model Assisted AI-Generated Image Quality Assessment](https://arxiv.org/abs/2404.17762) | 研究人员开发了一种利用大型多模态模型以更好地评估AI生成图像质量的替代方法，弥补了传统基于DNN的IQA模型在处理复杂语义特征时的不足。 |
| [^130] | [Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales](https://arxiv.org/abs/2404.11129) | 论文要点: Fact 范式通过生成多模态语言模型的忠实、简洁、可转移论据，解锁了他们在视觉任务中的深层理解和推理过程。努力解决当前的MLLMs在透明性、可解释性以及执行复杂任务方面的挑战。 |
| [^131] | [Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based sky-segmentation in urban canyon](https://arxiv.org/abs/2404.11070) | 本文提出了一种基于全卷积网络的天区分割算法，用于GNSS信号的NLOS检测，并在此基础上提出了Sky-GVIO系统，该系统通过结合GNSS、INS和视觉特征实现了在城市峡谷环境中的连续和精确定位。 |
| [^132] | [Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Synergistic Reconstruction](https://arxiv.org/abs/2404.08748) | 论文提出了一种多分支生成模型，用于提高PET/CT协同成像的质量，特别是在低剂量成像方面。 |
| [^133] | [OpenBias: Open-set Bias Detection in Text-to-Image Generative Models](https://arxiv.org/abs/2404.07990) | 本文提出OpenBias，一个检测文本到图像生成模型中开放集偏见的系统，无需预先定义的偏差集合，利用大型语言模型和视觉问答模型来识别和量化这些模型可能存在的偏差，解决了现有方法无法处理开放集偏差的问题，提供定量证据。 |
| [^134] | [Reinforcement Learning with Generalizable Gaussian Splatting](https://arxiv.org/abs/2404.07950) | 我们提出了一种基于可泛化高斯点积的强化学习方法，该方法在不同的环境中提供了更清晰和泛化的场景表示，并通过直接使用形状函数和概率分布去除了“黑箱”效应，使得方法更易于理解和可视化。 |
| [^135] | [VRSO: Visual-Centric Reconstruction for Static Object Annotation](https://arxiv.org/abs/2403.15026) | VRSO方法通过使用摄像头图像和参考深度，在3D空间中准确恢复静态对象，并且能够处理难以捕捉的细节，在Waymo Open Dataset上表现出色。 |
| [^136] | [SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications](https://arxiv.org/abs/2403.11515) | 本文提出了一种名为 SSAP（Shape-Sensitive Adversarial Patch）的新方法，旨在在自主导航应用程序中全面破坏单目Depth估计（MDE）。该方法通过设计具有形状感知特性的对抗性补丁，故意误导 MDE 系统，使其在噪声和不确定性中仍然有效。实验表明 SSAP 能够诱导深度估计错误和行为决策偏差。 |
| [^137] | [Motion Mamba: Efficient and Long Sequence Motion Generation](https://arxiv.org/abs/2403.07487) | 论文提出了一种名为Motion Mamba的简单且高效的运动生成模型，它在U-Net架构中使用了层次时序Mamba（HTM）块，同时在预测过程中采用移动平均预测以保持运动的一致性。 |
| [^138] | [PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering](https://arxiv.org/abs/2403.05053) | 研究提出了一种新的渐进式组合扩散方法，专注于图像合成中的前景生成，并通过改进的注意力引导策略显著提高了合成速度和质量。 |
| [^139] | [NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator](https://arxiv.org/abs/2403.02411) | 本文提出了 NiNformer，一种结合了网络在网络 (Network in Network) 和 Transformer 架构的新模型，使用 Token 混合作为门控函数生成器，以提高计算效率和减少数据集大小要求。 |
| [^140] | [Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks](https://arxiv.org/abs/2402.17976) | 本文提出了一个辅助预处理防御网络AADN，通过在输入图像上进行防御性转换，增强了视觉目标跟踪的对抗鲁棒性。 |
| [^141] | [AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene Understanding](https://arxiv.org/abs/2402.17521) | AVS-Net是一种自适应体素大小点采样方法，它提高了3D场景理解的高精度和效率。 |
| [^142] | [PCR-99: A Practical Method for Point Cloud Registration with 99 Percent Outliers](https://arxiv.org/abs/2402.16598) | 该方法通过改进的三点采样和排序机制在99%的异常值比例下实现了点云注册的高效性和准确性。 |
| [^143] | [Zero shot VLMs for hate meme detection: Are we there yet?](https://arxiv.org/abs/2402.12198) | 本文旨在探究视觉语言模型在零样本情况下识别仇恨meme的能力，并发现尽管当前零样本方法在准确性和泛化能力方面存在局限性，但未来研究有潜力解决这些局限性并扩大其在网络空间仇恨内容检测和响应中的应用。 |
| [^144] | [AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion](https://arxiv.org/abs/2402.03309) | 这是研究建立了一个能够有效融合声学和光学数据的神经渲染框架，即使在受限于小基线的测量条件下，也能准确重建水下高分辨率三维表面。 |
| [^145] | [Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding](https://arxiv.org/abs/2312.16477) | 本文提出了一种基于知识蒸馏的压缩方法，用于减少3D形状识别中视图级别方法的参数数量，同时保持性能。通过设计一种名为GMViT的高性能大型模型，以及引入空间自编码器增强特征表示，该方法在一个名为DeCoV的策略下实现了在图像分类任务上的较好性能。 |
| [^146] | [De-fine: Decomposing and Refining Visual Programs with Auto-Feedback](https://arxiv.org/abs/2311.12890) | 本研究提出了一种无需训练的框架“De-fine”，它能够自动分解复杂任务为简单子任务，并通过自动反馈来改进程序，显著提高了逻辑推理性能。en_tdlr:Our study introduces a training-free framework "De-fine" that automatically decomposes complex tasks into simpler subtasks and refines programs with auto-feedback, significantly improving logical reasoning performance. |
| [^147] | [Holistic Dynamic Frequency Transformer for Image Fusion and Exposure Correction](https://arxiv.org/abs/2309.01183) | 本文提出了一种基于频率域的全局动态频率变压器方法，用于统一和改进曝光校正任务，包括低光照增强、曝光校正和多曝光融合。 |
| [^148] | [Domain Reduction Strategy for Non Line of Sight Imaging](https://arxiv.org/abs/2308.10269) | 本文提出一种优化方法来重建隐藏场景的非视线成像，通过减少不必要计算，并使用表面法线准确高效地建模视向反射率，从而提高重构效率和质量。 |
| [^149] | [How Generalizable are Deepfake Image Detectors? An Empirical Study](https://arxiv.org/abs/2308.04177) | 这项研究表明，目前深度伪造图像检测器缺乏跨数据集的泛化能力，但通过模型增强训练可以提高这一能力。 |
| [^150] | [FreeDrag: Feature Dragging for Reliable Point-based Image Editing](https://arxiv.org/abs/2307.04684) | FreeDrag是一种创新的图像编辑方法，通过动态特征更新和回头搜索技术改善了点式拖拽编辑的稳定性和速度。 |
| [^151] | [Transformer-Based Visual Segmentation: A Survey](https://arxiv.org/abs/2304.09854) | 本文综述了基于Transformer的视觉分割技术，重点关注了其在分割任务中的有效应用和性能优势。 |
| [^152] | [Progressive Visual Prompt Learning with Contrastive Feature Re-formation](https://arxiv.org/abs/2304.08386) | 本文提出了渐进式视觉提示学习方法，结合了对比性特征重构技术，提高了视觉语言模型的泛化能力。 |
| [^153] | [Reinforcement Learning Friendly Vision-Language Model for Minecraft](https://arxiv.org/abs/2303.10571) | 本文提出了一种新型强化学习友好的视觉-语言模型CLIP4MC，它在Minecraft环境中通过结合任务完成度和语言描述的相似性，实现了对开放式任务的有效指导。 |
| [^154] | [Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need](https://arxiv.org/abs/2303.07338) | 类增量学习的研究表明，利用预训练模型和简单更新策略可以取得很好的性能，无需对下游任务进行额外训练。 |
| [^155] | [Vision Learners Meet Web Image-Text Pairs](https://arxiv.org/abs/2301.07088) | 新型视觉学习者MUG通过自监督学习，在网络图像文本配对数据上进行预训练，旨在改进视觉表示。 |

# 详细

[^1]: 隐式神经网络（INR）: 具有区分性语义的灵活视频隐式表示框架

    Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics

    [https://arxiv.org/abs/2408.02672](https://arxiv.org/abs/2408.02672)

    本文提出了一种名为Latent-INR的框架，该框架能够生成具有区分性语义的隐式视频表示，同时保持强大的压缩能力，使其适用于各种下游任务。

    

    arXiv:2408.02672v1 公告类型: 新  摘要: 隐式神经网络（INRs）已经成为编码各种数据形式（包括图像、视频、音频和场景）的强大表示。对于视频来说，已经提出了许多用于压缩任务的INRs，而且最近的方法在编码时间、存储和重建质量方面有了显著的改进。然而，这些编码的表示缺乏语义含义，因此无法用于任何需要此类属性的下游任务，比如检索。这可能导致INRs在对标传统编解码器时缺乏竞争力，因为它们除了压缩外没有任何显著的优势。为了解决这个问题，我们提出了一种灵活的框架，该框架解耦了视频INR的空间和时间方面。我们通过一组每帧的隐性词汇以及一组专门的视频专有超网络来实现这一点，这样，给定一个隐性词汇，这些超网络就能预测视频的时序特性，从而使得生成的表示不仅压缩能力强，而且具有语义意义，可以用于各种下游任务。

    arXiv:2408.02672v1 Announce Type: new  Abstract: Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes. With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality. However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval. This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression. To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR. We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict th
    
[^2]: 论文标题：多模态生成预训练照亮灵活的文本到图像的现实主义生成

    Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining

    [https://arxiv.org/abs/2408.02657](https://arxiv.org/abs/2408.02657)

    本研究提出了Lumina-mGPT，一种多模态自回归模型，它通过预训练的解码器-编码器变体在庞大的文本-图像序列上进行训练，能够生成灵活和现实主义的图像，并且通过预训练模型的渐进式强化正则化，在无监督的情形下取得了与监督图像生成模型相当甚至更好的性能。

    

    论文摘要：我们介绍了一种名为Lumina-mGPT的多模态自回归模型系列，它能够在各种视觉和语言任务中表现出色，尤其是在从文本描述生成灵活的现实主义图像方面尤为出色。与现有的自回归图像生成方法不同，Lumina-mGPT采用一个预训练的编码器-解码器变体作为统一框架，用于建模多模态的标记序列。我们的关键洞察是，一个简单的多模态生成预训练（mGPT）解码器-解码器变体，它利用随机序列的概率预测目标，而不是固定的语言模型，这种模型能够学习广泛和一般的跨模态能力，从而为灵活的文本到图像生成提供了光明。基于这些预训练模型，我们提出了一个称为渐进式强化正则化（Flexible Progressive Regularized Boosting，FPRB）的方法，该方法能够从高分辨率的数据集中对单个图像的高分辨率直接进行无监督图像生成。这种方法仅使用监督图像作为输入，该方法在一个实际的数据集中进行了广泛的实验，并证明了该方法在无监督情景下产生的图像的质量和多样性与监督图像生成模型相当，甚至在某些情况下更优。

    arXiv:2408.02657v1 Announce Type: new  Abstract: We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general m
    
[^3]: LaMamba-Diff: 基于局部注意力与Mamba的线性时间高保真扩散模型

    LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba

    [https://arxiv.org/abs/2408.02615](https://arxiv.org/abs/2408.02615)

    该论文提出了一种新的线性时间高保真扩散模型LaMamba-Diff，它结合了自注意力机制和Mamba块来同时处理全局和局部上下文，有效提高了处理长序列输入的效率和模型性能。

    

    最近基于Transformer的扩散模型表现出色，这主要归功于自注意力机制能够准确地捕捉输入单词的全连接交互，从而同时处理全局和局部上下文。然而，它们二次方的复杂性对于处理长序列输入构成了严重的计算挑战。相反，一个名为Mamba的状态空间模型通过将滤波后的全局上下文压缩到一个隐藏状态中来提供线性的复杂度。尽管效率很高，但这种压缩不可避免地会丢失局部词汇之间的重要细节信息，这对于有效的视觉生成建模至关重要。基于这些观察，我们提出了LaMamba块，它结合了自注意力和Mamba的优点，以线性复杂度捕获全局和局部细节。利用高效的无监督架构，我们的模型通过使用局部注意力和Mamba技术，能够在处理长序列输入时保持高保真度同时减少计算成本。

    arXiv:2408.02615v1 Announce Type: new  Abstract: Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exh
    
[^4]: 利用图文描述提取多模态讽刺检测中增强的多层级跨模态语义不匹配表示法，通过注意力机制进行视觉语义建模

    Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection

    [https://arxiv.org/abs/2408.02595](https://arxiv.org/abs/2408.02595)

    本文提出了一种新框架来检测多模态讽刺，该框架利用图文描述来捕捉和检测讽刺，并提高了模型处理图文不匹配的能力。

    

    arXiv:2408.02595v1 公告类型：新  摘要：讽刺是一种讽刺，以其固有的字面解读与意图内涵之间的差异为特征。尽管文本讽刺检测已经受到了广泛的探讨，但在一些情况下，仅凭文本输入可能不足以解读讽刺。为了在社交媒体数据中有效地识别讽刺，必须包含额外的情境线索，如图片。本研究提出了一个用于多模态讽刺检测的全新框架，它能够处理输入三元组。这些三元组中的两个组成元素是输入文本及其关联的图片，如数据集中所提供。此外，还引入了一种补充模态，即描述性的图片描述。引入这种视觉语义表示的动机是为了更准确地捕获文本与视觉内容之间的分歧，这对于讽刺检测任务来说是根本的。该研究的主要挑战在于如何更加敏感地捕捉图文之间的语义不匹配，以及如何通过引人注意机制来增强文本模块的表达能力。通过在Facebook和Instagram上分享的图片和图文帖子中加入描述性图片描述，该系统展示了如何通过多层级语义表示来捕捉讽刺的语义不匹配，并提出了一种有效的方法来检测其中的讽刺。

    arXiv:2408.02595v1 Announce Type: new  Abstract: Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primar
    
[^5]: 这里是翻译过的论文标题

    HQOD: Harmonious Quantization for Object Detection

    [https://arxiv.org/abs/2408.02561](https://arxiv.org/abs/2408.02561)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    arXiv:2408.02561v1 Announce Type: new  Abstract: Task inharmony problem commonly occurs in modern object detectors, leading to inconsistent qualities between classification and regression tasks. The predicted boxes with high classification scores but poor localization positions or low classification scores but accurate localization positions will worsen the performance of detectors after Non-Maximum Suppression. Furthermore, when object detectors collaborate with Quantization-Aware Training (QAT), we observe that the task inharmony problem will be further exacerbated, which is considered one of the main causes of the performance degradation of quantized detectors. To tackle this issue, we propose the Harmonious Quantization for Object Detection (HQOD) framework, which consists of two components. Firstly, we propose a task-correlated loss to encourage detectors to focus on improving samples with lower task harmony quality during QAT. Secondly, a harmonious Intersection over Union (IoU) 
    
[^6]: 自动评估多个队列中的不完全海马反转评分

    Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts

    [https://arxiv.org/abs/2408.02496](https://arxiv.org/abs/2408.02496)

    本文首次提出了自动评级不完全海马反转（IHI）的方法，该方法通过预测四个解剖学标准并将它们相加获得综合评分。尽管如此，研究中的挑战是如何克服模型对不同队列中特征空间的差异的敏感性。为此，作者提出了将不同队列的数据统一后进行训练，并在单独的队列上进行验证和测试的策略。实验结果表明，这种方法在保持较高评分准确性的同时，也加快了评估的速度和可行性，对于大规模研究和临床应用具有重要意义。

    

    arXiv:2408.02496v1 公告类型：交叉  摘要：在大约20%的普通人群中，不完全的海马反转（IHI），有时被称为海马旋转异常，是在T1加权磁共振图像的冠状切片上能够视觉评估的一种解剖模式。IHI与几种脑病（癫痫，精神分裂症）有关。然而，这些研究是基于较小样本进行的。此外，导致IHI产生的因素（遗传或环境因素）仍然知之甚少。因此，需要进行大规模研究，以进一步了解IHI以及它们与神经和精神疾病之间的关系。然而，视觉评估耗时且繁琐，因此需要开发一种自动方法。在本文中，我们提出了首次自动评级IHI的方法。我们通过预测四个解剖学标准来完成这项任务，然后将它们相加以获得一个综合评分。尽管研究的队列很大，但与之前的单队列研究相比，使用跨多个队列的验证数据进行评估的方法准确度可能较低。挑战在于克服模型对不同队列中特征空间的差异的敏感性。为了解决这一问题，我们提出了将不同队列的数据融合在一起进行训练，并在单独的队列上进行验证和测试的策略。我们的实验结果表明，将来自多个队列的数据统一后训练的模型是有希望的，可以在不同队列中保持较高的评分准确性。此外，使用自动评分系统代替手动评分，可以显著提高评估的速度和可行性，这对于大规模研究和临床应用具有重要意义。

    arXiv:2408.02496v1 Announce Type: cross  Abstract: Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal malrotation, is an atypical anatomical pattern of the hippocampus found in about 20% of the general population. IHI can be visually assessed on coronal slices of T1 weighted MR images, using a composite score that combines four anatomical criteria. IHI has been associated with several brain disorders (epilepsy, schizophrenia). However, these studies were based on small samples. Furthermore, the factors (genetic or environmental) that contribute to the genesis of IHI are largely unknown. Large-scale studies are thus needed to further understand IHI and their potential relationships to neurological and psychiatric disorders. However, visual evaluation is long and tedious, justifying the need for an automatic method. In this paper, we propose, for the first time, to automatically rate IHI. We proceed by predicting four anatomical criteria, which are then summed up to for
    
[^7]: HyperSpaceX: 超球面维度径向和角向探索

    HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions

    [https://arxiv.org/abs/2408.02494](https://arxiv.org/abs/2408.02494)

    本文提出了一种名为 HyperSpaceX 的技术，通过探索多径向超球面空间中的角向和径向特征，使用了一种新型的 DistArc 损失函数来提高分类任务的类间区分度。

    

    arXiv:2408.02494v1 公告类型：新 Abstract: 传统的深度学习模型依赖于诸如softmax交叉熵和ArcFace损失这种方法来处理分类和面部识别等任务。这些方法主要在超球形空间中探索角向特征，往往因为多个类别的密集角向数据而导致类内特征相互纠缠。在这篇论文中，提出了一种新的特征探索领域，称为HyperSpaceX，它通过一个新颖的DistArc损失函数增强了类区分度。所提出的DistArc损失函数包含了三个特征布局组件：两个角向和一个径向，强制实施类内绑定和类间分离在多径向安排中，改进了特征的区分度。对HyperSpaceX框架的新型表示进行了评估，采用了提出的预测度量，该度量兼顾了角向和径向元素，提供了更多关于特征分布的详细信息。在面部识别和动物纹理识别任务中进行的系统实验表明，与当前最佳的深度学习模型相比，HyperSpaceX架构能够在保持高分类准确性的同时，展现更强的类间区分能力。在进一步的工作中，研究团队计划扩展 HyperSpaceX 框架以支持更多类型的分类和识别任务，并探索其在其他数据集和现实世界场景中的有效性。

    arXiv:2408.02494v1 Announce Type: new  Abstract: Traditional deep learning models rely on methods such as softmax cross-entropy and ArcFace loss for tasks like classification and face recognition. These methods mainly explore angular features in a hyperspherical space, often resulting in entangled inter-class features due to dense angular data across many classes. In this paper, a new field of feature exploration is proposed known as HyperSpaceX which enhances class discrimination by exploring both angular and radial dimensions in multi-hyperspherical spaces, facilitated by a novel DistArc loss. The proposed DistArc loss encompasses three feature arrangement components: two angular and one radial, enforcing intra-class binding and inter-class separation in multi-radial arrangement, improving feature discriminability. Evaluation of HyperSpaceX framework for the novel representation utilizes a proposed predictive measure that accounts for both angular and radial elements, providing a mor
    
[^8]: 基于深度学习的金属粉末添加剂制造2D X射线图像孔隙缺陷校正

    Attenuation-adjusted deep learning of pore defects in 2D radiographs of additive manufacturing powders

    [https://arxiv.org/abs/2408.02427](https://arxiv.org/abs/2408.02427)

    本文提出的深度学习模型能够在单张2D X射线图像中准确分割孔隙，以提高金属粉末添加剂制造的孔隙度分析效率。与基线模型相比，该方法实现了F1分数的显著提升。

    

    本文旨在通过在高对比度2D X射线图像中识别孔隙的方法来提高金属粉末添加剂制造中的孔隙率分析效率。我们开发了一个结合X射线透过粒子模型和广泛应用于医学影像的UNet变体的深度学习模型，该模型能够在单张2D X射线图像中准确分割孔隙。与基线UNet模型相比，该方法实现了F1分数的显著提升，达到11.4%。我们的工作展示了以下三个关键技术的价值：1) 在合成数据上进行预训练，2) 制作精确的微粒裁剪图像，3) 使用基于距离映射的方法创建理想的微粒模型，灵感来自于Lambert中心投影。这些技术和方法对于实现在线制造过程中实时孔隙度分析具有重要意义。

    arXiv:2408.02427v1 Announce Type: new  Abstract: The presence of gas pores in metal feedstock powder for additive manufacturing greatly affects the final AM product. Since current porosity analysis often involves lengthy X-ray computed tomography (XCT) scans with a full rotation around the sample, motivation exists to explore methods that allow for high throughput -- possibly enabling in-line porosity analysis during manufacturing. Through labelling pore pixels on single 2D radiographs of powders, this work seeks to simulate such future efficient setups. High segmentation accuracy is achieved by combining a model of X-ray attenuation through particles with a variant of the widely applied UNet architecture; notably, F1-score increases by $11.4\%$ compared to the baseline UNet. The proposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2) making tight particle cutouts, and 3) subtracting an ideal particle without pores generated from a distance map inspired by Lamber
    
[^9]: FPT+: 一种针对高分辨率医学图像分类的高效参数和内存转移学习方法

    FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification

    [https://arxiv.org/abs/2408.02426](https://arxiv.org/abs/2408.02426)

    FPT+是一种高效的转移学习方法，特别适用于高分辨率医学图像分类，能够在减少内存消耗的同时，从大型预训练模型中学习知识。

    

    arXiv:2408.02426v1 公告类型：新篇 摘要：大规模预训练模型在下游任务上取得了巨大成功，建立了fine-tuning作为实现显著改进的标准方法。然而，将整个预训练模型的参数集进行fine-tuning代价高昂。近年来，高效参数转移学习（PETL）作为一种成本效益更高的替代方法，用于将预训练模型适应到下游任务上。尽管高效参数转移学习的优势显而易见，但随着模型大小的增加和输入分辨率的提高，PETL面临的挑战也随之增加，因为训练内存消耗并没有像参数使用那样得到有效降低。在此篇论文中，我们介绍了Fine-grained Prompt Tuning plus（FPT+），一种针对高分辨率医学图像分类的高效参数和内存转移学习方法，与其他的PETL方法相比，FPT+在减少内存消耗方面取得了显著进步。FPT+通过训练一个小型的侧网络并利用来自大型预训练模型（LPM）的精细粗粒度提示调优技术，访问预训练知识来实现高效迁移学习。

    arXiv:2408.02426v1 Announce Type: new  Abstract: The success of large-scale pre-trained models has established fine-tuning as a standard method for achieving significant improvements in downstream tasks. However, fine-tuning the entire parameter set of a pre-trained model is costly. Parameter-efficient transfer learning (PETL) has recently emerged as a cost-effective alternative for adapting pre-trained models to downstream tasks. Despite its advantages, the increasing model size and input resolution present challenges for PETL, as the training memory consumption is not reduced as effectively as the parameter usage. In this paper, we introduce Fine-grained Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical image classification, which significantly reduces memory consumption compared to other PETL methods. FPT+ performs transfer learning by training a lightweight side network and accessing pre-trained knowledge from a large pre-trained model (LPM) through fine
    
[^10]: 使用去噪扩散模型进行多天气跨视角地理定位

    Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models

    [https://arxiv.org/abs/2408.02408](https://arxiv.org/abs/2408.02408)

    论文提出 MCGF 多天气跨视角地理定位框架，通过使用去噪扩散模型动态适应未知天气条件，并采用联合优化方法提升跨视角地理定位的准确性。

    

    arXiv:2408.02408v1 公告类型: 新 Abstract: 跨视角地理定位在无GNSS信号的环境中旨在通过匹配无人机拍摄的图像与大量带有地理标签的卫星图像来确定未知位置。最近的研究表明，在特定天气条件下学习区分性图像表示可以显著提高性能。然而，不常见极端天气条件的频繁出现阻碍了进步。本文介绍了一个名为MCGF的多天气跨视角地理定位框架，它旨在使用去噪扩散模型动态适应未知天气条件。MCGF为图像恢复和地理定位建立了联合优化。为了图像恢复，MCGF引入了一个共享编码器和轻量级去噪模块来帮助主干消除与特定天气有关的信息。对于地理定位，MCGF使用EVA-02作为主干来提取特征，并使用交叉熵损失进行定位和天气调整。

    arXiv:2408.02408v1 Announce Type: new  Abstract: Cross-view geo-localization in GNSS-denied environments aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery. Recent research shows that learning discriminative image representations under specific weather conditions can significantly enhance performance. However, the frequent occurrence of unseen extreme weather conditions hinders progress. This paper introduces MCGF, a Multi-weather Cross-view Geo-localization Framework designed to dynamically adapt to unseen weather conditions. MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. For image restoration, MCGF incorporates a shared encoder and a lightweight restoration module to help the backbone eliminate weather-specific information. For geo-localization, MCGF uses EVA-02 as a backbone for feature extraction, with cross-entropy loss for
    
[^11]: 张量模板匹配算法：快速检测旋转物体在三维成像中的应用

    Tensorial template matching for fast cross-correlation with rotations and its application for tomography

    [https://arxiv.org/abs/2408.02398](https://arxiv.org/abs/2408.02398)

    本文通过张量模板匹配算法大幅提升了在三维图像中检测旋转物体的速度和精度。

    

    本文提出了一种新的算法--张量模板匹配，它基于一种数学框架，该框架能够将物体的旋转通过张量场来表示。与传统的模板匹配方法相比，张量模板匹配的计算复杂度不依赖于旋转精度，这对于处理大量三维图像（断层扫描图像）具有重要意义。使用合成数据和从断层扫描中获取的真实数据，我们证明了张量模板匹配算法显著快于传统模板匹配算法，并且可以提高其检测精度。

    arXiv:2408.02398v1 Announce Type: new  Abstract: Object detection is a main task in computer vision. Template matching is the reference method for detecting objects with arbitrary templates. However, template matching computational complexity depends on the rotation accuracy, being a limiting factor for large 3D images (tomograms). Here, we implement a new algorithm called tensorial template matching, based on a mathematical framework that represents all rotations of a template with a tensor field. Contrary to standard template matching, the computational complexity of the presented algorithm is independent of the rotation accuracy. Using both, synthetic and real data from tomography, we demonstrate that tensorial template matching is much faster than template matching and has the potential to improve its accuracy
    
[^12]: CMR-Agent: 一种用于迭代图像到点云注册的跨媒体代理学习

    CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration

    [https://arxiv.org/abs/2408.02394](https://arxiv.org/abs/2408.02394)

    研究提出了一种通过强化学习和模仿学习初始化的CMR-Agent，它可以实现迭代图像到点云注册。该方法通过提出2D-3D混合状态表示来充分利用2D图像和3D点云的特征，旨在提高注册精度和解释能力。

    

    arXiv:2408.02394v1 公告类型: cross  摘要: 图像到点云注册旨在确定RGB图像相对于点云的相对相机姿态。它在预先构建的LiDAR地图中的相机定位中起着重要作用。尽管存在媒体类型差距，但大多数基于学习的 methods 方法在没有任何反馈机制的情况下在特征空间中建立二维到三维点对，没有进行任何迭代的优化，结果导致了低精度和平庸的可解释性。在本论文中，我们提出将注册过程重新格式化为一个迭代马尔可夫决策过程，允许根据每个中间状态进行渐进的相机姿势调整。为此，我们将使用强化学习来发展一种跨媒体注册代理（CMR-Agent），并使用模仿学习来为它的注册策略初始化稳定性，并为训练提供快速启动。 根据交叉媒体的观察，我们提出了一种2D-3D混合状态表示，它充分利用了2D图像和3D点云的特征。

    arXiv:2408.02394v1 Announce Type: cross  Abstract: Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMR-Agent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits 
    
[^13]: 跨伪监督框架用于稀疏标注地理空间图像

    Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial Images

    [https://arxiv.org/abs/2408.02382](https://arxiv.org/abs/2408.02382)

    本研究提出了一种改进的跨伪监督框架，用于在稀疏标注的高分辨率卫星图像上训练精确的土地使用土地覆盖（LULC）分割模型，尤其适用于资源有限的繁忙城市环境。

    

    arXiv:2408.02382v1 新闻类型：新  摘要：土地使用土地覆盖（LULC）映射对于城市和资源规划至关重要，是发展智能和可持续城市的关键组成部分之一。本研究介绍了一种半监督分割模型，用于使用来自印度不同地区的多样性数据分布的高分辨率卫星图像进行LULC预测。我们的方法确保在different areas不同的地区内对建筑物、道路、树木和水体进行robust generalization稳健泛化。我们提出了一种经过修改的跨伪监督框架，用于在稀疏标注数据上训练图像分割模型。该提出的框架解决了半监督学习中流行的“跨伪监督”技术的一些限制。特别是，它解决了在带有sparse和inaccurate标签的noisy卫星图像数据上训练分割模型的挑战。这种全面的approach方法增强了模型在truncated edge area修剪边缘区域内的精准度和utility实用性。双向labeling methods反向标注方法在这些边缘区域内分割表现不佳，引入了significant bias极大的偏差。通过跨伪监督的改进，我们提出了一个先验的，intrinsic faithfulness intrinsic 相符性约束，以enhance 增强模型的CAUSALITY因果性。此外，我们还摒弃了泛化到整个feature space特征空间的概念，而是专注于 fine-grained 细致的segmentation细节分割，这对urban planning城市规划非常有益。Our proposed framework significantly outperforms conventional semisupervised models on various benchmarks. These experiments demonstrate the robustness and effectiveness of our approach, paving the way for cost-effective and efficient mapping strategies in resource-scarce urban environments.

    arXiv:2408.02382v1 Announce Type: new  Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource planning and is one of the key elements in developing smart and sustainable cities. This study introduces a semi-supervised segmentation model for LULC prediction using high-resolution satellite images with a huge diversity in data distributions in different areas from the country of India. Our approach ensures a robust generalization across different types of buildings, roads, trees, and water bodies within these distinct areas. We propose a modified Cross Pseudo Supervision framework to train image segmentation models on sparsely labelled data. The proposed framework addresses the limitations of the popular "Cross Pseudo Supervision" technique for semi-supervised learning. Specifically, it tackles the challenges of training segmentation models on noisy satellite image data with sparse and inaccurate labels. This comprehensive approach enhances the accuracy and utili
    
[^14]: 《CNVSRC 2024 视觉说话人识别系统的 NPU-ASLP 描述》

    The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024

    [https://arxiv.org/abs/2408.02369](https://arxiv.org/abs/2408.02369)

    本文介绍了NPU-ASLP团队在CNVSRC 2024竞赛中对VSR系统的技术细节，包括数据处理、模型架构和实现效果，并获得了良好成绩。

    

    本文详细介绍了 NPU-ASLP（团队237）在本届中连续视觉说话人识别挑战赛（CNVSRC 2024）中提出的视觉说话人识别（VSR）系统，并参与了所有四个赛道，包括单音说话人VSR任务和多音说话人VSR任务的固定和开放赛道。在数据处理方面，我们采用了基线提供的唇动提取器来生成多尺度视频数据。此外，训练期间应用了各种数据增强技术，包括速度扰动、随机旋转、水平翻转和颜色变换。VSR模型采用了端到端的架构，结合了CTC/注意力损失，引入了增强的ResNet3D视觉前端、E-Branchformer编码器和双向Transformer解码器。我们的方法为单音说话人任务和多音说话人任务分别达到了30.47%和34.30%的错误率，在单音说话人任务的开放赛道中获得了第二名。

    arXiv:2408.02369v1 Announce Type: new  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Ta
    
[^15]: StoDIP: 通过深度图像先验和随机迭代高效的3D MRF图像重建

    StoDIP: Efficient 3D MRF image reconstruction with deep image priors and stochastic iterations

    [https://arxiv.org/abs/2408.02367](https://arxiv.org/abs/2408.02367)

    该算法通过Deep Image Prior和随机迭代技术改进了3D MRF图像的准确重建，无需大量ground-truth数据即可进行高效的三维重建，克服了传统3D成像的挑战。

    

    arXiv:2408.02367v1 公告类型：交叉  摘要：磁共振指纹识别（MRF）是一种量化磁共振成像的方法，用于多参数组织映射，具有时间效率。定量映射重建要求去除压缩采样MRF采集中的伪影 artifacts。在已发表的方法中，许多方法仅关注二维（2D）图像重建，尽管它们在临床上的重要性更高，但忽视了向三维（3D）扫描的扩展。原因之一是在没有适当的缓解措施的情况下，向3D成像过渡会带来重大挑战，包括增加的计算成本和要求大量的ground-truth（无伪影）数据用于训练。为了解决这些问题，我们提出了StoDIP，一种新的算法，它将无ground-truth的Deep Image Prior（DIP）重建成用于3D MRF成像。StoDIP采用内存高效的随机更新方法，能够在不需要大量无伪影训练数据的情况下高效重建3D MRF数据。同时，该算法利用深度图像先验的强大能力，并结合随机迭代技术，能够在3D空间中捕获更多细节，有效地分离出不同类型的伪影。With the advancement of deep learning (DL), particularly in computer vision applications, the DIP approach gained attention for its ability to accurately reconstruct images by leveraging the inherent structures across a set of unlabeled images. However, despite its effectiveness in 2D image reconstruction, applying the DIP method to the 3D MRF imaging domain remained challenging due to the significant increase in data size and the computational complexity brought about by the transition to 3D.

    arXiv:2408.02367v1 Announce Type: cross  Abstract: Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to quantitative MRI for multiparametric tissue mapping. The reconstruction of quantitative maps requires tailored algorithms for removing aliasing artefacts from the compressed sampled MRF acquisitions. Within approaches found in the literature, many focus solely on two-dimensional (2D) image reconstruction, neglecting the extension to volumetric (3D) scans despite their higher relevance and clinical value. A reason for this is that transitioning to 3D imaging without appropriate mitigations presents significant challenges, including increased computational cost and storage requirements, and the need for large amount of ground-truth (artefact-free) data for training. To address these issues, we introduce StoDIP, a new algorithm that extends the ground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging. StoDIP employs memory-efficient stochastic updates
    
[^16]: 论文标题翻译成中文：本文提出了一种在长视频语言对准中注入环境描述的新方法

    Infusing Environmental Captions for Long-Form Video Language Grounding

    [https://arxiv.org/abs/2408.02336](https://arxiv.org/abs/2408.02336)

    引入与长视频对齐的丰富环境描述信息，有效排除无关内容，提高了视频语言对准任务的精确度。

    

    论文摘要翻译成中文：本文致力于解决长期视频-语言对准（VLG）问题。给定一个长视频和一个自然语言查询，模型应该能够准确定位到回答查询的时间点。人类可以轻松解决VLG任务，即使视频很长，他们也可以通过丰富的经验知识过滤掉不相关的时刻。与人类不同，现有的VLG方法经常在学习到的小规模数据集表面模式上失败，即使正确的时间点是在无关的帧中。为了克服这个问题，我们提出了一种名为EI-VLG的方法，该方法利用多模态大型语言模型（MLLM）提供的丰富文本信息作为人类经验的替代品，帮助我们有效排除无关帧。本文通过在具有挑战性的EgoNLQ数据集上的大量实验验证了所提出方法的效率。

    arXiv:2408.02336v1 Announce Type: new  Abstract: In this work, we tackle the problem of long-form video-language grounding (VLG). Given a long-form video and a natural language query, a model should temporally localize the precise moment that answers the query. Humans can easily solve VLG tasks, even with arbitrarily long videos, by discarding irrelevant moments using extensive and robust knowledge gained from experience. Unlike humans, existing VLG methods are prone to fall into superficial cues learned from small-scale datasets, even when they are within irrelevant frames. To overcome this challenge, we propose EI-VLG, a VLG method that leverages richer textual information provided by a Multi-modal Large Language Model (MLLM) as a proxy for human experiences, helping to effectively exclude irrelevant frames. We validate the effectiveness of the proposed method via extensive experiments on a challenging EgoNLQ benchmark.
    
[^17]: 通过噪声混合增强的伪造意识预测器用于多脸操纵检测和定位

    Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face Manipulation Detection and Localization

    [https://arxiv.org/abs/2408.02306](https://arxiv.org/abs/2408.02306)

    本研究提出了一种名为MoNFAP的新框架，该框架通过集成伪造意识统一预测和噪声混合模块增强了多脸操纵检测和定位的性能。

    

    随着人脸操纵技术的发展，多脸场景中的伪造图像逐渐成为一个更复杂、更具现实挑战的问题。尽管如此，针对这类多脸操纵的检测和定位方法仍然发展不足。传统的手动定位方法要么间接从定位掩膜中推导出检测结果，从而导致检测性能有限，要么采用一个简单的两分支结构同时获得检测和定位结果，由于两个任务之间没有有效的互动，这不能有效地提高定位能力。本研究提出了一种新框架，即MoNFAP，专门用于多脸操纵的检测和定位。MoNFAP主要引入了两项新的模块：伪造意识统一预测（FUP）模块和噪声混合模块（MNM）。FUP模块将检测和定位任务整合在一起使用。MNM模块通过提取噪声特征来增强预测器的性能。

    arXiv:2408.02306v1 Announce Type: new  Abstract: With the advancement of face manipulation technology, forgery images in multi-face scenarios are gradually becoming a more complex and realistic challenge. Despite this, detection and localization methods for such multi-face manipulations remain underdeveloped. Traditional manipulation localization methods either indirectly derive detection results from localization masks, resulting in limited detection performance, or employ a naive two-branch structure to simultaneously obtain detection and localization results, which cannot effectively benefit the localization capability due to limited interaction between two tasks. This paper proposes a new framework, namely MoNFAP, specifically tailored for multi-face manipulation detection and localization. The MoNFAP primarily introduces two novel modules: the Forgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module (MNM). The FUP integrates detection and localization tasks us
    
[^18]: 论文标题：低成本自我孤立集的网络分裂方法

    Network Fission Ensembles for Low-Cost Self-Ensembles

    [https://arxiv.org/abs/2408.02301](https://arxiv.org/abs/2408.02301)

    论文提出了一种名为Network Fission Ensembles的低成本集成学习与推论方法，通过在单个网络中生成多个输出来实现集成，消除了多重模型带来的成本。

    

    arXiv: 2408.02301v1 公告类型：新  摘要：近年来，用于图像分类的集成学习方法已显示出能够在不增加额外成本的情况下提高分类精度。然而，它们仍然需要在集成推理中使用多个训练模型，当模型大小增加时，这最终将成为一项重大负担。在本文中，我们提出了一个低成本集成学习和推理的方法，称为Network Fission Ensembles（NFE），通过将传统的网络自身转化为多出口结构。从一个给定的初始网络开始，我们首先剪枝一些权重以减少训练负担。然后，我们将剩余的权重分组为若干组，并为每组创建多个辅助路径以构造多出口。我们称这一过程为Network Fission。通过这种方式，可以从单个网络中获得多个输出，这使得集成学习成为可能。由于这个过程仅将现有网络的结构改变为多出口，而不需要使用额外的网络结构，因此实现了高效地创建多个输出。此外，通过使用预测概率的上采样的集成策略，可以进一步提高分类精度。实验表明，该方法在提高分类准确率的同时，没有显著增加训练成本。

    arXiv:2408.02301v1 Announce Type: new  Abstract: Recent ensemble learning methods for image classification have been shown to improve classification accuracy with low extra cost. However, they still require multiple trained models for ensemble inference, which eventually becomes a significant burden when the model size increases. In this paper, we propose a low-cost ensemble learning and inference, called Network Fission Ensembles (NFE), by converting a conventional network itself into a multi-exit structure. Starting from a given initial network, we first prune some of the weights to reduce the training burden. We then group the remaining weights into several sets and create multiple auxiliary paths using each set to construct multi-exits. We call this process Network Fission. Through this, multiple outputs can be obtained from a single network, which enables ensemble learning. Since this process simply changes the existing network structure to multi-exits without using additional net
    
[^19]: 感知至关重要：通过不确定性感知语义分割增强身体人工智能

    Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation

    [https://arxiv.org/abs/2408.02297](https://arxiv.org/abs/2408.02297)

    此研究通过校准感知概率和不确定性改进了身体人工智能的搜索任务，特别是在不同类型的语义感知模型中取得了显著进展。

    

    arXiv:2408.02297v1 宣布类型：新的摘要：身体人工智能在未知环境中执行任务方面取得了显著进展。然而，如物体搜索等任务主要关注有效策略的学习。在这项工作中，我们识别出当前搜索方法中的几个差距：它们主要集中在过时的感知模型上，忽略了时间聚合，并且从地面真实直接转移到测试时的噪声感知，而不考虑到感知状态的过度自信。通过校准的感知概率和不确定性以及聚合和决策过程中的不确定性来解决这些问题，从而对模型进行适应以处理序列任务。所得到的方法可以直接集成到一系列现有的搜索方法中，且没有额外的训练成本。我们在不同类型的语义感知模型和策略上进行了广泛的评估，确认了校准感知概率在聚合方法和不同感知模型和策略中的重要性。

    arXiv:2408.02297v1 Announce Type: new  Abstract: Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through calibrated perception probabilities and uncertainty across aggregation and found decisions, thereby adapting the models for sequential tasks. The resulting methods can be directly integrated with pretrained models across a wide family of existing search approaches at no additional training cost. We perform extensive evaluations of aggregation methods across both different semantic perception models and policies, confirming the importance of calib
    
[^20]: 视频中姿态估计的联合运动互学习方法

    Joint-Motion Mutual Learning for Pose Estimation in Videos

    [https://arxiv.org/abs/2408.02285](https://arxiv.org/abs/2408.02285)

    本文提出了一种新的视频姿态估计学习框架，通过结合原始关节信息和运动特征动态，有效提高了姿态估计的准确性。

    

    视频中的人脸姿态估计一直是在计算机视觉领域中既引人入胜又极具挑战性的任务。然而，由于复杂视频场景中的视频模糊和自我遮挡等原因，该任务仍然非常困难。最近的方法试图整合由骨干网络生成的多帧视觉特征来进行姿态估计。然而，它们常常忽视初始热图中的有用关节信息，热图是骨干生成的副产品。相比之下，试图细化初始热图的方法没有考虑任何空间和时间运动特征。因此，现有方法在姿态估计方面的性能不足，因为它们缺乏利用局部关节（热图）信息和全局运动（特征）动态的能力。为了解决这个问题，我们提出了一种新的用于姿态估计的联合运动互学习框架，该框架有效地集中在同时利用原始关节信息和运动特征动态上，从而显著提高了视频中姿态估计的准确性。

    arXiv:2408.02285v1 Announce Type: new  Abstract: Human pose estimation in videos has long been a compelling yet challenging task within the realm of computer vision. Nevertheless, this task remains difficult because of the complex video scenes, such as video defocus and self-occlusion. Recent methods strive to integrate multi-frame visual features generated by a backbone network for pose estimation. However, they often ignore the useful joint information encoded in the initial heatmap, which is a by-product of the backbone generation. Comparatively, methods that attempt to refine the initial heatmap fail to consider any spatio-temporal motion features. As a result, the performance of existing methods for pose estimation falls short due to the lack of ability to leverage both local joint (heatmap) information and global motion (feature) dynamics.   To address this problem, we propose a novel joint-motion mutual learning framework for pose estimation, which effectively concentrates on bo
    
[^21]: 自适应不确定性升采样视频去噪方法

    Cascading Refinement Video Denoising with Uncertainty Adaptivity

    [https://arxiv.org/abs/2408.02284](https://arxiv.org/abs/2408.02284)

    本文提出的级联精化视频去噪方法通过自适应不确定性升采样技术，成功实现了在CRVD数据集上的大幅度性能提升。通过创建不确定度图，该方法平均减少了25%的计算量。

    

    在arXiv:2408.02284v1中宣布，这是一个新的摘要。准确的位置匹配是视频去噪的关键。然而，在噪声环境中估计位置匹配是非常具有挑战性的。本文介绍了一种级联精化视频去噪方法，该方法可以在精化位置匹配的同时恢复图像。更好的图像质量可以恢复更多的细节信息。此外，更好的图像质量将带来更高的精确度匹配。在这个数据集上，这种方法显著超越了状态的最优性能。此外，为了处理不同级别的噪声，在每次迭代后创建了一个不确定度图。因此，对于那些容易恢复的视频，无需进行多余的计算。通过这种方式，平均减少了25%的总计算量。

    arXiv:2408.02284v1 Announce Type: new  Abstract: Accurate alignment is crucial for video denoising. However, estimating alignment in noisy environments is challenging. This paper introduces a cascading refinement video denoising method that can refine alignment and restore images simultaneously. Better alignment enables restoration of more detailed information in each frame. Furthermore, better image quality leads to better alignment. This method has achieved SOTA performance by a large margin on the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an uncertainty map was created after each iteration. Because of this, redundant computation on the easily restored videos was avoided. By applying this method, the entire computation was reduced by 25% on average.
    
[^22]: 几何代数与大型语言模型的结合：3D交互型可控场景中分离网格的指令驱动转换

    Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes

    [https://arxiv.org/abs/2408.02275](https://arxiv.org/abs/2408.02275)

    本系统利用几何代数与大型语言模型结合，实现了3D场景中对象精确重新定位，无需专业训练数据，仅通过自然语言指令即可操作。

    

    这篇论文介绍了一种新型的大语言模型(LLMs)与康式几何代数(CGA)的集成，它旨在革新可控3D场景编辑，特别是对象重新定位任务，这些任务通常需要复杂的手动流程和专业技能。这些传统的技艺通常依赖于大量的训练数据集或缺乏对精确编辑的正式语言描述。我们的系统“shenlong”使用CGA作为强大的正式语言，精确地建模了进行精确对象重新定位所需的空间变换。借助预训练LLMs的无监督学习能力，“shenlong”可以将自然语言指令翻译成CGA操作，并将这些操作应用于场景中，从而在三维场景中实现精确的空间变换，而无需专门的数据预训练。在现实的模拟环境中实施后，“shenlong”确保了与现有系统的兼容性。

    arXiv:2408.02275v1 Announce Type: new  Abstract: This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, shenlong ensures compatibility with existing
    
[^23]: COM厨房：作为视觉语言基准的未经编辑的上方视角视频数据集

    COM Kitchens: An Unedited Overhead-view Video Dataset as a Vision-Language Benchmark

    [https://arxiv.org/abs/2408.02272](https://arxiv.org/abs/2408.02272)

    本研究提出了一项名为COM厨房的数据集，它是由智能手机拍摄的未经编辑的上方视角烹饪视频组成，旨在帮助深度学习模型在视觉和语言社区中对程序视频进行理解和分析。

    

    arXiv:2408.02272v1 公告类型：新  摘要：在视觉和语言社区中，程序视频理解正得到关注。基于深度学习的视频分析需要大量的数据。因此，现有的工作通常使用网上的视频作为训练资源，这在从原始视频观察中查询指导内容方面造成了挑战。为了解决这个问题，我们提出了一个新的数据集，名为COM厨房。该数据集由智能手机拍摄的未经编辑的上方视角视频组成，参与者根据给出的食谱进行食品准备。由于高额的摄像机设置成本，固定视角的视频数据集通常缺乏环境多样性。我们利用现代的宽视角智能手机镜头覆盖从水槽到炉灶的上方视角烹饪台面，无需人工帮助即可捕捉活动。通过这种设置，我们向参与者分发了智能手机，收集了一个多样化的数据集。凭借这个数据集，我们提出了新型视频到文本检索任务“即时重新体现”和“了解文本的视觉反射”为一体的挑战。

    arXiv:2408.02272v1 Announce Type: new  Abstract: Procedural video understanding is gaining attention in the vision and language community. Deep learning-based video analysis requires extensive data. Consequently, existing works often use web videos as training resources, making it challenging to query instructional contents from raw video observations. To address this issue, we propose a new dataset, COM Kitchens. The dataset consists of unedited overhead-view videos captured by smartphones, in which participants performed food preparation based on given recipes. Fixed-viewpoint video datasets often lack environmental diversity due to high camera setup costs. We used modern wide-angle smartphone lenses to cover cooking counters from sink to cooktop in an overhead view, capturing activity without in-person assistance. With this setup, we collected a diverse dataset by distributing smartphones to participants. With this dataset, we propose the novel video-to-text retrieval task Online Re
    
[^24]: VoxelTrack：探索用于3D点云对象跟踪的体元表示

    VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking

    [https://arxiv.org/abs/2408.02263](https://arxiv.org/abs/2408.02263)

    VoxelTrack是一个新型3D对象跟踪框架，通过体元化和稀疏卷积块有效建模了精确的3D空间信息，实现了对跟踪对象位置预测的准确性。

    

    arXiv:2408.02263v1 公告类型：新  翻译摘要：当前基于激光雷达的3D单对象跟踪（SOT）方法通常依赖于基于点的表示网络。尽管这些网络已经展示出了成功，但它们确实存在一些根本问题：1）它包含池化操作以应对固有的无序点云，这阻碍了捕获对于跟踪，即预测任务有用的3D空间信息。2）采用的集合抽象操作很难处理密度不一致的点云，这也阻止了对3D空间信息的建模。为了解决这些问题，我们引入了一种新的跟踪框架，称为VoxelTrack。通过将固有无序的点云体元化并使用稀疏卷积块提取其特征，VoxelTrack有效地对3D空间信息进行了建模，从而引导了对跟踪对象精确位置的预测。此外，VoxelTrack结合了一个双流编码器，该编码器具有跨层特征融合的能力，并且采用了高维长的最短路径优化算法来进一步增强端到端训练的效率。通过实验验证，VoxelTrack在几种流行的3D SOT基准测试数据集上超过了现有的点云跟踪方法。

    arXiv:2408.02263v1 Announce Type: new  Abstract: Current LiDAR point cloud-based 3D single object tracking (SOT) methods typically rely on point-based representation network. Despite demonstrated success, such networks suffer from some fundamental problems: 1) It contains pooling operation to cope with inherently disordered point clouds, hindering the capture of 3D spatial information that is useful for tracking, a regression task. 2) The adopted set abstraction operation hardly handles density-inconsistent point clouds, also preventing 3D spatial information from being modeled. To solve these problems, we introduce a novel tracking framework, termed VoxelTrack. By voxelizing inherently disordered point clouds into 3D voxels and extracting their features via sparse convolution blocks, VoxelTrack effectively models precise and robust 3D spatial information, thereby guiding accurate position prediction for tracked objects. Moreover, VoxelTrack incorporates a dual-stream encoder with cros
    
[^25]: 基于课程学习的多模态对比掩码自编码器预训练方法

    Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders

    [https://arxiv.org/abs/2408.02245](https://arxiv.org/abs/2408.02245)

    本文提出了一种结合课程学习、多模态对比掩码自编码器和去噪技术的新预训练方法，专门用于改进图像理解的性能，特别是在RGB-D数据集上。

    

    arXiv:2408.02245v1 公告类型: 新  翻译摘要: 本文提出了一种在课程学习(CL)范式下用于图像理解任务的新的预训练方法，该方法利用了RGB-D技术。该方法利用了多模态对比掩码自编码器以及去噪技术。最近的方法要么使用掩码自编码（例如，MultiMAE），要么使用对比学习（例如，Pri3D），或者在单一的对比掩码自编码器架构中结合这两种方法，例如CMAE和CAV-MAE。然而，单一的对比掩码自编码器都不能适用于RGB-D数据集。为了提高这些方法的表现和效率，我们提出了基于CL的新预训练策略。具体来说，在第一阶段，我们使用对比学习方法来学习跨模态表示。在第二阶段，我们使用第一阶段获得的权重初始化模态特定的编码器，然后使用掩码自编码和去噪/噪声预测继续预训练模型。在第三阶段，我们利用监督学习进一步微调模型，并结合主动学习策略来选择最有信息量的样本。最后，我们在RGB-D数据集上进行了一系列实验以验证所提出方法的稳健性和有效性。

    arXiv:2408.02245v1 Announce Type: new  Abstract: In this paper, we propose a new pre-training method for image understanding tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Recent approaches either use masked autoencoding (e.g., MultiMAE) or contrastive learning(e.g., Pri3D, or combine them in a single contrastive masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the single contrastive masked autoencoder is applicable to RGB-D datasets. To improve the performance and efficacy of such methods, we propose a new pre-training strategy based on CL. Specifically, in the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we initialize the modality-specific encoders using the weights from the first stage and then pre-train the model using masked autoencoding and denoising/noise prediction us
    
[^26]: 创生而非复制！用于创意生成的推动能扩散法

    ProCreate, Don\'t Reproduce! Propulsive Energy Diffusion for Creative Generation

    [https://arxiv.org/abs/2408.02226](https://arxiv.org/abs/2408.02226)

    本研究提出 ProCreate 策略，旨在提高差异化图像生成模型的样本多样性和创造力，并防止训练数据的重复。它通过在生成过程中推动生成的图像远离参考图像，以实现这一目标。该策略在处理八个不同类别少样本创意生成数据集时表现出色，并且在大规模训练文本提示评估中也显示出了防止再现训练数据的能力。

    

    本研究提出了一种名为 ProCreate 的简单且易于实施的策略，旨在通过差异化基础图像生成模型的样本多样性来提高其创造力，并防止训练数据的重现。该策略在对一组参考图像进行操作时，主动推动生成的图像嵌入远离参考嵌入，并在生成过程中进行操作。我们还提出了“少样本创意生成8（Few-Shot Creative Generation 8）”，这是一个在八个不同类别（涵盖不同概念、风格和设置）上的少样本创意生成数据集，ProCreate在这个数据集上取得了样本多样性和保真度最高的成就。此外，我们展示了 ProCreate 在大规模训练文本提示评估中防止再现训练数据的有效性。代码和相关数据集都可在 https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public 上免费获取。更多详情可访问 https://procreate-diffusion.github.io。

    arXiv:2408.02226v1 Announce Type: new  Abstract: In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts. Code and FSCG-8 are available at https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The project page is available at https://procreate-diffusion.github.io.
    
[^27]: 不仅仅是阳性与阴性：在医学诊断中传达精细粒度的信息

    More Than Positive and Negative: Communicating Fine Granularity in Medical Diagnosis

    [https://arxiv.org/abs/2408.02214](https://arxiv.org/abs/2408.02214)

    研究提出了一种新的医学图像基准，以精细粒度学习为目标，证明了AI模型能够超越现有知识界限，捕捉并传递更详细的疾病状态信息，为医学图像的AI诊断提供了新的视角。

    

    arXiv:2408.02214v1 公告类型：新  摘要：随着深度学习的发展，在自动胸部X光片（CXR）分析的强大人工智能（AI）系统建设方面已经取得了显著的进展。现有的大部分AI模型都是在被训练成二分类器，旨在区分阳性与阴性案例。然而，简单二元设置与复杂的现实世界医学场景之间存在相当大的差距。在本工作中，我们对自动放射学诊断问题进行了重新评估。我们首先观察到阳性类别的案例之间存在显著差异，这意味着简单地将它们分类为阳性会丧失许多重要的细节。这激励我们构建能够从医疗图像中传达与人类专家相似的精细粒度知识的AI模型。为此，我们首先提出一个基于医学知识的新的精细粒度学习医学图像的基准。特别是，我们设计了一个基于医学知识的分割规则，将阳性案例分为更细粒度的子类别。通过在大型CXR数据集上训练和评估一系列的深度学习模型，我们验证了优秀模型能够跨越现有知识边界，捕获并传递关于疾病状态的更多细节。我们的研究提供了对这一重要但尚未充分探索的问题的见解，并为医学图像的AI诊断提供了新的视角。

    arXiv:2408.02214v1 Announce Type: new  Abstract: With the advance of deep learning, much progress has been made in building powerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR) analysis. Most existing AI models are trained to be a binary classifier with the aim of distinguishing positive and negative cases. However, a large gap exists between the simple binary setting and complicated real-world medical scenarios. In this work, we reinvestigate the problem of automatic radiology diagnosis. We first observe that there is considerable diversity among cases within the positive class, which means simply classifying them as positive loses many important details. This motivates us to build AI models that can communicate fine-grained knowledge from medical images like human experts. To this end, we first propose a new benchmark on fine granularity learning from medical images. Specifically, we devise a division rule based on medical knowledge to divide positive cases i
    
[^28]: ExoViP: 逐步验证与探索与外骨骼模块结合的构成性视觉推理

    ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning

    [https://arxiv.org/abs/2408.02210](https://arxiv.org/abs/2408.02210)

    ExoViP是一种新方法，通过验证模块的辅助，能够改善视觉语言编程方案中规划与执行错误的问题，确保任务的正确执行和决策过程的设计。

    

    在这里，arXiv:2408.02210v1 宣布类型为新。摘要：构成性视觉推理方法，能够将复杂的查询转换为可行的视觉任务的结构化组合，已经在复杂的跨模态任务上显示出强大的潜力。在大型语言模型（LLM）的最新进展的推动下，这种多模态挑战已经通过将LLM视为少样本/零样本规划者，即视觉-语言（VL）编程，达到一个新的阶段。尽管这些方法拥有许多优点，但它们也面临挑战，因为LLM规划错误或视觉执行模块的不准确，这使它们落后于非构成性模型。在本工作中，我们提出了“即插即用”的ExoViP方法，通过自我反省的验证来纠正规划和管理阶段中的错误。我们利用验证模块作为“外骨骼”来增强当前的VL编程方案。具体来说，我们提出的验证模块使用了三种子验证器来验证预测的准确性，并确保推理过程的可靠性。我们的方法通过引入外骨骼模块，不仅从错误中恢复，而且在外骨骼模块错误的情况下重新设计决策过程，从而显著提高了VL编程方案的可靠性和准确性。实验结果表明，在多个标准数据集上，ExoViP方法在正确执行指定的视觉任务和处理规划错误方面优于现有的方法。此外，我们演示了ExoViP不仅可以应用于视觉推理，还可以推广到其他多模态任务，从而为视觉语言编程提供了强大的赋能工具。

    arXiv:2408.02210v1 Announce Type: new  Abstract: Compositional visual reasoning methods, which translate a complex query into a structured composition of feasible visual tasks, have exhibited a strong potential in complicated multi-modal tasks. Empowered by recent advances in large language models (LLMs), this multi-modal challenge has been brought to a new stage by treating LLMs as few-shot/zero-shot planners, i.e., vision-language (VL) programming. Such methods, despite their numerous merits, suffer from challenges due to LLM planning mistakes or inaccuracy of visual execution modules, lagging behind the non-compositional models. In this work, we devise a "plug-and-play" method, ExoViP, to correct errors in both the planning and execution stages through introspective verification. We employ verification modules as "exoskeletons" to enhance current VL programming schemes. Specifically, our proposed verification module utilizes a mixture of three sub-verifiers to validate predictions a
    
[^29]: PanoFree: 无需调参的全局多视图图像生成方法，使用跨视角自我指导

    PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance

    [https://arxiv.org/abs/2408.02157](https://arxiv.org/abs/2408.02157)

    PanoFree是一种无需调参的全景图像生成方法，通过迭代变形和上色技术，无需微调即可解决多视角图形生成中的累积误差问题。

    

    arXiv:2408.02157v1 新闻类型：新 摘要：沉浸式的场景生成，特别是在创建全景图方面，从为多视图图像生成的大型预训练的文本到图像（T2I）模型受益匪浅。由于获取多视图图像的成本高昂，因此倾向于无调整的生成。然而，现有的方法要么限于简单的对应关系，要么需要进行大量的微调以捕捉复杂的对应关系。我们提出了一种名为PanoFree的无需调参的全局多视图图像生成方法，该方法支持广泛的不同对应关系的生成。PanoFree使用迭代变形和上色技术，不依赖于微调，解决了由于累积误差导致的不一致性和 artifacts 的问题。通过增强跨视角感知和通过跨视角指导、风险区域估计和擦除以及对称双向引导生成的进步，它改进了累积误差的状况。它还通过在进化过程中修正变形和上色过程来细化这些过程，同时最小化了那些在生成初期出现的不匹配。通过将多视角感知纳入迭代过程，PanoFree无需求助于任何形式的微调或预训练权重，就可以生成稳定而一致的、与多视角数据兼容的全景图像。

    arXiv:2408.02157v1 Announce Type: new  Abstract: Immersive scene generation, notably panorama creation, benefits significantly from the adaptation of large pre-trained text-to-image (T2I) models for multi-view image generation. Due to the high cost of acquiring multi-view images, tuning-free generation is preferred. However, existing methods are either limited to simple correspondences or require extensive fine-tuning to capture complex ones. We present PanoFree, a novel method for tuning-free multi-view image generation that supports an extensive array of correspondences. PanoFree sequentially generates multi-view images using iterative warping and inpainting, addressing the key issues of inconsistency and artifacts from error accumulation without the need for fine-tuning. It improves error accumulation by enhancing cross-view awareness and refines the warping and inpainting processes via cross-view guidance, risky area estimation and erasing, and symmetric bidirectional guided genera
    
[^30]: VidModEx: 一种可解释且高效的黑色盒子模型提取方法，适用于高维空间

    VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces

    [https://arxiv.org/abs/2408.02140](https://arxiv.org/abs/2408.02140)

    VidModEx方法使用SHAP增强了合成数据生成，提高了图像和视频分类模型的性能，在多个高维数据集上取得了显著的提升。

    

    arXiv:2408.02140v1 宣布类型：新  摘要：在黑箱模型提取领域，依赖于软标签或替代数据集的传统方法在扩展到高维输入空间和处理大量相关联的类别的复杂性方面遇到了挑战。在这项工作中，我们介绍了一种新的方法，该方法利用SHAP（SHapley Additive exPlanations）来增强合成数据生成。SHAP量化了每个输入特征对受害者模型的输出所做的个体贡献，这有助于优化一个基于能量的生成对抗网络以达到一个理想的输出。这种方法显著提升了性能，在图像分类模型上的准确度提升了16.45%，并且将这种方法扩展到了视频分类模型，在UCF11、UCF101、Kinetics 400、Kinetics 600和Something-Something V2等难以挑战的数据集上，平均提升了26.11%，最高提升了33.36%。我们进一步展示了这种方法的有效性和其实际实用性。

    arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of
    
[^31]: RICA^2：基于评分大纲的行动质量评估

    RICA^2: Rubric-Informed, Calibrated Assessment of Actions

    [https://arxiv.org/abs/2408.02138](https://arxiv.org/abs/2408.02138)

    RICA^2模型是一种深层概率模型，它通过整合评分大纲并量化预测不确定性，在行动质量评估方面取得了新纪录，特别是在FineDiving、MTL-AQA和JIGSAWS基准测试中表现优秀。

    

    本文研究了在视觉社区中吸引近期兴趣的行动质量评估(AQA)问题。不幸的是，以前的方法往往忽视了人类专家使用的评分大纲，并且未能量化模型的预测不确定性。为了弥合这一差距，我们提出了一种深层概率模型RICA^2，它整合了评分大纲，并对其预测不确定性进行了量化。我们的方法的核心在于动作步骤的随机嵌入，这些嵌入定义在一个编码评分大纲的图中。这些嵌入在潜在空间中分散概率密度，并且允许我们的方法表示模型不确定性。图编码了评分准则，基于这些准则，可以解码出质量分数。我们展示了我们的方法在包括FineDiving、MTL-AQA和JIGSAWS在内的公共基准测试中取得了新的记录，并且在预测性能上超过了现有技术。

    arXiv:2408.02138v1 Announce Type: new  Abstract: The ability to quantify how well an action is carried out, also known as action quality assessment (AQA), has attracted recent interest in the vision community. Unfortunately, prior methods often ignore the score rubric used by human experts and fall short of quantifying the uncertainty of the model prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model that integrates score rubric and accounts for prediction uncertainty for AQA. Central to our method lies in stochastic embeddings of action steps, defined on a graph structure that encodes the score rubric. The embeddings spread probabilistic density in the latent space and allow our method to represent model uncertainty. The graph encodes the scoring criteria, based on which the quality scores can be decoded. We demonstrate that our method establishes new state of the art on public benchmarks, including FineDiving, MTL-AQA, and JIGSAWS, with superior performance in
    
[^32]: 关于切比雪夫-索布列夫级数对数字墨的初步研究

    A First Look at Chebyshev-Sobolev Series for Digital Ink

    [https://arxiv.org/abs/2408.02135](https://arxiv.org/abs/2408.02135)

    本文探讨了使用切比雪夫-索布列夫级数对数字墨的初步研究，发现了这种表示方法可能在某些方面优于拉格朗日-索布列夫级数对数字墨表示。

    

    arXiv:2408.02135v1 公告类型：新  摘要：将数字墨视为平面曲线为各种应用提供了有价值的框架，包括签名验证、笔记编写和数学手写识别。这些平面曲线可以通过采样点确定的简化级数的参数化对 (x(s), y(s)) 获得。以前的工作已经发现，在这些简化级数（多项式）中采用拉格朗日或拉格朗日-索布列夫基具有许多理想的属性。这些属性包括数据表示的紧凑性、同种符号在多项式系数矢量空间中的有意义聚类、该空间中类间的线性可分离性，以及该空间中曲线之间变异性的高效计算。在这项工作中，我们迈出了探索切比雪夫-索布列夫级数在符号识别中应用的第一步。早期的迹象表明，这种表示对于某些目的来说可能优于拉格朗日-索布列夫表示。

    arXiv:2408.02135v1 Announce Type: new  Abstract: Considering digital ink as plane curves provides a valuable framework for various applications, including signature verification, note-taking, and mathematical handwriting recognition. These plane curves can be obtained as parameterized pairs of approximating truncated series (x(s), y(s)) determined by sampled points. Earlier work has found that representing these truncated series (polynomials) in a Legendre or Legendre-Sobolev basis has a number of desirable properties. These include compact data representation, meaningful clustering of like symbols in the vector space of polynomial coefficients, linear separability of classes in this space, and highly efficient calculation of variation between curves. In this work, we take a first step at examining the use of Chebyshev-Sobolev series for symbol recognition. The early indication is that this representation may be superior to Legendre-Sobolev representation for some purposes.
    
[^33]: 在这里是翻译过的论文标题

    View-consistent Object Removal in Radiance Fields

    [https://arxiv.org/abs/2408.02100](https://arxiv.org/abs/2408.02100)

    在这里是中文总结出的一句话要点

    

    在这里是翻译过的论文摘要

    arXiv:2408.02100v1 Announce Type: new  Abstract: Radiance Fields (RFs) have emerged as a crucial technology for 3D scene representation, enabling the synthesis of novel views with remarkable realism. However, as RFs become more widely used, the need for effective editing techniques that maintain coherence across different perspectives becomes evident. Current methods primarily depend on per-frame 2D image inpainting, which often fails to maintain consistency across views, thus compromising the realism of edited RF scenes. In this work, we introduce a novel RF editing pipeline that significantly enhances consistency by requiring the inpainting of only a single reference image. This image is then projected across multiple views using a depth-based approach, effectively reducing the inconsistencies observed with per-frame inpainting. However, projections typically assume photometric consistency across views, which is often impractical in real-world settings. To accommodate realistic varia
    
[^34]: 标题：释放数据巨浪的力量：用于语言模型指令训练的数据评价与选择综合调查

    Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models

    [https://arxiv.org/abs/2408.02085](https://arxiv.org/abs/2408.02085)

    本研究综述了评估与选择用于语言模型指令训练的数据方法的现有文献，揭示了不同评估方法的实际应用及未来研究的可能性，旨在为最优的数据驱动训练提供有价值的见解和策略。

    

    摘要：arXiv:2408.02085v1 公告类型：新文摘要：指令训练在使大型语言模型（LLMs）与人类喜好保持一致方面扮演着关键角色。尽管存在大量的开放式指令数据集，但盲目地在所有现有指令上训练一个LLM可能并不理想且不实用。为了确定最有利的训练数据点，自然语言处理（NLP）和深度学习领域已经提出了数据评估和选择的方法。然而，在指令训练的背景下，仍然存在一个知识差距，即哪些数据评估指标可以应用，以及它们是如何融入选择机制的。为了填补这一空白，我们提出了对用于指令训练的LLMs的数据评估和选择现有文献的全面回顾。我们系统地对所有适用的方法进行了分类，并将其分为基于质量的、基于多样性的和基于重要性的三类，其中细化了精细粒度的分类体系。对于每种的评估方法和它们的实际应用，我们都进行了详细的分析和对比。此外，我们还发现了未来研究可能的空白领域，提出了未来研究的方向。通过对现有方法和新兴技术的综合评估，我们相信可以为指导语言模型的最优数据驱动训练提供有价值的见解和策略。

    arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
    
[^35]: 使用多视图图像特征先验改进神经表面重建

    Improving Neural Surface Reconstruction with Feature Priors from Multi-View Image

    [https://arxiv.org/abs/2408.02079](https://arxiv.org/abs/2408.02079)

    本研究旨在通过探究七种预假视觉任务的多视图特征先验来改进神经表面重建，并利用迭代训练显著提升了重建质量。

    

    标题: arXiv:2408.02079v1 公告类型: 新的摘要: 最近在神经表面重建(NSR)方面的进展已显着提高了与体积渲染相结合的多视图重建。然而，仅在图像空间中依赖光度一致性并不能解决现实世界数据中提出的复杂问题，包括遮挡和非洛维埃表面。为了解决这些挑战，我们提出了一项对特征级一致性损失的调查，目的是利用多种预假视觉任务中的有价值特征先验，并克服目前的限制。需要指出的是，目前存在着确定最有效预假视觉任务以增强NSR所需方法的差距。在本研究中，我们全面探索了七项预假视觉任务的多视图特征先验，包括十三种方法。我们的主要目标是在广泛的可能性下强化NSR训练。此外，我们还研究了特征分辨率的变化影响，并评估了所有项目的性能和测量的数量级。结果表明，迭代训练能够显著提升重建质量，同时方法二的预训练模型提供了一个性能基准，对于理解NSR的最佳训练策略具有重要意义。

    arXiv:2408.02079v1 Announce Type: new  Abstract: Recent advancements in Neural Surface Reconstruction (NSR) have significantly improved multi-view reconstruction when coupled with volume rendering. However, relying solely on photometric consistency in image space falls short of addressing complexities posed by real-world data, including occlusions and non-Lambertian surfaces. To tackle these challenges, we propose an investigation into feature-level consistent loss, aiming to harness valuable feature priors from diverse pretext visual tasks and overcome current limitations. It is crucial to note the existing gap in determining the most effective pretext visual task for enhancing NSR. In this study, we comprehensively explore multi-view feature priors from seven pretext visual tasks, comprising thirteen methods. Our main goal is to strengthen NSR training by considering a wide range of possibilities. Additionally, we examine the impact of varying feature resolutions and evaluate both pi
    
[^36]: LDFaceNet: 基于潜扩散网络的高保真度Deepfake生成方法

    LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake Generation

    [https://arxiv.org/abs/2408.02078](https://arxiv.org/abs/2408.02078)

    LDFaceNet是使用指导潜扩散模型进行面部交换的视频生成方法，能够生成高质量Deepfake视频。

    

    在过去的十年中，在合成媒体生成领域取得了巨大的进步。这主要是由于基于生成对抗网络（GANs）的强大方法的推动。最近，受到非平衡热力学启发的扩散概率模型吸引了人们的注意。在图像生成领域，扩散模型（DMs）通过它们的随机抽样过程展现了在生成逼真和非同质图像方面的杰出能力。本文提出了一种名为LDFaceNet（潜扩散基于面部交换网络）的新型面部交换模块，它基于一个指导潜扩散模型，该模型采用了面部分割和面部识别模块进行有条件的去噪过程。模型采用了独特的损失函数来为扩散过程提供方向性指导。值得注意的是，LDFaceNet可以结合补充面部指导信息来实现高质量的Deepfake视频生成。模型采用了独特的损失函数来为扩散过程提供方向性指导。值得注意的是，LDFaceNet可以结合补充面部指导信息来实现高质量的Deepfake视频生成。

    arXiv:2408.02078v1 Announce Type: new  Abstract: Over the past decade, there has been tremendous progress in the domain of synthetic media generation. This is mainly due to the powerful methods based on generative adversarial networks (GANs). Very recently, diffusion probabilistic models, which are inspired by non-equilibrium thermodynamics, have taken the spotlight. In the realm of image generation, diffusion models (DMs) have exhibited remarkable proficiency in producing both realistic and heterogeneous imagery through their stochastic sampling procedure. This paper proposes a novel facial swapping module, termed as LDFaceNet (Latent Diffusion based Face Swapping Network), which is based on a guided latent diffusion model that utilizes facial segmentation and facial recognition modules for a conditioned denoising process. The model employs a unique loss function to offer directional guidance to the diffusion process. Notably, LDFaceNet can incorporate supplementary facial guidance fo
    
[^37]: 基于案例推理的儿童发展迟缓诊断筛查方法

    Case-based reasoning approach for diagnostic screening of children with developmental delays

    [https://arxiv.org/abs/2408.02073](https://arxiv.org/abs/2408.02073)

    本文提出了一种基于案例推理的儿童发展迟缓诊断筛查方法，旨在早发现、早干预，减少医疗和社会成本，并提高治疗效果。

    

    根据世界卫生组织的数据，全球约有6%至9%的人口患有发展迟缓。基于中国安徽省淮北市2023年新生儿的数量（94,420），我们估计每年有大约7,500例（疑似发展迟缓病例）的疑虑病例。对这些儿童进行早期识别并进行适当的早期干预，可以显著减少医疗资源的浪费和社会成本。国际研究指出，发展迟缓儿童的最佳干预期是六岁前，最好是在三岁半之前。研究表明，接受早期干预的发展迟缓儿童症状明显改善，甚至有些儿童可能完全康复。本研究采用了一种结合CNN-Trans的混合模型进行案例推理。

    arXiv:2408.02073v1 Announce Type: new  Abstract: According to the World Health Organization, the population of children with developmental delays constitutes approximately 6% to 9% of the total population. Based on the number of newborns in Huaibei, Anhui Province, China, in 2023 (94,420), it is estimated that there are about 7,500 cases (suspected cases of developmental delays) of suspicious cases annually. Early identification and appropriate early intervention for these children can significantly reduce the wastage of medical resources and societal costs. International research indicates that the optimal period for intervention in children with developmental delays is before the age of six, with the golden treatment period being before three and a half years of age. Studies have shown that children with developmental delays who receive early intervention exhibit significant improvement in symptoms; some may even fully recover. This research adopts a hybrid model combining a CNN-Tran
    
[^38]: 步速器：预测扩散模型图像生成所需的最小去噪步骤

    Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image Generation

    [https://arxiv.org/abs/2408.02054](https://arxiv.org/abs/2408.02054)

    该研究提出了一个经过调优的NLP模型，它能够实时推荐用于生成高质量图像的最优去噪步骤数，显著节约了计算资源。

    

    在这项研究中，我们介绍了一种经过精心调优的NLP模型，它能够确定任何给定文本提示所需的最小去噪步骤数。这个高级模型是一个实时工具，它为生成高质量图像推荐理想的去噪步骤，它与扩散模型无缝配合，确保在最短的时间内生产出质量更高的图像。我们的解释重点放在了DDIM调度器上，但这种方法是可以适应和应用于各种其他调度器，比如欧拉、欧拉祖先、椒盐法、DPM2 Karras、UniPC等等。通过这个模型，我们的客户能够节约计算资源，通过执行产生最优图像质量所需的最少去噪步骤。

    arXiv:2408.02054v1 Announce Type: new  Abstract: In this paper, we introduce an innovative NLP model specifically fine-tuned to determine the minimal number of denoising steps required for any given text prompt. This advanced model serves as a real-time tool that recommends the ideal denoise steps for generating high-quality images efficiently. It is designed to work seamlessly with the Diffusion model, ensuring that images are produced with superior quality in the shortest possible time. Although our explanation focuses on the DDIM scheduler, the methodology is adaptable and can be applied to various other schedulers like Euler, Euler Ancestral, Heun, DPM2 Karras, UniPC, and more. This model allows our customers to conserve costly computing resources by executing the fewest necessary denoising steps to achieve optimal quality in the produced images.
    
[^39]: 点云中三维单对象跟踪方法

    3D Single-object Tracking in Point Clouds with High Temporal Variation

    [https://arxiv.org/abs/2408.02049](https://arxiv.org/abs/2408.02049)

    我们提出了HVTrack框架，用于处理点云中具有高时序变化的三维单对象跟踪问题，通过相对姿态感知记忆模块、基扩张特征交叉注意力模块和上下文点引导自注意力模块来解决挑战。

    

    arXiv:2408.02049v1 公告类型：新  摘要：三维单对象跟踪（3D SOT）的关键挑战是点云的高时序变化。现有的方法依赖于一个假设，即点云的形状变化和对象在相邻帧之间的运动是平滑的，无法应对高时序变化的数据。本文提出了一个新的框架，用于处理点云中具有高时序变化的三维单对象跟踪问题，称为HVTrack。HVTrack提出了三个新的组件来处理高时序变化场景中的挑战：1）相对姿态感知记忆模块，用于处理时序变化的点云形状；2）基扩张特征交叉注意力模块，用于处理在扩展的搜索区域内与对象相似的干扰；3）上下文点引导自注意力模块，用于抑制背景噪声。我们通过在KITTI-HV数据集上设置不同的帧间隔来进行采样，构建了一个具有高时序变化的数据集。

    arXiv:2408.02049v1 Announce Type: new  Abstract: The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in 
    
[^40]: 像素级域适应：增强弱监督语义分割的新视角

    Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation

    [https://arxiv.org/abs/2408.02039](https://arxiv.org/abs/2408.02039)

    本文提出了一种名为 Pixel-Level Domain Adaptation (PLDA) 的全新方法，旨在通过对抗训练的方式，使模型学习到在物体不同部分之间具有一致性的像素级特征，从而在从图像标签进行弱监督语义分割的背景下，生成更清晰、更精确的伪 mask 信息。

    

    arXiv:2408.02039v1 公告类型：新  摘要：最近，已经有人关注从图像标签中学习 semantic segmentation 模型，这一范式被称为 image-level Weakly Supervised Semantic Segmentation (WSSS)。现有尝试采用 Class Activation Maps (CAMs) 作为先验来挖掘物体区域，但观察到激活不平衡问题，只有最具有辨别性的物体部分被定位在此篇文章中，我们认为仅从图像标签学习的模型难以生成完整和精确的伪 mask 做为 ground truths。为此，我们提出了一种 Pixel-Level Domain Adaptation (PLDA) 方法，以此鼓励模型学习像素级的域不变特征。具体地，通过一个对抗训练的多头域分类器，该分类器与特征提取器一起训练，以促进生成与域无关的像素特征。此外，我们还采用了多尺度细化和特征一致性正则化策略，以进一步提升伪 mask 的质量。我们通过实验验证了我们的 PLDA 方法，其在 image-level WSSS 任务中表现出了优越的性能。

    arXiv:2408.02039v1 Announce Type: new  Abstract: Recent attention has been devoted to the pursuit of learning semantic segmentation models exclusively from image tags, a paradigm known as image-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts adopt the Class Activation Maps (CAMs) as priors to mine object regions yet observe the imbalanced activation issue, where only the most discriminative object parts are located. In this paper, we argue that the distribution discrepancy between the discriminative and the non-discriminative parts of objects prevents the model from producing complete and precise pseudo masks as ground truths. For this purpose, we propose a Pixel-Level Domain Adaptation (PLDA) method to encourage the model in learning pixel-wise domain-invariant features. Specifically, a multi-head domain classifier trained adversarially with the feature extraction is introduced to promote the emergence of pixel features that are invariant with respect to the sh
    
[^41]: LEGO: 自监督代表学习方法在场景文本图像中的应用

    LEGO: Self-Supervised Representation Learning for Scene Text Images

    [https://arxiv.org/abs/2408.02036](https://arxiv.org/abs/2408.02036)

    我们提出了一种名为LEGO的自监督方法，用于学习场景文本图像的表示，通过创造性任务捕捉复杂结构，提升文本识别能力，适用于多种深度学习机器视觉任务。

    

    arXiv:2408.02036v1 公告类型：新  翻译摘要：近年来，基于数据的场景文本识别方法取得了重大进展。然而，由于标注的真实世界数据稀缺，这些方法的训练主要依赖于合成数据。合成和真实数据分布之间的差距限制了这些方法在实际应用中的进一步性能提升。为了解决这个问题，使用大量未标注的真实数据进行自监督训练是一种很有潜力的方法，这种方法在许多NLP和CV任务中被广泛证明是有效的。然而，由于场景文本图像的序列特性，对场景文本图像自监督训练的通用方法并不适用。为了解决这个问题，我们提出了一个Local Explicit and Global Order-aware self-supervised representation learning method（LEGO），它考虑了场景文本图像的特。受人类学习单词过程的启发，这个过程涉及高级的认知动机，我们的方法通过创造性地引入一系列自监督任务，成功地捕捉了场景文本图像的复杂结构。例如，我们的方法利用来自场景文本图像的不同角度和视角的信息，用以高效地捕捉文本文本的几何结构。此外，我们的方法还考虑了文本直连续性和文本块中单词之间的顺序关系，通过这些特性，我们能够提高场景文本识别在真实世界数据集上的性能，尤其是在一些难以编辑的文本图像上。为了增强场景文本识别方法的通用性和有效性，我们的方法不仅适用于特定的文本识别方法，还能为许多基于深度学习的机器视觉任务提供新的视角和有力工具。

    arXiv:2408.02036v1 Announce Type: new  Abstract: In recent years, significant progress has been made in scene text recognition by data-driven methods. However, due to the scarcity of annotated real-world data, the training of these methods predominantly relies on synthetic data. The distribution gap between synthetic and real data constrains the further performance improvement of these methods in real-world applications. To tackle this problem, a highly promising approach is to utilize massive amounts of unlabeled real data for self-supervised training, which has been widely proven effective in many NLP and CV tasks. Nevertheless, generic self-supervised methods are unsuitable for scene text images due to their sequential nature. To address this issue, we propose a Local Explicit and Global Order-aware self-supervised representation learning method (LEGO) that accounts for the characteristics of scene text images. Inspired by the human cognitive process of learning words, which involve
    
[^42]: 增强通过深度学习视听融合的人体动作识别和暴力检测

    Enhancing Human Action Recognition and Violence Detection Through Deep Learning Audiovisual Fusion

    [https://arxiv.org/abs/2408.02033](https://arxiv.org/abs/2408.02033)

    本文提出了一种基于深度学习视听融合的混合融合技术，显著提高了在公共场所的人体动作识别和暴力检测的准确性。

    

    这篇论文提出了一个基于两种不同媒体数据的融合型深度学习方法，即音频和视频，旨在提高在公共场所的人体活动识别和暴力检测能力。为了利用视听融合，使用了基于深度学习的晚期融合、中期融合和混合融合技术，并进行了比较。由于目标是在公共场所检测和识别暴力，我们对“现实生活暴力情景”(RLVS)数据集进行了扩展，并用于实验。对HFBDL（混合融合型深度学习）的模拟结果显示，在验证数据上达到了96.67%的准确率，比此数据集上的其他先进的方法要精确得多。为了展示模型在真实世界场景中的能力，我们还录制了54段有声音的视频，既有暴力场景也有非暴力场景。模型成功正确地识别了其中的52段视频。提出的这种方法在真实场景中展示了令人印象深刻的性能，因此，它可以用于人类行动识别和暴力检测的工业应用。我们的方法在视觉和音频数据上实现了高效的融合，显著提高了暴力检测的准确性。

    arXiv:2408.02033v1 Announce Type: new  Abstract: This paper proposes a hybrid fusion-based deep learning approach based on two different modalities, audio and video, to improve human activity recognition and violence detection in public places. To take advantage of audiovisual fusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning (HFBDL) are used and compared. Since the objective is to detect and recognize human violence in public places, Real-life violence situation (RLVS) dataset is expanded and used. Simulating results of HFBDL show 96.67\% accuracy on validation data, which is more accurate than the other state-of-the-art methods on this dataset. To showcase our model's ability in real-world scenarios, another dataset of 54 sounded videos of both violent and non-violent situations was recorded. The model could successfully detect 52 out of 54 videos correctly. The proposed method shows a promising performance on real scenarios. Thus, it can be used for hum
    
[^43]: 个体化多时间线MRI轨迹预测用于阿尔茨海默病

    Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease

    [https://arxiv.org/abs/2408.02018](https://arxiv.org/abs/2408.02018)

    本文通过使用条件变分AutoEncoder，构建了一个能在个体基础上进行MRI时间序列预测的模型，从而提高了阿尔茨海默病诊断的特异性。

    

    神经退行性改变通过磁共振成像（MRI）来测量，这是诊断阿尔茨海默病（AD）的一种潜在生物标志物，但其普遍认为不如淀粉样蛋白或tau生物标志物特异性。由于不同个体之间脑解剖结构的差异很大，我们假设利用MRI时间序列可以帮助提高特异性，即将每个患者视为其自身的基线。在这里，我们转向条件变分 Autoencoder 来生成个体化的MRI预测，这些预测基于患者的年龄、疾病状态和之前的扫描。使用阿尔茨海默病神经影像倡议（ADNI）的连续成像数据，我们训练了一个新的架构，以构建一个潜在空间分布，可以从该分布中抽样生成解剖结构变化的未来预测。这使我们可以超出数据集的范围进行外推，并对未来的MRI进行预测，最多可达10年。我们在一个保留的数据集中评估了模型，该数据集从未被用来训练或微调过模型。

    arXiv:2408.02018v1 Announce Type: new  Abstract: Neurodegeneration as measured through magnetic resonance imaging (MRI) is recognized as a potential biomarker for diagnosing Alzheimer's disease (AD), but is generally considered less specific than amyloid or tau based biomarkers. Due to a large amount of variability in brain anatomy between different individuals, we hypothesize that leveraging MRI time series can help improve specificity, by treating each patient as their own baseline. Here we turn to conditional variational autoencoders to generate individualized MRI predictions given the subject's age, disease status and one previous scan. Using serial imaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a novel architecture to build a latent space distribution which can be sampled from to generate future predictions of changing anatomy. This enables us to extrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated the model on a held-out set fr
    
[^44]: 肝脏创伤优先处理系统

    Decision Support System to triage of liver trauma

    [https://arxiv.org/abs/2408.02012](https://arxiv.org/abs/2408.02012)

    本文提出了一种决策支持系统，用于高效地评估和优先处理肝脏创伤病例，以减少治疗成本和继发并发症的风险。

    

    arXiv:2408.02012v1 公告类型：交叉 摘要：全球健康受到创伤的严重影响，每年超过500万人死于创伤，这与结核病、艾滋病和疟疾等疾病的死亡率相当。在伊朗，交通事故的经济后果每年约占国家国民生产总值（GNP）的2%。创伤患者在接受伤害后24小时内的出血是致死的主要原因，因此，迅速诊断和评估疾病的严重性至关重要。患者需要对所有器官进行全面的扫描，这会产生大量数据。对全身进行CT图像评估既耗时又需要大量的专业知识，这突显了在诊断过程中有效管理时间的重要性。高效的诊断流程能够在很大程度上减少治疗成本，并减少继发并发症的发生。在这种情况下，开发一个可靠的决策支持系统（D

    arXiv:2408.02012v1 Announce Type: cross  Abstract: Trauma significantly impacts global health, accounting for over 5 million deaths annually, which is comparable to mortality rates from diseases such as tuberculosis, AIDS, and malaria. In Iran, the financial repercussions of road traffic accidents represent approximately 2% of the nation's Gross National Product each year. Bleeding is the leading cause of mortality in trauma patients within the first 24 hours following an injury, making rapid diagnosis and assessment of severity crucial. Trauma patients require comprehensive scans of all organs, generating a large volume of data. Evaluating CT images for the entire body is time-consuming and requires significant expertise, underscoring the need for efficient time management in diagnosis. Efficient diagnostic processes can significantly reduce treatment costs and decrease the likelihood of secondary complications. In this context, the development of a reliable Decision Support System (D
    
[^45]: 没有背景会发生什么？为微细分任务构建仅包含前景的数据

    What Happens Without Background? Constructing Foreground-Only Data for Fine-Grained Tasks

    [https://arxiv.org/abs/2408.01998](https://arxiv.org/abs/2408.01998)

    该研究提出了一种通过SAM和Detic技术创建仅包含前景数据的工程化管道，以促进微细分任务中忽视背景噪声影响的科学研究。

    

    arXiv:2408.01998v1 新闻类型：新消息 摘要：在视觉信号处理中，微细分识别任务旨在根据样本中存在的鉴别性信息，区分相似的下位类。然而，现有的方法通常错误地关注背景区域，忽视了对主体有效鉴别信息的捕捉，这阻碍了其实际应用。为了促进对模型背景噪声影响的科学研究，并增强其对主体鉴别特征的集中能力，我们提出了一种工程化管道，它利用SAM和Detic的能力，创建了仅包含前景主题的数据集，不包含背景。广泛的交叉实验验证了这种方法作为一种训练前的预处理步骤的有效性，增强了算法性能，并为数据模式的进一步扩展奠定了潜力。

    arXiv:2408.01998v1 Announce Type: new  Abstract: Fine-grained recognition, a pivotal task in visual signal processing, aims to distinguish between similar subclasses based on discriminative information present in samples. However, prevailing methods often erroneously focus on background areas, neglecting the capture of genuinely effective discriminative information from the subject, thus impeding practical application. To facilitate research into the impact of background noise on models and enhance their ability to concentrate on the subject's discriminative features, we propose an engineered pipeline that leverages the capabilities of SAM and Detic to create fine-grained datasets with only foreground subjects, devoid of background. Extensive cross-experiments validate this approach as a preprocessing step prior to training, enhancing algorithmic performance and holding potential for further modal expansion of the data.
    
[^46]: SR-CIS: 自我反思增量系统与解耦记忆与推理

    SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning

    [https://arxiv.org/abs/2408.01970](https://arxiv.org/abs/2408.01970)

    SR-CIS通过结合小模型快速推理和慢速决策的大模型，并通过CA-OAD机制实现高效协作，提供了一种新的内存与推理解耦的机制，以解决当前深度学习模型在面对人类记忆和学习机制时的挑战。

    

    arXiv:2408.01970v1 公告类型：交叉  摘要：人类快速学习新知识同时保留旧记忆的能力为当前的深度学习模型提出了严峻的挑战。为了解决这个问题，我们借鉴了人类记忆和学习的机制，并提出了自反式互补增量系统（SR-CIS）。SR-CIS由解构化的互补推断模块（CIM）和互补记忆模块（CMM）组成，其中CIM通过置信度自适应在线异常检测（CA-OAD）机制实现快速推断和慢速决策的小模型，而CIM则由Confidence-Aware Online Anomaly Detection（CA-OAD）机制实现快速推断和慢速决策的大模型。CMM由任务特定的短期记忆（STM）区域和通用的长期记忆（LTM）区域组成。通过设定任务特定的低秩自适应（LoRA）和相关原型权重和偏差，它为参数和表示记忆建立了外部存储实例，从而解构了记忆模块与推理模块，并实现了记忆的解耦。

    arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere
    
[^47]: 数据集大小和社会一致性中介面部印象偏见的视觉语言AI

    Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI

    [https://arxiv.org/abs/2408.01959](https://arxiv.org/abs/2408.01959)

    研究揭示了43个CLIP视觉语言模型在面部印象偏见方面的学习情况，证明了社会一致性会中介模型反映的人类偏见程度，并且在数据集大规模训练下，模型更倾向于准确形成那些依赖于不可见属性的印象。

    

    arXiv:2408.01959v1公告类型：新摘要：能够将图像与文本关联的多模态AI模型在包括自动图像字幕在内的多个领域具有巨大潜力，甚至对盲人和低视力用户的安全访问应用程序也有帮助。然而，对偏见的不确定性在某些情况下限制了它们的采用和可用性。在本文中，我们对43个CLIP视觉语言模型进行了研究，以确定它们是否像人类一样学习面部印象偏见，并且我们首次发现，这些偏见在三种类型的CLIP模型家族中得到体现。我们还首次展示了，一个偏见的程度在社会中共享，预测了它在CLIP模型中反映的程度的程度。在仅对最大数据集进行训练的模型中，人类似的面部印象才会出现，这表明与未经审核的文化的更好匹配对于复制越来越微妙的社交偏见是必要的。此外，我们还发现，当对图像的某些不可见属性（如可信赖性和性取向）形成印象时，只有在大数据集上下文中基于模型的训练才更加准确。这可能意味着在生成与社会偏好高度一致的文本描述方面，视觉语言模型正在远离对现实数据集的简单映射，并开始学习如何了解和模仿人类的社会形态知觉。这对于理解模型的感知能力和揭示这些偏见对决策制定的潜在影响至关重要。

    arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u
    
[^48]: EqvAfford: SE(3)等变性对点级 affordance学习的影响

    EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning

    [https://arxiv.org/abs/2408.01953](https://arxiv.org/abs/2408.01953)

    我们的EqvAfford框架通过确保点级affordance学习的SE(3)等变性，为机器人操作任务提供优异的性能和泛化能力，无论物体姿态如何。

    

    arXiv:2408.01953v1公告类型：新  摘要：人类在感知和与世界互动时具有等变性意识，使我们能够在不同的物体姿态下进行操作。对于机器人操作，等变性在许多情况下也是存在的。例如，无论抽屉的姿态如何（平移、旋转和倾斜），操作策略都是一致的（抓住把手并沿直线拉动）。尽管传统模型通常不具有为机器人操作提供等变性的意识，这可能会导致训练所需的数据过多，并且在新的物体姿态下表现不佳，但我们提出我们的EqvAfford框架，具有新的设计来保证点级affordance学习中的等变性，对于下游机器人操作任务，在代表性的物体上具有在多样姿态下表现的优异性能和泛化能力。

    arXiv:2408.01953v1 Announce Type: new  Abstract: Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.
    
[^49]: 仿射n点问题广义最大似然估计

    Generalized Maximum Likelihood Estimation for Perspective-n-Point Problem

    [https://arxiv.org/abs/2408.01945](https://arxiv.org/abs/2408.01945)

    该研究提出了一个广义最大似然估计的PnP求解器，提高了对噪声数据的鲁棒性，并在多种姿势估计场景中提供了更准确的参数估计。

    

    arXiv:2408.01945v1 公告类型: 交叉 摘要: 视图n个点(PnP)问题在文献中得到了广泛的研究，并在各种基于视觉的姿态估计场景中得到了应用。然而，本文在几个现实世界的数据集中展示了现有方法的不足之处，这些方法忽略了观测数据的各向异性不确定性。这种忽视可能导致在存在噪声观测值的情况下得出次优和不准确的估计。因此，我们提出了一种广义最大似然PnP求解器，名为GMLPnP，它通过迭代GLS程序来同时估计姿态和不确定性。此外，所提出的方法与相机模型无关。合成和真实实验结果表明，我们的方法在常见的姿态估计情景中取得了更好的精度，与最佳基准方法相比，GMLPnP在TUM-RGBD和KITTI-360数据集上的旋转/平移精度分别提高了4.7%/2.0%和18.6%/18.4%。它对噪声的鲁棒性更好，对参数的估计也更准确。

    arXiv:2408.01945v1 Announce Type: cross  Abstract: The Perspective-n-Point (PnP) problem has been widely studied in the literature and applied in various vision-based pose estimation scenarios. However, existing methods ignore the anisotropy uncertainty of observations, as demonstrated in several real-world datasets in this paper. This oversight may lead to suboptimal and inaccurate estimation, particularly in the presence of noisy observations. To this end, we propose a generalized maximum likelihood PnP solver, named GMLPnP, that minimizes the determinant criterion by iterating the GLS procedure to estimate the pose and uncertainty simultaneously. Further, the proposed method is decoupled from the camera model. Results of synthetic and real experiments show that our method achieves better accuracy in common pose estimation scenarios, GMLPnP improves rotation/translation accuracy by 4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best baseline. It is more ac
    
[^50]: RobNODDI: 自适应采样下连续表示下NODDI参数估计的鲁棒性验证

    RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under Continuous Representation

    [https://arxiv.org/abs/2408.01944](https://arxiv.org/abs/2408.01944)

    本文提出了一种新的自适应多尺度扩散采样策略，提高了扩散磁共振成像（dMRI）参数估计的计算效率和模型对未知测试样品的泛化能力，特别是在测试扩散方向与训练方向不一致的情况下。

    

    arXiv:2408.01944v1 宣布类型：新 Abstract: 神经纤维取向分散与密度成像（NODDI）是一种用于评估脑组织微观结构的重要成像技术，对于发现和治疗各种神经系统疾病具有重要意义。目前基于深度学习的方法通过扩散磁共振成像（dMRI）来估计参数，使用数量较少的扩散梯度。这些方法加速了参数的估计并提高了准确性。然而，大多数现有深度学习模型在测试期间使用的扩散方向必须在严格的意义上与训练期间使用的扩散方向一致。这导致深度学习模型在dMRI参数估计方面的泛化性和鲁棒性不佳。在本工作中，我们首次验证了当测试时的扩散方向与训练时的扩散方向不一致时，目前的主流方法在dMRI参数估计方面的性能将显著下降。为此，我们提出了一种新的自适应多尺度扩散采样策略，该策略通过动态规划选择最具信息的扩散方向并组织的取样过程，大大提高了计算效率，增强了模型对未知测试样品的泛化能力。我们的方法在多个dMRI数据集上取得了显著的性能提升，表明了它在处理监督深度学习模型泛化性的挑战时，展现出了强大的潜在应用价值。

    arXiv:2408.01944v1 Announce Type: new  Abstract: Neurite Orientation Dispersion and Density Imaging (NODDI) is an important imaging technology used to evaluate the microstructure of brain tissue, which is of great significance for the discovery and treatment of various neurological diseases. Current deep learning-based methods perform parameter estimation through diffusion magnetic resonance imaging (dMRI) with a small number of diffusion gradients. These methods speed up parameter estimation and improve accuracy. However, the diffusion directions used by most existing deep learning models during testing needs to be strictly consistent with the diffusion directions during training. This results in poor generalization and robustness of deep learning models in dMRI parameter estimation. In this work, we verify for the first time that the parameter estimation performance of current mainstream methods will significantly decrease when the testing diffusion directions and the training diffus
    
[^51]: 《对抗攻击在对象检测中的调查与评估》

    A Survey and Evaluation of Adversarial Attacks for Object Detection

    [https://arxiv.org/abs/2408.01934](https://arxiv.org/abs/2408.01934)

    本文全面调查了专门针对对象检测的多种对抗攻击方法，提供了对现有攻击评价指标的回顾，并对多种开源攻击方法进行了系统性的评估。

    

    这篇论文对专门针对对象检测的对抗攻击进行了全面分类，并对现有的对抗攻击评价指标进行了回顾。系统地评估了开源攻击方法和模型的鲁棒性。并提供了一系列关键观察，以提高对攻击有效性和相应对策的理解。此外，我们还指出了用以指导未来安全自动化对象检测系统研究的关键挑战。

    arXiv:2408.01934v1 Announce Type: new  Abstract: Deep learning models excel in various computer vision tasks but are susceptible to adversarial examples-subtle perturbations in input data that lead to incorrect predictions. This vulnerability poses significant risks in safety-critical applications such as autonomous vehicles, security surveillance, and aircraft health monitoring. While numerous surveys focus on adversarial attacks in image classification, the literature on such attacks in object detection is limited. This paper offers a comprehensive taxonomy of adversarial attacks specific to object detection, reviews existing adversarial robustness evaluation metrics, and systematically assesses open-source attack methods and model robustness. Key observations are provided to enhance the understanding of attack effectiveness and corresponding countermeasures. Additionally, we identify crucial research challenges to guide future efforts in securing automated object detection systems.
    
[^52]: 乳腺癌中H&E染色至IHC染色翻译的进展：基于多倍率和注意力机制的方法

    Advancing H&E-to-IHC Stain Translation in Breast Cancer: A Multi-Magnification and Attention-Based Approach

    [https://arxiv.org/abs/2408.01929](https://arxiv.org/abs/2408.01929)

    本文提出了一种新的模型，该模型结合了注意力机制和多倍率信息处理，旨在解决从H&E染色切片合成IHC-HER2切片时遇到的挑战，特别是处理多倍率病理图像和在翻译过程中关注重要信息的问题。

    

    乳腺癌是全球范围内重要的健康挑战，需要精确的诊断和有效的治疗策略，其中组织病理学对Hematoxylin和Eosin（H&E）染色的组织切片起了核心作用。尽管它的作用非常重要，但是评估像表皮生长因子受体2（HER2）这样的特定生物标记以实现个性化的治疗仍然受到资源密集型的免疫组织化学（IHC）的限制。深度学习最近的一些进展，特别是在图像到图像翻译方面，为从H&E染色切片合成IHC-HER2切片提供了希望。然而，现有的方法遇到了一些挑战，包括处理病理图像的多倍率和在翻译过程中对重要信息的不足关注。为了解决这些问题，我们提出了一个结合注意力机制和多倍率信息处理的新模型。我们的模型使用了一种

    arXiv:2408.01929v1 Announce Type: cross  Abstract: Breast cancer presents a significant healthcare challenge globally, demanding precise diagnostics and effective treatment strategies, where histopathological examination of Hematoxylin and Eosin (H&E) stained tissue sections plays a central role. Despite its importance, evaluating specific biomarkers like Human Epidermal Growth Factor Receptor 2 (HER2) for personalized treatment remains constrained by the resource-intensive nature of Immunohistochemistry (IHC). Recent strides in deep learning, particularly in image-to-image translation, offer promise in synthesizing IHC-HER2 slides from H\&E stained slides. However, existing methodologies encounter challenges, including managing multiple magnifications in pathology images and insufficient focus on crucial information during translation. To address these issues, we propose a novel model integrating attention mechanisms and multi-magnification information processing. Our model employs a 
    
[^53]: CAF-YOLO：一种在生物医学影像中多尺度病变检测的坚固框架

    CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in Biomedical Imagery

    [https://arxiv.org/abs/2408.01897](https://arxiv.org/abs/2408.01897)

    本文提出了一种基于YOLOv8架构的CAF-YOLO方法，用于多尺度病变检测，通过引入ACFM和SFM模块，提高了对生物医学数据中异常细胞和肺结节等细小结构的检测精度。

    

    arXiv:2408.01897v1 公告类型：新  翻译摘要：在生物医学图像分析中，目标检测至关重要，特别是对于病变定位。虽然现有的方法在识别和标记病变方面非常出色，但它们往往缺乏检测极小的生物医学实体（如异常细胞、直径小于3毫米的肺结节）的精确性，这在贫血和肺病领域至关重要。为了解决这个问题，我们提出了基于YOLOv8架构的CAF-YOLO方法，这是一个灵活且坚固的医学目标检测方法，它利用卷积神经网络（CNN）和变换器的强大功能。为了克服卷积核的局限性，卷积核的能力有限，无法与远离的信息互动，我们引入了一个注意力和卷积融合模块（ACFM）。这个模块增强了全局和局部特征的建模，从而捕获了长期特征依赖性和空间自相关性。此外，我们还引入了时空融合模块（SFM），它通过模拟时空相关性，进一步提高了目标检测的准确性和召回率。通过对多种生物医学数据集的实验验证，CAF-YOLO在多尺度病变检测方面显示出优越的性能。它能够有效地区分真实病变与噪声和背景，从而显著提高了病变检测的准确性和可靠性。

    arXiv:2408.01897v1 Announce Type: new  Abstract: Object detection is of paramount importance in biomedical image analysis, particularly for lesion identification. While current methodologies are proficient in identifying and pinpointing lesions, they often lack the precision needed to detect minute biomedical entities (e.g., abnormal cells, lung nodules smaller than 3 mm), which are critical in blood and lung pathology. To address this challenge, we propose CAF-YOLO, based on the YOLOv8 architecture, a nimble yet robust method for medical object detection that leverages the strengths of convolutional neural networks (CNNs) and transformers. To overcome the limitation of convolutional kernels, which have a constrained capacity to interact with distant information, we introduce an attention and convolution fusion module (ACFM). This module enhances the modeling of both global and local features, enabling the capture of long-term feature dependencies and spatial autocorrelation. Additiona
    
[^54]: FBINeRF: 基于特征的递归神经网络集成方法用于鱼眼和标头神经辐射场

    FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and Fisheye Neural Radiance Fields

    [https://arxiv.org/abs/2408.01878](https://arxiv.org/abs/2408.01878)

    This study introduces FBINeRF, a novel technique integrating feature-based recurrent neural networks with adaptive GRUs for improved camera pose adjustment in capturing 3D scenes, especially for fisheye cameras, offering faster convergence and higher reconstruction quality relative to existing methods.

    

    我们已经在前面的研究中展示了优化和调整相机姿势的能力，比如使用基于神经辐射场的BARF和DBARF方法，为标头相机姿势的优化提供了一种有效的解决方案，但是这些方法并不能很好地处理包括鱼眼相机在内的各种相机类型的图像失真问题。此外，在DBARF方法中，初始深度值的误差会影响最终结果的收敛性和准确性。在本研究中，我们提出了一种自适应的GRU单元以及一种灵活的调整方法来解决斜径失真等问题，并采用基于特征的递归神经网络来从鱼眼相机图像数据集中生成连续的新图像。其他针对鱼眼相机图像的NeRF方法，如SCNeRF和OMNI-NeRF，使用了投影光束距离损失来细化失真姿势，这导致了严重的图像失真、较长的渲染时间和难以理解的计算过程。相比之下，我们的方法采用了一个新颖的封装光束损失函数，它不仅能够处理斜径失真，而且能够通过递归网络有效地集成特征信息，从而在保持图像细节的同时提高收敛速度和重建质量。

    arXiv:2408.01878v1 Announce Type: new  Abstract: Previous studies aiming to optimize and bundle-adjust camera poses using Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated impressive capabilities in 3D scene reconstruction. However, these approaches have been designed for pinhole-camera pose optimization and do not perform well under radial image distortions such as those in fisheye cameras. Furthermore, inaccurate depth initialization in DBARF results in erroneous geometric information affecting the overall convergence and quality of results. In this paper, we propose adaptive GRUs with a flexible bundle-adjustment method adapted to radial distortions and incorporate feature-based recurrent neural networks to generate continuous novel views from fisheye datasets. Other NeRF methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray distance loss for distorted pose refinement, causing severe artifacts, long rendering time, and are difficult to u
    
[^55]: 实体代理间的生成式通信对零样本目标导航有效吗？

    Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?

    [https://arxiv.org/abs/2408.01877](https://arxiv.org/abs/2408.01877)

    本研究通过在实体地面代理和空中代理之间实施基于视觉语言模型的生成式通信，显著提高了其在模拟环境中的零样本目标导航能力，同时减少了盲目探索的时间。

    

    arXiv:2408.01877v1 公告类型：交叉  翻译摘要：在零样本目标导航中，实体地面代理被期望在没有进行特定环境微调的情况下，沿着目标对象的自然语言标签导航。鉴于地面代理的有限视野和独立探索行为，这是一项挑战。为了解决这些问题，我们考虑了一个具有有限全局视野的空中代理，并与地面代理一起，提出了两种协同导航方案，以进行明智的探索。我们确立了具有视觉语言模型（VLMs）的实体代理之间的生成性通信（GC）在改进零样本ObjectNav中的影响，在模拟中，相对于一个没有协助的设置，实现了地面代理找到目标对象的10%的改进。我们还分析了对GC的分析，为量化幻觉和合作的存在确定了独特特征。特别地，我们识别了“预防性幻觉行为”的独特特征，这种行为有助于地面代理更早地定位目标对象，而不必盲目探索。此外，GC通过加深对空中代理输出的理解，提高了对目标对象位置的预测精确度，从而促进了地面代理的搜索策略。总之，我们的研究表明，通过合适的策略来集成GC，可以显著提高实体代理在无监督下导航到目标对象的能力。

    arXiv:2408.01877v1 Announce Type: cross  Abstract: In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a target object specified by a natural language label without any environment-specific fine-tuning. This is challenging, given the limited view of a ground agent and its independent exploratory behavior. To address these issues, we consider an assistive overhead agent with a bounded global view alongside the ground agent and present two coordinated navigation schemes for judicious exploration. We establish the influence of the Generative Communication (GC) between the embodied agents equipped with Vision-Language Models (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in the ground agent's ability to find the target object in comparison with an unassisted setup in simulation. We further analyze the GC for unique traits quantifying the presence of hallucination and cooperation. In particular, we identify a unique trait of "preemptive hallucin
    
[^56]: 使用与正样本相同的内分布数据实现安全的半监督对比学习

    Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples

    [https://arxiv.org/abs/2408.01872](https://arxiv.org/abs/2408.01872)

    

    

    arXiv:2408.01872v1 公告类型：跨领域 摘要：当只有少量的标签可用时，半监督学习方法在解决许多实际问题方面显示出良好的结果。现有方法假设标注和未标注数据的类分布相等；然而，当未标注数据中存在出分布（OOD）数据时，它们的表现会显著下降。以往的安全半监督学习研究已经通过基于标注数据的策略有效降低了出分布数据的负面影响。然而，即使这些研究能够有效地筛选出不必要的出分布数据，他们也可能会失去数据之间共享的基本信息，而不管其类别如何。为此，我们提出了将自监督对比学习方法应用于充分利用大量未标注数据的策略。我们还提出了一个带有系数调度比例的对比损失函数，以聚合作为锚定的标注负样本，从而确保所有数据在对比学习过程中的代表性，避免因过滤出分布数据而损失基本信息。

    arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex
    
[^57]: 由可见域到红外域的监督图像翻译方法用于物体检测

    Supervised Image Translation from Visible to Infrared Domain for Object Detection

    [https://arxiv.org/abs/2408.01843](https://arxiv.org/abs/2408.01843)

    本文提出了一种由可见域到红外域的监督图像翻译方法，通过将问题描述为图像翻译并采用两阶段训练策略，改进了物体检测等下游任务的准确性。通过集成生成对抗网络和物体检测模型，该方法学习了一种既能保留可见图像的结构细节又能保持红外图像纹理的转换过程。生成的图像被用于训练标准物体检测框架，实现了显著的准确性提升。

    

    本研究旨在学习一种从可见域到红外域图像的翻译方法，弥合这两种模态之间的领域差距，以提高包括物体检测在内的下游任务 accuracy。以往的方法试图通过迭代优化或端到端的深卷积网络来实现双域特征融合。然而，我们将问题描述为类似于图像翻译，采用两阶段训练策略，结合生成对抗网络（GAN）和物体检测模型。翻译模型在学习保留可见图像的结构细节的同时，保持了红外图像的纹理和其他特征。所生成的图像被用于训练标准物体检测框架，包括Yolov5、Mask和Faster R-CNN。我们还调查了在我们的管道中集成超分辨率步骤的有用性，以进一步改进模型的准确度，并实现显着改善。

    arXiv:2408.01843v1 Announce Type: new  Abstract: This study aims to learn a translation from visible to infrared imagery, bridging the domain gap between the two modalities so as to improve accuracy on downstream tasks including object detection. Previous approaches attempt to perform bi-domain feature fusion through iterative optimization or end-to-end deep convolutional networks. However, we pose the problem as similar to that of image translation, adopting a two-stage training strategy with a Generative Adversarial Network and an object detection model. The translation model learns a conversion that preserves the structural detail of visible images while preserving the texture and other characteristics of infrared images. Images so generated are used to train standard object detection frameworks including Yolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating a super-resolution step into our pipeline to further improve model accuracy, and achieve an improvem
    
[^58]: 使用深度卷积神经网络模型的Vibroseis数据串联效应减少

    A Deep CNN Model for Ringing Effect Attenuation of Vibroseis Data

    [https://arxiv.org/abs/2408.01831](https://arxiv.org/abs/2408.01831)

    本文提出了一种使用深度卷积神经网络的新方法，直接通过训练模型端到端地减少Vibroseis数据的串联效应，并通过跳过连接保持数据的细节，从而有效降低了串联效应。

    

    arXiv:2408.01831v1 公告类型：新摘要：在勘探地球物理领域，地震振动器是用于采集地震数据的广泛使用的地震源，通常被称为vibroseis。振动器数据的“串联效应”是由于振动器的频率带宽有限而常见的一个问题，它降低了第一突破挑选的性能。在此论文中，我们提出了一种使用深度卷积神经网络（CNN）的Vibroseis数据去噪模型。在这个模型中，我们使用端到端的训练策略直接获取去噪数据，并通过跳过连接来改进模型训练过程并保留Vibroseis数据的细节。对于现实中的Vibroseis去噪任务，我们从真实Vibroseis数据中合成训练数据和相应的标签，并使用它们来训练深度CNN模型。在合成数据和真实Vibroseis数据上进行了实验。实验结果表明，深度CNN模型可以减少串联效应。

    arXiv:2408.01831v1 Announce Type: new  Abstract: In the field of exploration geophysics, seismic vibrator is one of the widely used seismic sources to acquire seismic data, which is usually named vibroseis. "Ringing effect" is a common problem in vibroseis data processing due to the limited frequency bandwidth of the vibrator, which degrades the performance of first-break picking. In this paper, we proposed a novel deringing model for vibroseis data using deep convolutional neural network (CNN). In this model we use end-to-end training strategy to obtain the deringed data directly, and skip connections to improve model training process and preserve the details of vibroseis data. For real vibroseis deringing task we synthesize training data and corresponding labels from real vibroseis data and utilize them to train the deep CNN model. Experiments are conducted both on synthetic data and real vibroseis data. The experiment results show that deep CNN model can attenuate the ringing effect
    
[^59]: GLDiTalker:基于语音的3D面部动画生成与图谱潜在扩散变换器

    GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer

    [https://arxiv.org/abs/2408.01826](https://arxiv.org/abs/2408.01826)

    GLDiTalker是一种新方法，通过引入动作先验和随机性提高了基于语音的3D面部动画生成模型的多样性。

    

    arXiv:2408.01826v1 公告类型：新 摘要：基于语音的3D面部动画生成在工业应用和学术研究中已受到大量关注。由于现实中面部表情的非言语线索具有不确定性，生成结果应当具备多样性。然而，大多数最近的方法都是确定性模型，无法学习音频与面部动作的许多对许多映射，以至于无法生成多样性的面部动画。为了解决这一问题，我们提出了GLDiTalker，它引入了动作先验以及一定程度的随机性，以减少跨模态映射的不确定性，同时增加面部非言语线索的多样性。特别是，GLDiTalker在第一阶段使用VQ-VAE将面部动作网格序列映射到潜空间中，然后在第二阶段迭代地添加和去除噪声到潜面部动作特征中。为了整合不同层次的空间信息，还使用了分层注意机制来描述空间上下文及其它们之间的关系。通过详细分析和对比现有方法，GLDiTalker展示了更高的语声到表情转换准确性和动画的多样性。

    arXiv:2408.01826v1 Announce Type: new  Abstract: 3D speech-driven facial animation generation has received much attention in both industrial applications and academic research. Since the non-verbal facial cues that exist across the face in reality are non-deterministic, the generated results should be diverse. However, most recent methods are deterministic models that cannot learn a many-to-many mapping between audio and facial motion to generate diverse facial animations. To address this problem, we propose GLDiTalker, which introduces a motion prior along with some stochasticity to reduce the uncertainty of cross-modal mapping while increasing non-determinacy of the non-verbal facial cues that reside throughout the face. Particularly, GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in the first stage, and then iteratively adds and removes noise to the latent facial motion features in the second stage. In order to integrate different levels of spatial info
    
[^60]: MiniCPM-V: 一款可在您手机上运行的GPT-4V级别的MLLM

    MiniCPM-V: A GPT-4V Level MLLM on Your Phone

    [https://arxiv.org/abs/2408.01800](https://arxiv.org/abs/2408.01800)

    MiniCPM-V是一款能实现在手机上运行的强大且高效的多模态大型语言模型，它在性能上超越了GPT-4V-1106，Gemini Pro和Claudrat's Pluribus，并且大大减少了能耗和成本，可以为移动应用提供高质量的语言理解和生成能力，同时也保护了用户隐私和数据安全。

    

    arXiv:2408.01800v1 宣布类型：新  摘要：最近的多模态大型语言模型（MLLM）的兴起彻底改变了人工智能研究和行业的发展，并照亮了一条通往下一个人工智能里程碑的道路。然而，阻止MLLM在现实世界应用中变得实用的重大挑战仍然存在。最大的挑战之一来自于运行具有大量参数的MLLM所需的高昂成本和大量的计算需求。因此，大多数MLLM需要在高性能的云服务器上运行，这极大地限制了它们的应用范围，如移动、离线、能源敏感和隐私保护的场景。在这项工作中，我们推出了MiniCPM-V系列，这是一款可以在设备端部署的效率MLLM。通过在架构、预训练和校准方面集成最新的MLLM技术，最新的MiniCPM-Llama3-V 2.5具有以下特点：（1）强大的性能，超过了GPT-4V-1106、Gemini Pro和Claudrat’s Pluribus。（2）高效能，大大降低了模型运行的能耗和成本。（3）易部署，可以被轻松地集成到现有的移动应用程序中，为用户提供高质量的语言理解和生成能力，同时保护用户的隐私和数据安全。

    arXiv:2408.01800v1 Announce Type: new  Abstract: The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain preventing MLLMs from being practical in real-world applications. The most notable challenge comes from the huge cost of running an MLLM with a massive number of parameters and extensive computation. As a result, most MLLMs need to be deployed on high-performing cloud servers, which greatly limits their application scopes such as mobile, offline, energy-sensitive, and privacy-protective scenarios. In this work, we present MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By integrating the latest MLLM techniques in architecture, pretraining and alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1) Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claud
    
[^61]: 论文标题：νLite -- 一种轻量级且快速的核实例分割和分类模型

    NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation and Classification

    [https://arxiv.org/abs/2408.01797](https://arxiv.org/abs/2408.01797)

    我们提出了一种轻量级且高效的核实例分割和分类模型νLite，与SOTA模型CellViT相当，但参数和GFlops显著减少，为病理学提供了一种高效而强大的解决方案。

    

    arXiv:2408.01797v1 公告类型：交叉 摘要：在病理学中，对HE染色（H&E）玻片进行准确和高效的分析对于及时和有效地进行癌症诊断至关重要。尽管文献中存在许多用于核实例分割和分类的深度学习解决方案，但它们往往伴随着高昂的计算成本和资源需求，从而限制了它们在医疗应用中的实际使用。为了解决这个问题，我们介绍了一种新的卷积神经网络，νLite，这是一款基于Fast-ViT的U-Net类似架构，Fast-ViT是一款最新的（SOTA）轻量级CNN。我们获得了我们模型的三种版本，即νLite-S、νLite-M和νLite-H，这些模型在PanNuke数据集上进行了训练。实验结果证明，我们的模型在Panoptic Quality和检测方面与CellViT（SOTA）相当。然而，我们的 lightest模型，即NuLite-S，在参数上比CellViT小40倍，在GFlops上大约小了8倍。我们的最重的模型在参数上略微更小，但在GFlops上与CellViT相当。我们的工作为病理学提供了高性能和高效性的融合，并为医疗影像的深度学习解决方案开辟了新的空间。

    arXiv:2408.01797v1 Announce Type: cross  Abstract: In pathology, accurate and efficient analysis of Hematoxylin and Eosin (H\&E) slides is crucial for timely and effective cancer diagnosis. Although many deep learning solutions for nuclei instance segmentation and classification exist in the literature, they often entail high computational costs and resource requirements, thus limiting their practical usage in medical applications. To address this issue, we introduce a novel convolutional neural network, NuLite, a U-Net-like architecture designed explicitly on Fast-ViT, a state-of-the-art (SOTA) lightweight CNN. We obtained three versions of our model, NuLite-S, NuLite-M, and NuLite-H, trained on the PanNuke dataset. The experimental results prove that our models equal CellViT (SOTA) in terms of panoptic quality and detection. However, our lightest model, NuLite-S, is 40 times smaller in terms of parameters and about 8 times smaller in terms of GFlops, while our heaviest model is 17 ti
    
[^62]: 深度学习分类的不同嵌入空间比较

    Comparison of Embedded Spaces for Deep Learning Classification

    [https://arxiv.org/abs/2408.01767](https://arxiv.org/abs/2408.01767)

    本文对比了不同嵌入空间技术以设计用于深度学习分类的嵌入空间，提供了两种和三维嵌入技术的展示案例。

    

    arXiv:2408.01767v1 公告类型：cross  摘要：在深度学习中，嵌入空间是一个关键特性。良好的嵌入空间能有效地表示数据，以支持分类以及诸如开放式识别、小批量学习和可解释性等高级技术。本文提供了一个关于不同技术设计的嵌入空间用于分类的紧凑概述。它根据网络参数的损失函数和约束，对比了不同的嵌入空间的可达几何结构。这些技术在MNIST、Fashion MNIST和CIFAR-10数据集上进行了二维和三维嵌入的演示，允许检查嵌入空间的视觉表现。

    arXiv:2408.01767v1 Announce Type: cross  Abstract: Embedded spaces are a key feature in deep learning. Good embedded spaces represent the data well to support classification and advanced techniques such as open-set recognition, few-short learning and explainability. This paper presents a compact overview of different techniques to design embedded spaces for classification. It compares different loss functions and constraints on the network parameters with respect to the achievable geometric structure of the embedded space. The techniques are demonstrated with two and three-dimensional embeddings for the MNIST, Fashion MNIST and CIFAR-10 datasets, allowing visual inspection of the embedded spaces.
    
[^63]: MultiFuser: 多模态融合变换器用于增强驾驶员行为识别

    MultiFuser: Multimodal Fusion Transformer for Enhanced Driver Action Recognition

    [https://arxiv.org/abs/2408.01766](https://arxiv.org/abs/2408.01766)

    MultiFuser是一个多模态融合变换器，用于增强驾驶员行为识别，通过事件分解模块和模态合成器集成多种摄像头数据来提升表示力。

    

    驾驶员行为识别对于提高驾驶员与车辆之间的互动和安全驾驶至关重要。与一般的动作识别不同，驾驶员的环境往往很困难，光线昏暗，随着传感器的发展，各种摄像头，如红外和深度摄像头，被用于分析驾驶员的行为。因此，在这篇论文中，我们提出了一个新颖的多模态融合变换器，名为MultiFuser，它能够识别不同模态之间的相互关系和相互作用，并在驾驶员车厢视频中集成多种模态，从而得到改进的表示。具体来说，MultiFuser包括分层的事件分解模块来建模时空特征，以及一种模态合成器用于集成多模态特征。每个事件分解模块包括一个用于提取特定于模态的特征模块和一个基于图块的适应模块。

    arXiv:2408.01766v1 Announce Type: new  Abstract: Driver action recognition, aiming to accurately identify drivers' behaviours, is crucial for enhancing driver-vehicle interactions and ensuring driving safety. Unlike general action recognition, drivers' environments are often challenging, being gloomy and dark, and with the development of sensors, various cameras such as IR and depth cameras have emerged for analyzing drivers' behaviors. Therefore, in this paper, we propose a novel multimodal fusion transformer, named MultiFuser, which identifies cross-modal interrelations and interactions among multimodal car cabin videos and adaptively integrates different modalities for improved representations. Specifically, MultiFuser comprises layers of Bi-decomposed Modules to model spatiotemporal features, with a modality synthesizer for multimodal features integration. Each Bi-decomposed Module includes a Modal Expertise ViT block for extracting modality-specific features and a Patch-wise Adapt
    
[^64]: 提升绿色AI: 为水稻叶片病害识别设计的有效和高精度的轻量级CNN

    Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification

    [https://arxiv.org/abs/2408.01752](https://arxiv.org/abs/2408.01752)

    本次研究专注于建立轻量级CNN模型，如ShuffleNet、MobileNetV2和EfficientNet-B0，用于实现对水稻叶片病害的高效和准确识别。通过结合局部绝对差异操作、空间金字塔池化和改进的全连接层，该模型能够显著提高病害识别的准确率。

    

    论文摘要：水稻是全球一半以上人口的主要食物来源，其产量对于全球粮食安全至关重要。然而，水稻栽培经常受到多种疾病的侵扰，这些疾病可以严重降低产量和品质。因此，早期和准确地检测水稻疾病对于防止疾病蔓延和减轻作物损失至关重要。在本次研究中，我们探索了三种适用于移动设备的长短期记忆网络模型，即ShuffleNet、MobileNetV2和EfficientNet-B0，用于水稻叶片病害分类。选择这些模型是因为它们与移动设备兼容，与其他CNN模型相比，它们需要的计算能力和内存较少。为了提高这三款模型的性能，我们在全连接层之间添加了两个全连接层，并使用了一个防止模型过拟合的dropout层。我们使用早停方法来防止模型发生过拟合。研究结果显示，最佳性能来自……（由于内容整合中英文摘要较长，建议在生成摘要时进行适当的精简）

    arXiv:2408.01752v1 Announce Type: new  Abstract: Rice plays a vital role as a primary food source for over half of the world's population, and its production is critical for global food security. Nevertheless, rice cultivation is frequently affected by various diseases that can severely decrease yield and quality. Therefore, early and accurate detection of rice diseases is necessary to prevent their spread and minimize crop losses. In this research, we explore three mobile-compatible CNN architectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice leaf disease classification. These models are selected due to their compatibility with mobile devices, as they demand less computational power and memory compared to other CNN models. To enhance the performance of the three models, we added two fully connected layers separated by a dropout layer. We used early stop creation to prevent the model from being overfiting. The results of the study showed that the best performance wa
    
[^65]: 域惩罚改进的界限泛化

    Domain penalisation for improved Out-of-Distribution Generalisation

    [https://arxiv.org/abs/2408.01746](https://arxiv.org/abs/2408.01746)

    本文提出了一种域惩罚框架，用于平衡多源域数据集中对象检测的训练过程，解决了域泛化问题，并在GWHD 2021数据集上取得了提升。

    

    arXiv:2408.01746v1 公告类型：新  翻译摘要：在对象检测领域，域泛化（DG）旨在通过学习对应于感兴趣对象的多个源域中固有的稳健域不变特征，确保在不同和未知的目标域中表现出健壮性能。尽管对于分类任务已经有许多基于DG的方法，但对于对象检测领域，相关工作却相当少。在这篇论文中，我们提出了一种域惩罚（DP）框架，用于对象检测任务。在这种框架下，数据是从多个源域中抽取的，并且在完全未知的目标测试域中进行验证。我们为每个域分配惩罚权重，这些权重将根据检测网络在各自源域上的性能进行更新。通过优先考虑需要更多关注的域，我们的方法有效地平衡了训练过程。我们在GWHD 2021数据集上评估了我们的解决方案，它是Wathlon Object Detection Challenge的一部分。我们的方法在目标检测任务中成功地减少了源域和目标域之间的差距，实现了数据增强，同时最大限度地减少了过拟合的风险，最终实现了性能上的提升。

    arXiv:2408.01746v1 Announce Type: new  Abstract: In the field of object detection, domain generalisation (DG) aims to ensure robust performance across diverse and unseen target domains by learning the robust domain-invariant features corresponding to the objects of interest across multiple source domains. While there are many approaches established for performing DG for the task of classification, there has been a very little focus on object detection. In this paper, we propose a domain penalisation (DP) framework for the task of object detection, where the data is assumed to be sampled from multiple source domains and tested on completely unseen test domains. We assign penalisation weights to each domain, with the values updated based on the detection networks performance on the respective source domains. By prioritising the domains that needs more attention, our approach effectively balances the training process. We evaluate our solution on the GWHD 2021 dataset, a component of the W
    
[^66]: 基于特征引导的扩散模型用于高保真度和时间一致性说话人头像生成

    Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation

    [https://arxiv.org/abs/2408.01732](https://arxiv.org/abs/2408.01732)

    我们提出了一个两阶段扩散模型，使用音频同步面部特征点，并在此基础上生成高质量、同步且时间一致的说话人头像视频，解决了当前模型在图像质量和唇形同步方面的不足。

    

    arXiv:2408.01732v1 公告类型：新  翻译摘要：音频驱动的说话人头像生成是一项重要的但具有挑战性的任务，适用于包括虚拟头像、电影制作和在线会议在内的各种领域。然而，现有的基于GAN的方法过分强调生成与语音同步的唇形，而忽略了生成的帧的视觉质量，而基于扩散的方法则过分强调生成高质量的帧，忽视了唇形匹配，导致嘴部运动出现抖动。为了解决上述问题，我们引入了一种两阶段基于扩散的模型。第一阶段涉及根据给定的语音生成同步的面部特征点。在第二阶段，这些生成的特征点作为条件在去噪过程中使用，旨在优化嘴部抖动问题并生成高保真度、同步和在时间上一致的说话人头像视频。广泛的实验证明，我们的模型达到了最佳性能。

    arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
    
[^67]: 这里是一个用于评估文本生成图像描述质量的新框架

    A Novel Evaluation Framework for Image2Text Generation

    [https://arxiv.org/abs/2408.01723](https://arxiv.org/abs/2408.01723)

    该论文提出了一种基于现代大型语言模型的新框架，用于评估自动生成的图像描述质量，该框架结合了人类评价和模型性能度量，旨在提供更准确和可靠的评估结果。

    

    arXiv:2408.01723v1 公告类型: 新 Abstract: 自动生成图像描述的质量评估是一项挑战，它需要各种方面如句法、覆盖、正确性和真实性。虽然人工评估提供了宝贵的见解，但其成本和时间消耗的自然限制了其限制。现有的自动度量如BLEU、ROUGE、METEOR和CIDEr旨在弥合这一差距，但往往与人类判断的联系较弱。我们通过引入一个基于现代大型语言模型（LLM）的新评估框架来解决这一挑战，如GPT-4或Gemini，它们能够生成图像。在我们的提议框架中，我们首先将输入图像喂入指定的图像标题模型进行评估，然后生成文本描述。使用这个描述，LLM然后创建一个新的图像。通过从原始图像和由LLM创建的图像中提取特征，我们使用指定的相似度度量方法来衡量两者之间的相似度。我们进一步利用人类评价者的反馈作为标准，并提出了一套综合的评估指标来全面衡量图像生成模型的性能。我们的实验结果表明，与现有的自动评估方法相比，这种基于人类评价者和现代LLM的方法提供了更准确和可靠的结果。我们相信，这项工作将为自动生成图像描述的质量评估提供一个更精确、更高效的评估框架。

    arXiv:2408.01723v1 Announce Type: new  Abstract: Evaluating the quality of automatically generated image descriptions is challenging, requiring metrics that capture various aspects such as grammaticality, coverage, correctness, and truthfulness. While human evaluation offers valuable insights, its cost and time-consuming nature pose limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr aim to bridge this gap but often show weak correlations with human judgment. We address this challenge by introducing a novel evaluation framework rooted in a modern large language model (LLM), such as GPT-4 or Gemini, capable of image generation. In our proposed framework, we begin by feeding an input image into a designated image captioning model, chosen for evaluation, to generate a textual description. Using this description, an LLM then creates a new image. By extracting features from both the original and LLM-created images, we measure their similarity using a designated simil
    
[^68]: 农业机器人视觉-惯性SLAM: 用于基准测试闭环闭合的好处和计算成本的性能

    Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the Benefits and Computational Costs of Loop Closing

    [https://arxiv.org/abs/2408.01716](https://arxiv.org/abs/2408.01716)

    本文对多种视觉-惯性SLAM系统进行了农业机器人场景下的性能评估，重点分析了闭环闭合对定位精度和计算成本的影响。

    

    arXiv:2408.01716v1 公告类型: 新  摘要: 同步定位与地图构建(SLAM)对于移动机器人至关重要，它在不依赖于外部定位系统的情况下，允许在动态和非结构化的户外环境中进行自主导航。在农业应用中，由于照明或天气条件的可变性，视觉-惯性SLAM已成为潜在的解决方案。本文对几种开源视觉-惯性SLAM系统进行了基准测试，包括ORB-SLAM3、VINS-Fusion、OpenVINS、Kimera和SVO Pro，以评估这些系统在农业环境中的表现。我们侧重于闭环闭合对定位精度和计算需求的影响，提供了这些系统在实际工作环境中的有效性分析，尤其是它们在农业机器人嵌入式系统中的应用。我们的贡献还包括对帧率变化对定位精度影响的评估，以及对不同系统在阳光直射和遮挡下的性能比较，以及在高重复性场景中的表现评估。我们的实验结果表明，对于农业机器人导航，闭环闭合显著提高了定位精度，但同时也增加了计算成本。此外，我们还分析了不同系统在能量消耗方面的表现，为嵌入式系统中潜在的能源优化提供了见解。我们的工作将为农业机器人中的视觉-惯性SLAM的选型提供重要参考，并为未来研究指明了方向。

    arXiv:2408.01716v1 Announce Type: new  Abstract: Simultaneous Localization and Mapping (SLAM) is essential for mobile robotics, enabling autonomous navigation in dynamic, unstructured outdoor environments without relying on external positioning systems. In agricultural applications, where environmental conditions can be particularly challenging due to variable lighting or weather conditions, Visual-Inertial SLAM has emerged as a potential solution. This paper benchmarks several open-source Visual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS, Kimera, and SVO Pro, to evaluate their performance in agricultural settings. We focus on the impact of loop closing on localization accuracy and computational demands, providing a comprehensive analysis of these systems' effectiveness in real-world environments and especially their application to embedded systems in agricultural robotics. Our contributions further include an assessment of varying frame rates on localization acc
    
[^69]: 二值边缘图像的通用模糊模型，结合了边缘追踪及其实现

    A General Ambiguity Model for Binary Edge Images with Edge Tracing and its Implementation

    [https://arxiv.org/abs/2408.01712](https://arxiv.org/abs/2408.01712)

    该研究提出了一款适用于二值边缘图像模糊结构的一般模型，结合了边缘追踪技术，并解释了算法以实现后续任务的处理优化。

    

    在本次研究中，我们提出了一种适用于二值边缘图像中的交叉点、接合和其他结构的一般且直观的模糊模型。该模型与边缘追踪技术结合，其中边缘被看作是一系列连接的像素有序序列。我们的目标是提供一种通用的预处理方法，用于诸如图底分割、物体识别、拓扑分析等方面的任务。通过仅使用一系列简单的原则，结果描述起来直观易懂，有助于后续处理步骤的实现，如在接合处解决边缘连接的模糊问题。通过使用扩大的边缘图，可以利用快速的局部搜索操作直接访问相邻的边缘。我们的边缘追踪算法采用了递归技术，这简化了编程代码。随后我们将用伪代码详细介绍我们的算法，并与相关方法进行比较，并展示如何使用简单的模块化后处理步骤来优化结果。我们算法的完整性欢迎有兴趣的用户和研究人员进行测试和评估。

    arXiv:2408.01712v1 Announce Type: new  Abstract: We present a general and intuitive ambiguity model for intersections, junctions and other structures in binary edge images. The model is combined with edge tracing, where edges are ordered sequences of connected pixels. The objective is to provide a versatile preprocessing method for tasks such as figure-ground segmentation, object recognition, topological analysis, etc. By using only a small set of straightforward principles, the results are intuitive to describe. This helps to implement subsequent processing steps, such as resolving ambiguous edge connections at junctions. By using an augmented edge map, neighboring edges can be directly accessed using quick local search operations. The edge tracing uses recursion, which leads to compact programming code. We explain our algorithm using pseudocode, compare it with related methods, and show how simple modular postprocessing steps can be used to optimize the results. The complete algorith
    
[^70]: AVESFormer：实时音频视觉分割的高效变换器设计

    AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation

    [https://arxiv.org/abs/2408.01708](https://arxiv.org/abs/2408.01708)

    AVESFormer是一个高效的音频视觉变换器模型，实现了实时、高效和轻量级的AVS任务。

    

    arXiv:2408.01708v1 公告类型：新  翻译摘要：最近，基于transformer的模型在音频视觉分割（AVS）任务上展示了出色的性能。然而，它们的计算成本高昂，使得实时推理在实际中不可行。通过分析网络的注意图，我们识别出AVS模型的两个关键障碍：1）注意损耗，对应于在受限框架内由Softmax导致的注意力权值的过度集中，以及2）不高效的、沉重的变换器解码器，是由早期阶段出现的不广泛关注模式引起的。在这篇论文中，我们介绍了AVESFormer，这是第一个同时实现快速、高效和轻量级的实时音频视觉有效分割transformer模型。我们的模型采用高效的提示查询生成器来纠正跨注意力行为。此外，我们还提出了一种ELF解码器，通过促进适合局部特征的卷积，减少计算负担，从而实现更大效率。进行了广泛

    arXiv:2408.01708v1 Announce Type: new  Abstract: Recently, transformer-based models have demonstrated remarkable performance on audio-visual segmentation (AVS) tasks. However, their expensive computational cost makes real-time inference impractical. By characterizing attention maps of the network, we identify two key obstacles in AVS models: 1) attention dissipation, corresponding to the over-concentrated attention weights by Softmax within restricted frames, and 2) inefficient, burdensome transformer decoder, caused by narrow focus patterns in early stages. In this paper, we introduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation transformer that achieves fast, efficient and light-weight simultaneously. Our model leverages an efficient prompt query generator to correct the behaviour of cross-attention. Additionally, we propose ELF decoder to bring greater efficiency by facilitating convolutions suitable for local features to reduce computational burdens. Extensiv
    
[^71]: 信号-SGN：一种用于通过学习时空频率动态的脉冲图卷积网络，用于骨骼动作识别

    Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics

    [https://arxiv.org/abs/2408.01701](https://arxiv.org/abs/2408.01701)

    本文提出了一种名为信号-SGN的脉冲图卷积网络，该网络结合了图卷积特性和脉冲神经网络特性，旨在有效的处理骨骼动作序列的时间维度。信号-SGN能够通过在单个帧上进行图卷积，并在脉冲神经网络中捕获帧间的时序关系，以及在频率空间处理时空频率动态，以实现动作识别的高效和精确。

    

    arXiv:2408.01701v1 公告类型：新Abstract: 在基于骨骼的动作识别中，基于图卷积网络的(GCNs)的方法由于其复杂性和高能量消耗而存在限制。近年来，脉冲神经网络(SNNs)因其低能量消耗而备受关注，但现有结合GCNs和SNNs的方法未能充分利用骨骼序列的时间特征，导致存储器和计算成本增加。为了解决这一问题，我们提出了一个名为Signal-SGN(Spiking Graph Convolutional Network)的模型，它将骨骼序列的时间维度作为脉冲时间步长，并将特征视为离散的随机信号。网络的核心是由一个1D脉冲图卷积网络（1D-SGN）和一个频率脉冲卷积网络（FSN）组成的。SGN在单个帧上执行图卷积，并引入脉冲神经网络特性来捕获帧间的时序关系，与此同时，FSN则处理这些时空频率动态，从而实现骨骼动作的高效、精确识别。The proposed network effectively reduces energy consumption and computational costs while improving the spatial-temporal feature representation capability.通过实验验证了Signal-SGN在骨骼动作识别任务上的有效性和实用性。

    arXiv:2408.01701v1 Announce Type: new  Abstract: In skeletal-based action recognition, Graph Convolutional Networks (GCNs) based methods face limitations due to their complexity and high energy consumption. Spiking Neural Networks (SNNs) have gained attention in recent years for their low energy consumption, but existing methods combining GCNs and SNNs fail to fully utilize the temporal characteristics of skeletal sequences, leading to increased storage and computational costs. To address this issue, we propose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the temporal dimension of skeletal sequences as the spiking timestep and treats features as discrete stochastic signals. The core of the network consists of a 1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking Convolutional Network (FSN). The SGN performs graph convolution on single frames and incorporates spiking network characteristics to capture inter-frame temporal relationships, while th
    
[^72]: 基于贝叶斯主动学习的方法用于语义分割

    Bayesian Active Learning for Semantic Segmentation

    [https://arxiv.org/abs/2408.01694](https://arxiv.org/abs/2408.01694)

    我们提出了一个贝叶斯主动学习框架，通过平衡熵来优化对稀疏像素级标注的数据集的语义分割模型的训练，仅使用很少的像素标注即可达到监督训练水平。

    

    arXiv:2408.01694v1 公告类型：新摘要：语义分割模型的完全监督训练成本高昂且困难，因为每张图像中的像素都需要进行标注。因此，引入了稀疏像素级标注方法，该方法仅对每张图像中的子集像素进行标注来训练模型。我们介绍了一种基于稀疏像素级标注的贝叶斯主动学习框架，该框架利用基于平衡熵（Balanced Entropy）的像素级贝叶斯不确定性度量，其中Balanced Entropy是一种基于[84]的平衡熵，能够捕捉模型预测的几何分布和相关像素标签之间的信息。Balanced Entropy具有线性的可扩展性，并且具有封闭形式的解析解，并且可以在不考虑其他像素的情况下独立计算每个像素的Balanced Entropy值。我们在我提出的主动学习框架上培训了Cityscapes、Camvid、ADE20K和VOC2012基准数据集，并展示了它在使用仅占先前描述数据集一小部分的数据的情况下，达到了与监督水平相似的mIoU。同时，我们在Cityscapes、Camvid、ADE20K和VOC2012数据集中初步测试了我们的方法。

    arXiv:2408.01694v1 Announce Type: new  Abstract: Fully supervised training of semantic segmentation models is costly and challenging because each pixel within an image needs to be labeled. Therefore, the sparse pixel-level annotation methods have been introduced to train models with a subset of pixels within each image. We introduce a Bayesian active learning framework based on sparse pixel-level annotation that utilizes a pixel-level Bayesian uncertainty measure based on Balanced Entropy (BalEnt) [84]. BalEnt captures the information between the models' predicted marginalized probability distribution and the pixel labels. BalEnt has linear scalability with a closed analytical form and can be calculated independently per pixel without relational computations with other pixels. We train our proposed active learning framework for Cityscapes, Camvid, ADE20K and VOC2012 benchmark datasets and show that it reaches supervised levels of mIoU using only a fraction of labeled pixels while outpe
    
[^73]: 基于CNN的深度学习模型在滑坡检测中的比较分析

    A Comparative Analysis of CNN-based Deep Learning Models for Landslide Detection

    [https://arxiv.org/abs/2408.01692](https://arxiv.org/abs/2408.01692)

    这篇论文比较了四种传统语义分割模型（U-Net、LinkNet、PSPNet和FPN）和ResNet50 backbone encoder的应用，通过实验评估证明所提出的模型可以有效检测滑坡并提高检测精度。

    

    arXiv:2408.01692v1 公告类型：新  摘要：滑坡对社会的经济造成了重大损失，突显了它们作为全球范围内的频繁和破坏性自然灾害的重要性。印度和尼泊尔北部地区最近发生的滑坡造成了严重的破坏，损坏了基础设施，并对当地社区构成了威胁。卷积神经网络（CNNs）是一种深度学习技术，在图像处理领域表现出惊人的成功。由于其复杂的架构和高级的CNN-based模型在滑坡检测方面优于传统的算法。这项工作旨在更详细地研究CNNs的潜力，并着重于对基于CNN的模型进行比较，以便更好地进行滑坡检测。我们对四种传统的语义分割模型（U-Net、LinkNet、PSPNet和FPN）进行了比较，并将ResNet50 backbone encoder应用于它们。此外，我们还实验了诸如学习率、批大小和训练 epoch 等超参数对性能的影响。最后，我们通过随机森林分类器和多光谱遥感数据进行了实验评估，结果表明所提出的模型能够有效检测滑坡并提高了检测精度。

    arXiv:2408.01692v1 Announce Type: new  Abstract: Landslides inflict substantial societal and economic damage, underscoring their global significance as recurrent and destructive natural disasters. Recent landslides in northern parts of India and Nepal have caused significant disruption, damaging infrastructure and posing threats to local communities. Convolutional Neural Networks (CNNs), a type of deep learning technique, have shown remarkable success in image processing. Because of their sophisticated architectures, advanced CNN-based models perform better in landslide detection than conventional algorithms. The purpose of this work is to investigate CNNs' potential in more detail, with an emphasis on comparison of CNN based models for better landslide detection. We compared four traditional semantic segmentation models (U-Net, LinkNet, PSPNet, and FPN) and utilized the ResNet50 backbone encoder to implement them. Moreover, we have experimented with the hyperparameters such as learnin
    
[^74]: IDNet: 一个用于身份文档分析和欺诈检测的新型数据集

    IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection

    [https://arxiv.org/abs/2408.01690](https://arxiv.org/abs/2408.01690)

    本研究引入了一个新的数据集IDNet，旨在促进隐私保护的欺诈检测技术的进步。

    

    arXiv:2408.01690v1 公告类型: 新  翻译摘要: 在在线平台上挫败身份盗窃并加强安全，有效进行欺诈检测和对政府发行的身份文档(如护照、驾驶执照和身份证)的分析至关重要。训练准确的身份文档欺诈检测和分析工具依赖于大量身份文档数据集的可用性。然而，目前可公开获得的用于身份文档分析的基准数据集，包括MIDV-500、MIDV-2020和FMIDV，在多个方面存在缺陷：它们提供的样本数量有限，涉及的欺诈模式不足，而且很少包括对关键个人身份识别字段(如肖像图像)的更改，这限制了其在训练模型方面的实用性，这些模型能够检测真实生活中的欺诈行为，同时保护隐私。  为了回应这些不足，我们的研究引入了一个新的基准数据集——IDNet，旨在推进隐私保护的欺诈检测技术的进步。

    arXiv:2408.01690v1 Announce Type: new  Abstract: Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy.   In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection e
    
[^75]: 使用ε约束优化方法的图像到图像生成模型的可控制遗忘

    Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization

    [https://arxiv.org/abs/2408.01689](https://arxiv.org/abs/2408.01689)

    本文提出了一种使用ε控制因子调节图像到图像生成模型去学习程度的方法，旨在平衡去学习和模型性能，保护用户数据安全。

    

    虽然生成模型在近年来取得了显著的进步，但也引发了对隐私泄露和偏见等问题的担忧。机器去学习作为一种可行的解决方案，旨在从模型中移除特定的训练数据，例如含有私人信息和偏见的训练数据。在本文中，我们研究了Image-to-Image（I2I）生成模型中的机器去学习问题。以往的研究主要将这个问题视为一个单一目标优化问题，仅提供一种解决方案，这忽视了用户对去学习和模型效用之间的多样化期待。为了解决这个问题，我们提出了一种可控制去学习的框架，它使用控制系数ε来控制这种取舍。我们将I2I生成模型去学习的問題重新表述为一个ε约束优化问题，并使用一种基于梯度的方法来解决这个问题，以找到针对遗忘的优化的解决方案，同时能够保持模型的新鲜度和优越性。我们期望这种方法能够有效地应用于需要频繁更新模型的场景，如图形设计、自然语言处理和视觉内容创建等，以保护用户的数据安全和隐私。

    arXiv:2408.01689v1 Announce Type: cross  Abstract: While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\varepsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\varepsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearni
    
[^76]: 泰米尔莫：基于Siamese运动中心3D物体跟踪

    SiamMo: Siamese Motion-Centric 3D Object Tracking

    [https://arxiv.org/abs/2408.01688](https://arxiv.org/abs/2408.01688)

    SiamMo是一种新的3D物体跟踪方法，它基于Siamese运动中心范式，通过Siamese特征提取和Spatio-Temporal Feature Aggregation模块改善了跟踪性能，并使用了Box-aware Feature Encoding模块来编码物体框相关特征。

    

    arXiv:2408.01688v1 预报类型：新的 摘要：当前的三维单一物体跟踪方法主要依赖于Siamese匹配基础范式，这种范式在缺乏纹理和不完整的激光雷达点云情况下遇到困难。相反，运动中心范式避免了外观匹配，从而解决了这些问题。然而，其复杂的multistage管道和单个流架构的有限时间建模能力限制了其潜力。在这项研究中，我们介绍了一种名为SiamMo的新颖且简单的Siamese运动中心跟踪方法。与传统的单流架构不同，我们使用Siamese特征提取来进行运动中心跟踪。这种特征提取与时间融合的分离显著提高了跟踪性能。此外，我们设计了一个Spatio-Temporal Feature Aggregation模块，用于在多尺度下整合Siamese特征，有效捕获运动信息。我们还引入了一个Box-aware Feature Encoding模块，用于编码物体框相关特征。为了克服现有方法的缺点，我们进行了一系列优化，包括改进的Siamese网络结构和精简的模块设计。通过全面的实验，我们证明了SiamMo在多种高难度场景下的优越性能，包括长距离跟踪、动态场景和具有遮挡的图像序列。总的来说，SiamMo在减少跟踪误差和加速跟踪过程方面表现出出色的性能，为无人驾驶车辆和机器人领域提供了强大的3D物体跟踪解决方案。

    arXiv:2408.01688v1 Announce Type: new  Abstract: Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode obj
    
[^77]: iControl3D：可控3D场景生成系统

    iControl3D: An Interactive System for Controllable 3D Scene Generation

    [https://arxiv.org/abs/2408.01678](https://arxiv.org/abs/2408.01678)

    iControl3D是一个使用户能够精确控制并生成可定制3D场景的交互式系统。它使用3D网格作为中间代理，通过迭代合并2D扩散生成的图像来创建连贯统一的3D场景表示。

    

    arXiv:2408.01678v1 公告类型：新摘要：3D内容创建长期以来一直是一项复杂且耗时的任务，通常需要专业技能和资源。尽管最近的发展允许基于文本的3D对象和场景生成，但它们在提供生成过程的足够控制方面仍然存在不足，导致用户创意愿景与生成结果之间存在差距。在这篇论文中，我们介绍了iControl3D，这是一个全新的交互式系统，它使用户能够以精确的控制生成和渲染可定制的3D场景。为此，我们开发了一个3D创造者界面，为用户提供了对创建过程的精细控制。技术上，我们利用3D网格作为中间代理，将个体2D扩散生成的图像迭代合并为连贯且统一的3D场景表示。为了确保3D网格的无缝集成，我们在融合新生成的图像之前，提出了边界感知深度对齐的方法。

    arXiv:2408.01678v1 Announce Type: new  Abstract: 3D content creation has long been a complex and time-consuming process, often requiring specialized skills and resources. While recent advancements have allowed for text-guided 3D object and scene generation, they still fall short of providing sufficient control over the generation process, leading to a gap between the user's creative vision and the generated results. In this paper, we present iControl3D, a novel interactive system that empowers users to generate and render customizable 3D scenes with precise control. To this end, a 3D creator interface has been developed to provide users with fine-grained control over the creation process. Technically, we leverage 3D meshes as an intermediary proxy to iteratively merge individual 2D diffusion-generated images into a cohesive and unified 3D scene representation. To ensure seamless integration of 3D meshes, we propose to perform boundary-aware depth alignment before fusing the newly gener
    
[^78]: HIVE: 层次化体积编码像素隐式表面重建

    HIVE: HIerarchical Volume Encoding for Neural Implicit Surface Reconstruction

    [https://arxiv.org/abs/2408.01677](https://arxiv.org/abs/2408.01677)

    HIVE方法通过引入层次化体积编码和稀疏结构，实现了对三维形状的高精度重建，并显著提高了细节表现。

    

    arXiv:2408.01677v1 声明类型：新 摘要：神经隐式表面重建从图像中重建三维形状的趋势。然而，在以往的方法中，3D场景仅由MLPs编码，这些MLPs没有明确的三维结构。为了更好地表示3D形状，我们引入了一种体积编码方法，以明确编码空间信息。我们还设计了层次化体积，以在多个尺度上编码场景结构。高分辨率体积捕获高频几何细节，因为可以从不同的3D点学习空间变化特征，而低分辨率体积保证了形状的平滑性，因为相邻位置具有相同的低分辨率特征。此外，我们还采用了稀疏结构来减少在高分辨率体积中的内存消耗，以及两种正则化项来增强结果的平滑性。这种层次化体积编码可以附加到任何三维重建网络中，以提高重建的三维形状的精确度和细节表现。

    arXiv:2408.01677v1 Announce Type: new  Abstract: Neural implicit surface reconstruction has become a new trend in reconstructing a detailed 3D shape from images. In previous methods, however, the 3D scene is only encoded by the MLPs which do not have an explicit 3D structure. To better represent 3D shapes, we introduce a volume encoding to explicitly encode the spatial information. We further design hierarchical volumes to encode the scene structures in multiple scales. The high-resolution volumes capture the high-frequency geometry details since spatially varying features could be learned from different 3D points, while the low-resolution volumes enforce the spatial consistency to keep the shape smooth since adjacent locations possess the same low-resolution feature. In addition, we adopt a sparse structure to reduce the memory consumption at high-resolution volumes, and two regularization terms to enhance results smoothness. This hierarchical volume encoding could be appended to any 
    
[^79]: 自适应多情境与频率聚合网络对于 deepfake 检测

    Multiple Contexts and Frequencies Aggregation Network forDeepfake Detection

    [https://arxiv.org/abs/2408.01668](https://arxiv.org/abs/2408.01668)

    本文提出了一种名为 MkfaNet 的网络，通过自适应地聚合空间和频率域的特征，提升了深度假造检测的准确性和鲁棒性。

    

    arXiv:2408.01668v1 公告类型：新摘要：随着生成模型的快速成长，深度造假检测面临越来越多的挑战，因为深度造假技术也在不断发展。最近的技术进步依赖于通过空间或频率域的手动特征而不是在主干网络中建模一般造假特征。为了解决这个问题，我们将注意力转向了空间和频率检测器的两个直观先验，即学习能够在主干网络中建模真实和假样本之间细微面部差异的稳健空间属性和频率分布。为此，我们提出了一个高效的面部造假检测网络 MkfaNet，它由两个核心模块组成。对于空间情境，我们设计了一个自适应多内核聚合器，它能够通过多种卷积提取的组织特征来适应性地选择建模真实和假人脸之间的细微面部差异。对于频率成分，我们提出了一种多频率聚合器，用于处理频率域的归一化和嵌入，从而提高了对深度造假样本的检测能力。总的来说，我们的模型通过联合多核纹理分析、多频率特征聚合以及端对端训练提升了深度假造检测的准确性和鲁棒性。

    arXiv:2408.01668v1 Announce Type: new  Abstract: Deepfake detection faces increasing challenges since the fast growth of generative models in developing massive and diverse Deepfake technologies. Recent advances rely on introducing heuristic features from spatial or frequency domains rather than modeling general forgery features within backbones. To address this issue, we turn to the backbone design with two intuitive priors from spatial and frequency detectors, \textit{i.e.,} learning robust spatial attributes and frequency distributions that are discriminative for real and fake samples. To this end, we propose an efficient network for face forgery detection named MkfaNet, which consists of two core modules. For spatial contexts, we design a Multi-Kernel Aggregator that adaptively selects organ features extracted by multiple convolutions for modeling subtle facial differences between real and fake faces. For the frequency components, we propose a Multi-Frequency Aggregator to process 
    
[^80]: SAT3D:基于图像的3D语义属性转移

    SAT3D: Image-driven Semantic Attribute Transfer in 3D

    [https://arxiv.org/abs/2408.01664](https://arxiv.org/abs/2408.01664)

    这项研究提出了一种新的方法，可以基于参考图像在3D环境中实现更准确的语义属性转移，解决了以往方法在语义属性编辑方面的局限性。

    

    arXiv:2408.01664v1 Announce Type: 新的摘要: 基于GAN的图像编辑任务旨在在生成模型的潜在空间中对图像属性进行操作。大多数以前的2D和3D感知方法主要关注从参考图像中难以分辨的语义或区域编辑属性，这无法实现摄影风格的语义属性转移，如从一个男人的照片中转移胡须。在本文中，我们提出了一个基于图像的3D语义属性转移方法(SAT3D)，通过从参考图像中编辑语义属性。对于提出的方法，探索是在一个预先训练的3D感知StyleGAN基generator的样式空间中进行的，通过学习与样式代码通道相关的语义属性和样式码之间的相关性。为了指导，我们与一组基于短语的描述符组关联每个属性，并开发了一个定量测量模块(QMM)，以基于描述符组在图像中定量描述属性特征。

    arXiv:2408.01664v1 Announce Type: new  Abstract: GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor group
    
[^81]: 深度补丁视觉SLAM

    Deep Patch Visual SLAM

    [https://arxiv.org/abs/2408.01654](https://arxiv.org/abs/2408.01654)

    我们提出了一种名为“深度补丁视觉SLAM”的新方法，它能在单GPU上实现高质量的单目视觉SLAM，即使在大型数据集上也能保持接近真实时间的帧率，同时内存消耗仅为现有系统的很小一部分。

    

    arXiv:2408.01654v1 公告类型：新发表  摘要：近年来，视觉SLAM领域的研究显示了使用深度网络骨干结构的有效性。尽管准确性优异，然而，这些方法往往运行成本高昂，且在零样本学习时表现不佳。此外，它们的运行时性能波动较大，而前端的竞争也使得GPU资源有限。为了解决这些问题，我们提出了深度补丁视觉（DPV）SLAM方法，这是一种单GPU上的单目视觉SLAM技术。与现有的深度SLAM系统相比，DPV-SLAM在保持高最低帧率的同事，拥有较低的内存消耗（5-7G）。在现实世界数据集上，DPV-SLAM的帧率可以达到1x到4x的实际帧率。我们在EuRoC和TartanAir数据集上取得了与DROID-SLAM相当的准确性，而使用的是其内存消耗的一小部分，并且运行速度是其2.5倍。DPV-SLAM是DPVO视觉运动系统的一个扩展；它的代码可以在同一仓库中找到：https://github.com/princeton-vl/DPVO

    arXiv:2408.01654v1 Announce Type: new  Abstract: Recent work in visual SLAM has shown the effectiveness of using deep network backbones. Despite excellent accuracy, however, such approaches are often expensive to run or do not generalize well zero-shot. Their runtime can also fluctuate wildly while their frontend and backend fight for access to GPU resources. To address these problems, we introduce Deep Patch Visual (DPV) SLAM, a method for monocular visual SLAM on a single GPU. DPV-SLAM maintains a high minimum framerate and small memory overhead (5-7G) compared to existing deep SLAM systems. On real-world datasets, DPV-SLAM runs at 1x-4x real-time framerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir while running 2.5x faster using a fraction of the memory. DPV-SLAM is an extension to the DPVO visual odometry system; its code can be found in the same repository: https://github.com/princeton-vl/DPVO
    
[^82]: 多圆柱全景深度估计：通过多圆柱全景的立体匹配方法

    MCPDepth: Omnidirectional Depth Estimation via Stereo Matching from Multi-Cylindrical Panoramas

    [https://arxiv.org/abs/2408.01653](https://arxiv.org/abs/2408.01653)

    MCPDepth是一种使用圆柱全景的立体匹配方法来估计全景深度的新框架，它在不同视图的深度图融合中表现出优异的性能。这项技术特别设计为简化对嵌入式设备的部署，并显著提高了深度估计的准确性。

    

    我们介绍了一种名为Multi-Cylindrical Panoramic Depth Estimation（MCPDepth）的二阶段框架，用于通过多个圆柱全景的立体匹配进行全景深度估计。MCPDepth利用多个圆柱全景进行初始的立体匹配，然后将深度图在不同视图中融合。MCPDepth使用了一种环形注意力模块来克服垂直轴上的失真问题。MCPDepth仅使用标准网络组件，使其更容易部署到嵌入式设备上，并且在全球领先的方法中取得了显著的改进，这些方法通常需要特殊的算子。我们从理论和实验两个方面对比了立体匹配中的球体投影和圆柱投影，强调了圆柱投影的优点。MCPDepth在Deep360室外合成数据集和3D60室内真实场景数据集上分别实现了18.8%和19.9%的均绝对误差（MAE）的下降。

    arXiv:2408.01653v1 Announce Type: new  Abstract: We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a two-stage framework for omnidirectional depth estimation via stereo matching between multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for initial stereo matching and then fuses the resulting depth maps across views. A circular attention module is employed to overcome the distortion along the vertical axis. MCPDepth exclusively utilizes standard network components, simplifying deployment to embedded devices and outperforming previous methods that require custom kernels. We theoretically and experimentally compare spherical and cylindrical projections for stereo matching, highlighting the advantages of the cylindrical projection. MCPDepth achieves state-of-the-art performance with an 18.8% reduction in mean absolute error (MAE) for depth on the outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor real-scene dataset 3D60.
    
[^83]: 零样本手术工具在单目视频中使用Segmen Anything Model 2的分割

    Zero-Shot Surgical Tool Segmentation in Monocular Video Using Segment Anything Model 2

    [https://arxiv.org/abs/2408.01648](https://arxiv.org/abs/2408.01648)

    最新一代的图像和视频分割基础模型Segmen Anything Model 2在零样本手术工具分割方面取得了显著进展，尤其是在手术视频中，它能够适应不同手术类型和工具长度的复杂背景，并且性能得到了良好验证。

    

    arXiv:2408.01648v1 公告类型: 交叉  摘要: Segment Anything Model 2 (SAM 2)是用于图像和视频分割的最新一代基础模型。在庞大的Segment Anything Video（SA-V）数据集上进行训练，该数据集包含50.9K视频中的3550万个掩码，SAM 2通过支持各种提示（例如点、框和掩码）的零样本分割，推进了其前身的性能。其强大的零样本性能和高效的内存使用使其在手术工具视频分割方面特别吸引人，尤其是在标签数据稀缺以及手术程序多样化的背景下。在本文中，我们对SAM 2模型在多种手术类型（包括内窥术和显微术）中的零样本视频分割性能进行了评估。我们还对其在包含单工具和多工具（长度不同）的视频上的表现进行了评估，以展示SAM 2在手术领域应用的适用性和有效性。我们发现，SAM 2在零样本手术工具分割方面的性能是可扩展的，并且能够很好地适应手术视频中的复杂背景。在多种手术和工具长度的情况下，SAM 2的性能均得到了良好的验证。

    arXiv:2408.01648v1 Announce Type: cross  Abstract: The Segment Anything Model 2 (SAM 2) is the latest generation foundation model for image and video segmentation. Trained on the expansive Segment Anything Video (SA-V) dataset, which comprises 35.5 million masks across 50.9K videos, SAM 2 advances its predecessor's capabilities by supporting zero-shot segmentation through various prompts (e.g., points, boxes, and masks). Its robust zero-shot performance and efficient memory usage make SAM 2 particularly appealing for surgical tool segmentation in videos, especially given the scarcity of labeled data and the diversity of surgical procedures. In this study, we evaluate the zero-shot video segmentation performance of the SAM 2 model across different types of surgeries, including endoscopy and microscopy. We also assess its performance on videos featuring single and multiple tools of varying lengths to demonstrate SAM 2's applicability and effectiveness in the surgical domain. We found tha
    
[^84]: 利用消费型车辆上GNSS和图像数据进行道路网络估计的创新方法

    Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation

    [https://arxiv.org/abs/2408.01640](https://arxiv.org/abs/2408.01640)

    本文提出了一种利用消费型车辆上的GNSS数据和摄像头图像进行道路图自动创建的方法，该方法不仅提高了创建道路图的准确性，还大大减少了所需的工作量和成本。

    

    arXiv:2408.01640v1 公告类型: 交叉 摘要: 地图对于诸如车辆导航和自动驾驶机器人等众多应用至关重要。这些应用都需要有效的路线规划和定位空间模型。本文解决了为自动驾驶车辆构建道路图的挑战。尽管最近取得了进步，但创建一个准确的道路图仍然需要大量的工作，并且还没有实现完全的自动化。本文的目标是通过使用全球导航卫星系统（GNSS）轨迹和从现代车辆上标准传感器获取的基本图像数据自动且准确地生成此类图。我们通过将问题框架为使用卷积神经网络的语义分割任务来实现这一点，并利用数据中的空间信息。我们还利用数据中的时间信息来解决出现的问题，并且通过预处理和后处理步骤来提高数据的质量。此外，提出了一种新的监督学习框架，它能够从少量标注数据中学习，并去噪数据以减少不相关的边缘和噪点。在车辆收集的数据集上进行的实验验证了所提出方法的有效性和实用性。并通过与现有方法的对比，说明了使用建议的方法可以大幅度减少数据收集和自动化道路图创建过程中的努力和成本。

    arXiv:2408.01640v1 Announce Type: cross  Abstract: Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today's advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data's time 
    
[^85]: JambaTalk:基于混合Transformer-Mamba语言模型的3D说话头像生成

    JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Language Model

    [https://arxiv.org/abs/2408.01627](https://arxiv.org/abs/2408.01627)

    本文介绍了一种名为JambaTalk的3D说话头像生成方法，该方法基于混合Transformer-Mamba语言模型，旨在通过提高动作多样性实现更快的速度，并在与现有最先进模型的比较中取得了出色成绩。

    

    arXiv:2408.01627v1 新闻类型：新 Abstract: 近年来，说话头像生成已成为研究人员关注的焦点。人们投入了大量精力来改进唇同步运动、捕捉面部表情、生成自然头部姿态，以及实现高清视频质量。然而，到目前为止，还没有一个模型能够在所有这些指标上实现均衡。本文旨在使用Jamba，一种混合Transformer-Mamba模型的3D脸部动画。Mamba，一个开创性的结构状态空间模型（SSM）架构，被设计用来解决传统Transformer架构的限制。然而，它也有一些缺点。Jamba融合了Transformer和Mamba方法的优势，提供了一个全面的解决方案。基于Jamba的基本块，我们提出了JambaTalk，以通过多模态整合增强动作的多样性和速度。广泛的实验表明，我们的方法达到了与目前最先进模型相媲美或更优的表现。

    arXiv:2408.01627v1 Announce Type: new  Abstract: In recent years, talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high video quality. However, no single model has yet achieved equivalence across all these metrics. This paper aims to animate a 3D face using Jamba, a hybrid Transformers-Mamba model. Mamba, a pioneering Structured State Space Model (SSM) architecture, was designed to address the constraints of the conventional Transformer architecture. Nevertheless, it has several drawbacks. Jamba merges the advantages of both Transformer and Mamba approaches, providing a holistic solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and speed through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
    
[^86]: MedUHIP: 迈向人类参与的医疗分割

    MedUHIP: Towards Human-In-the-Loop Medical Segmentation

    [https://arxiv.org/abs/2408.01620](https://arxiv.org/abs/2408.01620)

    MedUHIP是一个结合了不确定性感知模型与人类参与循环的医疗分割新方法，旨在从具有不确定性的医疗图像中实现定量分割，并在临床实践中通过与医生的互动不断改进分割结果。

    

    虽然分割自然图像已经展示了令人印象深刻的性能，但这些技术不能直接应用于医疗图像分割。由于固有不确定性的存在，医疗图像分割尤其复杂。例如，组织边界的不明确性可能导致不同医生之间存在多样的但合理的标注。这些不确定性在临床解释上造成了显著差异，并影响了随后的医疗干预。因此，从具有不确定性的医疗图像中实现定量分割在临床实践中变得至关重要。为了解决这个问题，我们提出了一种新的方法，该方法结合了“不确定性感知模型”和“人类参与的循环”互动。不确定性感知模型提出了几种可能的分割以应对医疗图像固有的不确定性，而人类参与的循环互动则在临床实践中不断对分割结果进行迭代修改。

    arXiv:2408.01620v1 Announce Type: cross  Abstract: Although segmenting natural images has shown impressive performance, these techniques cannot be directly applied to medical image segmentation. Medical image segmentation is particularly complicated by inherent uncertainties. For instance, the ambiguous boundaries of tissues can lead to diverse but plausible annotations from different clinicians. These uncertainties cause significant discrepancies in clinical interpretations and impact subsequent medical interventions. Therefore, achieving quantitative segmentations from uncertain medical images becomes crucial in clinical practice. To address this, we propose a novel approach that integrates an \textbf{uncertainty-aware model} with \textbf{human-in-the-loop interaction}. The uncertainty-aware model proposes several plausible segmentations to address the uncertainties inherent in medical images, while the human-in-the-loop interaction iteratively modifies the segmentation under clinici
    
[^87]: 深度学习与OBIA的结合：任务、挑战、策略和视野

    Deep Learning Meets OBIA: Tasks, Challenges, Strategies, and Perspectives

    [https://arxiv.org/abs/2408.01607](https://arxiv.org/abs/2408.01607)

    本文详细阐述了深度学习在遥感领域中的OBIA任务、面临的挑战以及应对策略，并提出了未来的研究方向。

    

    arXiv:2408.01607v1 公告类型：新  摘要：在遥感领域，特别是像素级或块级应用中，深度学习取得了显著的关注。尽管已经尝试将深度学习融入到对象为基础图像分析（OBIA）中，但它的潜力尚未完全开发。本文对遥感领域中OBIA的使用日益广泛，我们进行了全面的研究和拓展，包括其任务子领域，无论是否整合了深度学习。此外，我们已识别和总结了五种流行的策略来应对在OBIA中直接处理对象数据的挑战，本文的回顾还提出了未来值得注意的研究方向。我们的目标是激发在这个有趣的但被忽视的领域中更多的探索，并促进深度学习在OBIA处理流程中更加广泛的使用。

    arXiv:2408.01607v1 Announce Type: new  Abstract: Deep learning has gained significant attention in remote sensing, especially in pixel- or patch-level applications. Despite initial attempts to integrate deep learning into object-based image analysis (OBIA), its full potential remains largely unexplored. In this article, as OBIA usage becomes more widespread, we conducted a comprehensive review and expansion of its task subdomains, with or without the integration of deep learning. Furthermore, we have identified and summarized five prevailing strategies to address the challenge of deep learning's limitations in directly processing unstructured object data within OBIA, and this review also recommends some important future research directions. Our goal with these endeavors is to inspire more exploration in this fascinating yet overlooked area and facilitate the integration of deep learning into OBIA processing workflows.
    
[^88]: 基于摄像机模型的自监督深度估计方法

    Self-Supervised Depth Estimation Based on Camera Models

    [https://arxiv.org/abs/2408.01565](https://arxiv.org/abs/2408.01565)

    该论文提出了一种基于摄像机模型的新方法，利用摄像机内部结构的信息来提高深度估计的质量，无需额外的数据和成本。

    

    arXiv:2408.01565v1 公告类型：新 Abstract: 对于机器人学和与视觉相关的任务而言，深度估计是一个至关重要的课题。在单目深度估计中，相较于需要昂贵地面真实标签的有监督学习，由于不需要任何标签成本，自监督方法具有巨大的潜力。然而，自监督学习在深度估计性能上仍然存在较大差距。同时，对于单目无监督深度估计而言，尺度问题是另一个主要问题。通常情况下，为了纠正深度估计，单目无监督深度估计方法仍然需要来自GPS、LiDAR或其他现有地图的地面真实尺度数据。在深度学习时代，虽然现有的方法主要依赖于探索图像之间的关系，以训练无监督的神经网络，但摄像机自身提供的基本信息被普遍忽略，这可以为深度估计提供免费的监督信息，而不需要任何额外的设备提供监督信号。利用摄像机自身的内部结构，我们提出了一种新的深度估计方法，这种方法利用了摄像机模型的信息来改进深度估计的质量，无须额外的数据和成本。我们的方法通过对比不同场景下摄像机模型的预测与实际图像之间的关系，进行深度估计的学习和改进，从而在保持性能的同时降低了成本。此外，这种方法还可以应用于各种不同的场景，适用于复杂的真实世界环境。

    arXiv:2408.01565v1 Announce Type: new  Abstract: Depth estimationn is a critical topic for robotics and vision-related tasks. In monocular depth estimation, in comparison with supervised learning that requires expensive ground truth labeling, self-supervised methods possess great potential due to no labeling cost. However, self-supervised learning still has a large gap with supervised learning in depth estimation performance. Meanwhile, scaling is also a major issue for monocular unsupervised depth estimation, which commonly still needs ground truth scale from GPS, LiDAR, or existing maps to correct. In deep learning era, while existing methods mainly rely on the exploration of image relationships to train the unsupervised neural networks, fundamental information provided by the camera itself has been generally ignored, which can provide extensive supervision information for free, without the need for any extra equipment to provide supervision signals. Utilizing the camera itself's int
    
[^89]: 使用带有合成数据和全局置信度评分的深度学习模型加速领域特定电子显微镜分析

    Accelerating Domain-Aware Electron Microscopy Analysis Using Deep Learning Models with Synthetic Data and Image-Wide Confidence Scoring

    [https://arxiv.org/abs/2408.01558](https://arxiv.org/abs/2408.01558)

    使用合成数据和全局置信度评分的深度学习模型，本研究有效提高了电子显微镜分析的领域特定性，减少了人类标注的依赖，并提升了分析的效率和可靠性。

    

    arXiv:2408.01558v1 公告类型：新发布 摘要：机器学习（ML）模型的引入改善了显微成像特征检测的效率、成本效益和可靠性，但其发展和适用性受限于对稀缺且常常有缺陷的人工标注数据的依赖，以及对领域意识的缺乏。通过创建一个基于物理的合成图像和数据生成器，我们开发了一个机器学习模型，其达到了与在人类标注数据上训练的模型相似的精确度（0.86）、召回率（0.63）、F1分数（0.71）以及工程属性预测（R2=0.82）。我们通过使用特征预测置信度分数来增强这两种模型，得到了一个全局置信度指标，能允许简单的阈值设置来消除不明确和不属于领域的图像，从而在不丢弃25%图像的情况下提高性能达5至30%。我们的研究显示合成数据可以消除机器学习中对人类的依赖，并提供了一种有效的数据技术，能够显著提升领域特定电子显微镜的图像分析效率。

    arXiv:2408.01558v1 Announce Type: new  Abstract: The integration of machine learning (ML) models enhances the efficiency, affordability, and reliability of feature detection in microscopy, yet their development and applicability are hindered by the dependency on scarce and often flawed manually labeled datasets and a lack of domain awareness. We addressed these challenges by creating a physics-based synthetic image and data generator, resulting in a machine learning model that achieves comparable precision (0.86), recall (0.63), F1 scores (0.71), and engineering property predictions (R2=0.82) to a model trained on human-labeled data. We enhanced both models by using feature prediction confidence scores to derive an image-wide confidence metric, enabling simple thresholding to eliminate ambiguous and out-of-domain images resulting in performance boosts of 5-30% with a filtering-out rate of 25%. Our study demonstrates that synthetic data can eliminate human reliance in ML and provides a 
    
[^90]: 增强的膝关节运动学：利用深度学习和变形算法为3D植入模型建模

    Enhanced Knee Kinematics: Leveraging Deep Learning and Morphing Algorithms for 3D Implant Modeling

    [https://arxiv.org/abs/2408.01557](https://arxiv.org/abs/2408.01557)

    论文提出了一种利用机器学习和变形技术自动且精确重建术前膝关节植入模型的方法，极大地方便了手术规划和生物力学评估。

    

    arXiv:2408.01557v1 通告类型：交叉  翻译摘要: 精确重建植入膝模型在骨科手术和生物医学工程中至关重要，这增强了术前规划，优化了植入设计，并改善了手术结果。传统方法依赖于费时且易出错的手动分割。本研究提出了一种新的方法，使用机器学习（ML）算法和变形技术来进行精确的3D植入膝模型重建。

    arXiv:2408.01557v1 Announce Type: cross  Abstract: Accurate reconstruction of implanted knee models is crucial in orthopedic surgery and biomedical engineering, enhancing preoperative planning, optimizing implant design, and improving surgical outcomes. Traditional methods rely on labor-intensive and error-prone manual segmentation. This study proposes a novel approach using machine learning (ML) algorithms and morphing techniques for precise 3D reconstruction of implanted knee models.   The methodology begins with acquiring preoperative imaging data, such as fluoroscopy or X-ray images of the patient's knee joint. A convolutional neural network (CNN) is then trained to automatically segment the femur contour of the implanted components, significantly reducing manual effort and ensuring high accuracy.   Following segmentation, a morphing algorithm generates a personalized 3D model of the implanted knee joint, using the segmented data and biomechanical principles. This algorithm conside
    
[^91]: 通过GAN基于的无监督操作的多任务SAR图像处理

    Multi-task SAR Image Processing via GAN-based Unsupervised Manipulation

    [https://arxiv.org/abs/2408.01553](https://arxiv.org/abs/2408.01553)

    本文提出了一种名为GUE的SAR图像处理框架，利用GAN在无监督的情况下处理图像，解决了语义方向在GAN潜在空间的分离和标记数据缺失的问题。

    

    arXiv:2408.01553v1 公告类型: 新 Abstract: 生成对抗网络(GANs)在通过学习数据分布中的模式合成大量逼真的SAR图像方面显示出了巨大的潜力。一些GAN能够在未引入任何标记数据的情况下实现图像编辑，展示了对SAR图像处理的关键作用。与传统SAR图像处理方法相比，基于GAN latent空间的控制是完全无监督的，允许在没有任何标签的情况下进行图像处理。此外，从数据中提取的信息更具可解释性。本文提出了一个名为GAN-based Unsupervised Editing (GUE)的SAR图像处理框架，旨在解决以下两个问题：(1) 在GAN的潜在空间中分离出语义方向并找到有意义的方向；(2) 建立一个全面的SAR图像处理框架，并实现多个图像处理功能。在实现的

    arXiv:2408.01553v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) have shown tremendous potential in synthesizing a large number of realistic SAR images by learning patterns in the data distribution. Some GANs can achieve image editing by introducing latent codes, demonstrating significant promise in SAR image processing. Compared to traditional SAR image processing methods, editing based on GAN latent space control is entirely unsupervised, allowing image processing to be conducted without any labeled data. Additionally, the information extracted from the data is more interpretable. This paper proposes a novel SAR image processing framework called GAN-based Unsupervised Editing (GUE), aiming to address the following two issues: (1) disentangling semantic directions in the GAN latent space and finding meaningful directions; (2) establishing a comprehensive SAR image processing framework while achieving multiple image processing functions. In the implementation of 
    
[^92]: 自训练点值解码器模块用于点云分割

    Trainable Pointwise Decoder Module for Point Cloud Segmentation

    [https://arxiv.org/abs/2408.01548](https://arxiv.org/abs/2408.01548)

    本文提出的一个自训练解码器模块旨在改善点云分割，尤其是在处理低密度点云时。它能够学习适应性点值转换，预测因投影而丢失的点类别，从而提高性能。

    

    arXiv:2408.01548v1 公告类型：新  摘要：点云分割（PCS）旨在进行点对点预测，并允许机器人和自动驾驶汽车理解其环境。范围图像是大范围室外点云的密集表示，基于图像的分割模型运行效率通常较高。然而，点云投射到范围图像上时，不可避免地会丢失点因为，在每个图像坐标上，尽管多个点被投射到同一个位置，但只有一个点被保留。更重要的是，很难为掉落的点分配正确的预测，这些点属于与保留点类不同的类。此外，现有的一些后处理方法，如K-最近邻（KNN）搜索和核点卷积（KPConv），无法与模型以端到端的方式进行训练，或者无法很好地处理密度不同的室外点云，从而使模型难以达到最佳性能。为此，本文提出一个自训练解码器模块，该模块可以处理具有不同密度的点云，并在保留点周围学习自适应点值之间的转换以预测掉落的点类别，从而大大提高了点云分割的性能。

    arXiv:2408.01548v1 Announce Type: new  Abstract: Point cloud segmentation (PCS) aims to make per-point predictions and enables robots and autonomous driving cars to understand the environment. The range image is a dense representation of a large-scale outdoor point cloud, and segmentation models built upon the image commonly execute efficiently. However, the projection of the point cloud onto the range image inevitably leads to dropping points because, at each image coordinate, only one point is kept despite multiple points being projected onto the same location. More importantly, it is challenging to assign correct predictions to the dropped points that belong to the classes different from the kept point class. Besides, existing post-processing methods, such as K-nearest neighbor (KNN) search and kernel point convolution (KPConv), cannot be trained with the models in an end-to-end manner or cannot process varying-density outdoor point clouds well, thereby enabling the models to achiev
    
[^93]: 守卫图像质量：对图像质量度量防御措施的对抗攻击基准测试

    Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics

    [https://arxiv.org/abs/2408.01541](https://arxiv.org/abs/2408.01541)

    本研究全面评估了25种防御策略，对其在对抗攻击下的表现进行了测试，并分析了对单个防御措施的差异。

    

    在图像质量评估(IQA)领域，指标的对抗鲁棒性是一个关键问题。本文对对抗攻击下的IQA防御策略进行了全面的基准测试。我们系统地评估了25种防御策略，包括对抗纯净度、对抗训练和认证 robustness 方法。在非适应性和适应性设置中，我们应用了14种不同类型的对抗攻击算法，并对这些防御措施进行了测试。我们分析了防御措施之间的差异以及它们在IQA任务中的适用性，考虑到它们应该保持图像质量得分和图像质量。提出的基准测试旨在指导未来的发展，接受新方法的提交，最新的结果可通过在线:https://videoprocessing.ai/benchmarks/iqa-defenses.html获取。

    arXiv:2408.01541v1 Announce Type: new  Abstract: In the field of Image Quality Assessment (IQA), the adversarial robustness of the metrics poses a critical concern. This paper presents a comprehensive benchmarking study of various defense mechanisms in response to the rise in adversarial attacks on IQA. We systematically evaluate 25 defense strategies, including adversarial purification, adversarial training, and certified robustness methods. We applied 14 adversarial attack algorithms of various types in both non-adaptive and adaptive settings and tested these defenses against them. We analyze the differences between defenses and their applicability to IQA tasks, considering that they should preserve IQA scores and image quality. The proposed benchmark aims to guide future developments and accepts submissions of new methods, with the latest results available online: https://videoprocessing.ai/benchmarks/iqa-defenses.html.
    
[^94]: SceneMotion: 从Agent-Centric嵌入到场景级预测

    SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts

    [https://arxiv.org/abs/2408.01537](https://arxiv.org/abs/2408.01537)

    SceneMotion模型通过使用一个新的潜在上下文模块将局部agent-centric嵌入转换为全景级预测，在Waymo Open Interaction Prediction Challenge中表现出色。

    

    arXiv:2408.01537v1 公告类型: cross

    arXiv:2408.01537v1 Announce Type: cross  Abstract: Self-driving vehicles rely on multimodal motion forecasts to effectively interact with their environment and plan safe maneuvers. We introduce SceneMotion, an attention-based model for forecasting scene-wide motion modes of multiple traffic agents. Our model transforms local agent-centric embeddings into scene-wide forecasts using a novel latent context module. This module learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling. The competitive performance in the Waymo Open Interaction Prediction Challenge demonstrates the effectiveness of our approach. Moreover, we cluster future waypoints in time and space to quantify the interaction between agents. We merge all modes and analyze each mode independently to determine which clusters are resolved through interaction or result in conflict. Our implementation is available at: https://github.com/kit-mrt/future-motion
    
[^95]: 基于上下文跨模态注意力的音频-视觉深度伪造检测与定位

    Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization

    [https://arxiv.org/abs/2408.01532](https://arxiv.org/abs/2408.01532)

    本文提出了一种基于RNN的音频-视觉深度伪造检测和定位的新框架，通过跨模态注意力和上下文信息的学习，提高检测准确性并定位伪造图像的不自然区域。

    

    arXiv:2408.01532v1 公告类型: 交叉 摘要: 在数字时代，深度伪造和合成媒体的出现对社会政治的完整性和诚信构成了重大威胁。基于多模态操纵的深度伪造，如音频-视觉深度伪造，更加逼真，威胁更大。现有的多模态深度伪造检测器通常基于跨模态数据的异构流融合，如音频和视频信号。然而，数据的异构性在有效融合和因此多模态深度伪造检测中创造了分布模态差距。在本论文中，我们提出了一种基于循环神经网络（RNNs）的新颖多模态注意力框架，用于音频-视觉深度伪造的检测。该提出的框架应用注意力到多模态多序列表示上，并学习它们之间的贡献性特征，用于深度伪造检测和位置识别。通过这种方式，我们的模型不仅能够提高深度伪造检测的准确率，而且还能对伪造图像中的不自然区域进行定位。我们的实验结果表明，与现有的多模态方法相比，我们的模型在多个公开数据集上的性能更为优越。

    arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo
    
[^96]: 论文标题：增强在线道路网络感知与推理的标准定义图谱

    Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps

    [https://arxiv.org/abs/2408.01471](https://arxiv.org/abs/2408.01471)

    我们的研究提出了一种新的混合制图框架，该框架通过利用实时地标数据和标准定义地图，实现了高效的在线高清地图生成。

    

    论文摘要：自动驾驶对于城市和高速公路驾驶通常需要高清地图（HD）来生成导航计划。然而，在成规模生成和维护HD地图时，存在各种挑战。尽管在线制图方法已经开始出现，但它们在动态环境中对于长距离的意义上，其性能受到了重影遮挡的限制。基于这些考虑，我们的工作集中于在开发在线矢量化HD地图表示时，利用轻量级可扩展的先验——标准定义（SD）地图。我们首先检查将轮廓化的SD地图表示集成到各种在线制图架构中的问题。此外，为了识别轻量级的策略，我们将OpenLane-V2数据集与OpenStreetMaps扩展，并评估图形化SD地图表示的好处。设计SD地图集成组件的关键发现是，SD地图编码能够提高在线道路网络感知和推理的效能。由于相关技术的进步，我们的研究提出了一种新的混合制图框架，该框架通过利用实时地标数据和标准定义地图，实现了高效的在线高清地图生成。该框架在开放道路环境中进行了一系列的实验验证，表明了其对于高清地图的实时在线更新具有显著的优势。未来的研究将进一步验证这些结果，并探索如何将其应用到更广泛的道路场景中。

    arXiv:2408.01471v1 Announce Type: cross  Abstract: Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definition (SD) maps-in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encode
    
[^97]: 一种新的神经网络环境成本估计模型及其在适应性生命周期中的应用

    Estimating Environmental Cost Throughout Model's Adaptive Life Cycle

    [https://arxiv.org/abs/2408.01446](https://arxiv.org/abs/2408.01446)

    本文提出了一种预测指数PreIndex，用来估计在模型适应性生命周期的再培训过程中，从当前数据分布到新数据分布的变化中，所涉及的环境成本（如碳排放和能源消耗）。

    

    arXiv:2408.01446v1 公告类型: 交叉  翻译摘要: 随着神经网络在当前时代的快速发展，需要越来越多的能源来训练和使用模型。特别是，这伴随着碳排放到环境中的增加。为了减少人工智能/深度学习时代的碳足迹以及相关的能源需求，我们提出了一种模型再培训的预测指数，PreIndex，以估计与模型再培训相关的环境成本，如碳排放和能源消耗。模型再培训是为了适应数据分布的变化或在数据分布之间的变化/差异。PreIndex指数可以用来估算从当前数据分布到新数据分布的重新训练的成本，它也与Model的适应性生命周期有关。我们的研究旨在为模型的适应性生命周期的设计提供支持，从而帮助降低再培训的成本和环境影响。

    arXiv:2408.01446v1 Announce Type: cross  Abstract: With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with
    
[^98]: 基于聚类的新建筑检查无人机视角规划方法

    A New Clustering-based View Planning Method for Building Inspection with Drone

    [https://arxiv.org/abs/2408.01435](https://arxiv.org/abs/2408.01435)

    本文提出了一种基于聚类的两步计算方法，使用谱聚类、局部潜在场方法和超遗传算法来为无人机建筑检查找到近似最优视角。

    

    arXiv:2408.01435v1 公告类型: 交叉 上线的摘要: 随着无人机技术的迅速发展，配备视觉传感器的无人机在建筑检查和监控方面的应用引起了许多关注。视角规划旨在为基于视线的任务找到一组近似最优的视角，以实现视野覆盖目标。本文提出了一种新的基于聚类的方法，该方法使用谱聚类、局部潜在场方法和超遗传算法来找到覆盖目标建筑表面的近似最优视角。在第一步，该提议的方法基于谱聚类生成候选视角，并基于我们提出的新型局部潜在场方法纠正候选视角的位置。在第二部，我们将优化问题转换为Set Covering Problem (SCP)，并使用我们提出的超遗传算法来解决最佳视角子集的问题。实验结果表明，该减少规划步骤的数量并提高效率的新方法，在建筑检查任务中具有实用性。

    arXiv:2408.01435v1 Announce Type: cross  Abstract: With the rapid development of drone technology, the application of drones equipped with visual sensors for building inspection and surveillance has attracted much attention. View planning aims to find a set of near-optimal viewpoints for vision-related tasks to achieve the vision coverage goal. This paper proposes a new clustering-based two-step computational method using spectral clustering, local potential field method, and hyper-heuristic algorithm to find near-optimal views to cover the target building surface. In the first step, the proposed method generates candidate viewpoints based on spectral clustering and corrects the positions of candidate viewpoints based on our newly proposed local potential field method. In the second step, the optimization problem is converted into a Set Covering Problem (SCP), and the optimal viewpoint subset is solved using our proposed hyper-heuristic algorithm. Experimental results show that the pro
    
[^99]: VLG-CBM：利用视觉语言指导的训练概念瓶颈模型

    VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance

    [https://arxiv.org/abs/2408.01432](https://arxiv.org/abs/2408.01432)

    VLG-CBM是一种新的框架，旨在通过引入视觉语言指导机制，提高概念瓶颈模型的解释性和一致性，并解决概念预测与输入图像不一致的问题。

    

    arXiv:2408.01432v1 公告类型：新  摘要：概念瓶颈模型（CBMs）通过引入一个中间的概念瓶颈层（CBL）提供可解释的预测，该层编码了人类可理解的概念来解释模型的决策。最近的工作提出使用大型语言模型（LLMs）和预训练的视觉语言模型（VLMs）自动训练CBMs，这使得它更加可扩展和自动化。然而，现有方法在两个方面仍有不足：首先，CBL预测的概念经常与输入图像不符，这让人对解释的准确性表示怀疑。其次，已经表明概念值编码了不必要的信息：即使是随机概念集也能获得与当前最先进CBMs相当的测试准确度。为了解决这些关键的局限性，在本工作中，我们提出了一种名为“视觉语言指导的概念瓶颈模型”（VLG-CBM）的全新框架，以实现具有増强解释性的训练，同时享受自动化的收益。通过引入一种新的视觉语言指导机制，VLG-CBM能够确保概念瓶颈层的概念与图像更紧密相关，并增强模型的预测可解释性。此外，我们还发现，概念值中存在冗余信息，这会影响模型的预测决策。通过处理这些额外的信息，VLG-CBM能够进一步提高模型的性能并保证预测解释的一致性和可靠性。我们的实验结果表明，使用VLG-CBM进行训练的模型在保持高准确性的同时，实现了更好的解释性和概念一致性。

    arXiv:2408.01432v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models (LLMs) and pre-trained Vision-Language Models (VLMs) to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boos
    
[^100]: 迁移式对抗性面部图像对隐私保护

    Transferable Adversarial Facial Images for Privacy Protection

    [https://arxiv.org/abs/2408.01428](https://arxiv.org/abs/2408.01428)

    本文提出了一种新的面部隐私保护方案，能在保持高视觉质量的同时提高转移能力。通过直接塑造整个面部空间，而不是仅利用妆容信息等一种特征来整合对抗性噪声，该方案能够在黑盒场景中生成自然且高度可转移的对抗性面部图像。

    

    arXiv:2408.01428v1 宣布类型：新 摘要：由于深度人脸识别（FR）系统的成功，人们对使用这些系统在数字世界中追踪用户的能力给予了高度重视。之前的研究通过在面部图像中引入不可察觉的对抗性噪声来误导这些人脸识别模型，实现了增强面部隐私保护的目标。然而，他们高度依赖于用户选择的参考来指导对抗性噪声的生成，并且在黑盒场景中无法同时创建自然且高度可转移的对抗性面部图像。因此，我们提出了一个全新的面部隐私保护方案，该方案在保持高视觉质量的同时提高了转移能力。我们提议直接塑造整个面部空间，而不是利用类似于妆容信息的一种面部特征来整合对抗性噪声。为实现这一目标，我们首先利用全局对抗性潜变量搜索

    arXiv:2408.01428v1 Announce Type: new  Abstract: The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent sear
    
[^101]: Siamese Transformer 网络对于少样本图像分类

    Siamese Transformer Networks for Few-shot Image Classification

    [https://arxiv.org/abs/2408.01427](https://arxiv.org/abs/2408.01427)

    本研究提出了基于Siamese Transformer的网络，通过提取全局和局部特征，结合了Euclidean distance measure，在少样本图像分类任务中取得了显著提升。

    

    arXiv:2408.01427v1 宣布类型: 新 摘要: 人类在视觉分类任务中表现出非凡的效率，能够准确识别和分类新图像，需要的示例很少。这种能力归功于他们能够专注于细节并识别之前看到的和新图像之间的共同特征。相比之下，现有的少样本图像分类方法往往侧重于全局特征或局部特征，很少有研究考虑这两种特征的整合。为了解决这个限制，我们提出了一种基于 Siamese Transformer 网络(STN)的新方法。我们的方法使用两个并行分支网络，利用预训练的视觉Transformer(ViT)架构来提取全局和局部特征。特别是，我们实现了一个使用ViT-Small网络架构的网络，并使用通过自我监督学习获得预训练的模型参数对分支网络进行初始化。我们采用了欧氏距离度量来对全局特征和局部特征进行量化，并将两者结合起来，以提高少数样本图像分类的性能。在多个公开数据集上的实验结果表明，与现有的先进方法相比，我们的方法在相应少样本和单一样本条件下均取得了显著的提升。

    arXiv:2408.01427v1 Announce Type: new  Abstract: Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global featur
    
[^102]: 这里是翻译过的论文标题

    Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs

    [https://arxiv.org/abs/2408.01355](https://arxiv.org/abs/2408.01355)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    arXiv:2408.01355v1 Announce Type: new  Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable performance on various visual-language understanding and generation tasks. However, MLLMs occasionally generate content inconsistent with the given images, which is known as "hallucination". Prior works primarily center on evaluating hallucination using standard, unperturbed benchmarks, which overlook the prevalent occurrence of perturbed inputs in real-world scenarios-such as image cropping or blurring-that are critical for a comprehensive assessment of MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI, the first benchmark designed to evaluate Hallucination in MLLMs within Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios, containing 1,260 perturbed images from 11 object types. Each image is accompanied by detailed annotations, which include fine-grained hallucination types, such as existence, attribute, and rel
    
[^103]: 这里是翻译过的论文标题

    CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression

    [https://arxiv.org/abs/2408.00938](https://arxiv.org/abs/2408.00938)

    本研究提出了一种基于临床知识改进的扩散模型，用于更准确地预测特发性肺纤维化（IPF）的进展。

    

    这里是翻译过的论文摘要

    arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
    
[^104]: 逆向扩展：最小合成预训练？

    Scaling Backwards: Minimal Synthetic Pre-training?

    [https://arxiv.org/abs/2408.00677](https://arxiv.org/abs/2408.00677)

    论文发现即使在最基本的合成图像上进行预训练也能达到与大型数据集类似的性能，证明了预训练即使在假想情境下也有效。

    

    arXiv:2408.00677v2 公告类型：替换 摘要：当前计算机视觉系统的一个重要组成部分是预训练和转移学习。虽然预训练通常在大型真实世界图像数据集上进行，但在这篇论文中，我们提出了一个问题：这真的是必要的吗？为此，我们寻找了一个最小化的、纯粹的合成预训练数据集，该数据集允许我们获得与ImageNet-1k类似的性能。我们从一个单一的几何图形中构造了这样一个数据集，并对它进行了扰动。通过这种方式，我们对三个主要发现做出了贡献。(i) 我们展示了即使是在最小的合成图像上进行预训练，对于完全的微调，性能也与像ImageNet-1k这样的大型预训练数据集相当。(ii) 我们研究了我们构造人工类别所需的唯一参数。我们发现，尽管形状差异可能对人类来说难以区分，但对于获得强大的性能至关重要。(iii) 最后，我们研究了如何在合成预训练数据集上使用相同的模型Zero-shot Learning也能取得良好的表现。

    arXiv:2408.00677v2 Announce Type: replace  Abstract: Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we inve
    
[^105]: MSA²Net: 针对医学图像分割的多尺度自适应注意力导向网络

    MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation

    [https://arxiv.org/abs/2407.21640](https://arxiv.org/abs/2407.21640)

    MSA²Net 是一种新的深度分割框架，它通过设计快捷的跳转连接，解决了医学图像分割中对本地和全局特征进行有效融合的问题，以处理结构变化，促进特征的动态加权和组合。

    

    arXiv:2407.21640v2 公告类型：替换交叉  翻译摘要：医学图像分割涉及在医学图像中识别和分离对象实例，以绘制各种组织和结构，这项任务因这些特征在大小、形状和密度上的显著差异而变得复杂。传统的卷积神经网络（CNNs）被用于这项任务，但它们在捕获长距离相关方面存在局限性。变换器，配备有自注意力机制，旨在解决这一问题。然而，在医学图像分割中，合并局部和全局特征是有益的，以有效地在各种尺度上整合特征映射，捕捉结构变化的详细特征和更广泛的意义要素。在这项研究中，我们提出了MSA²Net，一个新的深层分割框架，具有快捷的设计的跳转连接。这些连接通过动态加权和组合粗细特征，促进了特征融合，从而有效地处理了结构的变化。

    arXiv:2407.21640v2 Announce Type: replace-cross  Abstract: Medical image segmentation involves identifying and separating object instances in a medical image to delineate various tissues and structures, a task complicated by the significant variations in size, shape, and density of these features. Convolutional neural networks (CNNs) have traditionally been used for this task but have limitations in capturing long-range dependencies. Transformers, equipped with self-attention mechanisms, aim to address this problem. However, in medical image segmentation it is beneficial to merge both local and global features to effectively integrate feature maps across various scales, capturing both detailed features and broader semantic elements for dealing with variations in structures. In this paper, we introduce MSA$^2$Net, a new deep segmentation framework featuring an expedient design of skip-connections. These connections facilitate feature fusion by dynamically weighting and combining coarse-
    
[^106]: 在整个人体运动生成中添加多模态控制

    Adding Multimodal Controls to Whole-body Human Motion Generation

    [https://arxiv.org/abs/2407.21136](https://arxiv.org/abs/2407.21136)

    我们提出了一种名为ControlMM的框架，它使用统一的模型来同时控制整个身体的运动生成，并处理不同的条件模式，包括文本、语音和音乐。ControlMM解决了运动分布在不同生成任务中的偏移问题，同时通过使用一种粗粒度的方法来处理不同粒度的条件。

    

    arXiv:2407.21136v2 新闻类型：替换 摘要：整个人体多模态运动生成，通过文本，语音或音乐控制，具有多种应用，包括视频生成和角色动画。然而，使用统一模型来实现不同条件模式的各种生成任务有两个主要挑战：在不同生成场景之间运动分布会发生偏移，并且对于具有不同粒度的混合条件进行优化是一个复杂的问题。此外，现有数据集中不一致的运动格式进一步阻碍了有效多模态运动生成。在此论文中，我们提出ControlMM，一个统一框架，用于在多模态运动生成的可插拔方式中控制整个人体运动。为了有效地学习和在不同的运动分布之间转移运动知识，我们提出了ControlMM-Attn，用于并行建模静态和动态的人类拓扑图。为了处理具有不同粒度的条件，ControlMM employs a coarse-grained

    arXiv:2407.21136v2 Announce Type: replace  Abstract: Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to accomplish various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different generation scenarios and the complex optimization of mixed conditions with varying granularity. Furthermore, inconsistent motion formats in existing datasets further hinder effective multimodal motion generation. In this paper, we propose ControlMM, a unified framework to Control whole-body Multimodal Motion generation in a plug-and-play manner. To effectively learn and transfer motion knowledge across different motion distributions, we propose ControlMM-Attn, for parallel modeling of static and dynamic human topology graphs. To handle conditions with varying granularity, ControlMM employs a coarse-
    
[^107]: EAR: 边缘感知法从双平面X射线图像重建3D椎体结构

    EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from bi-planar X-ray images

    [https://arxiv.org/abs/2407.20937](https://arxiv.org/abs/2407.20937)

    EAR网络通过自编码器架构的改进，加强了边缘信息的感知并提高了椎体形状的重建精度。

    

    arXiv:2407.20937v2 公告类型: 替换交叉Abstract: X射线摄影因其快速的成像速度和较高分辨率而使诊断和治疗过程变得更为简便。然而，由于X射线摄影的投影性质，图像空间中丢失了大量的空间信息。准确提供脊椎的形态结构信息至关重要，这要求从2D X射线图像重建出3D结构的脊椎。由于椎体的不对称结构，现有的重建方法在保留边缘信息和形状方面面临着巨大挑战。本文提出了一种新的边缘感知重建网络（EAR），旨在提升边缘信息和椎体形状的重建性能。在我们的网络中，通过使用自编码器架构作为核心，添加了边缘注意力模块和频谱增强模块，以增强边缘重建的能力。同时还结合了四个损失项，包括重建损失、边缘损失、频率损失和节点损失，以提高重建质量。在我们的模型中，还集成了对抗损失来改进重建结构的细化程度，并对椎体的局部几何结构进行了保护。实验结果表明，与传统方法相比，EAR在边缘信息和椎体形状的保留方面取得了显著的性能提升。

    arXiv:2407.20937v2 Announce Type: replace-cross  Abstract: X-ray images ease the diagnosis and treatment process due to their rapid imaging speed and high resolution. However, due to the projection process of X-ray imaging, much spatial information has been lost. To accurately provide efficient spinal morphological and structural information, reconstructing the 3-D structures of the spine from the 2-D X-ray images is essential. It is challenging for current reconstruction methods to preserve the edge information and local shapes of the asymmetrical vertebrae structures. In this study, we propose a new Edge-Aware Reconstruction network (EAR) to focus on the performance improvement of the edge information and vertebrae shapes. In our network, by using the auto-encoder architecture as the backbone, the edge attention module and frequency enhancement module are proposed to strengthen the perception of the edge reconstruction. Meanwhile, we also combine four loss terms, including reconstruc
    
[^108]: 多视图姿态精度评价的稳健度量分数

    Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation

    [https://arxiv.org/abs/2407.20391](https://arxiv.org/abs/2407.20391)

    本研究提出了一种稳健的度量方法，用于评估相机姿态估计的准确性，该方法旨在识别和纠正姿态估计中的误差，通过构建一个可靠的评估框架，帮助提高图像序列中相机位置和方向的精确度。

    

    我们提出三种新的度量方法，用于评估给定地面真值的情况下估计相机姿态集的准确性。这三种度量方法是翻译对齐分数（TAS）、旋转对齐分数（RAS）和姿态对齐分数（PAS）。TAS独立评估翻译的准确性，而RAS独立评估旋转的准确性，PAS则是两者分数的平均值，用于评估翻译和旋转的整体准确性。TAS的计算步骤如下：（1）找到最近对距离的上四分位数，d。（2）使用稳健注册方法对估计的轨迹进行对齐，使其与地面真值对齐。（3）收集所有距离误差，并获取从0.01d到d的多个阈值范围内的累积频数，分辨率是0.01d。（4）将这些累积频数相加并对其进行归一化处理，确保理论上的最大值是1。TAS在实践中具有很高的稳健性和可靠性，可以帮助我们对相机姿态估计的结果进行更客观的评价。

    arXiv:2407.20391v2 Announce Type: replace-cross  Abstract: We propose three novel metrics for evaluating the accuracy of a set of estimated camera poses given the ground truth: Translation Alignment Score (TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS evaluates the translation accuracy independently of the rotations, and the RAS evaluates the rotation accuracy independently of the translations. The PAS is the average of the two scores, evaluating the combined accuracy of both translations and rotations. The TAS is computed in four steps: (1) Find the upper quartile of the closest-pair-distances, $d$. (2) Align the estimated trajectory to the ground truth using a robust registration method. (3) Collect all distance errors and obtain the cumulative frequencies for multiple thresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up these cumulative frequencies and normalize them such that the theoretical maximum is 1. The TAS has practical
    
[^109]: 革命性城市安全感知评估的新方法：将多模态大型语言模型与街景图像集成

    Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images

    [https://arxiv.org/abs/2407.19719](https://arxiv.org/abs/2407.19719)

    该论文提出了一种使用多模态大型语言模型和街景图像来评估城市安全的新方法，旨在自动化大规模的安全评估过程，提高效率和准确性。

    

    我们谈到的论文提供了一种革命性的城市安全感知评估方法，该方法整合了多模态大型语言模型与街景图像。这些街景图像可以适用于安全检测，并且能够在不依赖于依靠人类资源的情况下大规模地进行安全检测。该模型在理论和实践上都取得了重大突破，有望彻底改变城市安全评估的方法论。

    arXiv:2407.19719v2 Announce Type: replace  Abstract: Measuring urban safety perception is an important and complex task that traditionally relies heavily on human resources. This process often involves extensive field surveys, manual data collection, and subjective assessments, which can be time-consuming, costly, and sometimes inconsistent. Street View Images (SVIs), along with deep learning methods, provide a way to realize large-scale urban safety detection. However, achieving this goal often requires extensive human annotation to train safety ranking models, and the architectural differences between cities hinder the transferability of these models. Thus, a fully automated method for conducting safety evaluations is essential. Recent advances in multimodal large language models (MLLMs) have demonstrated powerful reasoning and analytical capabilities. Cutting-edge models, e.g., GPT-4 have shown surprising performance in many tasks. We employed these models for urban safety ranking o
    
[^110]: VersusDebias: 通用的零样本文本到图像模型的基于小语言模型的提示工程和生成对手的偏激消除

    VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary

    [https://arxiv.org/abs/2407.19524](https://arxiv.org/abs/2407.19524)

    我们提出了一种通用的文本到图像模型偏激消除框架VersusDebias，使用小语言模型和生成对手机制来减少偏激想象的影响，并通过提示工程生成无偏的提示。

    

    arXiv:2407.19524v2 公告类型：替换 摘要：随着文本到图像（T2I）模型的快速发展，人类图像生成中对某些社会群体的偏见越来越受到关注。现有方法仅基于特定模型的固定提示，无法适应T2I模型更新速度快和实际场景中提示多样性的趋势。此外，它们还不能考虑到想象的风险，导致预期结果和实际结果之间存在偏差。为了解决这个问题，我们引入了VersusDebias，这是一个针对T2I模型中偏见的全新且通用的偏激消除框架，它由一个生成对手机制（GAM）和一个使用小语言模型的偏激消除生成机制组成。自适应的GAM为每个提示生成专用的属性数组，以减少T2I模型中想象的影响。SLM使用提示工程为T2I模型生成无偏提示。

    arXiv:2407.19524v2 Announce Type: replace  Abstract: With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I
    
[^111]: 通过从2D变压器中提炼关系先验知识来提升跨域点分类性能

    Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers

    [https://arxiv.org/abs/2407.18534](https://arxiv.org/abs/2407.18534)

    本文提出的RPD方法通过蒸馏机制让3D网络从预训练的2D视觉网络中学习关系先验知识，提高了其在跨域点分类任务上的性能。

    

    arXiv:2407.18534v2 公告类型：替换摘要：对象点云的语义模式是由其局部几何配置决定的。学习区分性表示在局部区域形状多变和全局视角下表面不完备的情况下可能会非常困难，这在无监督域调整（UDA）的背景下更是如此。具体来说，传统的3D网络主要集中在局部几何细节上，忽视了局部几何之间的拓扑结构，这极大地限制了它们在跨域泛化方面的性能。最近，基于视觉的变压器模型在各种图像任务中取得了显著的性能提升，得益于它强大的概括能力和可扩展性，这些能力源于在局部图斑间捕捉长距离的相关性。受到此类视觉变压器成功经验的启发，我们提出了一种新颖的“关系先验知识提炼”（RPD）方法，以提取点云样本之间的关系先验知识，从而使其在跨域学习中更加稳健。我们的方法首先利用一系列预训练的2D视觉网络对关系先验进行编码，然后将编码的信息通过蒸馏机制传递给3D网络。通过这种方式，3D网络可以从2D网络中“学习”到关系先验，从而提升其对不同域数据集的点分类任务的适应性。

    arXiv:2407.18534v2 Announce Type: replace  Abstract: Semantic pattern of an object point cloud is determined by its topological configuration of local geometries. Learning discriminative representations can be challenging due to large shape variations of point sets in local regions and incomplete surface in a global perspective, which can be made even more severe in the context of unsupervised domain adaptation (UDA). In specific, traditional 3D networks mainly focus on local geometric details and ignore the topological structure between local geometries, which greatly limits their cross-domain generalization. Recently, the transformer-based models have achieved impressive performance gain in a range of image-based tasks, benefiting from its strong generalization capability and scalability stemming from capturing long range correlation across local patches. Inspired by such successes of visual transformers, we propose a novel Relational Priors Distillation (RPD) method to extract relat
    
[^112]: MARINE: 一种用于在动物视频中检测罕见捕食者-猎物交互的计算机视觉模型

    MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos

    [https://arxiv.org/abs/2407.18289](https://arxiv.org/abs/2407.18289)

    模型MARINE采用了高效的动作识别技术，成功地识破了动物视频中罕见捕食者-猎物的交互行为，在多个数据集上表现优异，显示出其在动物行为分析平台中的潜在应用价值。

    

    arXiv:2407.18289v2 公告类型：替换

    arXiv:2407.18289v2 Announce Type: replace  Abstract: Encounters between predator and prey play an essential role in ecosystems, but their rarity makes them difficult to detect in video recordings. Although advances in action recognition (AR) and temporal action detection (AD), especially transformer-based models and vision foundation models, have achieved high performance on human action datasets, animal videos remain relatively under-researched. This thesis addresses this gap by proposing the model MARINE, which utilizes motion-based frame selection designed for fast animal actions and DINOv2 feature extraction with a trainable classification head for action recognition. MARINE outperforms VideoMAE in identifying predator attacks in videos of fish, both on a small and specific coral reef dataset (81.53\% against 52.64\% accuracy), and on a subset of the more extensive Animal Kingdom dataset (94.86\% against 83.14\% accuracy). In a multi-label setting on a representative sample of Anim
    
[^113]: 通过知识蒸馏利用基础模型在多目标跟踪中的创新：将DINOv2特征蒸馏到FairMOT

    Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT

    [https://arxiv.org/abs/2407.18288](https://arxiv.org/abs/2407.18288)

    本文通过知识蒸馏利用了DINOv2基础模型，将其特征蒸馏到FairMOT中，虽然取得了一定的改进，但仍需要更多研究来验证其实际应用的有效性。

    

    arXiv:2407.18288v2 公告类型：替换  摘要：多目标跟踪（MOT）是一种计算机视觉任务，已经在许多领域得到了应用。MOT的一些常见局限性包括对象外观的变化、遮挡或拥挤的场景。为了应对这些挑战，机器学习方法被大量采用，并利用了大量的数据集、复杂的模型和大量的计算资源。由于实际限制，不一定总能获得上述资源。然而，随着知名人工智能公司最近发布的预训练的基础模型，这些模型已经在使用最先进的方法对庞大的数据集进行了训练。这项工作尝试通过知识蒸馏来利用这样一个基础模型，即DINOv2。提出的策略利用了一种老师-学生架构，其中DINOv2作为老师，FairMOT的后端HRNetv2 W18作为学生。结果表明，尽管提出的策略显示出改进，但仍需进一步研究以验证其在实际应用中的有效性。

    arXiv:2407.18288v2 Announce Type: replace  Abstract: Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements 
    
[^114]: 基于空间时间图的多模态网络直播产品检索

    Spatiotemporal Graph Guided Multi-modal Network for Livestreaming Product Retrieval

    [https://arxiv.org/abs/2407.16248](https://arxiv.org/abs/2407.16248)

    该研究提出了一种名为SGMN的模型，利用文本指导注意力机制和视频空间时间信息，有效地捕捉并识别直播中的产品特征，为未来的实时产品识别与推荐提供支持。

    

    arXiv:2407.16248v3 通告类型: 替换 Abstract: 在电子商务快速扩张的背景下，越来越多的消费者习惯于通过直播进行购物。准确识别销售人员正在销售的产品，即直播产品检索（LPR），是一项基础且艰巨的挑战。LPR任务在现实场景中面临三个主要难题：1）从背景中的干扰产品中识别出意图产品；2）直播中展示的产品外观常常与商店中的标准化产品图片存在显著差异；3）商店中有许多视觉细微差别的参考产品。为了解决这些挑战，我们提出了基于空间时间图的多模态网络（SGMN）。首先，我们采用了文本指导注意力机制，利用销售人员的口头内容来指导模型将注意力集中在意图产品上，强调它们在静态图像中的重要特征和形态信息。同时，视频流内的产品空间和时间信息也被提取和利用，使得网络能够根据产品在视频流中的动态变化捕捉产品回放的特征。最后，我们设计了相应的损失函数，综合考虑了视频流中产品的外观特征、位置信息以及文本描述。通过大量的直播视频数据进行训练，SGMN旨在有效地捕捉和识别直播中的产品特征，为未来的实时产品识别与推荐提供关键的技术支持。

    arXiv:2407.16248v3 Announce Type: replace  Abstract: With the rapid expansion of e-commerce, more consumers have become accustomed to making purchases via livestreaming. Accurately identifying the products being sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a fundamental and daunting challenge. The LPR task encompasses three primary dilemmas in real-world scenarios: 1) the recognition of intended products from distractor products present in the background; 2) the video-image heterogeneity that the appearance of products showcased in live streams often deviates substantially from standardized product images in stores; 3) there are numerous confusing products with subtle visual nuances in the shop. To tackle these challenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN). First, we employ a text-guided attention mechanism that leverages the spoken content of salespeople to guide the model to focus toward intended products, emphasizing their s
    
[^115]: 自适应PointFormer: 通过适应2D视觉变换器分析3D点云

    Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual Transformers

    [https://arxiv.org/abs/2407.13200](https://arxiv.org/abs/2407.13200)

    该研究提出了一种自适应PointFormer（APF），通过微调2D视觉模型直接处理3D点云，从而无需将点云映射到图像上。这种方法可以有效学习低维空间中的点云表示，同时显著提高现有方法性能。

    

    arXiv:2407.13200v2 公告类型: 替换 摘要: 预训练的大型模型在计算机视觉领域表现出了惊人的有效性，特别是在2D图像分析方面。然而，当涉及到3D点云时，与图像的大量数据库相比，数据访问的局限性为开发3D预训练模型带来了挑战。因此，本研究试图直接利用具有2D先验知识的预训练模型来完成3D点云分析的任务。因此，我们提出了自适应PointFormer（APF），它仅使用少量参数对预训练的2D模型进行微调，以便直接处理点云，无需映射到图像上。具体来说，我们将原始点云转换为点嵌入，以与图像符号的维度对齐。考虑到点云的内在无序性与图像的结构性质之间的差异，我们接着对点嵌入进行序列化，以优化2D注意力机制的使用。我们还采用了一种新颖的多尺度自回归生成网络来辅助生成细化后的点云，并利用正则化策略来减少稀疏性，从而提升模型性能。在各类点云数据集上的实验表明，APF不仅能有效地学习低维空间中的点云表示，同时还能显著提高FD²-Net等现有方法的性能。

    arXiv:2407.13200v2 Announce Type: replace  Abstract: Pre-trained large-scale models have exhibited remarkable efficacy in computer vision, particularly for 2D image analysis. However, when it comes to 3D point clouds, the constrained accessibility of data, in contrast to the vast repositories of images, poses a challenge for the development of 3D pre-trained models. This paper therefore attempts to directly leverage pre-trained models with 2D prior knowledge to accomplish the tasks for 3D point cloud analysis. Accordingly, we propose the Adaptive PointFormer (APF), which fine-tunes pre-trained 2D models with only a modest number of parameters to directly process point clouds, obviating the need for mapping to images. Specifically, we convert raw point clouds into point embeddings for aligning dimensions with image tokens. Given the inherent disorder in point clouds, in contrast to the structured nature of images, we then sequence the point embeddings to optimize the utilization of 2D a
    
[^116]: HPC: 分层渐进式编码框架用于体积视频

    HPC: Hierarchical Progressive Coding Framework for Volumetric Video

    [https://arxiv.org/abs/2407.09026](https://arxiv.org/abs/2407.09026)

    本文提出了一种使用单一模型的分层渐进式体积视频编码框架HPC，可以实现灵活的比特率调整，以适应各种网络和设备能力。

    

    arXiv:2407.09026v2 宣布类型：替换  摘要：基于神经辐射场（NeRF）的体积视频具有多种3D应用的巨大潜力，但其巨大的数据量对于压缩和传输提出了重大挑战。NeRF压缩目前缺乏在单个模型中对视频质量和比特率进行灵活调整的能力，以适应各种网络和设备能力。为了解决这些问题，我们提出了一种新颖的分层渐进式体积视频编码框架，它使用单一模型实现可变比特率压缩。具体来说，HPC引入了一个具有多分辨率残差辐射场的分层表示，以减少长时间序列中的时间冗余，同时产生各种细节级别。然后，我们提出了一个端到端的渐进式学习方法，以及一个多速率失真损失函数，以同时优化层次结构和压缩。我们的HPC只需训练一次，就可以实现多个比特率的编码效率，对于低带宽和低延迟的要求提供了灵活的解决方案。

    arXiv:2407.09026v2 Announce Type: replace  Abstract: Volumetric video based on Neural Radiance Field (NeRF) holds vast potential for various 3D applications, but its substantial data volume poses significant challenges for compression and transmission. Current NeRF compression lacks the flexibility to adjust video quality and bitrate within a single model for various network and device capacities. To address these issues, we propose HPC, a novel hierarchical progressive volumetric video coding framework achieving variable bitrate using a single model. Specifically, HPC introduces a hierarchical representation with a multi-resolution residual radiance field to reduce temporal redundancy in long-duration sequences while simultaneously generating various levels of detail. Then, we propose an end-to-end progressive learning approach with a multi-rate-distortion loss function to jointly optimize both hierarchical representation and compression. Our HPC trained only once can realize multiple
    
[^117]: 数据与多模态大型语言模型协同演进的调查研究

    The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective

    [https://arxiv.org/abs/2407.08583](https://arxiv.org/abs/2407.08583)

    论文讨论了多模态大型语言模型与数据协同发展的关系，表明两者在模型性能和数据质量提升中起到相互促进的作用。

    

    arXiv:2407.08583v2 通告类型：替换交叉

    arXiv:2407.08583v2 Announce Type: replace-cross  Abstract: The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs; on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approache
    
[^118]: 利用可变尺度注意力对小医疗对象进行分割

    Exploiting Scale-Variant Attention for Segmenting Small Medical Objects

    [https://arxiv.org/abs/2407.07720](https://arxiv.org/abs/2407.07720)

    本文提出了一种名为SvANet的新网络，旨在利用可变注意力机制提高在小医疗图片中识别小区域的目标效率。此网络能更精确地分割图片，并且能应用于实际医疗诊断中，从而提高疾病的早期识别率和治疗成功率。

    

    arXiv:2407.07720v4 公告类型：替换交叉 摘要：早期检测和准确诊断可以预测恶性疾病转化风险，从而增加有效治疗的可能性。识别轻微综合征的小病理区域是疾病早期诊断的基础。虽然深度学习算法，特别是卷积神经网络（CNNs），在分割医学对象方面显示出潜力，但对于医学图像中小区域的分析仍然是一个挑战。这种困难源于卷积和池化运算在CNN中丢失的信息和压缩缺陷，尤其是在网络加深时，这对小医疗对象尤为明显。为了解决这些挑战，我们提出了一种新颖的可变尺度注意力网络（SvANet），用于准确分割医学图像中的小尺度对象。SvANet由可变尺度注意力、跨尺度引导和蒙特卡洛优化组成。通过设计可变尺度注意力机制和细化模块，SvANet能够捕捉不同尺度上的对象特徵，并利用深度指导的小尺度特征，提高边缘精度和像素级分割质量。此外，我们还开发了一种实时多尺度网络，该网络可以在不牺牲准确性的情况下，以更快的速度进行实时的图像分割操作。实验结果表明，SvANet在专业诊断中的应用提高了对小肿瘤和小病变的精确检出率，特别是在低对比度和高噪声的医学图像中。我们的方法为目前小规模医学对象分割的研究提供了有效的解决方案，并对数字病理学和医学成像领域有重要的应用价值。

    arXiv:2407.07720v4 Announce Type: replace-cross  Abstract: Early detection and accurate diagnosis can predict the risk of malignant disease transformation, thereby increasing the probability of effective treatment. Identifying mild syndrome with small pathological regions serves as an ominous warning and is fundamental in the early diagnosis of diseases. While deep learning algorithms, particularly convolutional neural networks (CNNs), have shown promise in segmenting medical objects, analyzing small areas in medical images remains challenging. This difficulty arises due to information losses and compression defects from convolution and pooling operations in CNNs, which become more pronounced as the network deepens, especially for small medical objects. To address these challenges, we propose a novel scale-variant attention-based network (SvANet) for accurately segmenting small-scale objects in medical images. The SvANet consists of scale-variant attention, cross-scale guidance, Monte 
    
[^119]: 超越美学：文本到图像模型的文化能力

    Beyond Aesthetics: Cultural Competence in Text-to-Image Models

    [https://arxiv.org/abs/2407.06863](https://arxiv.org/abs/2407.06863)

    该研究提出了一种评估文本到图像模型文化能力的框架，并建立了一个覆盖多个国家、多个文化的文化基准测试，以评估模型在文化意识和多样性方面的表现。

    

    arXiv:2407.06863v4 公告类型：替换  摘要：文本到图像（T2I）模型正在全球各地不断被采用，它们创造出各自独特文化的视觉表现。目前T2I模型的评价主要集中在生成图像的忠实性，美感和现实感上，而忽略了一个关键的维度：文化能力。在这项工作中，我们介绍了一种框架，用于评估T2I模型的文化能力，该框架涉及两个关键维度：对文化的意识和对文化多样性的尊重，并提出了一个使用结构化知识库和大型语言模型相结合的方法，以构建一个大型的文化 artefacts数据集，以实现这一评价。特别是，我们将这种方法应用于建立CUBE（文化基准测试，以测试文本到图像模型的表现），这是一项首次通过不同地理和文化区域和相关概念来评估文化能力的第一项基准测试。CUBE覆盖了与8个不同国家相关联的文化 artefacts，并沿着3个不同的概念进行筛查。

    arXiv:2407.06863v4 Announce Type: replace  Abstract: Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concept
    
[^120]: ViG-Bias：基于视觉的偏差发现与缓解

    ViG-Bias: Visually Grounded Bias Discovery and Mitigation

    [https://arxiv.org/abs/2407.01996](https://arxiv.org/abs/2407.01996)

    ViG-Bias是一种新方法，用于在视觉模型中自动发现和缓解潜藏的偏差，通过大型视觉语言模型提取跨模态嵌入，即使在属性未知的场景中也能识别出系统失败的模式。

    

    arXiv:2407.01996v3 公告类型：替换  摘要：在关键决策过程中广泛采用机器学习模型，凸显了发现和缓解偏差策略的必要性。由于很多时候这些偏差与难以察觉的隐藏的伪相关性有关，因此确定系统表现出偏差的根本原因并不直接。标准方法依赖于通过对模型在数据样本预定义子群上的性能进行分析来执行偏差审核，尤其是在涉及到人类时，通常会基于性别或种族等常见属性，或者是为了图像等其他特定属性定义的语义上有意义的组。然而，并不总是有可能事先知道定义视觉识别系统失败模式的详细属性。最近的方法提出通过利用大型视觉语言模型来发现这些小组，这些模型能够提取跨模态嵌入并生成描述性文本，从而可以在未预先定义的情况下发现导致模型表现不佳的属性组合。

    arXiv:2407.01996v3 Announce Type: replace  Abstract: The proliferation of machine learning models in critical decision making processes has underscored the need for bias discovery and mitigation strategies. Identifying the reasons behind a biased system is not straightforward, since in many occasions they are associated with hidden spurious correlations which are not easy to spot. Standard approaches rely on bias audits performed by analyzing model performance in pre-defined subgroups of data samples, usually characterized by common attributes like gender or ethnicity when it comes to people, or other specific attributes defining semantically coherent groups of images. However, it is not always possible to know a-priori the specific attributes defining the failure modes of visual recognition systems. Recent approaches propose to discover these groups by leveraging large vision language models, which enable the extraction of cross-modal embeddings and the generation of textual descripti
    
[^121]: 解决混合专家模型中的大视觉语言模型中的项目标梯度冲突

    Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model

    [https://arxiv.org/abs/2406.19905](https://arxiv.org/abs/2406.19905)

    本文提出的方法旨在解决在大视觉语言模型中使用混合专家模式时出现的项目标梯度冲突问题，通过使用专门的学习项和初步的部署标签来优化模型性能。

    

    arXiv:2406.19905v2 公告类型：替换 摘要：在研究大型视觉语言模型（LVLMs）的过程中，混合专家（MoE）模型引起了越来越多的关注。它使用稀疏模型来替换稠密模型，在推理期间激活较少的数据量和参数，从而显著降低了推理成本。在LVLMs中的现有MoE方法鼓励不同的专家处理不同的项目标，它们通常使用一个路由器来预测每个项目标的派遣。然而，这些预测仅基于示例特征，并不能真正揭示被分配到专家中的项目的优化方向。这可能在分配给同一个专家的不同项目中引起严重的优化干扰。为了解决这个问题，本文提出了一种基于项目标级梯度分析的新方法，即解决项目标梯度冲突（STGC）。具体来说，我们首先使用项目标级梯度来识别专家中的冲突项目标。然后，我们为每个冲突项目标添加了一个专门的学习项，并通过使用一组初步的部署标签，它们的更新将成为主监督的补充。在我们的实验中，SimP已成功应用于Transformer-XL和T5模型，并验证了其有效性。它在不增加模型复杂性的情况下，可提高模型性能，尤其是语义下游任务的性能。此外，我们还发现，通过逐步从低层级到高层级传递预测标签，可以进一步提升性能。

    arXiv:2406.19905v2 Announce Type: replace  Abstract: The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLMs encourage different experts to handle different tokens, and they usually employ a router to predict the routing of each token. However, the predictions are based solely on sample features and do not truly reveal the optimization directions of tokens. This may lead to severe optimization interference between different tokens assigned to an expert. To address this problem, this paper proposes a novel method based on token-level gradient analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a specialized lo
    
[^122]: InterCLIP-MEP：交互式CLIP和增强记忆预测器在多模态 sarcasm 检测中的应用

    InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection

    [https://arxiv.org/abs/2406.16464](https://arxiv.org/abs/2406.16464)

    InterCLIP-MEP 框架使用交互式 CLIP 和增强的记忆预测器改进了对社交媒体上多模态 sarcasm 的检测效果。

    

    arXiv:2406.16464v3 公告类型：替换交叉 摘要：社交媒体上 sarcasm 的普遍存在，通过文字和图像的组合表达，为情感分析和意图挖掘带来了巨大挑战。现有的多模态 sarcasm 检测方法已经证明存在高估性能的问题，因为它们很难有效地捕捉到文字和图像之间互动产生的精细 sarcastic 线索。为了解决这些问题，我们提出了 InterCLIP-MEP，一个用于多模态 sarcasm 检测的全新框架。特别是，我们引入了交互式 CLIP（InterCLIP）作为骨干，提取文字图像表示，通过在每个编码器中直接嵌入跨模态信息，从而改善了表示，更好地捕捉文字和图像之间的互动。此外，我们还设计了一种有效的培训策略，为了适应我们提出的增强记忆预测器（MEP）调整InterCLIP。MEP使用一个动态的、固定长度的双通道记忆系统来增强对 sarcasm 的检测，并且准确率大幅度提升。

    arXiv:2406.16464v3 Announce Type: replace-cross  Abstract: The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Existing multi-modal sarcasm detection methods have been proven to overestimate performance, as they struggle to effectively capture the intricate sarcastic cues that arise from the interaction between an image and text. To address these issues, we propose InterCLIP-MEP, a novel framework for multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP (InterCLIP) as the backbone to extract text-image representations, enhancing them by embedding cross-modality information directly within each encoder, thereby improving the representations to capture text-image interactions better. Furthermore, an efficient training strategy is designed to adapt InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic, fixed-length dual-channel mem
    
[^123]: CM2-Net：不断的多模态映射网络用于驾驶员行为识别

    CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition

    [https://arxiv.org/abs/2406.11340](https://arxiv.org/abs/2406.11340)

    CM2-Net提出了一种创新的不断学习新模态特征表示的多模态映射网络，能够在多种模态数据上提高驾驶员行为识别的精度。

    

    arXiv:2406.11340v3 公告类型：替换  翻译：驾驶员行为识别在通过整合多种模态（如红外和深度）来增强驾驶员与车辆互动并确保行车安全方面取得了显著进展。然而，与仅使用RGB模态相比，在车舱环境中收集所有类型非RGB模态的数据总是既费时又昂贵。因此， previous works 建议独立地通过在RGB视频上预先训练的模型来微调每个非RGB模态，但这些方法在面对新的模态时提取有信息特征时不太有效，因为存在较大的领域差距。相反，我们提出了一个不断的多模态映射网络（CM2-Net），可以在不断学习的每种新模态上提供从先前学习的模态的指导性提示。特别是，我们开发了一种累积的跨模态映射提示（ACMP）技术，用于将辨别性和信息丰富的特征从不同模态映射出来。为了有效地解决多个模态的跨界学习难题，我们创新地提出了一种模态级和通道级的多级映射策略，该策略能够捕获和整合来自不同模态和通道多尺度、多角度的空间和时间特征。通过在真实世界数据集上的实验结果证明，CM2-Net不仅能够不断学习新模态的特征表示，而且还能够在多种模态的数据上训练出一个泛化性强的模型，实现更好的驾驶员行为识别精度。

    arXiv:2406.11340v3 Announce Type: replace  Abstract: Driver action recognition has significantly advanced in enhancing driver-vehicle interactions and ensuring driving safety by integrating multiple modalities, such as infrared and depth. Nevertheless, compared to RGB modality only, it is always laborious and costly to collect extensive data for all types of non-RGB modalities in car cabin environments. Therefore, previous works have suggested independently learning each non-RGB modality by fine-tuning a model pre-trained on RGB videos, but these methods are less effective in extracting informative features when faced with newly-incoming modalities due to large domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network (CM2-Net) to continually learn each newly-incoming modality with instructive prompts from the previously-learned modalities. Specifically, we have developed Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative and informative fea
    
[^124]: 用于遥感图像检索的深度度量学习高效主动学习Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval

    Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval

    [https://arxiv.org/abs/2406.10107](https://arxiv.org/abs/2406.10107)

    本研究提出了一种名为ANNEAL的主动学习方法，用于在遥感图像检索中高效地学习深度度量学习模型。该方法通过结合不确定性与多样性的评价标准，创造了一个小但信息丰富的训练集，显著提升了检索性能。

    

    arXiv替换公告类型：replace，摘要：深度度量学习（DML）在遥感（RS）图像检索（CBIR）中显示出其有效性。大多数CBIR的DML方法依赖于大量标注图的数以准确学习深度神经网络（DNNs）的模型参数。然而，获取这样的数据既耗时又昂贵。为了解决这个问题，我们提出了一个专门用于RS中DML驱动的CBIR的标注成本高效的主动学习方法（ANNEAL）。ANNEAL旨在创建一个由类似和不同图像对组成的较小但信息丰富的训练集，以便准确地学习一个度量空间。图像对的信息量通过结合不确定性和多样性标准来评估。为了评估图像对的不确定性，我们引入了两项算法：1）基于度量的不确定性估计（MGUE）；和2）基于二元分类器的不确定性估计（BCGUE）。MGUE算法自动估计一个阈值，该阈值在估计不确定性时起到关键作用。BCGUE算法利用二元分类器精确指定需要额外标注的图像对。通过一个遥感数据集进行的实验结果表明，与传统的对数损失和IoU准则相比，ANNEAL在准确性和召回率上实现了更高的性能。为了进一步提高检索性能，我们还探讨了ANNEAL与其他主动学习策略和度量标准的潜力，并提出了一个基于分数的回溯策略。这些实验验证了ANNEAL的有效性和在遥感CBIR中的潜在应用。

    arXiv:2406.10107v2 Announce Type: replace  Abstract: Deep metric learning (DML) has shown to be effective for content-based image retrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a high number of annotated images to accurately learn model parameters of deep neural networks (DNNs). However, gathering such data is time-consuming and costly. To address this, we propose an annotation cost-efficient active learning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to create a small but informative training set made up of similar and dissimilar image pairs to be utilized for accurately learning a metric space. The informativeness of image pairs is evaluated by combining uncertainty and diversity criteria. To assess the uncertainty of image pairs, we introduce two algorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary classifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically estimates a threshold value that acts
    
[^125]: VIP: 多模态大型语言模型增强的无界图像描摹

    VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model

    [https://arxiv.org/abs/2406.01059](https://arxiv.org/abs/2406.01059)

    本文提出了一个基于多模态大型语言模型的图像无界描摹框架，能够根据用户需求定制结果。引入了一种特殊的交叉注意力模块，促进了中心和周边图像区域的交互。

    

    arXiv:2406.01059v2 公告类型：替换 摘要：本文针对图像无界描摹问题进行了研究，该问题旨在给定中心图像内容时，延伸周边图像区域。尽管最近的工作已经取得了显著的性能，但缺乏适用性和定制化能力阻碍了其在更广泛场景中的实际应用。因此，本文提出了一种新的图像无界描摹框架，它能够根据用户的需求定制结果。首先，我们利用了一种多模态大型语言模型（MLLM），该模型能够自动提取并组织给定图像的遮罩部分和非遮罩部分的相应文本描述。因此，所获得的文本提示被介绍给我们的模型，以赋予其定制无界描摹结果的能力。此外，精心设计的特殊交叉注意力模块——“中心-总量-周边”（CTS），进一步增强了……

    arXiv:2406.01059v2 Announce Type: replace  Abstract: In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirement of users. First of all, we take advantage of a Multimodal Large Language Model (MLLM) that automatically extracts and organizes the corresponding textual descriptions of the masked and unmasked part of a given image. Accordingly, the obtained text prompts are introduced to endow our model with the capacity to customize the outpainting results. In addition, a special Cross-Attention module, namely Center-Total-Surrounding (CTS), is elaborately designed to enhance further the the interact
    
[^126]: SE3D: 三维成像中注意力方法评估框架

    SE3D: A Framework For Saliency Method Evaluation In 3D Imaging

    [https://arxiv.org/abs/2405.14584](https://arxiv.org/abs/2405.14584)

    我们提出了SE3D框架，用于评估三维成像中的注意力方法，并针对3D CNN进行了基准测试和评价指标的改进。

    

    在过去十多年的时间里，深度学习模型已经在各种二维成像任务中占据了主导地位。现在，它们的应用范围正在扩展到三维成像领域，三维卷积神经网络（3D CNNs）能够处理LiDAR、MRI和CT扫描，这对自动驾驶和医疗成像等领域有着重大影响。在这些关键领域中，解释模型决策的基本原则极为重要。尽管在可解释的智能AI方面取得了最近的进展，但很少有人致力于解释3D CNN模型，而许多研究通过不充分的二维注意力方法的扩展来解释这些模型。

    arXiv:2405.14584v2 Announce Type: replace  Abstract: For more than a decade, deep learning models have been dominating in various 2D imaging tasks. Their application is now extending to 3D imaging, with 3D Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and CT scans, with significant implications for fields such as autonomous driving and medical imaging. In these critical settings, explaining the model's decisions is fundamental. Despite recent advances in Explainable Artificial Intelligence, however, little effort has been devoted to explaining 3D CNNs, and many works explain these models via inadequate extensions of 2D saliency methods.   A fundamental limitation to the development of 3D saliency methods is the lack of a benchmark to quantitatively assess these on 3D data. To address this issue, we propose SE3D: a framework for Saliency method Evaluation in 3D imaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and evaluation metrics 
    
[^127]: 使用减少存储的直接张量环分解技术压缩卷积神经网络

    Reduced storage direct tensor ring decomposition for convolutional neural networks compression

    [https://arxiv.org/abs/2405.10802](https://arxiv.org/abs/2405.10802)

    本文提出了一种基于减少存储直接张量环分解的新型CNNs压缩方法，该方法在保持高分类准确性的同时，实现了参数和计算复杂度的大幅降低，并在CIFAR-10和ImageNet数据集上得到了验证。

    

    arXiv:2405.10802v2 公告类型：替换

    arXiv:2405.10802v2 Announce Type: replace  Abstract: Convolutional neural networks (CNNs) are among the most widely used machine learning models for computer vision tasks, such as image classification. To improve the efficiency of CNNs, many CNNs compressing approaches have been developed. Low-rank methods approximate the original convolutional kernel with a sequence of smaller convolutional kernels, which leads to reduced storage and time complexities. In this study, we propose a novel low-rank CNNs compression method that is based on reduced storage direct tensor ring decomposition (RSDTR). The proposed method offers a higher circular mode permutation flexibility, and it is characterized by large parameter and FLOPS compression rates, while preserving a good classification accuracy of the compressed network. The experiments, performed on the CIFAR-10 and ImageNet datasets, clearly demonstrate the efficiency of RSDTR in comparison to other state-of-the-art CNNs compression approaches.
    
[^128]: CoReX：基于关系的概念解释器在探索和评价分类器决定中的应用

    When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX

    [https://arxiv.org/abs/2405.01661](https://arxiv.org/abs/2405.01661)

    CoReX是一种新的基于概念和关系的解释器，可以帮助更深入理解模型预测的内在机制，并提高模型在生物医学图像分析等领域的透明性和可解释性。

    

    arXiv:2405.01661v2公告类型：替换交叉 摘要：基于输入像素相关性的卷积神经网络（CNN）解释可能过于不具体，无法评估哪些和如何输入特征影响模型决策。特别是在生物等复杂现实世界领域中，特定概念的存在及其之间的关系可能会区分于不同的类。像素相关性不足以传达此类信息。因此，模型的评价受到限制，数据中存在的相关方面以及影响模型决策的因素可能会被忽略。本工作提出了一种新的方法来解释和评价CNN模型，该方法使用基于概念和关系的解释器（CoReX）。它通过从决策过程中屏蔽（不）相关的概念以及通过限制学习到的可解释的替代模型中的关系，对一组图像的预测行为进行解释。我们使用多种图像数据集对我们的方法进行了测试，并且证明CoReX能够为决策提供更明确的概念和关系的解释，从而有助于更深入理解模型预测的内在机制。此外，我们还通过实验验证了CoReX能够在生物医学图像分析等领域中利用概念和关系的解释来有效提高模型的透明性和可解释性。

    arXiv:2405.01661v2 Announce Type: replace-cross  Abstract: Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biology, the presence of specific concepts and of relations between concepts might be discriminating between classes. Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image dat
    
[^129]: 使用大型多模态模型辅助的AI生成图像质量评估方法

    Large Multi-modality Model Assisted AI-Generated Image Quality Assessment

    [https://arxiv.org/abs/2404.17762](https://arxiv.org/abs/2404.17762)

    研究人员开发了一种利用大型多模态模型以更好地评估AI生成图像质量的替代方法，弥补了传统基于DNN的IQA模型在处理复杂语义特征时的不足。

    

    arXiv:2404.17762v2 公告类型：替换摘要：传统基于深度神经网络（DNN）的图像质量评估（IQA）模型利用卷积神经网络（CNN）或Transformer来学习质量的特征表示，在自然场景图像上的性能表现出色。然而，当应用于AI生成的图像（AGI）时，这些基于DNN的IQA模型表现出较低的性能。这种情况很大程度上是由于某些AGI固有的语义不准确，这由图像生成过程的不确定性所引起。因此，对于评估AGI的质量，能够分辨出语义内容变得至关重要。由于有限的参数复杂性和训练数据，传统基于DNN的IQA模型在捕捉复杂的精细语义特征方面遇到困难，这使得它们难以把握整个图像的语义内容的存在和一致性。为了解决当前IQA模型在语义内容感知方面的不足，我们引入了一种利用大型多模态模型来增强AI图像质量的评估能力的方法。

    arXiv:2404.17762v2 Announce Type: replace  Abstract: Traditional deep neural network (DNN)-based image quality assessment (IQA) models leverage convolutional neural networks (CNN) or Transformer to learn the quality-aware feature representation, achieving commendable performance on natural scene images. However, when applied to AI-Generated images (AGIs), these DNN-based IQA models exhibit subpar performance. This situation is largely due to the semantic inaccuracies inherent in certain AGIs caused by uncontrollable nature of the generation process. Thus, the capability to discern semantic content becomes crucial for assessing the quality of AGIs. Traditional DNN-based IQA models, constrained by limited parameter complexity and training data, struggle to capture complex fine-grained semantic features, making it challenging to grasp the existence and coherence of semantic content of the entire image. To address the shortfall in semantic content perception of current IQA models, we intro
    
[^130]: 论文标题: Fact: 使用忠实、简洁且可转移的论据教 LLM 多模态语言模型

    Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales

    [https://arxiv.org/abs/2404.11129](https://arxiv.org/abs/2404.11129)

    论文要点: Fact 范式通过生成多模态语言模型的忠实、简洁、可转移论据，解锁了他们在视觉任务中的深层理解和推理过程。努力解决当前的MLLMs在透明性、可解释性以及执行复杂任务方面的挑战。

    

    论文摘要: arXiv:2404.11129v2 公告类型: 替换 论文摘要: 多模态大型语言模型 (MLLMs) 的卓越性能无可争议地展示了它们在处理广泛的视觉任务方面的理解能力。然而，它们的黑盒推理过程仍然是未解之谜，使它们不可解释，并且在幻觉方面挣扎。这些模型执行复杂的组成推理任务的能力也受到了限制，最终导致这些模型学习的停滞不前。在这项工作中，我们介绍了一种名为Fact的新范式，用于为多模态语言模型生成忠实、简洁且可转移的论据。这种范式使用了可验证的视觉编程来生成可执行代码，保证了忠实性和精确性。随后，通过一系列操作，包括修剪、合并和桥接，理由增强了其简洁性。此外，我们还过滤了智能体的论据，以确保它们拥有更加丰富的知识结构。我们还构建了一个多个视觉任务的数据集，集成了MLLMs+Fact的优势，证明了我们方法的具体优势，以及对未来的研究方向进行了一系列探讨。事实这一范式提供了一种革命性的解决方案，以此来解锁大型模态语言模型在视觉任务中的深层理解和推理过程。

    arXiv:2404.11129v2 Announce Type: replace  Abstract: The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks. Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision. Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales th
    
[^131]: 天空中增强型GNSS/INS/视觉导航系统：城市峡谷中风车网络结构基于天区分割的改进

    Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based sky-segmentation in urban canyon

    [https://arxiv.org/abs/2404.11070](https://arxiv.org/abs/2404.11070)

    本文提出了一种基于全卷积网络的天区分割算法，用于GNSS信号的NLOS检测，并在此基础上提出了Sky-GVIO系统，该系统通过结合GNSS、INS和视觉特征实现了在城市峡谷环境中的连续和精确定位。

    

    arXiv:2404.11070v2公告类型：替换摘要：精确、连续和可靠的定位是实现自动驾驶的关键组成部分。然而，在高建筑物、树木和 elevated structures 等反常视路（NLOS）的复杂城市峡谷环境中，单个传感器的脆弱性和视野外（NLOS）问题严重影响了定位结果。为了解决这些挑战，本文提出了一种基于全卷积网络（FCN）的天区分割算法，用于GNSS NLOS检测。在此基础上，扩展了一种名为S-NDM的NLOS检测和缓解算法，将其与紧密耦合的全球导航卫星系统（GNSS）、加速度计和视觉特征系统相结合，该系统被称为Sky-GVIO，旨在实现城市峡谷环境中连续和精确的定位。此外，该系统将单点定位（SPP）与实时动态（RTK）方法结合起来，以增强其运行的可靠性和精确性。

    arXiv:2404.11070v2 Announce Type: replace  Abstract: Accurate, continuous, and reliable positioning is a critical component of achieving autonomous driving. However, in complex urban canyon environments, the vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused by high buildings, trees, and elevated structures seriously affect positioning results. To address these challenges, a sky-view images segmentation algorithm based on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection. Building upon this, a novel NLOS detection and mitigation algorithm (named S-NDM) is extended to the tightly coupled Global Navigation Satellite Systems (GNSS), Inertial Measurement Units (IMU), and visual feature system which is called Sky-GVIO, with the aim of achieving continuous and accurate positioning in urban canyon environments. Furthermore, the system harmonizes Single Point Positioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its operational ver
    
[^132]: 多分支生成模型在多通道成像中的应用，特别是在PET/CT协同重建中的应用

    Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Synergistic Reconstruction

    [https://arxiv.org/abs/2404.08748](https://arxiv.org/abs/2404.08748)

    论文提出了一种多分支生成模型，用于提高PET/CT协同成像的质量，特别是在低剂量成像方面。

    

    这篇论文提出了一个使用多分支生成模型的新型医学图像协同重建方法。通过结合可变自编码器（VAEs），我们的模型能够在同一时间处理成对的图像，从而实现有效的去噪和重建。协同图像重建是通过在我们的模型中纳入一个评价图像与模型距离的正规化器来实现的。我们在MNIST和Positron Emission Tomography（PET）/Computed Tomography（CT）数据集上展示了我们的方法的有效性，证明了在低剂量成像方面的图像质量提升。尽管存在诸如块分解和模型限制等挑战，我们的结果强调了生成模型在提高医学成像重建方面的潜力。

    arXiv:2404.08748v2 Announce Type: replace-cross  Abstract: This paper presents a novel approach for learned synergistic reconstruction of medical images using multi-branch generative models. Leveraging variational autoencoders (VAEs), our model learns from pairs of images simultaneously, enabling effective denoising and reconstruction. Synergistic image reconstruction is achieved by incorporating the trained models in a regularizer that evaluates the distance between the images and the model. We demonstrate the efficacy of our approach on both Modified National Institute of Standards and Technology (MNIST) and positron emission tomography (PET)/computed tomography (CT) datasets, showcasing improved image quality for low-dose imaging. Despite challenges such as patch decomposition and model limitations, our results underscore the potential of generative models for enhancing medical imaging reconstruction.
    
[^133]: 开放偏见：文本到图像生成模型中的开放集偏差检测

    OpenBias: Open-set Bias Detection in Text-to-Image Generative Models

    [https://arxiv.org/abs/2404.07990](https://arxiv.org/abs/2404.07990)

    本文提出OpenBias，一个检测文本到图像生成模型中开放集偏见的系统，无需预先定义的偏差集合，利用大型语言模型和视觉问答模型来识别和量化这些模型可能存在的偏差，解决了现有方法无法处理开放集偏差的问题，提供定量证据。

    

    arXiv:2404.07990v2 公告类型：替换  摘要：文本到图像生成模型正变得越来越受欢迎，并且对普通大众越来越可访问。随着这些模型的大规模部署，深入探究它们的稳定性和公平性变得至关重要，以避免传播和延续任何类型的偏见。然而，现有的工作集中在检测预先定义的有限集偏差，限制研究局限于众所周知的概念。在本文中，我们挑战文本到图像生成模型中的开放集偏差检测问题，提出OpenBias，一个新的管道，可以在没有访问任何预编译集合的情况下识别和量化偏见的严重性。OpenBias由三个阶段组成。在第一个阶段，我们利用大型语言模型（LLM）基于一组提示提出偏差。其次，目标生成模型使用相同的提示集生成图像。最后，一个视觉问答模型识别先前提议偏差的程度。我们对11个类别的自然语言描述（包括“人种”、“性别”和“经济状态”）进行实验，证明了OpenBias能够检测模型生成的图像中的开放集偏差。我们的方法不仅解决了现有工作无法处理的开放集问题，而且能够提供有关模型中实际存在的偏见的定量证据。

    arXiv:2404.07990v2 Announce Type: replace  Abstract: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previou
    
[^134]: 使用可泛化高斯点积的强化学习

    Reinforcement Learning with Generalizable Gaussian Splatting

    [https://arxiv.org/abs/2404.07950](https://arxiv.org/abs/2404.07950)

    我们提出了一种基于可泛化高斯点积的强化学习方法，该方法在不同的环境中提供了更清晰和泛化的场景表示，并通过直接使用形状函数和概率分布去除了“黑箱”效应，使得方法更易于理解和可视化。

    

    arXiv:2404.07950v2 公告类型：替换 摘要：在基于视觉的强化学习任务中，优秀的数据表示至关重要。环境表示的质量直接影响了学习任务的成功。以往的基于视觉的强化学习通常使用显式或隐式的方法来表示环境，如图像、点、体素和神经辐射场。然而，这些表示方法存在若干缺点。它们要么无法描述复杂的局部几何，要么在从未见过的新场景中泛化能力差，或者需要精确的背景遮罩。此外，这些隐式的神经表示就像是一个“黑箱”，严重阻碍了可解释性。与传统的隐式神经表示方法相比，3D高斯点积(3DGS)作为一种显式的场景表示和可微的渲染方法，被认为是对重排和表示方法的一种革命性改变。在本文中，我们提出了一种新颖的可泛化高斯点积方法的强化学习。这种方法能够提供更清晰的场景描述，并且在不同的环境中表现出优异的泛化能力。更重要的是，我们的方法通过直接使用形状函数和概率分布来表示场景，使得所提出的方法易于理解和去“黑箱”，从而可以为强化学习任务提供更深入的见解。

    arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
    
[^135]: VRSO：基于视觉的静态对象重建方法

    VRSO: Visual-Centric Reconstruction for Static Object Annotation

    [https://arxiv.org/abs/2403.15026](https://arxiv.org/abs/2403.15026)

    VRSO方法通过使用摄像头图像和参考深度，在3D空间中准确恢复静态对象，并且能够处理难以捕捉的细节，在Waymo Open Dataset上表现出色。

    

    arXiv:2403.15026v2 公告类型：替换 摘要：作为智能驾驶系统感知结果的一部分，空间中静态对象的检测（SOD）为驾驶环境理解提供了关键提示。随着深层神经网络在SOD任务上的快速部署，对高质量训练样本的需求正在飙升。传统且可靠的方法是对密集激光雷达点云和参考图像进行手动标记。尽管大多数公共驾驶数据集都采用这种方式来提供SOD的 ground truth (GT)，但在实践中，它仍然非常昂贵和耗时。本研究介绍了VRSO，一种基于视觉的静态对象标注方法。在Waymo Open Dataset上的实验表明，VRSO标注的重投影误差只有2.6个像素，大约是Waymo Open Dataset标签（10.6像素）的四倍。VRSO在成本低、效率高和质量高方面表现突出：（1）它只需要摄像头图像和参考深度，就能在3D空间中恢复静态对象；（2）与其他基于视觉的方法相比，VRSO能够提供更准确的三维重建；（3）VRSO还能够处理难以捕捉的细节，如行人和非固定的交通标志。此外，VRSO的训练和评估过程不需要特定的3D场景，可以是完全数据驱动的，并在实际驾驶场景中通过真实车辆进行测试。目前在公共数据集上测试VRSO，你可以在arXiv上获取代码和数据，并在未来的驾驶数据集中应用我们的方法。

    arXiv:2403.15026v2 Announce Type: replace  Abstract: As a part of the perception results of intelligent driving systems, static object detection (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labelling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive and time-consuming in practice. This paper introduces VRSO, a visual-centric approach for static object annotation. Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static objects in 3D space with only camer
    
[^136]: SSAP：面向自主导航应用的形状敏感对抗性补丁全面破坏单目 Depth 估计

    SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications

    [https://arxiv.org/abs/2403.11515](https://arxiv.org/abs/2403.11515)

    本文提出了一种名为 SSAP（Shape-Sensitive Adversarial Patch）的新方法，旨在在自主导航应用程序中全面破坏单目Depth估计（MDE）。该方法通过设计具有形状感知特性的对抗性补丁，故意误导 MDE 系统，使其在噪声和不确定性中仍然有效。实验表明 SSAP 能够诱导深度估计错误和行为决策偏差。

    

    arXiv:2403.11515v2 公告类型：替换交叉 摘要：单目深度估计（MDE）已经通过卷积神经网络（CNNs）的整合以及最近，Transformer 的使用取得了显著的进展。但是，对于它们在包括自动驾驶和机器人导航等安全关键领域对对抗性攻击的易感性的担忧已经出现。评估基于 CNN 的深度预测方法的现有方法在诱导对视觉系统的全面破坏方面已经不足，往往限于特定区域。在本文中，我们介绍了一种名为 SSAP（Shape-Sensitive Adversarial Patch）的新型方法，旨在在自主导航应用程序中全面破坏单目深度估计（MDE）。我们的补丁被精心设计成以两种不同的方式来破坏 MDE：通过扭曲估计的距离或通过在系统视角中创建物体消失的错觉。值得注意的是，我们的补丁具有形状感知特异性，意味着它可以选择性地影响 MDE，即使在深度估计中的噪声和不确定性中也保持有效。实验结果表明，SSAP 能够有效地在多个模型和场景中诱导深度估计的错误和行为决策的偏差。我们的研究结果为理解深度估计系统的鲁棒性提供深刻的洞见，并为设计和开发更强大的防御策略提供基础。

    arXiv:2403.11515v2 Announce Type: replace-cross  Abstract: Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensi
    
[^137]: Motion Mamba：高效且长序列的运动生成

    Motion Mamba: Efficient and Long Sequence Motion Generation

    [https://arxiv.org/abs/2403.07487](https://arxiv.org/abs/2403.07487)

    论文提出了一种名为Motion Mamba的简单且高效的运动生成模型，它在U-Net架构中使用了层次时序Mamba（HTM）块，同时在预测过程中采用移动平均预测以保持运动的一致性。

    

    arXiv:2403.07487v4 宣布类型：替换 摘要：在生成计算机视觉中，人类运动生成是一个重要的追求，而实现长期序列和高效的运动生成仍然是一个挑战。最近状态空间模型（SSM）在长期序列建模中的进步，特别是Mamba，已经在硬件友好的设计上展示了相当的潜力，这似乎是建立在运动生成模型之上的一个有希望的方向。然而，将SSM应用于运动生成面临障碍，因为缺乏一种专门的设计架构来建模运动序列。为了解决这些挑战，我们提出了Motion Mamba，这是一个简单且高效的方法，它展现了一个开创性的运动生成模型，使用了SSM。具体来说，我们设计了一个层次时序Mamba（HTM）块，用于通过U-Net架构中集成的各种隔离SSM模块来处理时间数据，这些模块的大小在统计对称性上有所不同，旨在保留帧之间的运动一致性。此外，为了维护运动一致性，我们实施了移动平均预测以减少长时间间隔中的运动变化，并且在生成运动时使用了代理编码器来提高性能。我们在多种运动数据集上验证了我们方法的有效性，表明Motion Mamba在长期运动预测中的表现优于现有的运动生成模型。

    arXiv:2403.07487v4 Announce Type: replace  Abstract: Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between fr
    
[^138]: 《PrimeComposer: 使用注意力引导加速渐进式组合扩散的图像合成技术》

    PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering

    [https://arxiv.org/abs/2403.05053](https://arxiv.org/abs/2403.05053)

    研究提出了一种新的渐进式组合扩散方法，专注于图像合成中的前景生成，并通过改进的注意力引导策略显著提高了合成速度和质量。

    

    arXiv:2403.05053v2 公告类型：替换 摘要：图像合成涉及将给定对象无缝地整合到特定的视觉环境中。当前的训练免费方法依赖于从几个采样器中提取注意力权重，以指导生成器。然而，由于这些权重是源自不同的上下文，它们的组合导致了一致性的困惑和外观信息的损失。这些问题在它们过分关注背景生成时变得更加严重，即使在这种情况下这是不必要的任务。这不仅妨碍了它们的快速实施，而且也影响了前景生成的质量。此外，这些方法在过渡区域引入了不必要的艺术颜料。在本论文中，我们将图像合成视为一种基于主题的局部编辑任务，仅专注于前景的生成。在每一次编辑中，所编辑的前景与噪声背景相结合，以维持场景的一致性。为了解决剩余的问题，我们提出了PrimeComposer，这是一种更快的前景为主提升注意力引导加速的渐进式组合扩散图像合成方法。

    arXiv:2403.05053v2 Announce Type: replace  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. Current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only impedes their swift implementation but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tra
    
[^139]: NiNformer: 一种使用 Token 混合作为门控函数生成器的网络在网络 transformer

    NiNformer: A Network in Network Transformer with Token Mixing as a Gating Function Generator

    [https://arxiv.org/abs/2403.02411](https://arxiv.org/abs/2403.02411)

    本文提出了 NiNformer，一种结合了网络在网络 (Network in Network) 和 Transformer 架构的新模型，使用 Token 混合作为门控函数生成器，以提高计算效率和减少数据集大小要求。

    

    arXiv:2403.02411v5 公告类型：替换 摘要：自引入以来，注意机制一直是 transformer 架构的主要组件，并引领了深度学习领域的重大进步，涵盖多个领域和多种任务。注意机制被用作计算机视觉中的 Vision Transformer ViT，其应用扩展到了多个视觉领域任务，如分类、分割、物体检测和图像生成。这个机制虽然表达能力非常强，但它也有一些不足，如计算成本高昂，且需要较大的数据集才能有效优化。为了解决这些问题，文献中提出了许多设计，旨在减少计算负担，并缓解对数据大小的要求。在计算机视觉领域中，一些这方面的尝试包括 MLP-Mixer、Conv-Mixer、Perciver-IO 等。本论文介绍了作为门控函数生成器的新的计算块。

    arXiv:2403.02411v5 Announce Type: replace  Abstract: The attention mechanism is the main component of the transformer architecture, and since its introduction, it has led to significant advancements in deep learning that span many domains and multiple tasks. The attention mechanism was utilized in computer vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as a
    
[^140]: 使用辅助对抗防御网络增强跟踪鲁棒性

    Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks

    [https://arxiv.org/abs/2402.17976](https://arxiv.org/abs/2402.17976)

    本文提出了一个辅助预处理防御网络AADN，通过在输入图像上进行防御性转换，增强了视觉目标跟踪的对抗鲁棒性。

    

    arXiv:2402.17976v3 宣布类型：替换 摘要：视觉目标跟踪中的对抗攻击通过在图像中引入肉眼难以察觉的扰动，已经严重降低了高级跟踪器的性能。然而，针对目标跟踪的对抗防御方法的研究仍然不足。为了解决这些问题，我们提出了一个有效的辅助预处理防御网络AADN，它在对输入图像进行防御性转换，并将它们馈送至跟踪器之前。此外，它还可以无缝地集成到其他视觉跟踪器中作为即插即用模块，而不需要参数调整。我们使用对抗训练训练AADN，特别是使用DuAL-Loss生成同时攻击跟踪器分类和回归分支的对抗样本。在OTB100、LaSOT和VOT2018基准上进行的广泛实验表明，AADN在对抗攻击方面的防御鲁棒性维持得很好，同时保持了跟踪精度和准确性。

    arXiv:2402.17976v3 Announce Type: replace  Abstract: Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. However, there is still a lack of research on designing adversarial defense methods for object tracking. To address these issues, we propose an effective auxiliary pre-processing defense network, AADN, which performs defensive transformations on the input images before feeding them into the tracker. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without parameter adjustments. We train AADN using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that AADN maintains excellent defense robustness against adversarial att
    
[^141]: AVS-Net：自适应体素大小点采样网络用于3D场景理解

    AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene Understanding

    [https://arxiv.org/abs/2402.17521](https://arxiv.org/abs/2402.17521)

    AVS-Net是一种自适应体素大小点采样方法，它提高了3D场景理解的高精度和效率。

    

    arXiv:2402.17521v3 公告类型：替换  摘要：点云学习领域的最新进展促进了智能车辆和机器人在3D环境理解方面的智能。然而，处理大规模3D场景仍然是一个挑战，因此，高效的点云降采样方法是点云学习中不可或缺的一部分。现有的降采样方法要么需要巨大的计算负担，要么牺牲了精细的立体信息。为了解决这个问题，本文提出了一种高级采样器，它实现了高精度和效率的结合。提出的该方法以体素中心点采样为基础，却能有效地克服体素大小确定和关键几何特征保存的挑战。具体来说，我们提出了一个体素适应模块，它根据点云基的降采样比例适当地调整体素大小，确保了采样结果在理解变量场景方面的表现力。

    arXiv:2402.17521v3 Announce Type: replace  Abstract: The recent advancements in point cloud learning have enabled intelligent vehicles and robots to comprehend 3D environments better. However, processing large-scale 3D scenes remains a challenging problem, such that efficient downsampling methods play a crucial role in point cloud learning. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. For such purpose, this paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel centroid sampling as a foundation but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures that the sampling results exhibit a favorable distribution for comprehending var
    
[^142]: PCR-99: 一种适用于99%异常值点云注册的实用方法

    PCR-99: A Practical Method for Point Cloud Registration with 99 Percent Outliers

    [https://arxiv.org/abs/2402.16598](https://arxiv.org/abs/2402.16598)

    该方法通过改进的三点采样和排序机制在99%的异常值比例下实现了点云注册的高效性和准确性。

    

    我们提出了一种有效的方法来处理点云注册问题，能够处理未知比例尺和极高的异常值比例。我们提出的方法，称为PCR-99，使用一种基于三点采样方法的确定性方法，该方法是两者的创新机制：（1）基于成对比例尺一致性的样本排序，优先考虑更有可能成为内点的点对应关系；以及（2）基于三元组比例尺一致性的高效异常值拒绝方案，预先筛选不良样本，减少需要被测试的假设数量。我们的评估显示，在98%异常值比例时，所提出的方法与现有技术相当。然而，在99%异常值比例时，它在已知比例尺和未知比例尺问题方面都显著优于现有技术。尤其是在后者方面，我们在鲁棒性和速度上观察到明显的优势。

    arXiv:2402.16598v5 Announce Type: replace-cross  Abstract: We propose a robust method for point cloud registration that can handle both unknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a deterministic 3-point sampling approach with two novel mechanisms that significantly boost the speed: (1) an improved ordering of the samples based on pairwise scale consistency, prioritizing the point correspondences that are more likely to be inliers, and (2) an efficient outlier rejection scheme based on triplet scale consistency, prescreening bad samples and reducing the number of hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio, the proposed method achieves comparable performance to the state of the art. At 99% outlier ratio, however, it outperforms the state of the art for both known-scale and unknown-scale problems. Especially for the latter, we observe a clear superiority in terms of robustness and speed.
    
[^143]: 零样本VLM在仇恨meme检测中的应用：我们还不足以应对吗？

    Zero shot VLMs for hate meme detection: Are we there yet?

    [https://arxiv.org/abs/2402.12198](https://arxiv.org/abs/2402.12198)

    本文旨在探究视觉语言模型在零样本情况下识别仇恨meme的能力，并发现尽管当前零样本方法在准确性和泛化能力方面存在局限性，但未来研究有潜力解决这些局限性并扩大其在网络空间仇恨内容检测和响应中的应用。

    

    arXiv:2402.12198v2 更改消息类型：替换证书  摘要更改通知类型：替换-交叉  摘要：社交媒体上的多媒体内容正在迅速发展，meme作为一种独特的形式越来越受到人们的关注。不幸的是，一些不良用户利用meme来攻击个人或脆弱的社区，这使得识别和解决这类仇恨meme变得至关重要。这项研究已经开发出了多种仇恨meme检测模型，以解决这个问题。然而，传统机器/深度学习模型的一个显着局限性是需要大量的标签数据来进行准确的分类。最近，研究界见证了各种视觉语言模型的出现，这些模型在各种任务上表现出色。在本研究中，我们旨在调查这些视觉语言模型在处理复杂的任务，如仇恨meme检测方面的有效性。我们使用了各种提示设置，专注于仇恨/有害meme的零样本分类。通过我们的分析表明，某些零样本设置显示出处理复杂矢量任务的潜力和可行性。然而，我们也要指出，目前的零样本方法在准确性和泛化能力方面还存在局限性。尽管如此，随着研究的不断深入，我们有理由相信，这些由最新技术驱动的模型将为识别和缓解网上仇恨meme的传播做出重大贡献。  我们在使用大量示例短语的基础上进了一步，应用了一系列用于检测和分类仇恨meme的零样本多模态提示词（prompts）。我们发现，在某些情况下，维基百科的词条和描述性短语可用于指定特定类型的仇恨内容，即使在不需要预先训练的分类器的情况下，它们也能促进模型的有效判别式学习。在后续的实验中，我们探索了将零样本多模态Prompt改写（rephrasing）策略应用于极端言论、敏感主题等相关领域的多模态学习前训练（prompt rephrasing for multi-modal learning pretraining），并发现该方法可在不牺牲模型性能的情况下有效提高零样本到实验域的泛化能力。整体而言，我们的结果表明，尽管面临挑战，零样本多模态提示词（prompts）作为一种新颖的范式，其潜力已被证明是广大的。我们的研究指出，将来的工作应致力于进一步的研究和开发，以解决此类方法的局限性，并扩展它们在网络空间仇恨内容检测和响应中的应用。

    arXiv:2402.12198v2 Announce Type: replace-cross  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our anal
    
[^144]: AONeuS: 一个结合声学-光学传感融合的神经渲染框架

    AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion

    [https://arxiv.org/abs/2402.03309](https://arxiv.org/abs/2402.03309)

    这是研究建立了一个能够有效融合声学和光学数据的神经渲染框架，即使在受限于小基线的测量条件下，也能准确重建水下高分辨率三维表面。

    

    arXiv:2402.03309v3 公告类型：替换  摘要：水下感知和三维表面重建是具有广泛应用前景的难题，这些应用包括建筑、安全、海洋考古和环境监测等领域。由于恶劣的操作条件、脆弱的环境以及有限的导航控制，潜水器经常被限制在其可捕获测量值的基线范围内。在三维场景重建的背景下，我们都知道，较小的基线会使重建更加困难。我们的工作开发了一种基于物理的多模态声学-光学神经表面重建框架（AONeuS），它能够有效地整合高分辨率RGB测量值与低分辨率深度分辨成像声纳测量值。通过融合这些互补的模态，我们的框架可以从在受限制的基线范围内捕获的测量值中重建出准确的超高分辨率三维表面。通过广泛的模拟和实验，我们的研究成果表明，AONeuS能够处理和整合声学和光学数据，即使在水深和成像模糊的条件下也是如此。这是在受限基线条件下实现高分辨率3D表面重建的一项关键技术进步。

    arXiv:2402.03309v3 Announce Type: replace  Abstract: Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive s
    
[^145]: 组多视图自编码器用于空间编码的3D形状分析

    Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding

    [https://arxiv.org/abs/2312.16477](https://arxiv.org/abs/2312.16477)

    本文提出了一种基于知识蒸馏的压缩方法，用于减少3D形状识别中视图级别方法的参数数量，同时保持性能。通过设计一种名为GMViT的高性能大型模型，以及引入空间自编码器增强特征表示，该方法在一个名为DeCoV的策略下实现了在图像分类任务上的较好性能。

    

    arXiv:2312.16477v3 公告类型：替换  摘要：近年来，基于视图的3D形状识别方法的结果已经饱和，并且由于参数尺寸巨大，性能优异的模型无法部署在内存受限的设备上。为了解决这个问题，我们为这一领域引入了一种基于知识蒸馏的压缩方法，这种方法在尽可能保留模型性能的同时显著减少了参数的数量。具体来说，为了提高小型模型的能力，我们设计了一个高性能的大型模型，称为组多视图视觉Transformer（GMViT）。在GMViT中，视图级别的ViT首先建立了视图级别特征之间的关系。此外，为了捕获更深层次的特征，我们对视图级别特征进行了分组模块的处理，使其提升到了组级别特征。最后，组级别ViT将组级别特征整合成完整的、结构良好的3D形状描述符。值得注意的是，在这两个ViT中，我们都引入了空间自编码器来进一步增强特征表示。我们的压缩策略叫做DeCoV,它可以在图像分类任务上达到比当前最佳压缩技术更好的性能。在论文中，我们详细说明了如何使用知识蒸馏来实现这一愿景，并讨论了DeCoV在另一方面的影响可能给我们带来的便利。

    arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e
    
[^146]: 分解与细化视觉程序的自动反馈

    De-fine: Decomposing and Refining Visual Programs with Auto-Feedback

    [https://arxiv.org/abs/2311.12890](https://arxiv.org/abs/2311.12890)

    本研究提出了一种无需训练的框架“De-fine”，它能够自动分解复杂任务为简单子任务，并通过自动反馈来改进程序，显著提高了逻辑推理性能。en_tdlr:Our study introduces a training-free framework "De-fine" that automatically decomposes complex tasks into simpler subtasks and refines programs with auto-feedback, significantly improving logical reasoning performance.

    

    arXiv:2311.12890v3 公告类型：替换 摘要：视觉程序设计是一种模块化和可泛化的设计范式，它将不同的模块和Python运算符集成在一起，以解决各种视觉-语言任务。与需要特定任务数据的端到端模型不同，它在无监督的情况下取得了进步，能够进行视觉处理和推理。目前，视觉程序设计的方法为每个任务生成程序，一次性通过，遗憾的是，缺乏根据反馈评估和优化程序的能力，这最终限制了它们在处理复杂、多步问题时的有效性。受到Benders解聚灵感的启发，我们提出了一种名为“Define”的训练免费框架，它可以通过自动解聚复杂任务为简单的子任务，并通过自动反馈来细化程序。这个模型无关的方法可以通过整合多个模型的优势来改进逻辑推理性能。我们在各种视觉任务上的实验表明，Define创建了更多具有逻辑推理性能的程序片段，从而实现了更好的错误恢复和决策过程。原文：Visual programming, a modular and generalizable paradigm, integrates different modules and Python operators to solve various vision-language tasks. Unlike end-to-end models that need task-specific data, it advances in performing visual processing and reasoning in an unsupervised manner. Current visual programming methods generate programs in a single pass for each task where the ability to evaluate and optimize based on feedback, unfortunately, is lacking, which consequentially limits their effectiveness for complex, multi-step problems. Drawing inspiration from benders decomposition, we introduce De-fine, a training-free framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback. This model-agnostic approach can improve logical reasoning performance by integrating the strengths of multiple models. Our experiments across various visual tasks show that De-fine creates more robust program fragments with better error recovery and decision-making processes.

    arXiv:2311.12890v3 Announce Type: replace  Abstract: Visual programming, a modular and generalizable paradigm, integrates different modules and Python operators to solve various vision-language tasks. Unlike end-to-end models that need task-specific data, it advances in performing visual processing and reasoning in an unsupervised manner. Current visual programming methods generate programs in a single pass for each task where the ability to evaluate and optimize based on feedback, unfortunately, is lacking, which consequentially limits their effectiveness for complex, multi-step problems. Drawing inspiration from benders decomposition, we introduce De-fine, a training-free framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback. This model-agnostic approach can improve logical reasoning performance by integrating the strengths of multiple models. Our experiments across various visual tasks show that De-fine creates more ro
    
[^147]: 全局动态频率变压器用于图像融合和曝光校正

    Holistic Dynamic Frequency Transformer for Image Fusion and Exposure Correction

    [https://arxiv.org/abs/2309.01183](https://arxiv.org/abs/2309.01183)

    本文提出了一种基于频率域的全局动态频率变压器方法，用于统一和改进曝光校正任务，包括低光照增强、曝光校正和多曝光融合。

    

    arXiv:2309.01183v2 宣布类型：替换 摘要：在提高图像质量的过程中，曝光相关问题的校正是非常重要的一个组成部分，这对于各种计算机视觉任务有重大的意义。历史上，大多数方法主要在空间域内进行恢复，对频率域的可能性的考虑有限。此外，在低光照增强、曝光校正和多曝光融合方面，缺乏统一的视角，这使得图像处理优化变得复杂和困难。为了应对这些挑战，本文提出了一个创新的方法，该方法利用频率域来改进和统一曝光校正任务的处理。我们的方法引入了全局频率注意力（Holistic Frequency Attention）和动态频率反馈前向网络（Dynamic Frequency Feed-Forward Network），它们取代了空间域中常用的相关运算。它们构成了一个基础构建块，有助于实现U形结构。

    arXiv:2309.01183v2 Announce Type: replace  Abstract: The correction of exposure-related issues is a pivotal component in enhancing the quality of images, offering substantial implications for various computer vision tasks. Historically, most methodologies have predominantly utilized spatial domain recovery, offering limited consideration to the potentialities of the frequency domain. Additionally, there has been a lack of a unified perspective towards low-light enhancement, exposure correction, and multi-exposure fusion, complicating and impeding the optimization of image processing. In response to these challenges, this paper proposes a novel methodology that leverages the frequency domain to improve and unify the handling of exposure correction tasks. Our method introduces Holistic Frequency Attention and Dynamic Frequency Feed-Forward Network, which replace conventional correlation computation in the spatial-domain. They form a foundational building block that facilitates a U-shaped
    
[^148]: 非视线成像中的域减少策略

    Domain Reduction Strategy for Non Line of Sight Imaging

    [https://arxiv.org/abs/2308.10269](https://arxiv.org/abs/2308.10269)

    本文提出一种优化方法来重建隐藏场景的非视线成像，通过减少不必要计算，并使用表面法线准确高效地建模视向反射率，从而提高重构效率和质量。

    

    本文提出了一种基于优化的非视线（NLOS）成像新方法，旨在在通用配置下大大减少重构时间的同时准确重建隐藏场景。在非视线成像中，目标对象的可视表面显著稀疏。为了减轻不必要的计算，这些计算源自空洞区域，我们设计了该方法，允许从隐藏空间中连续采样的一组点进行部分传播，从而渲染瞬态现象。我们的方法能够准确且高效地利用表面法线建模视向反射率，从而使得能够获得表面几何形状以及反照率。在优化过程中，我们提出了一种新的域减少策略，该策略在粗到细的步骤中定期从采样域中剔除空洞区域，导致计算减少。这促进了整体优化过程的效率，并显著提高了隐藏场景的重构质量。

    arXiv:2308.10269v2 Announce Type: replace  Abstract: This paper presents a novel optimization-based method for non-line-of-sight (NLOS) imaging that aims to reconstruct hidden scenes under general setups with significantly reduced reconstruction time. In NLOS imaging, the visible surfaces of the target objects are notably sparse. To mitigate unnecessary computations arising from empty regions, we design our method to render the transients through partial propagations from a continuously sampled set of points from the hidden space. Our method is capable of accurately and efficiently modeling the view-dependent reflectance using surface normals, which enables us to obtain surface geometry as well as albedo. In this pipeline, we propose a novel domain reduction strategy to eliminate superfluous computations in empty regions. During the optimization process, our domain reduction procedure periodically prunes the empty regions from our sampling domain in a coarse-to-fine manner, leading to 
    
[^149]: 深度伪造图像检测器的泛化能力有多强？一项实证研究

    How Generalizable are Deepfake Image Detectors? An Empirical Study

    [https://arxiv.org/abs/2308.04177](https://arxiv.org/abs/2308.04177)

    这项研究表明，目前深度伪造图像检测器缺乏跨数据集的泛化能力，但通过模型增强训练可以提高这一能力。

    

    arXiv:2308.04177v2 宣布类型：替换摘要：随着深度伪造变得越来越逼真，其潜在的欺诈行为或绕过访问控制系统的风险日益增大。因此，人们开发了深度伪造检测方法，这些方法利用深度学习模型来区分真实与合成视频。不幸的是，现有的检测器在检测与训练数据不同的深度伪造方面遇到了困难，但尚未有人对这种局限性进行研究或探讨如何解决。尤其是，单模态深度伪造图像揭示的伪造证据很少，这相对于检测深度伪造视频来说是一个更大的挑战。在本研究中，我们首次对深度伪造检测器的泛化能力进行了研究，这是检测器必须实现的目标，以确保始终领先于攻击者一步。我们的研究利用了六个深度伪造数据集、五种深度伪造图像检测方法以及两种模型增强方法，旨在评估不同数据集、评估标准和模型自身属性对检测器泛化能力的影响。我们还发现在模型训练过程中采取模型增强可以显著提高检测器的泛化能力。

    arXiv:2308.04177v2 Announce Type: replace  Abstract: Deepfakes are becoming increasingly credible, posing a significant threat given their potential to facilitate fraud or bypass access control systems. This has motivated the development of deepfake detection methods, in which deep learning models are trained to distinguish between real and synthesized footage. Unfortunately, existing detectors struggle to generalize to deepfakes from datasets they were not trained on, but little work has been done to examine why or how this limitation can be addressed. Especially, those single-modality deepfake images reveal little available forgery evidence, posing greater challenges than detecting deepfake videos. In this work, we present the first empirical study on the generalizability of deepfake detectors, an essential goal for detectors to stay one step ahead of attackers. Our study utilizes six deepfake datasets, five deepfake image detection methods, and two model augmentation approaches, con
    
[^150]: FreeDrag：基于特征拖拽的可靠点式图像编辑

    FreeDrag: Feature Dragging for Reliable Point-based Image Editing

    [https://arxiv.org/abs/2307.04684](https://arxiv.org/abs/2307.04684)

    FreeDrag是一种创新的图像编辑方法，通过动态特征更新和回头搜索技术改善了点式拖拽编辑的稳定性和速度。

    

    arXiv:2307.04684v4 公告类型：替换 摘要：为了满足图像编辑的复杂和多样化的需求，在图像内容上进行精确和灵活的编辑是不可或缺的。最近，基于拖拽的编辑方法取得了令人印象深刻的性能。然而，这些方法主要集中在点拖拽上，导致两个值得注意的缺点，即“错过追踪”，在对预先确定的手柄点进行精确追踪时遇到困难；以及“模糊追踪”，追踪到的点有可能被错误地定位在类似于手柄点的区域中。为了解决上述问题，我们提出了FreeDrag，一种基于特征拖拽的方法，旨在免除点追踪的负担。FreeDrag包含了两个关键设计，即通过自适应更新实现的模板特征和带有回头搜索的线搜索，前者通过精心控制每次拖拽后特征更新的大小，从而改善了对内容剧烈变化时的稳定性；而后者则减轻了对鼠标和手柄点之间对准精度的高要求。FreeDrag提供了一种动态的特征更新机制，可以在不牺牲稳定性的同时，显著提高拖拽编辑操作的速度。通过大量的实验，我们证明了FreeDrag在提高拖拽编辑精度和减少用户操作上的优越性。

    arXiv:2307.04684v4 Announce Type: replace  Abstract: To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely "miss tracking", where difficulties arise in accurately tracking the predetermined handle points, and "ambiguous tracking", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter allev
    
[^151]: 基于Transformer的视觉分割综述

    Transformer-Based Visual Segmentation: A Survey

    [https://arxiv.org/abs/2304.09854](https://arxiv.org/abs/2304.09854)

    本文综述了基于Transformer的视觉分割技术，重点关注了其在分割任务中的有效应用和性能优势。

    

    arXiv:2304.09854v4 公告类型：替换 摘要：视觉分割旨在将图像、视频帧或点云分割成多个区段或组。这项技术在自动驾驶、图像编辑、机器视觉和医疗分析等众多实际应用中都有着重要作用。在过去的十年里，基于深度学习的方法在这一点上取得了显著的进步。最近，在自然语言处理中被设计为 self-attention 机制的Transformer类型神经网络在各种视觉处理任务中超越了先前的卷积或循环方法。特别是，视觉Transformer为各种分割任务提供了解决方案，即使在复杂场景下也保持了鲁棒性、统一性和简洁性。本文综述了基于Transformer的视觉分割的最新进展。首先回顾了背景，包括问题定义、数据集和先前的卷积方法。接下来，我们总结了Transformer在视觉分割方面的一些核心技术的发展。我们详细介绍了视觉Transformer的基本网络结构和自注意力机制，以及在分割任务中如何有效应用该机制。此外，我们还回顾了Transformer的变体，如自生成Transformer（AutoFormer）、Scaled Dot-Product Attention（官网加粗）和Multi-Head attention modules，以及在分割任务中它们的实现和优势。最后，本文讨论了迁移学习和预训练模型的应用，以及如何优化网络结构和训练过程以提高性能。此外，我们还介绍了目前存在的问题和未来的研究方向，为该领域的发展提供了新的视角。

    arXiv:2304.09854v4 Announce Type: replace  Abstract: Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-archit
    
[^152]: 渐进式视觉提示学习与对比性特征重构

    Progressive Visual Prompt Learning with Contrastive Feature Re-formation

    [https://arxiv.org/abs/2304.08386](https://arxiv.org/abs/2304.08386)

    本文提出了渐进式视觉提示学习方法，结合了对比性特征重构技术，提高了视觉语言模型的泛化能力。

    

    arXiv:2304.08386v3 公告类型：替换  摘要：提示学习被设计为适应视觉语言（V-L）模型到下游任务的一种替代方法。前人工作主要集中在文本提示上，而视觉提示的工作对于V-L模型来说是非常缺乏的。现有的视觉提示方法要么性能平平，要么训练过程不稳定，这表明视觉提示学习具有难度。在本文中，我们提出了一个新的渐进式视觉提示（ProVP）结构来加强不同层级提示之间的相互作用。更重要的是，我们的ProVP能够有效地将图像嵌入传播到深层层中，并在一定程度上表现出与实例适应性提示方法的类似行为。为了缓解泛化恶化，我们还提出了新的对比性特征重构方法，该方法能够防止经过提示的视觉特征严重偏离固定的CLIP视觉特征分布。结合两者，我们的方法（ProVP-Ref）能够在多种视觉语言任务上取得更好的泛化性能。

    arXiv:2304.08386v3 Announce Type: replace  Abstract: Prompt learning has been designed as an alternative to fine-tuning for adapting Vision-language (V-L) models to the downstream tasks. Previous works mainly focus on text prompt while visual prompt works are limited for V-L models. The existing visual prompt methods endure either mediocre performance or unstable training process, indicating the difficulty of visual prompt learning. In this paper, we propose a new Progressive Visual Prompt (ProVP) structure to strengthen the interactions among prompts of different layers. More importantly, our ProVP could effectively propagate the image embeddings to deep layers and behave partially similar to an instance adaptive prompt method. To alleviate generalization deterioration, we further propose a new contrastive feature re-formation, which prevents the serious deviation of the prompted visual feature from the fixed CLIP visual feature distribution. Combining both, our method (ProVP-Ref) is 
    
[^153]: 面向强化学习友好型视觉-语言模型的《我的世界》模型

    Reinforcement Learning Friendly Vision-Language Model for Minecraft

    [https://arxiv.org/abs/2303.10571](https://arxiv.org/abs/2303.10571)

    本文提出了一种新型强化学习友好的视觉-语言模型CLIP4MC，它在Minecraft环境中通过结合任务完成度和语言描述的相似性，实现了对开放式任务的有效指导。

    

    arXiv:2303.10571v2 公告类型：替换交叉  摘要：在人工智能研究社区中，建立一个能够在高水平任务上实现广泛任务自主嵌入式代理是至关重要的。然而，为所有开放式任务获取或手动设计奖励是不现实的。在这篇文章中，我们提出了一个新型的跨模态对比学习框架架构，CLIP4MC，旨在学习一种基于强化学习的视频-语言模型（VLM），它作为开放式任务的内在奖励函数。由于标准VLMs可能只在大范围内捕获相似性，因此仅利用视频快照和语言提示之间的相似性并不是RL友好的。为了实现RL友好性，我们将在VLM训练目标中融入任务完成程度的信息，因为这种信息可以帮助代理区分不同状态的重要性。此外，我们还提供了一个干净的YouTube数据集，基于《我的世界》的任务，用于训练VLM，以提升模型在开放式任务中的RL性能。我们还通过在Minecraft环境中的实验，对CLIP4MC进行验证，结果表明，我们的方法能够有效地区分最重要的奖励，并促进了任务执行的策略。

    arXiv:2303.10571v2 Announce Type: replace-cross  Abstract: One of the essential missions in the AI research community is to build an autonomous embodied agent that can achieve high-level performance across a wide spectrum of tasks. However, acquiring or manually designing rewards for all open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-ended tasks. Simply utilizing the similarity between the video snippet and the language prompt is not RL-friendly since standard VLMs may only capture the similarity at a coarse level. To achieve RL-friendliness, we incorporate the task completion degree into the VLM training objective, as this information can assist agents in distinguishing the importance between different states. Moreover, we provide neat YouTube datasets based on the l
    
[^154]: 重新审视预训练模型下的类增量学习：通用性和适应性就是你所需的一切

    Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need

    [https://arxiv.org/abs/2303.07338](https://arxiv.org/abs/2303.07338)

    类增量学习的研究表明，利用预训练模型和简单更新策略可以取得很好的性能，无需对下游任务进行额外训练。

    

    arXiv:2303.07338v2 公告类型：替换交叉  摘要：类增量学习（CIL）旨在不断获得知识的同时不会忘记学过的旧知识。传统的CIL模型是从零开始训练的，以便随着数据的发展不断获得知识。最近，预训练技术已经取得了显著的进展，使得大量的预训练模型（PTMs）可用于CIL。与传统方法相反，PTMs提供了一种易于转移的通用的嵌入表示，可以为CIL所用。在这项工作中，我们对以PTMs为基础的CIL进行了重新审视，并认为CIL的核心因素在于模型的更新能力和知识的转移能力。1）首先，我们揭示了冻结的PTM能够为CIL提供通用的嵌入表示。令人惊讶的是，一个简单的基线（SimpleCIL），它不断地将PTM的类器设置为原型特征，可以在不进行下游任务训练的情况下击败最先进的算法。2）由于预训练数据集和下游数据集之间的分布差距，PTM的通用嵌入可能不适合立即用于CIL。为了克服这一障碍，我们需要一套适应性和通用性的训练策略，以确保模型的有效适应性和知识的高效利用。

    arXiv:2303.07338v2 Announce Type: replace-cross  Abstract: Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM 
    
[^155]: 视觉学习者与网络图像文本对

    Vision Learners Meet Web Image-Text Pairs

    [https://arxiv.org/abs/2301.07088](https://arxiv.org/abs/2301.07088)

    新型视觉学习者MUG通过自监督学习，在网络图像文本配对数据上进行预训练，旨在改进视觉表示。

    

    arXiv:2301.07088v3 公告类型：替换 摘要： 许多自监督学习方法在一组精心编排的图像Net-1K数据集上进行预训练。在本工作中，考虑到网络数据出色的可扩展性，我们考虑在由网络来源的图像文本配对数据上进行自监督预训练。首先，我们在类似配置的设置中，对代表性的自监督预训练方法在大型网络数据上的表现进行了基准测试。我们比较了一系列方法，包括使用掩码训练目标的单模态方法，以及使用图像文本对比性训练的多模态方法。我们观察到，现有的多模态方法在视觉转移学习任务上并没有超过单模态方法的表现。我们推导出一个信息论观点来解释这些基准结果，该观点为设计一种新型视觉学习者提供了洞察。受此洞察的启发，我们介绍了一种新的视觉表示预训练方法MUlti-modal Generator（MUG），它从

    arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from
    

