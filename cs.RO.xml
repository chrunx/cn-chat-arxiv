<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.RO.xml</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27169;&#24577;&#27493;&#24577;&#35268;&#21010;&#21644;&#27169;&#24577;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#36275;&#37096;&#34892;&#36208;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#20498;&#31435;&#25670;&#27169;&#22411;&#39044;&#27979;&#36275;&#37096;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#36275;&#37096;&#36319;&#36394;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#29289;&#29702;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02662</link><description>&lt;p&gt;
Integrating Model-Based Footstep Planning with Model-Free Reinforcement Learning for Dynamic Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27169;&#24577;&#27493;&#24577;&#35268;&#21010;&#21644;&#27169;&#24577;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#36275;&#37096;&#34892;&#36208;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#20498;&#31435;&#25670;&#27169;&#22411;&#39044;&#27979;&#36275;&#37096;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#36275;&#37096;&#36319;&#36394;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#29289;&#29702;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02662v1 Announce Type: new  Abstract: In this work, we introduce a control framework that combines model-based footstep planning with Reinforcement Learning (RL), leveraging desired footstep patterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing the LIP model, our method forward predicts robot states and determines the desired foot placement given the velocity commands. We then train an RL policy to track the foot placements without following the full reference motions derived from the LIP model. This partial guidance from the physics model allows the RL policy to integrate the predictive capabilities of the physics-informed dynamics and the adaptability characteristics of the RL controller without overfitting the policy to the template model. Our approach is validated on the MIT Humanoid, demonstrating that our policy can achieve stable yet dynamic locomotion for walking and turning. We further validate the adaptability and generalizability of our poli
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#22522;&#20110;Mamba&#27169;&#22411;&#30340;Context-Aware Mamba-based Reinforcement Learning&#65288;CAMRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#22312;&#20154;&#21475;&#23494;&#38598;&#29615;&#22659;&#20013;&#30340;&#25928;&#29575;&#21644;&#25509;&#21463;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;Mamba&#27169;&#22411;&#26469;&#30830;&#23450;&#26426;&#22120;&#20154;&#30340;&#19979;&#19968;&#27493;&#21160;&#20316;&#65292;CAMRL&#24110;&#21161;&#26426;&#22120;&#20154;&#26681;&#25454;&#39044;&#27979;&#30340;&#19979;&#19968;&#20010;&#29366;&#24577;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#20174;&#32780;&#26377;&#25928;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2408.02661</link><description>&lt;p&gt;
Context-aware Mamba-based Reinforcement Learning for social robot navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02661
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#22522;&#20110;Mamba&#27169;&#22411;&#30340;Context-Aware Mamba-based Reinforcement Learning&#65288;CAMRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#22312;&#20154;&#21475;&#23494;&#38598;&#29615;&#22659;&#20013;&#30340;&#25928;&#29575;&#21644;&#25509;&#21463;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;Mamba&#27169;&#22411;&#26469;&#30830;&#23450;&#26426;&#22120;&#20154;&#30340;&#19979;&#19968;&#27493;&#21160;&#20316;&#65292;CAMRL&#24110;&#21161;&#26426;&#22120;&#20154;&#26681;&#25454;&#39044;&#27979;&#30340;&#19979;&#19968;&#20010;&#29366;&#24577;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#20174;&#32780;&#26377;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02661v1 Announce Type: new  Abstract: Social robot navigation (SRN) is a relevant problem that involves navigating a pedestrian-rich environment in a socially acceptable manner. It is an essential part of making social robots effective in pedestrian-rich settings. The use cases of such robots could vary from companion robots to warehouse robots to autonomous wheelchairs. In recent years, deep reinforcement learning has been increasingly used in research on social robot navigation. Our work introduces CAMRL (Context-Aware Mamba-based Reinforcement Learning). Mamba is a new deep learning-based State Space Model (SSM) that has achieved results comparable to transformers in sequencing tasks. CAMRL uses Mamba to determine the robot's next action, which maximizes the value of the next state predicted by the neural network, enabling the robot to navigate effectively based on the rewards assigned. We evaluate CAMRL alongside existing solutions (CADRL, LSTM-RL, SARL) using a rigorous
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31616;&#21333;&#20219;&#21153;&#24320;&#22987;&#36880;&#27493;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#36827;&#34892;&#22797;&#26434;&#36339;&#36291;&#65292;&#35299;&#20915;&#20102;&#33151;&#37096;&#26426;&#22120;&#20154;&#31934;&#30830;&#36339;&#36291;&#30340;&#38590;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#36339;&#36291;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38271;&#26399;&#39134;&#34892;&#38454;&#27573;&#20013;&#36755;&#20837;&#25511;&#21046;&#19981;&#21487;&#29992;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#22312;&#30828;&#20214;&#19978;&#20165;&#38656;&#20960;&#36718;&#35797;&#39564;&#23601;&#33021;&#23558;&#31616;&#21333;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#24212;&#29992;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.02619</link><description>&lt;p&gt;
Mastering Agile Jumping Skills from Simple Practices with Iterative Learning Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31616;&#21333;&#20219;&#21153;&#24320;&#22987;&#36880;&#27493;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#36827;&#34892;&#22797;&#26434;&#36339;&#36291;&#65292;&#35299;&#20915;&#20102;&#33151;&#37096;&#26426;&#22120;&#20154;&#31934;&#30830;&#36339;&#36291;&#30340;&#38590;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#36339;&#36291;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38271;&#26399;&#39134;&#34892;&#38454;&#27573;&#20013;&#36755;&#20837;&#25511;&#21046;&#19981;&#21487;&#29992;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#22312;&#30828;&#20214;&#19978;&#20165;&#38656;&#20960;&#36718;&#35797;&#39564;&#23601;&#33021;&#23558;&#31616;&#21333;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#24212;&#29992;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02619v1 Announce Type: new  Abstract: Achieving precise target jumping with legged robots poses a significant challenge due to the long flight phase and the uncertainties inherent in contact dynamics and hardware. Forcefully attempting these agile motions on hardware could result in severe failures and potential damage. Motivated by these challenging problems, we propose an Iterative Learning Control (ILC) approach that aims to learn and refine jumping skills from easy to difficult, instead of directly learning these challenging tasks. We verify that learning from simplicity can enhance safety and target jumping accuracy over trials. Compared to other ILC approaches for legged locomotion, our method can tackle the problem of a long flight phase where control input is not available. In addition, our approach allows the robot to apply what it learns from a simple jumping task to accomplish more challenging tasks within a few trials directly in hardware, instead of learning fro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#20154;-&#32676;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#25511;&#21046;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#38598;&#20013;&#24335;&#19982;&#20998;&#24067;&#24335;&#25511;&#21046;&#31574;&#30053;&#30340;&#20248;&#21155;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23618;&#27425;&#24335;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#29305;&#28857;&#65292;&#20197;&#25552;&#39640;&#25191;&#34892;&#29615;&#22659;&#30417;&#27979;&#20219;&#21153;&#26102;&#30340;&#25928;&#29575;&#12290;&#28151;&#21512;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#19978;&#21462;&#24471;&#24179;&#34913;&#65292;&#33021;&#22815;&#24212;&#23545;&#36328;&#26102;&#27573;&#30340;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#31181;&#28151;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#38598;&#20013;&#24335;&#31995;&#32479;&#65292;&#38477;&#20302;&#20102;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#35748;&#30693;&#36127;&#25285;&#65292;&#20026;&#26426;&#22120;&#20154;&#38598;&#32676;&#30340;&#26234;&#33021;&#21270;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.02605</link><description>&lt;p&gt;
Trade-offs of Dynamic Control Structure in Human-swarm Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02605
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#20154;-&#32676;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#25511;&#21046;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#38598;&#20013;&#24335;&#19982;&#20998;&#24067;&#24335;&#25511;&#21046;&#31574;&#30053;&#30340;&#20248;&#21155;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23618;&#27425;&#24335;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#29305;&#28857;&#65292;&#20197;&#25552;&#39640;&#25191;&#34892;&#29615;&#22659;&#30417;&#27979;&#20219;&#21153;&#26102;&#30340;&#25928;&#29575;&#12290;&#28151;&#21512;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#19978;&#21462;&#24471;&#24179;&#34913;&#65292;&#33021;&#22815;&#24212;&#23545;&#36328;&#26102;&#27573;&#30340;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#31181;&#28151;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#38598;&#20013;&#24335;&#31995;&#32479;&#65292;&#38477;&#20302;&#20102;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#35748;&#30693;&#36127;&#25285;&#65292;&#20026;&#26426;&#22120;&#20154;&#38598;&#32676;&#30340;&#26234;&#33021;&#21270;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02605v1 Announce Type: new  Abstract: Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment. The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics. Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems. While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload. We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems. We find that a flexible hybrid system can outperform a centralised system (in our environmental mon
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;&#35299;&#30721;&#32908;&#32905;&#21151;&#33021;&#24615;&#32593;&#32476;&#26469;&#25552;&#39640;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32908;&#32905;&#21516;&#27493;&#32780;&#38750;&#21333;&#19968;&#32908;&#32905;&#27963;&#21160;&#26469;&#32534;&#30721;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#20154;&#31867;-&#26426;&#22120;&#30028;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#24863;&#30693;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.02547</link><description>&lt;p&gt;
The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;&#35299;&#30721;&#32908;&#32905;&#21151;&#33021;&#24615;&#32593;&#32476;&#26469;&#25552;&#39640;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32908;&#32905;&#21516;&#27493;&#32780;&#38750;&#21333;&#19968;&#32908;&#32905;&#27963;&#21160;&#26469;&#32534;&#30721;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#20154;&#31867;-&#26426;&#22120;&#30028;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#24863;&#30693;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02547v1 Announce Type: new  Abstract: Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots. Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems. The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions. This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation. We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle con
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20197;&#20026;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38598;&#25104;&#22320;&#25552;&#21462;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#65288;VLN-EventKG&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02535</link><description>&lt;p&gt;
Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20197;&#20026;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38598;&#25104;&#22320;&#25552;&#21462;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#65288;VLN-EventKG&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02535v1 Announce Type: cross  Abstract: Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Multi-Finger Grasping&#20195;&#34920;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20915;&#23450;&#27169;&#22411;&#20165;&#38656;&#25968;&#21315;&#35757;&#32451;&#26679;&#26412;&#21363;&#21487;&#39640;&#25928;&#26144;&#23556;&#33267;multi-finger grasp&#31354;&#38388;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02455</link><description>&lt;p&gt;
A Surprisingly Efficient Representation for Multi-Finger Grasping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Multi-Finger Grasping&#20195;&#34920;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20915;&#23450;&#27169;&#22411;&#20165;&#38656;&#25968;&#21315;&#35757;&#32451;&#26679;&#26412;&#21363;&#21487;&#39640;&#25928;&#26144;&#23556;&#33267;multi-finger grasp&#31354;&#38388;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02455v1 Announce Type: new  Abstract: The problem of grasping objects using a multi-finger hand has received significant attention in recent years. However, it remains challenging to handle a large number of unfamiliar objects in real and cluttered environments. In this work, we propose a representation that can be effectively mapped to the multi-finger grasp space. Based on this representation, we develop a simple decision model that generates accurate grasp quality scores for different multi-finger grasp poses using only hundreds to thousands of training samples. We demonstrate that our representation performs well on a real robot and achieves a success rate of 78.64% after training with only 500 real-world grasp attempts and 87% with 4500 grasp attempts. Additionally, we achieve a success rate of 84.51% in a dynamic human-robot handover scenario using a multi-finger hand.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36712;&#36857;&#29983;&#25104;&#19982;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26080;&#22320;&#22270;&#30340;&#25143;&#22806;&#29615;&#22659;&#20013;&#22788;&#29702;&#22797;&#26434;&#30340;&#23548;&#33322;&#20219;&#21153;&#65292;&#30830;&#20445;&#36335;&#24452;&#26082;&#28385;&#36275;&#29305;&#23450;&#29615;&#22659;&#30340;&#21487;&#36890;&#34892;&#24615;&#32422;&#26463;&#21448;&#33021;&#31526;&#21512;&#20154;&#31867;&#30340;&#36335;&#24452;&#20559;&#22909;&#12290;&#25991;&#31456;&#36890;&#36807;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#27169;&#22411;&#29983;&#25104;&#22810;&#26465;&#20505;&#36873;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#21644;VLMs&#30340;&#38646;&#23556;&#33021;&#21147;&#36827;&#34892;&#36335;&#24452;&#30340;&#36873;&#25321;&#65292;&#20197;&#36866;&#37197;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#20840;&#23616;&#23548;&#33322;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02454</link><description>&lt;p&gt;
TGS: Trajectory Generation and Selection using Vision Language Models in Mapless Outdoor Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36712;&#36857;&#29983;&#25104;&#19982;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26080;&#22320;&#22270;&#30340;&#25143;&#22806;&#29615;&#22659;&#20013;&#22788;&#29702;&#22797;&#26434;&#30340;&#23548;&#33322;&#20219;&#21153;&#65292;&#30830;&#20445;&#36335;&#24452;&#26082;&#28385;&#36275;&#29305;&#23450;&#29615;&#22659;&#30340;&#21487;&#36890;&#34892;&#24615;&#32422;&#26463;&#21448;&#33021;&#31526;&#21512;&#20154;&#31867;&#30340;&#36335;&#24452;&#20559;&#22909;&#12290;&#25991;&#31456;&#36890;&#36807;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#27169;&#22411;&#29983;&#25104;&#22810;&#26465;&#20505;&#36873;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#21644;VLMs&#30340;&#38646;&#23556;&#33021;&#21147;&#36827;&#34892;&#36335;&#24452;&#30340;&#36873;&#25321;&#65292;&#20197;&#36866;&#37197;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#20840;&#23616;&#23548;&#33322;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02454v1 Announce Type: new  Abstract: We present a multi-modal trajectory generation and selection algorithm for real-world mapless outdoor navigation in challenging scenarios with unstructured off-road features like buildings, grass, and curbs. Our goal is to compute suitable trajectories that (1) satisfy the environment-specific traversability constraints and (2) match human-like paths while navigating in crosswalks, sidewalks, etc. Our formulation uses a Conditional Variational Autoencoder (CVAE) generative model enhanced with traversability constraints to generate multiple candidate trajectories for global navigation. We use VLMs and a visual prompting approach with their zero-shot ability of semantic understanding and logical reasoning to choose the best trajectory given the contextual information about the task. We evaluate our methods in various outdoor scenes with wheeled robots and compare the performance with other global navigation algorithms. In practice, we obse
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;RIs-Calib&#26159;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#20272;&#35745;&#30340;3D&#38647;&#36798;&#21644;IMU&#30340;&#26102;&#31354;&#26657;&#20934;&#22120;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20154;&#24037;&#22522;&#30784;&#35774;&#26045;&#25110;&#20808;&#39564;&#30693;&#35782;&#21363;&#21487;&#23454;&#29616;&#20934;&#30830;&#30340;&#26102;&#31354;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2408.02444</link><description>&lt;p&gt;
RIs-Calib: An Open-Source Spatiotemporal Calibrator for Multiple 3D Radars and IMUs Based on Continuous-Time Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;RIs-Calib&#26159;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#20272;&#35745;&#30340;3D&#38647;&#36798;&#21644;IMU&#30340;&#26102;&#31354;&#26657;&#20934;&#22120;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20154;&#24037;&#22522;&#30784;&#35774;&#26045;&#25110;&#20808;&#39564;&#30693;&#35782;&#21363;&#21487;&#23454;&#29616;&#20934;&#30830;&#30340;&#26102;&#31354;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02444v1 Announce Type: new  Abstract: Aided inertial navigation system (INS), typically consisting of an inertial measurement unit (IMU) and an exteroceptive sensor, has been widely accepted as a feasible solution for navigation. Compared with vision-aided and LiDAR-aided INS, radar-aided INS could achieve better performance in adverse weather conditions since the radar utilizes low-frequency measuring signals with less attenuation effect in atmospheric gases and rain. For such a radar-aided INS, accurate spatiotemporal transformation is a fundamental prerequisite to achieving optimal information fusion. In this work, we present RIs-Calib: a spatiotemporal calibrator for multiple 3D radars and IMUs based on continuous-time estimation, which enables accurate spatiotemporal calibration and does not require any additional artificial infrastructure or prior knowledge. Our approach starts with a rigorous and robust procedure for state initialization, followed by batch optimizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#27880;&#20876;&#20195;&#29702;&#65288;CMR-Agent&#65289;&#65292;&#35813;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36880;&#27493;&#35843;&#25972;&#30456;&#26426;&#23039;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02394</link><description>&lt;p&gt;
CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#27880;&#20876;&#20195;&#29702;&#65288;CMR-Agent&#65289;&#65292;&#35813;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36880;&#27493;&#35843;&#25972;&#30456;&#26426;&#23039;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02394v1 Announce Type: cross  Abstract: Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMR-Agent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#36828;&#31243;&#21576;&#29616;&#21644;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#26080;&#25163;&#25805;&#20316;&#30340;3&#33258;&#30001;&#24230;&#33050;&#36367;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#33050;&#36367;&#36890;&#36807;&#20004;&#20010;&#36724;&#21521;&#20542;&#26012;&#23454;&#29616;&#21521;&#21069;&#12289;&#21521;&#21518;&#21644;&#20391;&#21521;&#31227;&#21160;&#65292;&#24182;&#20801;&#35768;&#22260;&#32469;&#22402;&#30452;&#36724;&#26059;&#36716;&#12290;&#25511;&#21046;&#22120;&#20351;&#29992;&#33258;&#35843;&#25972;&#26426;&#21046;&#20445;&#25345;&#21021;&#22987;&#20301;&#32622;&#65292;&#32780;HTC Vive&#36861;&#36394;&#22120;&#34987;&#29992;&#26469;&#23558;&#36861;&#36394;&#22120;&#30340;&#26041;&#21521;&#36716;&#25442;&#20026;&#36816;&#21160;&#21629;&#20196;&#12290;&#36890;&#36807;&#22312;&#33879;&#21517;&#30340;ANA Avatar XPRIZE&#31454;&#36187;&#20013;&#30340;&#25104;&#21151;&#23454;&#25112;&#65292;&#35813;&#33050;&#36367;&#25511;&#21046;&#22120;&#26174;&#31034;&#20986;&#20854;&#23545;&#20110;&#38750;&#19987;&#19994;&#25805;&#20316;&#21592;&#30340;&#25805;&#20316;&#20415;&#21033;&#24615;&#21644;&#22312;&#31359;&#36234;&#38556;&#30861;&#29289;&#12289;&#35299;&#20915;&#20132;&#20114;&#21644;&#25805;&#20316;&#20219;&#21153;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#22823;&#37096;&#20998;&#30001;3D&#25171;&#21360;&#21046;&#36896;&#30340;&#33050;&#36367;&#25511;&#21046;&#22120;&#30340;&#27169;&#22411;&#65292;&#20197;&#20379;&#22797;&#21046;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2408.02319</link><description>&lt;p&gt;
Self-centering 3-DOF feet controller for hands-free locomotion control in telepresence and virtual reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#36828;&#31243;&#21576;&#29616;&#21644;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#26080;&#25163;&#25805;&#20316;&#30340;3&#33258;&#30001;&#24230;&#33050;&#36367;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#33050;&#36367;&#36890;&#36807;&#20004;&#20010;&#36724;&#21521;&#20542;&#26012;&#23454;&#29616;&#21521;&#21069;&#12289;&#21521;&#21518;&#21644;&#20391;&#21521;&#31227;&#21160;&#65292;&#24182;&#20801;&#35768;&#22260;&#32469;&#22402;&#30452;&#36724;&#26059;&#36716;&#12290;&#25511;&#21046;&#22120;&#20351;&#29992;&#33258;&#35843;&#25972;&#26426;&#21046;&#20445;&#25345;&#21021;&#22987;&#20301;&#32622;&#65292;&#32780;HTC Vive&#36861;&#36394;&#22120;&#34987;&#29992;&#26469;&#23558;&#36861;&#36394;&#22120;&#30340;&#26041;&#21521;&#36716;&#25442;&#20026;&#36816;&#21160;&#21629;&#20196;&#12290;&#36890;&#36807;&#22312;&#33879;&#21517;&#30340;ANA Avatar XPRIZE&#31454;&#36187;&#20013;&#30340;&#25104;&#21151;&#23454;&#25112;&#65292;&#35813;&#33050;&#36367;&#25511;&#21046;&#22120;&#26174;&#31034;&#20986;&#20854;&#23545;&#20110;&#38750;&#19987;&#19994;&#25805;&#20316;&#21592;&#30340;&#25805;&#20316;&#20415;&#21033;&#24615;&#21644;&#22312;&#31359;&#36234;&#38556;&#30861;&#29289;&#12289;&#35299;&#20915;&#20132;&#20114;&#21644;&#25805;&#20316;&#20219;&#21153;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#22823;&#37096;&#20998;&#30001;3D&#25171;&#21360;&#21046;&#36896;&#30340;&#33050;&#36367;&#25511;&#21046;&#22120;&#30340;&#27169;&#22411;&#65292;&#20197;&#20379;&#22797;&#21046;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02319v1 Announce Type: new  Abstract: We present a novel seated foot controller for handling 3-DOF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering foot controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20855;&#26377;&#32930;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#30830;&#20445;&#20854;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#29289;&#20307;&#25628;&#32034;&#31561;&#20219;&#21153;&#26102;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2408.02297</link><description>&lt;p&gt;
Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20855;&#26377;&#32930;&#20307;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#30830;&#20445;&#20854;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#29289;&#20307;&#25628;&#32034;&#31561;&#20219;&#21153;&#26102;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02297v1 Announce Type: new  Abstract: Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through calibrated perception probabilities and uncertainty across aggregation and found decisions, thereby adapting the models for sequential tasks. The resulting methods can be directly integrated with pretrained models across a wide family of existing search approaches at no additional training cost. We perform extensive evaluations of aggregation methods across both different semantic perception models and policies, confirming the importance of calib
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25512;&#20986;&#20102;OPENGRASP-LITE 1.0&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;&#26580;&#39034;&#36830;&#26438;&#26426;&#21046;&#30340;&#35302;&#35273;&#20223;&#29983;&#25163;&#65292;&#34701;&#21512;&#20102;MEMS&#21387;&#21147;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#35302;&#35273;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#25569;&#25345;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#29616;&#26377;&#30340;&#35774;&#35745;&#20013;&#39318;&#27425;&#25552;&#20379;&#20102;&#20351;&#29992;MEMS&#20256;&#24863;&#22120;&#21644;&#26580;&#39034;&#36830;&#26438;&#26426;&#21046;&#30340;&#36731;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02293</link><description>&lt;p&gt;
OPENGRASP-LITE Version 1.0: A Tactile Artificial Hand with a Compliant Linkage Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25512;&#20986;&#20102;OPENGRASP-LITE 1.0&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;&#26580;&#39034;&#36830;&#26438;&#26426;&#21046;&#30340;&#35302;&#35273;&#20223;&#29983;&#25163;&#65292;&#34701;&#21512;&#20102;MEMS&#21387;&#21147;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#35302;&#35273;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#25569;&#25345;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#29616;&#26377;&#30340;&#35774;&#35745;&#20013;&#39318;&#27425;&#25552;&#20379;&#20102;&#20351;&#29992;MEMS&#20256;&#24863;&#22120;&#21644;&#26580;&#39034;&#36830;&#26438;&#26426;&#21046;&#30340;&#36731;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02293v1 Announce Type: new  Abstract: Recent research has seen notable progress in the development of linkage-based artificial hands. While previous designs have focused on adaptive grasping, dexterity and biomimetic artificial skin, only a few systems have proposed a lightweight, accessible solution integrating tactile sensing with a compliant linkage-based mechanism. This paper introduces OPENGRASP LITE, an open-source, highly integrated, tactile, and lightweight artificial hand. Leveraging compliant linkage systems and MEMS barometer-based tactile sensing, it offers versatile grasping capabilities with six degrees of actuation. By providing tactile sensors and enabling soft grasping, it serves as an accessible platform for further research in tactile artificial hands.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;ZEST&#30340;&#38646;&#25490;&#25918;&#28023;&#36816;&#22120;&#30340;&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#30701;&#36884;&#28023;&#36335;&#36816;&#36755;&#30340;&#38646;&#25490;&#25918;&#21452;&#20307;&#33337;&#65292;&#26088;&#22312;&#35299;&#20915;&#27839;&#28023;&#21644;&#36817;&#28023;&#33322;&#36816;&#30340;&#30899;&#25490;&#25918;&#38382;&#39064;&#12290;&#27492;&#39033;&#30446;&#30340;&#23454;&#26045;&#23545;&#20110;&#23454;&#29616;&#22269;&#38469;&#28023;&#20107;&#32452;&#32455;&#65288;IMO&#65289;&#21644;&#27431;&#30431;&#32511;&#33394;&#21327;&#35758;&#20013;&#35268;&#23450;&#30340;&#30899;&#25490;&#25918;&#20943;&#25490;&#30446;&#26631;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.02277</link><description>&lt;p&gt;
Integrating a Digital Twin Concept in the Zero Emission Sea Transporter (ZEST) Project for Sustainable Maritime Transport using Stonefish Simulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;ZEST&#30340;&#38646;&#25490;&#25918;&#28023;&#36816;&#22120;&#30340;&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#30701;&#36884;&#28023;&#36335;&#36816;&#36755;&#30340;&#38646;&#25490;&#25918;&#21452;&#20307;&#33337;&#65292;&#26088;&#22312;&#35299;&#20915;&#27839;&#28023;&#21644;&#36817;&#28023;&#33322;&#36816;&#30340;&#30899;&#25490;&#25918;&#38382;&#39064;&#12290;&#27492;&#39033;&#30446;&#30340;&#23454;&#26045;&#23545;&#20110;&#23454;&#29616;&#22269;&#38469;&#28023;&#20107;&#32452;&#32455;&#65288;IMO&#65289;&#21644;&#27431;&#30431;&#32511;&#33394;&#21327;&#35758;&#20013;&#35268;&#23450;&#30340;&#30899;&#25490;&#25918;&#20943;&#25490;&#30446;&#26631;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02277v1 Announce Type: new  Abstract: In response to stringent emission reduction targets imposed by the International Maritime Organization (IMO) and the European Green Deal's Fit for 55 legislation package, the maritime industry has shifted its focus towards decarbonization. While significant attention has been placed on vessels exceeding 5,000 gross tons (GT), emissions from coastal and short sea shipping, amounting to approximately 13% of global shipping transportation and 15% within the European Union (EU), have not received adequate consideration. This abstract introduces the Zero Emission Sea Transporter (ZEST) project, designed to address this issue by developing a zero-emissions multi-purpose catamaran for short sea route
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#37096;&#32626;&#22312;&#22810;&#25351;&#22841; grippers &#19978;&#30340;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25163;&#25351;&#30340;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#26469;&#22686;&#24378;&#35302;&#35273;&#24863;&#30693;&#24615;&#33021;&#12290;&#25991;&#31456;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21516;&#27493;&#22270;&#20687;&#37319;&#38598;&#31995;&#32479;&#65292;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#35774;&#35745;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#20010;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#32852;&#21512;&#26657;&#20934;&#25928;&#29575;&#12290;&#22312;&#39564;&#35777;&#36825;&#19968;&#31995;&#32479;&#30340;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#19977;&#25351;&#22841;&#25345;&#26426;&#22120;&#20154;&#35013;&#22791;&#26377;7&#20010;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#34920;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#35302;&#35273;&#34920;&#38754;&#35206;&#30422;&#19979;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02206</link><description>&lt;p&gt;
Large-scale Deployment of Vision-based Tactile Sensors on Multi-fingered Grippers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#37096;&#32626;&#22312;&#22810;&#25351;&#22841; grippers &#19978;&#30340;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25163;&#25351;&#30340;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#26469;&#22686;&#24378;&#35302;&#35273;&#24863;&#30693;&#24615;&#33021;&#12290;&#25991;&#31456;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21516;&#27493;&#22270;&#20687;&#37319;&#38598;&#31995;&#32479;&#65292;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#35774;&#35745;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#20010;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#32852;&#21512;&#26657;&#20934;&#25928;&#29575;&#12290;&#22312;&#39564;&#35777;&#36825;&#19968;&#31995;&#32479;&#30340;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#19977;&#25351;&#22841;&#25345;&#26426;&#22120;&#20154;&#35013;&#22791;&#26377;7&#20010;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#34920;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#35302;&#35273;&#34920;&#38754;&#35206;&#30422;&#19979;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02206v1 Announce Type: new  Abstract: Vision-based Tactile Sensors (VBTSs) show significant promise in that they can leverage image measurements to provide high-spatial-resolution human-like performance. However, current VBTS designs, typically confined to the fingertips of robotic grippers, prove somewhat inadequate, as many grasping and manipulation tasks require multiple contact points with the object. With an end goal of enabling large-scale, multi-surface tactile sensing via VBTSs, our research (i) develops a synchronized image acquisition system with minimal latency,(ii) proposes a modularized VBTS design for easy integration into finger phalanges, and (iii) devises a zero-shot calibration approach to improve data efficiency in the simultaneous calibration of multiple VBTSs. In validating the system within a miniature 3-fingered robotic gripper equipped with 7 VBTSs we demonstrate improved tactile perception performance by covering the contact surfaces of both gripper 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoPotter&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32467;&#26500;&#20808;&#39564;&#30340;&#36830;&#32493;&#21487;&#21464;&#24418;&#29289;&#20307;&#65288;&#22914;&#38518;&#22303;&#65289;&#30340;&#25805;&#32437;&#65292;&#20026;&#23454;&#29616;&#26426;&#22120;&#20154;&#25163;&#24037;&#33402;&#25216;&#33021;&#21644;&#24863;&#30693;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02184</link><description>&lt;p&gt;
RoPotter: Toward Robotic Pottery and Deformable Object Manipulation with Structural Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoPotter&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32467;&#26500;&#20808;&#39564;&#30340;&#36830;&#32493;&#21487;&#21464;&#24418;&#29289;&#20307;&#65288;&#22914;&#38518;&#22303;&#65289;&#30340;&#25805;&#32437;&#65292;&#20026;&#23454;&#29616;&#26426;&#22120;&#20154;&#25163;&#24037;&#33402;&#25216;&#33021;&#21644;&#24863;&#30693;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02184v1 Announce Type: new  Abstract: Humans are capable of continuously manipulating a wide variety of deformable objects into complex shapes. This is made possible by our intuitive understanding of material properties and mechanics of the object, for reasoning about object states even when visual perception is occluded. These capabilities allow us to perform diverse tasks ranging from cooking with dough to expressing ourselves with pottery-making. However, developing robotic systems to robustly perform similar tasks remains challenging, as current methods struggle to effectively model volumetric deformable objects and reason about the complex behavior they typically exhibit. To study the robotic systems and algorithms capable of deforming volumetric objects, we introduce a novel robotics task of continuously deforming clay on a pottery wheel. We propose a pipeline for perception and pottery skill-learning, called RoPotter, wherein we demonstrate that structural priors spec
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#24494;&#22609;&#26009;&#25910;&#38598;&#21322;&#28508;&#27700;&#24179;&#21488;&#65292;&#26088;&#22312;&#25913;&#21892;&#24494;&#22609;&#26009;&#22312;&#28023;&#27700;&#20013;&#30340;&#28165;&#38500;&#25928;&#29575;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#28023;&#27915;&#29615;&#22659;&#25913;&#21892;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.02162</link><description>&lt;p&gt;
Improvement and Empirical Testing of a Novel Autonomous Microplastics-Collecting Semisubmersible
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#24494;&#22609;&#26009;&#25910;&#38598;&#21322;&#28508;&#27700;&#24179;&#21488;&#65292;&#26088;&#22312;&#25913;&#21892;&#24494;&#22609;&#26009;&#22312;&#28023;&#27700;&#20013;&#30340;&#28165;&#38500;&#25928;&#29575;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#28023;&#27915;&#29615;&#22659;&#25913;&#21892;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02162v1 Announce Type: new  Abstract: Since their invention, plastics have become ubiquitous in modern societies all around the world, and their impact on the environment has, in recent years, become nearly as well-known. Plastics produced by humans have reached nearly every corner of the world, and throughout their centuries-long lifetimes, plastics continually break down into smaller and smaller particles due to the physical stresses which they are subjected to. These stresses eventually, inevitably, break these plastics down into microplastics -pieces of plastic small enough to be consumed by organisms in bodies of water throughout the globe. These microplastics can very easily bioaccumulate, and have been found everywhere from the Great Lakes to the bloodstreams of humans. The effects of these plastics are poorly understood, however, they have been linked to infertility, halted growth, and a host of other maladies in aquatic organisms. Currently, removal of these plastic
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24102;&#26377;&#32518;&#32499;&#30340;&#36127;&#40736;&#22411;&#26426;&#22120;&#20154;&#31995;&#32479;&#35774;&#35745;&#30340;&#39640;&#25928;&#36335;&#24452;&#35268;&#21010;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#21253;&#25324;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;(UGV)&#12289;&#26080;&#20154;&#31354;&#20013;&#36710;&#36742;(UAV)&#21644;&#36830;&#25509;&#20004;&#32773;&#30340;&#32518;&#32499;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#35299;&#20915;&#19968;&#20010;&#29305;&#23450;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#36991;&#24320;&#22320;&#38754;&#21644;&#31354;&#20013;&#38556;&#30861;&#29289;&#65292;&#36825;&#20123;&#38556;&#30861;&#29289;&#34987;&#24314;&#27169;&#20026;&#19977;&#32500;&#31435;&#26041;&#20307;&#12290;&#21021;&#22987;&#37197;&#32622;&#20013;&#65292;UAV&#20301;&#20110;UGV&#39030;&#37096;&#65292;&#30446;&#26631;&#26159;&#20351;UAV&#21040;&#36798;&#31354;&#20013;&#30446;&#26631;&#28857;&#12290;&#20551;&#35774;UGV&#20808;&#31227;&#21160;&#21040;&#19968;&#20010;&#20301;&#32622;&#65292;&#20351;UAV&#21487;&#20197;&#20174;&#37027;&#37324;&#36215;&#39134;&#24182;&#22312;&#22402;&#30452;&#24179;&#38754;&#20869;&#39134;&#34892;&#20197;&#21040;&#36798;&#31354;&#20013;&#30446;&#26631;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#20197;&#36817;&#20284;&#26368;&#20248;&#35299;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#22320;&#38754;&#21644;&#31354;&#20013;&#36335;&#24452;&#30340;&#24635;&#38271;&#24230;&#12290;&#25991;&#31456;&#20551;&#35774;&#32518;&#32499;&#26159;&#26494;&#24347;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#36951;&#20256;&#31639;&#27861;&#21644;&#38556;&#30861;&#29289;&#36991;&#35753;&#31574;&#30053;&#65292;&#20197;&#22312;&#22810;&#21464;&#37327;&#35268;&#21010;&#38382;&#39064;&#20013;&#25214;&#21040;&#19968;&#26465;&#36335;&#24452;&#65292;&#20351;UAV&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#23433;&#20840;&#21040;&#36798;&#30446;&#26631;&#65292;&#19988;&#21516;&#26102;&#32771;&#34385;&#21040;UGV&#30340;&#27963;&#21160;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2408.02141</link><description>&lt;p&gt;
An efficient strategy for path planning with a tethered marsupial robotics system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02141
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24102;&#26377;&#32518;&#32499;&#30340;&#36127;&#40736;&#22411;&#26426;&#22120;&#20154;&#31995;&#32479;&#35774;&#35745;&#30340;&#39640;&#25928;&#36335;&#24452;&#35268;&#21010;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#21253;&#25324;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;(UGV)&#12289;&#26080;&#20154;&#31354;&#20013;&#36710;&#36742;(UAV)&#21644;&#36830;&#25509;&#20004;&#32773;&#30340;&#32518;&#32499;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#35299;&#20915;&#19968;&#20010;&#29305;&#23450;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#36991;&#24320;&#22320;&#38754;&#21644;&#31354;&#20013;&#38556;&#30861;&#29289;&#65292;&#36825;&#20123;&#38556;&#30861;&#29289;&#34987;&#24314;&#27169;&#20026;&#19977;&#32500;&#31435;&#26041;&#20307;&#12290;&#21021;&#22987;&#37197;&#32622;&#20013;&#65292;UAV&#20301;&#20110;UGV&#39030;&#37096;&#65292;&#30446;&#26631;&#26159;&#20351;UAV&#21040;&#36798;&#31354;&#20013;&#30446;&#26631;&#28857;&#12290;&#20551;&#35774;UGV&#20808;&#31227;&#21160;&#21040;&#19968;&#20010;&#20301;&#32622;&#65292;&#20351;UAV&#21487;&#20197;&#20174;&#37027;&#37324;&#36215;&#39134;&#24182;&#22312;&#22402;&#30452;&#24179;&#38754;&#20869;&#39134;&#34892;&#20197;&#21040;&#36798;&#31354;&#20013;&#30446;&#26631;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#20197;&#36817;&#20284;&#26368;&#20248;&#35299;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#22320;&#38754;&#21644;&#31354;&#20013;&#36335;&#24452;&#30340;&#24635;&#38271;&#24230;&#12290;&#25991;&#31456;&#20551;&#35774;&#32518;&#32499;&#26159;&#26494;&#24347;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#36951;&#20256;&#31639;&#27861;&#21644;&#38556;&#30861;&#29289;&#36991;&#35753;&#31574;&#30053;&#65292;&#20197;&#22312;&#22810;&#21464;&#37327;&#35268;&#21010;&#38382;&#39064;&#20013;&#25214;&#21040;&#19968;&#26465;&#36335;&#24452;&#65292;&#20351;UAV&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#23433;&#20840;&#21040;&#36798;&#30446;&#26631;&#65292;&#19988;&#21516;&#26102;&#32771;&#34385;&#21040;UGV&#30340;&#27963;&#21160;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02141v1 Announce Type: new  Abstract: A marsupial robotics system comprises three components: an Unmanned Ground Vehicle (UGV), an Unmanned Aerial Vehicle (UAV), and a tether connecting both robots. Marsupial systems are highly beneficial in industry as they extend the UAV's battery life during flight. This paper introduces a novel strategy for a specific path planning problem in marsupial systems, where each of the components must avoid collisions with ground and aerial obstacles modeled as 3D cuboids. Given an initial configuration in which the UAV is positioned atop the UGV, the goal is to reach an aerial target with the UAV. We assume that the UGV first moves to a position from which the UAV can take off and fly through a vertical plane to reach an aerial target. We propose an approach that discretizes the space to approximate an optimal solution, minimizing the sum of the lengths of the ground and air paths. First, we assume a taut tether and use a novel algorithm that 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20572;&#36710;&#22330;&#20840;&#31243;&#35268;&#21010;&#31995;&#32479;&#65288;ParkingE2E&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#22270;&#20687;&#21040;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23436;&#25104;&#22797;&#26434;&#20572;&#36710;&#22330;&#26223;&#19979;&#30340;&#20219;&#21153;&#65292;&#20026;&#26234;&#33021;&#39550;&#39542;&#36710;&#36742;&#30340;&#33258;&#20027;&#27850;&#36710;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02061</link><description>&lt;p&gt;
ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20572;&#36710;&#22330;&#20840;&#31243;&#35268;&#21010;&#31995;&#32479;&#65288;ParkingE2E&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#22270;&#20687;&#21040;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23436;&#25104;&#22797;&#26434;&#20572;&#36710;&#22330;&#26223;&#19979;&#30340;&#20219;&#21153;&#65292;&#20026;&#26234;&#33021;&#39550;&#39542;&#36710;&#36742;&#30340;&#33258;&#20027;&#27850;&#36710;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02061v1 Announce Type: cross  Abstract: Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conducted extensive experiments in real-world scenarios, and the resul
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EqvAfford&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20445;&#35777;&#28857;&#32423; affordance&#23398;&#20064;&#20013;&#30340;equivariance&#65292;&#20026;&#19979;&#28216;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#23039;&#24577;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01953</link><description>&lt;p&gt;
EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EqvAfford&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20445;&#35777;&#28857;&#32423; affordance&#23398;&#20064;&#20013;&#30340;equivariance&#65292;&#20026;&#19979;&#28216;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#23039;&#24577;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01953v1 Announce Type: new  Abstract: Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#35266;&#27979;&#19981;&#30830;&#23450;&#24615;&#21516;&#26102;&#20272;&#35745;&#23039;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;PnP&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.01945</link><description>&lt;p&gt;
Generalized Maximum Likelihood Estimation for Perspective-n-Point Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#35266;&#27979;&#19981;&#30830;&#23450;&#24615;&#21516;&#26102;&#20272;&#35745;&#23039;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;PnP&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01945v1 Announce Type: cross  Abstract: The Perspective-n-Point (PnP) problem has been widely studied in the literature and applied in various vision-based pose estimation scenarios. However, existing methods ignore the anisotropy uncertainty of observations, as demonstrated in several real-world datasets in this paper. This oversight may lead to suboptimal and inaccurate estimation, particularly in the presence of noisy observations. To this end, we propose a generalized maximum likelihood PnP solver, named GMLPnP, that minimizes the determinant criterion by iterating the GLS procedure to estimate the pose and uncertainty simultaneously. Further, the proposed method is decoupled from the camera model. Results of synthetic and real experiments show that our method achieves better accuracy in common pose estimation scenarios, GMLPnP improves rotation/translation accuracy by 4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best baseline. It is more ac
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21010;&#26102;&#20195;&#30340;&#36129;&#29486;&#22312;&#20110;&#65292;&#21033;&#29992;&#33258;&#28982;&#30028;&#20013;&#30340;&#29983;&#29289;&#26234;&#33021;&#65292;&#21457;&#23637;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#65292;&#25353;&#29031;&#30740;&#31350;&#32773;&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#65292;&#25104;&#21151;&#22320;&#23558;&#30005;&#21050;&#28608;&#24212;&#29992;&#20110;&#27169;&#20223;&#28023;&#34544;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20854;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#25805;&#25511;&#33021;&#21147;&#65292;&#20197;&#21450;&#23454;&#26102;&#39044;&#27979;&#20854;&#34892;&#20026;&#30340;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01941</link><description>&lt;p&gt;
A Jellyfish Cyborg: Exploiting Natural Embodied Intelligence as Soft Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21010;&#26102;&#20195;&#30340;&#36129;&#29486;&#22312;&#20110;&#65292;&#21033;&#29992;&#33258;&#28982;&#30028;&#20013;&#30340;&#29983;&#29289;&#26234;&#33021;&#65292;&#21457;&#23637;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#65292;&#25353;&#29031;&#30740;&#31350;&#32773;&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#65292;&#25104;&#21151;&#22320;&#23558;&#30005;&#21050;&#28608;&#24212;&#29992;&#20110;&#27169;&#20223;&#28023;&#34544;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20854;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#25805;&#25511;&#33021;&#21147;&#65292;&#20197;&#21450;&#23454;&#26102;&#39044;&#27979;&#20854;&#34892;&#20026;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01941v1 Announce Type: new  Abstract: In the advanced field of bio-inspired robotics, the emergence of cyborgs represents the successful integration of engineering and biological systems. Building on previous research that showed how electrical stimuli could initiate and speed up a jellyfish's movement, this study presents a groundbreaking approach that explores how the natural embodied intelligence of the animal can be harnessed to address pivotal challenges such as spontaneous exploration, navigation in various environments, control of whole-body motion, and real-time predictions of behavior. We have developed a comprehensive data acquisition system and a unique setup for stimulating jellyfish, allowing for a detailed study of their movements. Through careful analysis of both spontaneous behaviors and behaviors induced by targeted stimulation, we have identified subtle differences between natural and induced motion patterns. By using a machine learning method called physic
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFSTL&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20248;&#21270;&#20215;&#20540;&#20989;&#25968;&#31354;&#38388;&#26469;&#25552;&#39640;Signal Temporal Logic&#65288;STL&#65289;&#23450;&#20041;&#30340;&#35268;&#26684;&#30340;&#40065;&#26834;&#24615;&#20215;&#20540;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#25163;&#32534;&#21046;&#30340;&#35859;&#35789;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39044;&#20808;&#35757;&#32451;&#30340;&#25216;&#33021;&#24182;&#26681;&#25454;&#26410;&#35265;&#30340;STL&#35268;&#26684;&#23545;&#20854;&#36827;&#34892;&#20248;&#20808;&#32423;&#23433;&#25490;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38271;&#26102;&#38388;&#35745;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01923</link><description>&lt;p&gt;
Scalable Signal Temporal Logic Guided Reinforcement Learning via Value Function Space Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFSTL&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20248;&#21270;&#20215;&#20540;&#20989;&#25968;&#31354;&#38388;&#26469;&#25552;&#39640;Signal Temporal Logic&#65288;STL&#65289;&#23450;&#20041;&#30340;&#35268;&#26684;&#30340;&#40065;&#26834;&#24615;&#20215;&#20540;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#25163;&#32534;&#21046;&#30340;&#35859;&#35789;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39044;&#20808;&#35757;&#32451;&#30340;&#25216;&#33021;&#24182;&#26681;&#25454;&#26410;&#35265;&#30340;STL&#35268;&#26684;&#23545;&#20854;&#36827;&#34892;&#20248;&#20808;&#32423;&#23433;&#25490;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38271;&#26102;&#38388;&#35745;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01923v1 Announce Type: new  Abstract: The integration of reinforcement learning (RL) and formal methods has emerged as a promising framework for solving long-horizon planning problems. Conventional approaches typically involve abstraction of the state and action spaces and manually created labeling functions or predicates. However, the efficiency of these approaches deteriorates as the tasks become increasingly complex, which results in exponential growth in the size of labeling functions or predicates. To address these issues, we propose a scalable model-based RL framework, called VFSTL, which schedules pre-trained skills to follow unseen STL specifications without using hand-crafted predicates. Given a set of value functions obtained by goal-conditioned RL, we formulate an optimization problem to maximize the robustness value of Signal Temporal Logic (STL) defined specifications, which is computed using value functions as predicates. To further reduce the computation burde
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22312;&#38646; Shot ObjectNav &#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#19982;&#20855;&#26377;&#19968;&#23450;&#20840;&#23616;&#35270;&#37326;&#30340;&#31354;&#20013;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#30340;&#20027;&#20307;&#20195;&#29702;&#65292;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#36890;&#20449;&#65288;GC&#65289;&#30340;&#24773;&#20917;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#36825;&#31181;&#36890;&#20449;&#26041;&#24335;&#33021;&#22815;&#24110;&#21161;&#20027;&#20307;&#20195;&#29702;&#22312;&#27809;&#26377;&#29615;&#22659;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23547;&#25214;&#30446;&#26631;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20998;&#26512;&#20102;&#36825;&#31181;&#20132;&#20114;&#20013;&#20986;&#29616;&#30340;&#24187;&#24819;&#21644;&#21512;&#20316;&#34892;&#20026;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#24187;&#24819;&#34892;&#20026;&#65292;&#36825;&#25552;&#39640;&#20102;&#20027;&#20307;&#20195;&#29702;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01877</link><description>&lt;p&gt;
Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22312;&#38646; Shot ObjectNav &#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#19982;&#20855;&#26377;&#19968;&#23450;&#20840;&#23616;&#35270;&#37326;&#30340;&#31354;&#20013;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#30340;&#20027;&#20307;&#20195;&#29702;&#65292;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#36890;&#20449;&#65288;GC&#65289;&#30340;&#24773;&#20917;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#36825;&#31181;&#36890;&#20449;&#26041;&#24335;&#33021;&#22815;&#24110;&#21161;&#20027;&#20307;&#20195;&#29702;&#22312;&#27809;&#26377;&#29615;&#22659;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23547;&#25214;&#30446;&#26631;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20998;&#26512;&#20102;&#36825;&#31181;&#20132;&#20114;&#20013;&#20986;&#29616;&#30340;&#24187;&#24819;&#21644;&#21512;&#20316;&#34892;&#20026;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#24187;&#24819;&#34892;&#20026;&#65292;&#36825;&#25552;&#39640;&#20102;&#20027;&#20307;&#20195;&#29702;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01877v1 Announce Type: new  Abstract: In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a target object specified by a natural language label without any environment-specific fine-tuning. This is challenging, given the limited view of a ground agent and its independent exploratory behavior. To address these issues, we consider an assistive overhead agent with a bounded global view alongside the ground agent and present two coordinated navigation schemes for judicious exploration. We establish the influence of the Generative Communication (GC) between the embodied agents equipped with Vision-Language Models (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in the ground agent's ability to find the target object in comparison with an unassisted setup in simulation. We further analyze the GC for unique traits quantifying the presence of hallucination and cooperation. In particular, we identify a unique trait of "preemptive hallucinat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;TrustNavGPT&#27169;&#22411;&#65292;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#24182;&#35780;&#20272;&#35821;&#38899;&#25351;&#20196;&#20013;&#30340;&#24773;&#24863;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#20154;&#22522;&#20110;&#38899;&#39057;&#23548;&#33322;&#30340;&#20449;&#20219;&#24230;&#65292;&#24182;&#36991;&#20813;&#35823;&#35299;&#20154;&#31867;&#25351;&#20196;&#23548;&#33268;&#30340;&#38169;&#35823;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2408.01867</link><description>&lt;p&gt;
TrustNavGPT: Modeling Uncertainty to Improve Trustworthiness of Audio-Guided LLM-Based Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;TrustNavGPT&#27169;&#22411;&#65292;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#24182;&#35780;&#20272;&#35821;&#38899;&#25351;&#20196;&#20013;&#30340;&#24773;&#24863;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#20154;&#22522;&#20110;&#38899;&#39057;&#23548;&#33322;&#30340;&#20449;&#20219;&#24230;&#65292;&#24182;&#36991;&#20813;&#35823;&#35299;&#20154;&#31867;&#25351;&#20196;&#23548;&#33268;&#30340;&#38169;&#35823;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01867v1 Announce Type: new  Abstract: While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present TrustNavGPT, an LLM based audio guided navigation agent that uses affective cues in spoken communication elements such as tone and inflection that convey meaning beyond words, allowing it to assess the trustworthiness of human commands and make effective, safe decisions. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;BEVPlace++&#65292;&#19968;&#31181;&#22312;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#19978;&#20351;&#29992;&#30340;&#24555;&#36895;&#12289;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#30340;&#28608;&#20809;&#38647;&#36798;&#20840;&#23616;&#23450;&#20301;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#29983;&#25104;&#30340;&#40479;&#30640;&#35270;&#22270;&#65288;BEV&#65289;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#36890;&#36807;Place Recognition&#21644;3&#20010;&#33258;&#30001;&#24230;&#65288;DoF&#65289;&#23039;&#24577;&#20272;&#35745;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#23616;&#23450;&#20301;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;CNN&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26469;&#33258;BEV&#22270;&#20687;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#21305;&#37197;&#20004;&#24352;&#20855;&#26377;&#36739;&#22823;&#24179;&#31227;&#37327;&#30340;BEV&#22270;&#20687;&#30340;keypoints&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25991;&#31456;&#35774;&#35745;&#20102;&#19968;&#20010;&#26059;&#36716;&#31561;&#21464;&#27169;&#22359;&#65288;REM&#65289;&#26469;&#25552;&#21462;&#29420;&#29305;&#30340;&#29305;&#24449;&#24182;&#22686;&#24378;&#20102;&#23545;&#26059;&#36716;&#21464;&#21270;&#40065;&#26834;&#24615;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#36890;&#36807;&#23558;REM&#21644;&#25551;&#36848;&#31526;&#29983;&#25104;&#22120;NetVLAD&#20018;&#32852;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26059;&#36716;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#32593;&#32476;&#65288;REIN&#65289;&#65292;&#20174;&#32780;&#29983;&#25104;&#26059;&#36716;&#31561;&#21464;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24555;&#36895;&#24615;&#21644;&#31934;&#24230;&#65292;&#34920;&#26126;&#23427;&#22312;&#23454;&#29616;&#39640;&#31934;&#24230;&#20840;&#23616;&#23450;&#20301;&#30340;&#21516;&#26102;&#65292;&#36824;&#20445;&#25345;&#20102;&#36739;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01841</link><description>&lt;p&gt;
BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for Unmanned Ground Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;BEVPlace++&#65292;&#19968;&#31181;&#22312;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#19978;&#20351;&#29992;&#30340;&#24555;&#36895;&#12289;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#30340;&#28608;&#20809;&#38647;&#36798;&#20840;&#23616;&#23450;&#20301;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#29983;&#25104;&#30340;&#40479;&#30640;&#35270;&#22270;&#65288;BEV&#65289;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#36890;&#36807;Place Recognition&#21644;3&#20010;&#33258;&#30001;&#24230;&#65288;DoF&#65289;&#23039;&#24577;&#20272;&#35745;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#23616;&#23450;&#20301;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;CNN&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26469;&#33258;BEV&#22270;&#20687;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#21305;&#37197;&#20004;&#24352;&#20855;&#26377;&#36739;&#22823;&#24179;&#31227;&#37327;&#30340;BEV&#22270;&#20687;&#30340;keypoints&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25991;&#31456;&#35774;&#35745;&#20102;&#19968;&#20010;&#26059;&#36716;&#31561;&#21464;&#27169;&#22359;&#65288;REM&#65289;&#26469;&#25552;&#21462;&#29420;&#29305;&#30340;&#29305;&#24449;&#24182;&#22686;&#24378;&#20102;&#23545;&#26059;&#36716;&#21464;&#21270;&#40065;&#26834;&#24615;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#36890;&#36807;&#23558;REM&#21644;&#25551;&#36848;&#31526;&#29983;&#25104;&#22120;NetVLAD&#20018;&#32852;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26059;&#36716;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#32593;&#32476;&#65288;REIN&#65289;&#65292;&#20174;&#32780;&#29983;&#25104;&#26059;&#36716;&#31561;&#21464;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24555;&#36895;&#24615;&#21644;&#31934;&#24230;&#65292;&#34920;&#26126;&#23427;&#22312;&#23454;&#29616;&#39640;&#31934;&#24230;&#20840;&#23616;&#23450;&#20301;&#30340;&#21516;&#26102;&#65292;&#36824;&#20445;&#25345;&#20102;&#36739;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01841v1 Announce Type: new  Abstract: This article introduces BEVPlace++, a novel, fast, and robust LiDAR global localization method for unmanned ground vehicles. It uses lightweight convolutional neural networks (CNNs) on Bird's Eye View (BEV) image-like representations of LiDAR data to achieve accurate global localization through place recognition followed by 3-DoF pose estimation. Our detailed analyses reveal an interesting fact that CNNs are inherently effective at extracting distinctive features from LiDAR BEV images. Remarkably, keypoints of two BEV images with large translations can be effectively matched using CNN-extracted features. Building on this insight, we design a rotation equivariant module (REM) to obtain distinctive features while enhancing robustness to rotational changes. A Rotation Equivariant and Invariant Network (REIN) is then developed by cascading REM and a descriptor generator, NetVLAD, to sequentially generate rotation equivariant local features a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20559;&#24046;&#30693;&#24773; situational graphs&#8221;&#65288;diS-Graphs&#65289;&#30340;&#23454;&#26102;&#29615;&#22659;&#23450;&#20301;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#24314;&#31569;&#29289;&#23454;&#38469;&#35268;&#21010;&#21644;&#35774;&#35745;&#20043;&#38388;&#30340;&#20559;&#24046;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#36825;&#20123;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#23450;&#20301;&#21644;&#26144;&#23556;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01737</link><description>&lt;p&gt;
Real-time Localization and Mapping in Architectural Plans with Deviations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20559;&#24046;&#30693;&#24773; situational graphs&#8221;&#65288;diS-Graphs&#65289;&#30340;&#23454;&#26102;&#29615;&#22659;&#23450;&#20301;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#24314;&#31569;&#29289;&#23454;&#38469;&#35268;&#21010;&#21644;&#35774;&#35745;&#20043;&#38388;&#30340;&#20559;&#24046;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#36825;&#20123;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#23450;&#20301;&#21644;&#26144;&#23556;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01737v1 Announce Type: new  Abstract: Having prior knowledge of an environment boosts the localization and mapping accuracy of robots. Several approaches in the literature have utilized architectural plans in this regard. However, almost all of them overlook the deviations between actual as-built environments and as-planned architectural designs, introducing bias in the estimations. To address this issue, we present a novel localization and mapping method denoted as deviations-informed Situational Graphs or diS-Graphs that integrates prior knowledge from architectural plans even in the presence of deviations. It is based on Situational Graphs (S-Graphs) that merge geometric models of the environment with 3D scene graphs into a multi-layered jointly optimizable factor graph. Our diS-Graph extracts information from architectural plans by first modeling them as a hierarchical factor graph, which we will call an Architectural Graph (A-Graph). While the robot explores the real en
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#32508;&#36848;&#20102;&#30005;&#21160;&#20551;&#32930;&#25216;&#26415;&#65292;&#20854;&#20013;&#28041;&#21450;&#31070;&#32463;&#20551;&#20307;&#12289;&#36719;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#19982;&#25511;&#21046;&#31574;&#30053;&#12290;&#25991;&#31456;&#27010;&#36848;&#20102;&#30005;&#21160;&#20551;&#32930;&#25216;&#26415;&#30340;&#33539;&#22260;&#65292;&#20171;&#32461;&#31070;&#32463;&#20551;&#20307;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#23545;&#22810;&#31181;&#36719;&#20307;&#26426;&#22120;&#20154;&#21160;&#21147;&#28304;&#21644;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#20256;&#32479;&#30828;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21033;&#24330;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#23637;&#26395;&#12290;</title><link>https://arxiv.org/abs/2408.01729</link><description>&lt;p&gt;
A Survey on Robotic Prosthetics: Neuroprosthetics, Soft Actuators, and Control Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#32508;&#36848;&#20102;&#30005;&#21160;&#20551;&#32930;&#25216;&#26415;&#65292;&#20854;&#20013;&#28041;&#21450;&#31070;&#32463;&#20551;&#20307;&#12289;&#36719;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#19982;&#25511;&#21046;&#31574;&#30053;&#12290;&#25991;&#31456;&#27010;&#36848;&#20102;&#30005;&#21160;&#20551;&#32930;&#25216;&#26415;&#30340;&#33539;&#22260;&#65292;&#20171;&#32461;&#31070;&#32463;&#20551;&#20307;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#23545;&#22810;&#31181;&#36719;&#20307;&#26426;&#22120;&#20154;&#21160;&#21147;&#28304;&#21644;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#20256;&#32479;&#30828;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21033;&#24330;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01729v1 Announce Type: new  Abstract: The field of robotics is a quickly evolving feat of technology that accepts contributions from various genres of science. Neuroscience, Physiology, Chemistry, Material science, Computer science, and the wide umbrella of mechatronics have all simultaneously contributed to many innovations in the prosthetic applications of robotics. This review begins with a discussion of the scope of the term robotic prosthetics and discusses the evolving domain of Neuroprosthetics. The discussion is then constrained to focus on various actuation and control strategies for robotic prosthetic limbs. This review discusses various soft robotic actuators such as EAP, SMA, FFA, etc., and the merits of such actuators over conventional hard robotic actuators. Options in control strategies for robotic prosthetics, that are in various states of research and development, are reviewed. This paper concludes the discussion with an analysis regarding the prospective di
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#35780;&#20272;&#20102;&#20960;&#31181;&#24320;&#28304;&#30340;&#35270;&#35273;&#24815;&#24615;SLAM&#31995;&#32479;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#22238;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22238;&#29615;&#38381;&#21512;&#23545;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#20855;&#26377;&#26174;&#33879;&#25928;&#24212;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#19981;&#21516;&#22270;&#20687;&#25293;&#25668;&#39057;&#29575;&#23545;&#23450;&#20301;&#31934;&#24230;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#25991;&#31456;&#25552;&#20986;&#30340;&#36825;&#20123;&#35780;&#20272;&#23545;&#20110;&#23558;&#36825;&#20123;SLAM&#31995;&#32479;&#24212;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2408.01716</link><description>&lt;p&gt;
Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the Benefits and Computational Costs of Loop Closing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01716
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#35780;&#20272;&#20102;&#20960;&#31181;&#24320;&#28304;&#30340;&#35270;&#35273;&#24815;&#24615;SLAM&#31995;&#32479;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#22238;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22238;&#29615;&#38381;&#21512;&#23545;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#20855;&#26377;&#26174;&#33879;&#25928;&#24212;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#19981;&#21516;&#22270;&#20687;&#25293;&#25668;&#39057;&#29575;&#23545;&#23450;&#20301;&#31934;&#24230;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#25991;&#31456;&#25552;&#20986;&#30340;&#36825;&#20123;&#35780;&#20272;&#23545;&#20110;&#23558;&#36825;&#20123;SLAM&#31995;&#32479;&#24212;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01716v1 Announce Type: new  Abstract: Simultaneous Localization and Mapping (SLAM) is essential for mobile robotics, enabling autonomous navigation in dynamic, unstructured outdoor environments without relying on external positioning systems. In agricultural applications, where environmental conditions can be particularly challenging due to variable lighting or weather conditions, Visual-Inertial SLAM has emerged as a potential solution. This paper benchmarks several open-source Visual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS, Kimera, and SVO Pro, to evaluate their performance in agricultural settings. We focus on the impact of loop closing on localization accuracy and computational demands, providing a comprehensive analysis of these systems' effectiveness in real-world environments and especially their application to embedded systems in agricultural robotics. Our contributions further include an assessment of varying frame rates on localization acc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#26059;&#32764;&#25925;&#38556;&#24182;&#23454;&#29616;&#31934;&#30830;&#30528;&#38470;&#30340;&#22810;&#26059;&#32764;&#26080;&#20154;&#26426;&#21407;&#22411;&#35774;&#35745;&#12290;&#35813;&#35774;&#35745;&#21253;&#25324;&#25925;&#38556; tolerant &#25511;&#21046;&#25216;&#26415;&#21644;&#26426;&#26800;&#26041;&#26696;&#65292;&#20197;&#21450;&#22522;&#20110;&#35270;&#35273;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#20197;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#30340;&#22522;&#26412;&#32467;&#26524;&#39564;&#35777;&#20854;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01676</link><description>&lt;p&gt;
Prototyping of a multirotor UAV for precision landing under rotor failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#26059;&#32764;&#25925;&#38556;&#24182;&#23454;&#29616;&#31934;&#30830;&#30528;&#38470;&#30340;&#22810;&#26059;&#32764;&#26080;&#20154;&#26426;&#21407;&#22411;&#35774;&#35745;&#12290;&#35813;&#35774;&#35745;&#21253;&#25324;&#25925;&#38556; tolerant &#25511;&#21046;&#25216;&#26415;&#21644;&#26426;&#26800;&#26041;&#26696;&#65292;&#20197;&#21450;&#22522;&#20110;&#35270;&#35273;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#20197;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#30340;&#22522;&#26412;&#32467;&#26524;&#39564;&#35777;&#20854;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01676v1 Announce Type: new  Abstract: This work presents a prototype of a multirotor aerial vehicle capable of precision landing, even under the effects of rotor failures. The manuscript presents the fault-tolerant techniques and mechanical designs to achieve a fault-tolerant multirotor, and a vision-based navigation system required to achieve a precision landing. Preliminary experimental results will be shown, to validate on one hand the fault-tolerant control vehicle and, on the other hand, the autonomous landing algorithm. Also, a prototype of the fault-tolerant UAV is presented, capable of precise autonomous landing, which will be used in future experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;SPORT&#26694;&#26550;&#36890;&#36807;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20998;&#20026;&#29289;&#20307;&#23450;&#20301;&#12289;&#30446;&#26631;&#24819;&#35937;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#19977;&#20010;&#37096;&#20998;&#65292;&#20197;&#21450;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#29289;&#20307;&#31354;&#38388;&#35782;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#25193;&#25955;&#30340;3D&#23039;&#24577;&#20272;&#35745;&#22120;&#20197;&#30830;&#20445;&#29289;&#29702;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#20165;&#22312;&#29289;&#20307;&#31867;&#22411;&#30340;&#23450;&#20041;&#20043;&#38388;&#36827;&#34892;&#20132;&#27969;&#65292;SPORT&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20805;&#20998;&#22320;&#21033;&#29992;&#24320;&#25918;&#29615;&#22659;&#29289;&#20307;&#23450;&#20301;&#21644;&#35782;&#21035;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#8220;&#24819;&#35937;&#8221;&#29289;&#20307;&#30456;&#23545;&#20110;&#21442;&#32771;&#29289;&#20307;&#30340;&#23039;&#24577;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#36890;&#29992;&#29615;&#22659;&#20013;&#20934;&#30830;&#22320;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307;&#65292;&#20174;&#32780;&#20026;&#23454;&#29616;&#36890;&#29992;&#30446;&#30340;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01655</link><description>&lt;p&gt;
Stimulating Imagination: Towards General-purpose Object Rearrangement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;SPORT&#26694;&#26550;&#36890;&#36807;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20998;&#20026;&#29289;&#20307;&#23450;&#20301;&#12289;&#30446;&#26631;&#24819;&#35937;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#19977;&#20010;&#37096;&#20998;&#65292;&#20197;&#21450;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#29289;&#20307;&#31354;&#38388;&#35782;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#25193;&#25955;&#30340;3D&#23039;&#24577;&#20272;&#35745;&#22120;&#20197;&#30830;&#20445;&#29289;&#29702;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#20165;&#22312;&#29289;&#20307;&#31867;&#22411;&#30340;&#23450;&#20041;&#20043;&#38388;&#36827;&#34892;&#20132;&#27969;&#65292;SPORT&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20805;&#20998;&#22320;&#21033;&#29992;&#24320;&#25918;&#29615;&#22659;&#29289;&#20307;&#23450;&#20301;&#21644;&#35782;&#21035;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#8220;&#24819;&#35937;&#8221;&#29289;&#20307;&#30456;&#23545;&#20110;&#21442;&#32771;&#29289;&#20307;&#30340;&#23039;&#24577;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#36890;&#29992;&#29615;&#22659;&#20013;&#20934;&#30830;&#22320;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307;&#65292;&#20174;&#32780;&#20026;&#23454;&#29616;&#36890;&#29992;&#30446;&#30340;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01655v1 Announce Type: new  Abstract: General-purpose object placement is a fundamental capability of an intelligent generalist robot, i.e., being capable of rearranging objects following human instructions even in novel environments. To achieve this, we break the rearrangement down into three parts, including object localization, goal imagination and robot control, and propose a framework named SPORT. SPORT leverages pre-trained large vision models for broad semantic reasoning about objects, and learns a diffusion-based 3D pose estimator to ensure physically-realistic results. Only object types (to be moved or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object localization and recognition since no specific fine-tuning is needed for robotic scenarios. Furthermore, the diffusion-based estimator only need to "imagine" the poses of the moving and reference objects after the pl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LiDAR&#30340;&#24863;&#30693;-&#24863;&#30693;&#35268;&#21010;&#26694;&#26550;LF-3PM&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#24863;&#30693;&#25439;&#22833;&#22270;&#65288;SOLM&#65289;&#21644;&#24212;&#29992;&#25200;&#21160;&#35825;&#23548;&#24230;&#37327;&#26469;&#20248;&#21270;&#36335;&#24452;&#35268;&#21010;&#21644;&#25552;&#39640;&#24863;&#30693;&#31283;&#23450;&#24615;&#65292;&#19981;&#20165;&#26377;&#25928;&#25552;&#21319;&#20102;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#19981;&#21033;&#29615;&#22659;&#20013;&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#32780;&#19988;&#22823;&#22823;&#21152;&#36895;&#20102;&#35268;&#21010;&#36807;&#31243;&#30340;&#25191;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01649</link><description>&lt;p&gt;
LF-3PM: a LiDAR-based Framework for Perception-aware Planning with Perturbation-induced Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LiDAR&#30340;&#24863;&#30693;-&#24863;&#30693;&#35268;&#21010;&#26694;&#26550;LF-3PM&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#24863;&#30693;&#25439;&#22833;&#22270;&#65288;SOLM&#65289;&#21644;&#24212;&#29992;&#25200;&#21160;&#35825;&#23548;&#24230;&#37327;&#26469;&#20248;&#21270;&#36335;&#24452;&#35268;&#21010;&#21644;&#25552;&#39640;&#24863;&#30693;&#31283;&#23450;&#24615;&#65292;&#19981;&#20165;&#26377;&#25928;&#25552;&#21319;&#20102;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#19981;&#21033;&#29615;&#22659;&#20013;&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#32780;&#19988;&#22823;&#22823;&#21152;&#36895;&#20102;&#35268;&#21010;&#36807;&#31243;&#30340;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01649v1 Announce Type: new  Abstract: Just as humans can become disoriented in featureless deserts or thick fogs, not all environments are conducive to the Localization Accuracy and Stability (LAS) of autonomous robots. This paper introduces an efficient framework designed to enhance LiDAR-based LAS through strategic trajectory generation, known as Perception-aware Planning. Unlike vision-based frameworks, the LiDAR-based requires different considerations due to unique sensor attributes. Our approach focuses on two main aspects: firstly, assessing the impact of LiDAR observations on LAS. We introduce a perturbation-induced metric to provide a comprehensive and reliable evaluation of LiDAR observations. Secondly, we aim to improve motion planning efficiency. By creating a Static Observation Loss Map (SOLM) as an intermediary, we logically separate the time-intensive evaluation and motion planning phases, significantly boosting the planning process. In the experimental section
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36733;GNSS&#21644;&#26222;&#36890;&#36710;&#36742;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#22270;&#20687;&#25968;&#25454;&#33258;&#21160;&#26500;&#24314;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36947;&#36335;&#20013;&#24515;&#32447;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26368;&#23567;&#21270;&#20102;&#23545;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#20351;&#36947;&#36335;&#22270;&#30340;&#33258;&#21160;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01640</link><description>&lt;p&gt;
Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36733;GNSS&#21644;&#26222;&#36890;&#36710;&#36742;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#22270;&#20687;&#25968;&#25454;&#33258;&#21160;&#26500;&#24314;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36947;&#36335;&#20013;&#24515;&#32447;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26368;&#23567;&#21270;&#20102;&#23545;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#20351;&#36947;&#36335;&#22270;&#30340;&#33258;&#21160;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01640v1 Announce Type: cross  Abstract: Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today's advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data's time 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PUCL&#30340;&#31639;&#27861;&#65292;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#38750;&#32447;&#24615;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#12290;&#36825;&#19968;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#30340;&#30693;&#35782;&#23601;&#33021;&#25512;&#26029;&#32422;&#26463;&#21442;&#25968;&#21270;&#65292;&#19988;&#26080;&#38656;&#29615;&#22659;&#27169;&#22411;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26410;&#30693;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35268;&#21010;&#21644;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.01622</link><description>&lt;p&gt;
Positive-Unlabeled Constraint Learning (PUCL) for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PUCL&#30340;&#31639;&#27861;&#65292;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#38750;&#32447;&#24615;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#12290;&#36825;&#19968;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#30340;&#30693;&#35782;&#23601;&#33021;&#25512;&#26029;&#32422;&#26463;&#21442;&#25968;&#21270;&#65292;&#19988;&#26080;&#38656;&#29615;&#22659;&#27169;&#22411;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26410;&#30693;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35268;&#21010;&#21644;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01622v1 Announce Type: new  Abstract: Planning for a wide range of real-world robotic tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. This paper presents a novel Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a continuous arbitrary constraint function from demonstration, without requiring prior knowledge of the true constraint parameterization or environmental model as existing works. Within our framework, we treat all data in demonstrations as positive (feasible) data, and learn a control policy to generate potentially infeasible trajectories, which serve as unlabeled data. In each iteration, we first update the policy and then a two-step positive-unlabeled learning procedure is applied, where it first identifies reliable infeasible data using a distance metric, an
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#30446;&#31435;&#20307;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#20165;&#26377;&#20960;&#27627;&#31859;&#30452;&#24452;&#30340;&#24102;&#29366;&#24494;&#23610;&#24230;&#36830;&#32493;&#26426;&#22120;&#20154;&#65288;NTCR&#65289;&#30340;&#19977;&#32500;&#24418;&#24577;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#21488;&#30456;&#23545;&#25918;&#32622;&#30340;&#38745;&#24577;&#31435;&#20307;&#25668;&#20687;&#22836;&#65292;&#21363;&#20351;&#21407;&#22987;&#28857;&#20113;&#37319;&#38598;&#36136;&#37327;&#19981;&#39640;&#65292;&#20063;&#33021;&#22815;&#36890;&#36807;&#39044;&#35774;&#20960;&#20309;&#21442;&#32771;&#19979;&#30340;KD&#26641;&#31639;&#27861;&#23545;&#37319;&#38598;&#30340;&#28857;&#20113;&#36827;&#34892;&#27491;&#30830;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;NTCR&#24418;&#24577;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01615</link><description>&lt;p&gt;
Three-dimensional Morphological Reconstruction of Millimeter-Scale Soft Continuum Robots based on Dual Stereo Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#30446;&#31435;&#20307;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#20165;&#26377;&#20960;&#27627;&#31859;&#30452;&#24452;&#30340;&#24102;&#29366;&#24494;&#23610;&#24230;&#36830;&#32493;&#26426;&#22120;&#20154;&#65288;NTCR&#65289;&#30340;&#19977;&#32500;&#24418;&#24577;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#21488;&#30456;&#23545;&#25918;&#32622;&#30340;&#38745;&#24577;&#31435;&#20307;&#25668;&#20687;&#22836;&#65292;&#21363;&#20351;&#21407;&#22987;&#28857;&#20113;&#37319;&#38598;&#36136;&#37327;&#19981;&#39640;&#65292;&#20063;&#33021;&#22815;&#36890;&#36807;&#39044;&#35774;&#20960;&#20309;&#21442;&#32771;&#19979;&#30340;KD&#26641;&#31639;&#27861;&#23545;&#37319;&#38598;&#30340;&#28857;&#20113;&#36827;&#34892;&#27491;&#30830;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;NTCR&#24418;&#24577;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01615v1 Announce Type: new  Abstract: Continuum robots can be miniaturized to just a few millimeters in diameter. Among these, notched tubular continuum robots (NTCR) show great potential in many delicate applications. Existing works in robotic modeling focus on kinematics and dynamics but still face challenges in reproducing the robot's morphology -- a significant factor that can expand the research landscape of continuum robots, especially for those with asymmetric continuum structures. This paper proposes a dual stereo vision-based method for the three-dimensional morphological reconstruction of millimeter-scale NTCRs. The method employs two oppositely located stationary binocular cameras to capture the point cloud of the NTCR, then utilizes predefined geometry as a reference for the KD tree method to relocate the capture point clouds, resulting in a morphologically correct NTCR despite the low-quality raw point cloud collection. The method has been proved feasible for an
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25253;&#21578;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#26657;&#20934;&#20855;&#26377;&#30005;&#32518;&#39537;&#21160;&#33218;&#21644;&#24037;&#20855;&#30340;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#20165;&#32791;&#26102;8&#21040;21&#20998;&#38047;&#65292;&#21363;&#20351;&#22312;&#38271;&#26102;&#38388;&#19988;&#36127;&#36733;&#37325;&#30340;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#26657;&#20934;&#21518;&#30340;&#20851;&#33410;&#31934;&#24230;&#20173;&#28982;&#20445;&#25345;&#39640;&#27700;&#20934;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01604</link><description>&lt;p&gt;
Efficient Data-driven Joint-level Calibration of Cable-driven Surgical Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25253;&#21578;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#26657;&#20934;&#20855;&#26377;&#30005;&#32518;&#39537;&#21160;&#33218;&#21644;&#24037;&#20855;&#30340;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#20165;&#32791;&#26102;8&#21040;21&#20998;&#38047;&#65292;&#21363;&#20351;&#22312;&#38271;&#26102;&#38388;&#19988;&#36127;&#36733;&#37325;&#30340;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#26657;&#20934;&#21518;&#30340;&#20851;&#33410;&#31934;&#24230;&#20173;&#28982;&#20445;&#25345;&#39640;&#27700;&#20934;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01604v1 Announce Type: new  Abstract: Knowing accurate joint positions is crucial for safe and precise control of laparoscopic surgical robots, especially for the automation of surgical sub-tasks. These robots have often been designed with cable-driven arms and tools because cables allow for larger motors to be placed at the base of the robot, further from the operating area where space is at a premium. However, by connecting the joint to its motor with a cable, any stretch in the cable can lead to errors in kinematic estimation from encoders at the motor, which can result in difficulties for accurate control of the surgical tool. In this work, we propose an efficient data-driven calibration of positioning joints of such robots, in this case the RAVEN-II surgical robotics research platform. While the calibration takes only 8-21 minutes, the accuracy of the calibrated joints remains high during a 6-hour heavily loaded operation, suggesting desirable feasibility in real practi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335; guided search &#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20197;&#25552;&#39640;&#22312;&#19981;&#26126;&#29615;&#22659;&#20013;&#22303;&#22756;&#26679;&#26412;&#25628;&#32034;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01589</link><description>&lt;p&gt;
Soil Sample Search in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335; guided search &#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20197;&#25552;&#39640;&#22312;&#19981;&#26126;&#29615;&#22659;&#20013;&#22303;&#22756;&#26679;&#26412;&#25628;&#32034;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01589v1 Announce Type: new  Abstract: To work in unknown outdoor environments, autonomous sampling machines need the ability to target samples despite limited visibility and robotic arm reach distance. We design a heuristic guided search method to speed up the search process and more efficiently localize the approximate center of soil regions. Through simulation experiments, we assess the effectiveness of the proposed algorithm and discover superior performance in terms of speed, distance traveled, and success rate compared to naive baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25104;&#21151;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#21270;&#25805;&#20316;&#24120;&#35268;&#30340;&#23454;&#39564;&#23460;&#35774;&#22791;&#65292;&#22914;&#31163;&#24515;&#26426;&#65292;&#23637;&#31034;&#20102;&#23567;&#22411;&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01576</link><description>&lt;p&gt;
Autonomous Integration of Bench-Top Wet Lab Equipment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25104;&#21151;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#21270;&#25805;&#20316;&#24120;&#35268;&#30340;&#23454;&#39564;&#23460;&#35774;&#22791;&#65292;&#22914;&#31163;&#24515;&#26426;&#65292;&#23637;&#31034;&#20102;&#23567;&#22411;&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01576v1 Announce Type: new  Abstract: Laboratory automation is an expensive and complicated endeavor with limited inflexible options for small-scale labs. We develop a prototype system for tending to a bench-top centrifuge using computer vision methods for color detection and circular Hough Transforms to detect and localize centrifuge buckets. Initial results show that the prototype is capable of automating the usage of regular bench-top lab equipment.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;TURTLMap&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20302;&#32441;&#29702;&#28023;&#24213;&#29615;&#22659;&#30340;&#23454;&#26102;&#23450;&#20301;&#21644;&#23494;&#38598;&#21046;&#22270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#20302;&#20215;&#20301;&#30340;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;&#12290;&#36825;&#39033;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#36861;&#36394;&#33322;&#34892;&#22120;&#30340;&#20301;&#32622;&#24182;&#23454;&#26102;&#26500;&#24314;&#20302;&#32441;&#29702;&#29615;&#22659;&#30340;&#39640;&#23494;&#24230;&#22320;&#22270;&#65292;&#21363;&#20351;&#22312;&#27874;&#28010;&#31561;&#33258;&#28982;&#26465;&#20214;&#30340;&#24433;&#21709;&#19979;&#20063;&#33021;&#20445;&#25345;&#31934;&#24230;&#12290;&#22312;&#23460;&#20869;&#27700;&#27133;&#20013;&#20351;&#29992;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#21644;&#30495;&#23454;&#22320;&#22270;&#20316;&#20026;&#21442;&#32771;&#65292;&#23454;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01569</link><description>&lt;p&gt;
TURTLMap: Real-time Localization and Dense Mapping of Low-texture Underwater Environments with a Low-cost Unmanned Underwater Vehicle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;TURTLMap&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20302;&#32441;&#29702;&#28023;&#24213;&#29615;&#22659;&#30340;&#23454;&#26102;&#23450;&#20301;&#21644;&#23494;&#38598;&#21046;&#22270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#20302;&#20215;&#20301;&#30340;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;&#12290;&#36825;&#39033;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#36861;&#36394;&#33322;&#34892;&#22120;&#30340;&#20301;&#32622;&#24182;&#23454;&#26102;&#26500;&#24314;&#20302;&#32441;&#29702;&#29615;&#22659;&#30340;&#39640;&#23494;&#24230;&#22320;&#22270;&#65292;&#21363;&#20351;&#22312;&#27874;&#28010;&#31561;&#33258;&#28982;&#26465;&#20214;&#30340;&#24433;&#21709;&#19979;&#20063;&#33021;&#20445;&#25345;&#31934;&#24230;&#12290;&#22312;&#23460;&#20869;&#27700;&#27133;&#20013;&#20351;&#29992;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#21644;&#30495;&#23454;&#22320;&#22270;&#20316;&#20026;&#21442;&#32771;&#65292;&#23454;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01569v1 Announce Type: new  Abstract: Significant work has been done on advancing localization and mapping in underwater environments. Still, state-of-the-art methods are challenged by low-texture environments, which is common for underwater settings. This makes it difficult to use existing methods in diverse, real-world scenes. In this paper, we present TURTLMap, a novel solution that focuses on textureless underwater environments through a real-time localization and mapping method. We show that this method is low-cost, and capable of tracking the robot accurately, while constructing a dense map of a low-textured environment in real-time. We evaluate the proposed method using real-world data collected in an indoor water tank with a motion capture system and ground truth reference map. Qualitative and quantitative results validate the proposed system achieves accurate and robust localization and precise dense mapping, even when subject to wave conditions. The project page fo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#39318;&#27425;&#23558;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#23545;&#32963;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#35786;&#26029;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#25968;&#25454;&#31232;&#32570;&#21644;&#20559;&#35265;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20811;&#26381;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#26465;&#20214;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01554</link><description>&lt;p&gt;
Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#39318;&#27425;&#23558;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#23545;&#32963;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#35786;&#26029;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#25968;&#25454;&#31232;&#32570;&#21644;&#20559;&#35265;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20811;&#26381;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#26465;&#20214;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01554v1 Announce Type: new  Abstract: In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21327;&#21516;&#25805;&#20316;&#20013;&#20132;&#20114;&#21147;&#30340;&#26032;&#20998;&#35299;&#26041;&#27861;&#65292;&#33021;&#22815;&#30830;&#23450;&#20195;&#29702;&#30456;&#23545;&#20110;&#32676;&#20307;&#30340;&#29366;&#24577;&#65292;&#24182;&#25193;&#23637;&#20102;&#23450;&#20041;&#20197;&#21253;&#21547;&#22235;&#20010;&#33258;&#30001;&#24230;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#29289;&#20307;&#36335;&#24452;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#12289;&#20301;&#32622;&#25110;&#36755;&#20837;&#21147;&#30697;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#33258;&#21253;&#21547;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#23454;&#29616;&#65292;&#20026;&#21327;&#21516;&#25805;&#20316;&#20013;&#30340;&#36890;&#20449;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.01543</link><description>&lt;p&gt;
A Decomposition of Interaction Force for Multi-Agent Co-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21327;&#21516;&#25805;&#20316;&#20013;&#20132;&#20114;&#21147;&#30340;&#26032;&#20998;&#35299;&#26041;&#27861;&#65292;&#33021;&#22815;&#30830;&#23450;&#20195;&#29702;&#30456;&#23545;&#20110;&#32676;&#20307;&#30340;&#29366;&#24577;&#65292;&#24182;&#25193;&#23637;&#20102;&#23450;&#20041;&#20197;&#21253;&#21547;&#22235;&#20010;&#33258;&#30001;&#24230;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#29289;&#20307;&#36335;&#24452;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#12289;&#20301;&#32622;&#25110;&#36755;&#20837;&#21147;&#30697;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#33258;&#21253;&#21547;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#23454;&#29616;&#65292;&#20026;&#21327;&#21516;&#25805;&#20316;&#20013;&#30340;&#36890;&#20449;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01543v1 Announce Type: new  Abstract: Multi-agent human-robot co-manipulation is a poorly understood process with many inputs that potentially affect agent behavior. This paper explores one such input known as interaction force. Interaction force is potentially a primary component in communication that occurs during co-manipulation. There are, however, many different perspectives and definitions of interaction force in the literature. Therefore, a decomposition of interaction force is proposed that provides a consistent way of ascertaining the state of an agent relative to the group for multi-agent co-manipulation. This proposed method extends a current definition from one to four degrees of freedom, does not rely on a predefined object path, and is independent of the number of agents acting on the system and their locations and input wrenches (forces and torques). In addition, all of the necessary measures can be obtained by a self-contained robotic system, allowing for a m
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;SceneMotion&#27169;&#22411;&#65292;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#22810;&#20256;&#24863;&#22120;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#23558;&#23616;&#37096;&#20195;&#29702;&#29305;&#24449;&#26144;&#23556;&#21040;&#22330;&#26223;&#32423;&#21035;&#30340;&#39044;&#27979;&#20013;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#22810;&#20010;&#20132;&#36890;&#20195;&#29702;&#22330;&#26223;&#32423;&#36816;&#21160;&#27169;&#24335;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01537</link><description>&lt;p&gt;
SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;SceneMotion&#27169;&#22411;&#65292;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#22810;&#20256;&#24863;&#22120;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#23558;&#23616;&#37096;&#20195;&#29702;&#29305;&#24449;&#26144;&#23556;&#21040;&#22330;&#26223;&#32423;&#21035;&#30340;&#39044;&#27979;&#20013;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#22810;&#20010;&#20132;&#36890;&#20195;&#29702;&#22330;&#26223;&#32423;&#36816;&#21160;&#27169;&#24335;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01537v1 Announce Type: cross  Abstract: Self-driving vehicles rely on multimodal motion forecasts to effectively interact with their environment and plan safe maneuvers. We introduce SceneMotion, an attention-based model for forecasting scene-wide motion modes of multiple traffic agents. Our model transforms local agent-centric embeddings into scene-wide forecasts using a novel latent context module. This module learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling. The competitive performance in the Waymo Open Interaction Prediction Challenge demonstrates the effectiveness of our approach. Moreover, we cluster future waypoints in time and space to quantify the interaction between agents. We merge all modes and analyze each mode independently to determine which clusters are resolved through interaction or result in conflict. Our implementation is available at: https://github.com/kit-mrt/future-motion
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#21033;&#29992;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65288;SD&#65289;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#22312;&#32447;&#36947;&#36335;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;SD&#22320;&#22270;&#30340;rasterized&#34920;&#31034;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#22312;&#32447;&#26144;&#23556;&#26550;&#26500;&#20013;&#65292;&#24182;&#25193;&#23637;OpenLane-V2&#25968;&#25454;&#38598;&#20197;&#32467;&#21512;OpenStreetMaps&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#22270;&#24418;SD&#22320;&#22270;&#34920;&#31034;&#65292;&#33021;&#22815;&#20026;&#33258;&#21160;&#36710;&#36742;&#22312;&#22478;&#24066;&#21644;&#39640;&#36895;&#20844;&#36335;&#29615;&#22659;&#19979;&#23548;&#33322;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;HD&#22320;&#22270;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;SD&#22320;&#22270;&#31574;&#30053;&#65292;&#25552;&#39640;&#22312;&#32447;&#26144;&#23556;&#31995;&#32479;&#23545;&#20110;&#21160;&#24577;&#29615;&#22659;&#20013;&#22823;&#37327;&#36974;&#25377;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01471</link><description>&lt;p&gt;
Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#21033;&#29992;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65288;SD&#65289;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#22312;&#32447;&#36947;&#36335;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;SD&#22320;&#22270;&#30340;rasterized&#34920;&#31034;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#22312;&#32447;&#26144;&#23556;&#26550;&#26500;&#20013;&#65292;&#24182;&#25193;&#23637;OpenLane-V2&#25968;&#25454;&#38598;&#20197;&#32467;&#21512;OpenStreetMaps&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#22270;&#24418;SD&#22320;&#22270;&#34920;&#31034;&#65292;&#33021;&#22815;&#20026;&#33258;&#21160;&#36710;&#36742;&#22312;&#22478;&#24066;&#21644;&#39640;&#36895;&#20844;&#36335;&#29615;&#22659;&#19979;&#23548;&#33322;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;HD&#22320;&#22270;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;SD&#22320;&#22270;&#31574;&#30053;&#65292;&#25552;&#39640;&#22312;&#32447;&#26144;&#23556;&#31995;&#32479;&#23545;&#20110;&#21160;&#24577;&#29615;&#22659;&#20013;&#22823;&#37327;&#36974;&#25377;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01471v1 Announce Type: cross  Abstract: Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definition (SD) maps-in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encode
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#24182;&#37319;&#29992;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#20102;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#20013;&#30340;&#35270;&#28857;&#35268;&#21010;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23545;&#24314;&#31569;&#34920;&#38754;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2408.01435</link><description>&lt;p&gt;
A New Clustering-based View Planning Method for Building Inspection with Drone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#24182;&#37319;&#29992;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#20102;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#20013;&#30340;&#35270;&#28857;&#35268;&#21010;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23545;&#24314;&#31569;&#34920;&#38754;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01435v1 Announce Type: cross  Abstract: With the rapid development of drone technology, the application of drones equipped with visual sensors for building inspection and surveillance has attracted much attention. View planning aims to find a set of near-optimal viewpoints for vision-related tasks to achieve the vision coverage goal. This paper proposes a new clustering-based two-step computational method using spectral clustering, local potential field method, and hyper-heuristic algorithm to find near-optimal views to cover the target building surface. In the first step, the proposed method generates candidate viewpoints based on spectral clustering and corrects the positions of candidate viewpoints based on our newly proposed local potential field method. In the second step, the optimization problem is converted into a Set Covering Problem (SCP), and the optimal viewpoint subset is solved using our proposed hyper-heuristic algorithm. Experimental results show that the pro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#36129;&#29486;&#20026;&#65306;&#39318;&#27425;&#21457;&#24067;&#20102;&#19968;&#27454;&#22823;&#22411;UAV&#38388;&#21512;&#20316;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25552;&#21319;&#20302;&#31354;&#32463;&#27982;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;U2UData&#30340;&#22810;&#26080;&#20154;&#26426;&#21327;&#21516;&#24863;&#30693;&#31995;&#32479;&#65292;&#21487;&#22312;&#38271;&#36317;&#31163;&#33539;&#22260;&#20869;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#39134;&#34892;&#12290;&#36890;&#36807;&#20351;&#29992;LiDAR&#21644;&#20854;&#20182;&#20256;&#24863;&#22120;&#65292;&#35813;&#31995;&#32479;&#33021;&#26377;&#25928;&#24212;&#23545;&#38556;&#30861;&#29289;&#36974;&#25377;&#38382;&#39064;&#65292;&#39044;&#26399;&#23558;&#25512;&#21160;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00606</link><description>&lt;p&gt;
U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#36129;&#29486;&#20026;&#65306;&#39318;&#27425;&#21457;&#24067;&#20102;&#19968;&#27454;&#22823;&#22411;UAV&#38388;&#21512;&#20316;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25552;&#21319;&#20302;&#31354;&#32463;&#27982;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;U2UData&#30340;&#22810;&#26080;&#20154;&#26426;&#21327;&#21516;&#24863;&#30693;&#31995;&#32479;&#65292;&#21487;&#22312;&#38271;&#36317;&#31163;&#33539;&#22260;&#20869;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#39134;&#34892;&#12290;&#36890;&#36807;&#20351;&#29992;LiDAR&#21644;&#20854;&#20182;&#20256;&#24863;&#22120;&#65292;&#35813;&#31995;&#32479;&#33021;&#26377;&#25928;&#24212;&#23545;&#38556;&#30861;&#29289;&#36974;&#25377;&#38382;&#39064;&#65292;&#39044;&#26399;&#23558;&#25512;&#21160;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00606v2 Announce Type: replace  Abstract: Modern perception systems for autonomous flight are sensitive to occlusion and have limited long-range capability, which is a key bottleneck in improving low-altitude economic task performance. Recent research has shown that the UAV-to-UAV (U2U) cooperative perception system has great potential to revolutionize the autonomous flight industry. However, the lack of a large-scale dataset is hindering progress in this area. This paper presents U2UData, the first large-scale cooperative perception dataset for swarm UAVs autonomous flight. The dataset was collected by three UAVs flying autonomously in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames, 945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes. It also includes brightness, temperature, humidity, smoke, and airflow values covering all flight routes. U2USim is the first real-world mapping swarm UAVs simulation environment. It take
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19977;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#25351;&#26631;&#65292;&#21253;&#25324;&#29420;&#31435;&#30340;&#32763;&#35793;&#21644;&#26059;&#36716;&#31934;&#24230;&#35780;&#20215;&#65292;&#20197;&#21450;&#32508;&#21512;&#30340;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36317;&#31163;&#35823;&#24046;&#30340;&#32047;&#31215;&#39057;&#25968;&#26041;&#27861;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20026;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2407.20391</link><description>&lt;p&gt;
Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19977;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#25351;&#26631;&#65292;&#21253;&#25324;&#29420;&#31435;&#30340;&#32763;&#35793;&#21644;&#26059;&#36716;&#31934;&#24230;&#35780;&#20215;&#65292;&#20197;&#21450;&#32508;&#21512;&#30340;&#23039;&#24577;&#31934;&#24230;&#35780;&#20215;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36317;&#31163;&#35823;&#24046;&#30340;&#32047;&#31215;&#39057;&#25968;&#26041;&#27861;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20026;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20391v2 Announce Type: replace-cross  Abstract: We propose three novel metrics for evaluating the accuracy of a set of estimated camera poses given the ground truth: Translation Alignment Score (TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS evaluates the translation accuracy independently of the rotations, and the RAS evaluates the rotation accuracy independently of the translations. The PAS is the average of the two scores, evaluating the combined accuracy of both translations and rotations. The TAS is computed in four steps: (1) Find the upper quartile of the closest-pair-distances, $d$. (2) Align the estimated trajectory to the ground truth using a robust registration method. (3) Collect all distance errors and obtain the cumulative frequencies for multiple thresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up these cumulative frequencies and normalize them such that the theoretical maximum is 1. The TAS has practical
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#36801;&#31227;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#22797;&#26434;&#30340;&#37117;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#35268;&#21010;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#20351;&#29992;&#25552;&#39640;&#35268;&#21010;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#19987;&#23478;&#25968;&#25454;&#20013;&#33719;&#24471;&#30693;&#35782;&#65292;&#20877;&#23558;&#20854;&#36801;&#31227;&#21040;&#29992;&#25143;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.18569</link><description>&lt;p&gt;
PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#36801;&#31227;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#22797;&#26434;&#30340;&#37117;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#35268;&#21010;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#20351;&#29992;&#25552;&#39640;&#35268;&#21010;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#19987;&#23478;&#25968;&#25454;&#20013;&#33719;&#24471;&#30693;&#35782;&#65292;&#20877;&#23558;&#20854;&#36801;&#31227;&#21040;&#29992;&#25143;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18569v3 Announce Type: replace  Abstract: Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a fundamental resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tunin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#26426;&#22120;&#20154;&#38598;&#32676;&#25511;&#21046;&#22120;&#20449;&#24687;&#21363;&#21487;&#37325;&#24314;&#38598;&#32676;&#34892;&#20026;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#28436;&#31034;&#36716;&#25442;&#20026;&#25551;&#36848;&#22810;agent&#20132;&#20114;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;agent&#29983;&#25104;&#24335;&#23545;&#25239;&#24335;&#27169;&#20223;&#23398;&#20064;&#65288;MA-GAIL&#65289;&#30340;&#24110;&#21161;&#19979;&#36827;&#34892;&#34892;&#20026;&#37325;&#24314;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31354;&#38388;&#32452;&#32455;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20026;&#35266;&#23519;&#21644;&#37325;&#24314;&#38598;&#32676;&#34892;&#20026;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#21644;&#27979;&#35797;&#21487;&#33021;&#20250;&#22312;&#20854;&#20182;&#26041;&#27861;&#20013;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2407.11592</link><description>&lt;p&gt;
Learning to Imitate Spatial Organization in Multi-robot Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#26426;&#22120;&#20154;&#38598;&#32676;&#25511;&#21046;&#22120;&#20449;&#24687;&#21363;&#21487;&#37325;&#24314;&#38598;&#32676;&#34892;&#20026;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#28436;&#31034;&#36716;&#25442;&#20026;&#25551;&#36848;&#22810;agent&#20132;&#20114;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;agent&#29983;&#25104;&#24335;&#23545;&#25239;&#24335;&#27169;&#20223;&#23398;&#20064;&#65288;MA-GAIL&#65289;&#30340;&#24110;&#21161;&#19979;&#36827;&#34892;&#34892;&#20026;&#37325;&#24314;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31354;&#38388;&#32452;&#32455;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20026;&#35266;&#23519;&#21644;&#37325;&#24314;&#38598;&#32676;&#34892;&#20026;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#21644;&#27979;&#35797;&#21487;&#33021;&#20250;&#22312;&#20854;&#20182;&#26041;&#27861;&#20013;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11592v2 Announce Type: replace  Abstract: Understanding collective behavior and how it evolves is important to ensure that robot swarms can be trusted in a shared environment. One way to understand the behavior of the swarm is through collective behavior reconstruction using prior demonstrations. Existing approaches often require access to the swarm controller which may not be available. We reconstruct collective behaviors in distinct swarm scenarios involving shared environments without using swarm controller information. We achieve this by transforming prior demonstrations into features that describe multi-agent interactions before behavior reconstruction with multi-agent generative adversarial imitation learning (MA-GAIL). We show that our approach outperforms existing algorithms in spatial organization, and can be used to observe and reconstruct a swarm's behavior for further analysis and testing, which might be impractical or undesirable on the original robot swarm.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31283;&#23450;&#36870;&#21521;&#29702;&#35770;&#30340; novel &#26041;&#27861;&#26469;&#25511;&#21046;&#22810;&#20010;&#36719;&#27668;&#21160;&#25191;&#34892;&#22120;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36719;&#22841;&#29226;&#20013;&#36719;&#25163;&#25351;&#30340;&#21327;&#35843;&#21160;&#20316;&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#20010;&#27880;&#23556;&#27893;&#20316;&#20026;&#39537;&#21160;&#26426;&#21046;&#65292;&#25991;&#31456;&#35299;&#20915;&#20102;&#21327;&#35843;&#22810;&#20010;&#33258;&#30001;&#24230;&#19979;&#36719;&#26426;&#26800;&#31995;&#32479;&#36816;&#21160;&#25511;&#21046;&#30340;&#25216;&#26415;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2406.04666</link><description>&lt;p&gt;
Underactuated Control of Multiple Soft Pneumatic Actuators via Stable Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31283;&#23450;&#36870;&#21521;&#29702;&#35770;&#30340; novel &#26041;&#27861;&#26469;&#25511;&#21046;&#22810;&#20010;&#36719;&#27668;&#21160;&#25191;&#34892;&#22120;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36719;&#22841;&#29226;&#20013;&#36719;&#25163;&#25351;&#30340;&#21327;&#35843;&#21160;&#20316;&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#20010;&#27880;&#23556;&#27893;&#20316;&#20026;&#39537;&#21160;&#26426;&#21046;&#65292;&#25991;&#31456;&#35299;&#20915;&#20102;&#21327;&#35843;&#22810;&#20010;&#33258;&#30001;&#24230;&#19979;&#36719;&#26426;&#26800;&#31995;&#32479;&#36816;&#21160;&#25511;&#21046;&#30340;&#25216;&#26415;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04666v2 Announce Type: replace  Abstract: Soft grippers, with their inherent compliance and adaptability, show advantages for delicate and versatile manipulation tasks in robotics. This paper presents a novel approach to underactuated control of multiple soft actuators, explicitly focusing on the coordination of soft fingers within a soft gripper. Utilizing a single syringe pump as the actuation mechanism, we address the challenge of coordinating multiple degrees of freedom of a compliant system. The theoretical framework applies concepts from stable inversion theory, adapting them to the unique dynamics of the underactuated soft gripper. Through meticulous mechatronic system design and controller synthesis, we demonstrate the efficacy and applicability of our approach in achieving precise and coordinated manipulation tasks in simulation and experimentation. Our findings not only contribute to the advancement of soft robot control but also offer practical insights into the d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Visual Language Model&#65288;VLM&#65289;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#29992;&#25143;&#20132;&#20114;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20219;&#21153;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2405.11537</link><description>&lt;p&gt;
VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Visual Language Model&#65288;VLM&#65289;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#29992;&#25143;&#20132;&#20114;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20219;&#21153;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11537v3 Announce Type: replace  Abstract: The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;LiDAR&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#21644;&#34920;&#38754;&#29305;&#24615;&#24433;&#21709;&#30340;&#32508;&#21512;&#28857;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22522;&#20110;LiDAR&#30340;SLAM&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#20998;&#31163;&#27861;&#32447;&#19981;&#30830;&#23450;&#24615;&#19982;&#23556;&#32447;&#26041;&#21521;&#19981;&#30830;&#23450;&#24615;&#30340;&#25237;&#24433;&#65292;&#24182;&#32467;&#21512;&#24555;&#36895;&#22686;&#37327;&#30697;&#38453;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2405.01316</link><description>&lt;p&gt;
LOG-LIO2: A LiDAR-Inertial Odometry with Efficient Uncertainty Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;LiDAR&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#21644;&#34920;&#38754;&#29305;&#24615;&#24433;&#21709;&#30340;&#32508;&#21512;&#28857;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22522;&#20110;LiDAR&#30340;SLAM&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#20998;&#31163;&#27861;&#32447;&#19981;&#30830;&#23450;&#24615;&#19982;&#23556;&#32447;&#26041;&#21521;&#19981;&#30830;&#23450;&#24615;&#30340;&#25237;&#24433;&#65292;&#24182;&#32467;&#21512;&#24555;&#36895;&#22686;&#37327;&#30697;&#38453;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01316v2 Announce Type: replace  Abstract: Uncertainty in LiDAR measurements, stemming from factors such as range sensing, is crucial for LIO (LiDAR-Inertial Odometry) systems as it affects the accurate weighting in the loss function. While recent LIO systems address uncertainty related to range sensing, the impact of incident angle on uncertainty is often overlooked by the community. Moreover, the existing uncertainty propagation methods suffer from computational inefficiency. This paper proposes a comprehensive point uncertainty model that accounts for both the uncertainties from LiDAR measurements and surface characteristics, along with an efficient local uncertainty analytical method for LiDAR-based state estimation problem. We employ a projection operator that separates the uncertainty into the ray direction and its orthogonal plane. Then, we derive incremental Jacobian matrices of eigenvalues and eigenvectors w.r.t. points, which enables a fast approximation of uncertai
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;iOS&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#20351;&#26222;&#36890;&#29992;&#25143;&#33021;&#22815;&#26080;&#38656;&#23454;&#38469;&#29289;&#29702;&#26426;&#22120;&#20154;&#65292;&#20415;&#33021;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20256;&#32479;&#26426;&#22120;&#20154;&#35757;&#32451;&#20013;&#23545;&#20110;&#19987;&#19994;&#20154;&#21592;&#21644;&#26114;&#36149;&#35774;&#22791;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.06089</link><description>&lt;p&gt;
EVE: Enabling Anyone to Train Robots using Augmented Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;iOS&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#20351;&#26222;&#36890;&#29992;&#25143;&#33021;&#22815;&#26080;&#38656;&#23454;&#38469;&#29289;&#29702;&#26426;&#22120;&#20154;&#65292;&#20415;&#33021;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20256;&#32479;&#26426;&#22120;&#20154;&#35757;&#32451;&#20013;&#23545;&#20110;&#19987;&#19994;&#20154;&#21592;&#21644;&#26114;&#36149;&#35774;&#22791;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.06089v3 Announce Type: replace-cross  Abstract: The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task requires expensive trajectory data where a trained human annotator moves a physical robot to train it. Consequently, only those with access to robots produce demonstrations to train robots. In this work, we remove this restriction with EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations, without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study (N=14, D=30) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically movi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#31995;&#32479;&#30340;&#22312;&#32447;&#37096;&#32626;&#26399;&#38388;&#20934;&#30830;&#30830;&#23450;&#21644;&#36866;&#24212;&#27169;&#22411;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#21464;&#21270;&#65292;&#20026;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#20855;&#26377;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#40065;&#26834;&#31070;&#32463;&#25511;&#21046;&#31574;&#30053;&#21644;&#35777;&#20070;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#40065;&#26834;&#24418;&#24335;&#21270;&#65292;&#23558;Lyapunov&#23548;&#25968;&#26426;&#20250;&#32422;&#26463;&#32435;&#20837;&#32771;&#34385;&#33539;&#22260;&#65292;&#30830;&#20445;Lyapunov&#35777;&#20070;&#30340;&#36880;&#28176;&#20943;&#23569;&#12290;&#20026;&#20102;&#20943;&#23569;&#22788;&#29702;&#27010;&#29575;&#25514;&#26045;&#31354;&#38388;&#25152;&#38656;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#35813;&#25991;&#36890;&#36807;&#30830;&#23450;&#20984;&#32422;&#26463;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36275;&#22815;&#20445;&#35777;Lyapunov&#23548;&#25968;&#32422;&#26463;&#34987;&#28385;&#36275;&#30340;&#26465;&#20214;&#12290;&#30740;&#31350;&#23558;&#36825;&#19968;&#26465;&#20214;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#38381;&#21512;&#22238;&#36335;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.03017</link><description>&lt;p&gt;
Distributionally Robust Policy and Lyapunov-Certificate Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.03017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#31995;&#32479;&#30340;&#22312;&#32447;&#37096;&#32626;&#26399;&#38388;&#20934;&#30830;&#30830;&#23450;&#21644;&#36866;&#24212;&#27169;&#22411;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#21464;&#21270;&#65292;&#20026;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#20855;&#26377;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#40065;&#26834;&#31070;&#32463;&#25511;&#21046;&#31574;&#30053;&#21644;&#35777;&#20070;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#40065;&#26834;&#24418;&#24335;&#21270;&#65292;&#23558;Lyapunov&#23548;&#25968;&#26426;&#20250;&#32422;&#26463;&#32435;&#20837;&#32771;&#34385;&#33539;&#22260;&#65292;&#30830;&#20445;Lyapunov&#35777;&#20070;&#30340;&#36880;&#28176;&#20943;&#23569;&#12290;&#20026;&#20102;&#20943;&#23569;&#22788;&#29702;&#27010;&#29575;&#25514;&#26045;&#31354;&#38388;&#25152;&#38656;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#35813;&#25991;&#36890;&#36807;&#30830;&#23450;&#20984;&#32422;&#26463;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36275;&#22815;&#20445;&#35777;Lyapunov&#23548;&#25968;&#32422;&#26463;&#34987;&#28385;&#36275;&#30340;&#26465;&#20214;&#12290;&#30740;&#31350;&#23558;&#36825;&#19968;&#26465;&#20214;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#38381;&#21512;&#22238;&#36335;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.03017v2 Announce Type: replace-cross  Abstract: This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty. A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment. We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied. We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the gl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSAP&#30340;&#24418;&#29366;&#25935;&#24863;&#24615;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#23427;&#33021;&#22815;&#23545;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#30772;&#22351;&#12290;</title><link>https://arxiv.org/abs/2403.11515</link><description>&lt;p&gt;
SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSAP&#30340;&#24418;&#29366;&#25935;&#24863;&#24615;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#23427;&#33021;&#22815;&#23545;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11515v2 Announce Type: replace-cross  Abstract: Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#20174;&#23398;&#20064;&#21040;&#30340;&#22122;&#22768;&#27979;&#37327;&#20013;&#20272;&#35745;&#30340;&#20845;&#24230;&#33258;&#30001;&#24230;&#23039;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#31181;&#20960;&#20309;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#26368;&#23567;&#21253;&#21547;&#27979;&#22320;&#32447;&#29699;&#65288;MEGB&#65289;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#21487;&#33021;&#30340;&#38169;&#35823;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#23039;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#35299;&#37322;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09990</link><description>&lt;p&gt;
CLOSURE: Fast Quantification of Pose Uncertainty Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09990
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#20174;&#23398;&#20064;&#21040;&#30340;&#22122;&#22768;&#27979;&#37327;&#20013;&#20272;&#35745;&#30340;&#20845;&#24230;&#33258;&#30001;&#24230;&#23039;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#31181;&#20960;&#20309;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#26368;&#23567;&#21253;&#21547;&#27979;&#22320;&#32447;&#29699;&#65288;MEGB&#65289;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#21487;&#33021;&#30340;&#38169;&#35823;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#23039;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#35299;&#37322;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09990v3 Announce Type: replace  Abstract: We investigate uncertainty quantification of 6D pose estimation from learned noisy measurements (e.g. keypoints and pose hypotheses). Assuming unknown-but-bounded measurement noises, a pose uncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D poses compatible with the measurements. Despite being simple to formulate and its ability to embed uncertainty, the PURSE is difficult to manipulate and interpret due to the many abstract nonconvex polynomial constraints. An appealing simplification of PURSE is to find its minimum enclosing geodesic ball (MEGB), i.e., a point pose estimation with minimum worst-case error bound. We contribute (i) a geometric interpretation of the nonconvex PURSE, and (ii) a fast algorithm to inner approximate the MEGB. Particularly, we show the PURSE corresponds to the feasible set of a constrained dynamical system or the intersection of multiple geodesic balls, and this perspective allows u
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25253;&#36947;&#20102;&#19968;&#31181;&#20154;&#22411;&#26426;&#22120;&#20154;&#34892;&#36208;&#25216;&#26415;&#30340;&#21019;&#26032;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#20154;&#22411;&#26426;&#22120;&#20154;&#8220;Adam&#8221;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#23545;&#25239;&#36816;&#21160;&#20808;&#39564;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#20197;&#24448;&#26426;&#22120;&#20154;&#26356;&#20026;&#25509;&#36817;&#20154;&#31867;&#30340;&#34892;&#36208;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.18294</link><description>&lt;p&gt;
Whole-body Humanoid Robot Locomotion with Human Reference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18294
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25253;&#36947;&#20102;&#19968;&#31181;&#20154;&#22411;&#26426;&#22120;&#20154;&#34892;&#36208;&#25216;&#26415;&#30340;&#21019;&#26032;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#20154;&#22411;&#26426;&#22120;&#20154;&#8220;Adam&#8221;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#23545;&#25239;&#36816;&#21160;&#20808;&#39564;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#20197;&#24448;&#26426;&#22120;&#20154;&#26356;&#20026;&#25509;&#36817;&#20154;&#31867;&#30340;&#34892;&#36208;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18294v3 Announce Type: replace  Abstract: Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, "Adam", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the prop
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30524;&#22806;&#31185;&#25163;&#26415;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#33258;&#36866;&#24212;&#30524;&#30545;&#21147;&#25511;&#21046;&#23454;&#29616;&#21452;&#25163;&#25805;&#20316;&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#25506;&#35752;&#20102;&#21512;&#20316;&#24335;&#25805;&#20316;&#21644;&#36965;&#25511;&#25805;&#20316;&#20004;&#31181;&#31574;&#30053;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#30524;&#22806;&#31185;&#25163;&#26415;&#20013;&#23384;&#22312;&#30340;&#25805;&#20316;&#31934;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18088</link><description>&lt;p&gt;
Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera Force Control: Cooperative vs. Teleoperation Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30524;&#22806;&#31185;&#25163;&#26415;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#33258;&#36866;&#24212;&#30524;&#30545;&#21147;&#25511;&#21046;&#23454;&#29616;&#21452;&#25163;&#25805;&#20316;&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#25506;&#35752;&#20102;&#21512;&#20316;&#24335;&#25805;&#20316;&#21644;&#36965;&#25511;&#25805;&#20316;&#20004;&#31181;&#31574;&#30053;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#30524;&#22806;&#31185;&#25163;&#26415;&#20013;&#23384;&#22312;&#30340;&#25805;&#20316;&#31934;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18088v2 Announce Type: replace  Abstract: Performing retinal vein cannulation (RVC) as a potential treatment for retinal vein occlusion (RVO) without the assistance of a surgical robotic system is very challenging to do safely. The main limitation is the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not safely minimize the contact force between the surgical instrument and the sclera to prevent tissue damage. Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. This work presents a bi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR-99&#30340;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22788;&#29702;&#26410;&#30693;&#27604;&#20363;&#21644;&#26497;&#31471;&#27604;&#20363;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;3&#28857;&#25277;&#26679;&#21644;&#39034;&#24207;&#20248;&#20808;&#21450;&#22522;&#20110;&#19977;&#20803;&#32452;&#27604;&#20363;&#19968;&#33268;&#24615;&#39044;&#31579;&#36873;&#30340;&#26426;&#21046;&#65292;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#30340;&#22330;&#26223;&#20013;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#21363;&#20351;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#29575;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#22312;&#20445;&#25345;&#26497;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26174;&#31034;&#20986;&#20102;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#38382;&#39064;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16598</link><description>&lt;p&gt;
PCR-99: A Practical Method for Point Cloud Registration with 99 Percent Outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR-99&#30340;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22788;&#29702;&#26410;&#30693;&#27604;&#20363;&#21644;&#26497;&#31471;&#27604;&#20363;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;3&#28857;&#25277;&#26679;&#21644;&#39034;&#24207;&#20248;&#20808;&#21450;&#22522;&#20110;&#19977;&#20803;&#32452;&#27604;&#20363;&#19968;&#33268;&#24615;&#39044;&#31579;&#36873;&#30340;&#26426;&#21046;&#65292;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#30340;&#22330;&#26223;&#20013;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#21363;&#20351;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#29575;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#22312;&#20445;&#25345;&#26497;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26174;&#31034;&#20986;&#20102;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#27604;&#20363;&#38382;&#39064;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16598v5 Announce Type: replace-cross  Abstract: We propose a robust method for point cloud registration that can handle both unknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a deterministic 3-point sampling approach with two novel mechanisms that significantly boost the speed: (1) an improved ordering of the samples based on pairwise scale consistency, prioritizing the point correspondences that are more likely to be inliers, and (2) an efficient outlier rejection scheme based on triplet scale consistency, prescreening bad samples and reducing the number of hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio, the proposed method achieves comparable performance to the state of the art. At 99% outlier ratio, however, it outperforms the state of the art for both known-scale and unknown-scale problems. Especially for the latter, we observe a clear superiority in terms of robustness and speed.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;GM Cruise&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#22312;2023&#24180;10&#26376;&#19982;&#19968;&#21517;&#34892;&#20154;&#21457;&#29983;&#30896;&#25758;&#30340;&#26696;&#20363;&#65292;&#20998;&#26512;&#20102;&#20844;&#21496;&#22312;&#24212;&#23545;&#34892;&#20154;&#34987;&#25302;&#34892;&#20107;&#25925;&#19978;&#30340;&#19981;&#24403;&#22788;&#29702;&#65292;&#24182;&#20174;&#20013;&#25552;&#28860;&#20986;&#23545;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#23433;&#20840;&#24037;&#31243;&#26377;&#37325;&#35201;&#21551;&#31034;&#30340;&#25945;&#35757;&#65292;&#21253;&#25324;&#23545;&#21608;&#36793;&#21457;&#29983;&#20107;&#20214;&#30340;&#35748;&#35782;&#19982;&#21453;&#24212;&#12289;&#20934;&#30830;&#26500;&#24314;&#30896;&#25758;&#21518;&#22330;&#26223;&#27169;&#22411;&#20197;&#21450;&#25913;&#21892;&#20107;&#25925;&#21709;&#24212;&#26426;&#21046;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;GM Cruise&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#22312;2023&#24180;10&#26376;&#19982;&#19968;&#21517;&#34892;&#20154;&#21457;&#29983;&#30896;&#25758;&#30340;&#26696;&#20363;&#65292;&#20998;&#26512;&#20102;&#20844;&#21496;&#22312;&#24212;&#23545;&#34892;&#20154;&#34987;&#25302;&#34892;&#20107;&#25925;&#19978;&#30340;&#19981;&#24403;&#22788;&#29702;&#65292;&#24182;&#20174;&#20013;&#25552;&#28860;&#20986;&#23545;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#23433;&#20840;&#24037;&#31243;&#26377;&#37325;&#35201;&#21551;&#31034;&#30340;&#25945;&#35757;&#65292;&#21253;&#25324;&#23545;&#21608;&#36793;&#21457;&#29983;&#20107;&#20214;&#30340;&#35748;&#35782;&#19982;&#21453;&#24212;&#12289;&#20934;&#30830;&#26500;&#24314;&#30896;&#25758;&#21518;&#22330;&#26223;&#27169;&#22411;&#20197;&#21450;&#25913;&#21892;&#20107;&#25925;&#21709;&#24212;&#26426;&#21046;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.06046v3 Announce Type: replace  Abstract: An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. Is-sues stem not just from the loss events themselves, but also from how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. External investigation reports provide raw material describing the incident and critique the company's response from a regulatory point of view, but exclude safety engineering recommendations from scope. We highlight specific facts and relationships among events by tying together different pieces of the external report material. We then explore safety lessons that might be learned related to: recognizing and responding to nearby mishaps, building an accurate world model of a post-collision scenario, the in-adequacy of 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36187;&#36710;&#22312;&#24212;&#23545;&#23454;&#38469;&#36710;&#36742;&#27169;&#22411;&#35823;&#24046;&#65288;&#27169;&#24577;&#24046;&#24322;&#65289;&#26102;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#37319;&#29992;&#35299;&#32806;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31639;&#27861;&#22312;&#38754;&#23545;&#27169;&#22411;&#35823;&#24046;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06406</link><description>&lt;p&gt;
Partial End-to-end Reinforcement Learning for Robustness Against Modelling Error in Autonomous Racing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36187;&#36710;&#22312;&#24212;&#23545;&#23454;&#38469;&#36710;&#36742;&#27169;&#22411;&#35823;&#24046;&#65288;&#27169;&#24577;&#24046;&#24322;&#65289;&#26102;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#37319;&#29992;&#35299;&#32806;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31639;&#27861;&#22312;&#38754;&#23545;&#27169;&#22411;&#35823;&#24046;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06406v2 Announce Type: replace  Abstract: In this paper, we address the issue of increasing the performance of reinforcement learning (RL) solutions for autonomous racing cars when navigating under conditions where practical vehicle modelling errors (commonly known as \emph{model mismatches}) are present. To address this challenge, we propose a partial end-to-end algorithm that decouples the planning and control tasks. Within this framework, an RL agent generates a trajectory comprising a path and velocity, which is subsequently tracked using a pure pursuit steering controller and a proportional velocity controller, respectively. In contrast, many current learning-based (i.e., reinforcement and imitation learning) algorithms utilise an end-to-end approach whereby a deep neural network directly maps from sensor data to control commands. By leveraging the robustness of a classical controller, our partial end-to-end driving algorithm exhibits better robustness towards model mis
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#32463;&#26631;&#27880;&#30340;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#39062;&#26080;&#30417;&#30563;&#22330;&#26223;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#24110;&#21161;&#33258;&#21160;&#26426;&#22120;&#20154;&#32500;&#25345;&#26410;&#26469;&#31354;&#38388;&#26646;&#24687;&#22320;&#65292;&#22914;&#22826;&#31354;&#31449; Gateway&#12290;&#36890;&#36807;&#20462;&#25913; Expectation-Maximization &#39640;&#26031;&#28151;&#21512;&#27169;&#22411; (GMM) &#32858;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992; Earth Mover's Distance &#24230;&#37327;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#27604;&#20004;&#20010;&#26102;&#31354;&#28857;&#30340;&#21464;&#21270;&#65292;&#20026;&#26410;&#26469;&#30340;&#31354;&#38388;&#33258;&#21160;&#21270;&#30417;&#25511;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.02396</link><description>&lt;p&gt;
Unsupervised Change Detection for Space Habitats Using 3D Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#32463;&#26631;&#27880;&#30340;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#26032;&#39062;&#26080;&#30417;&#30563;&#22330;&#26223;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#24110;&#21161;&#33258;&#21160;&#26426;&#22120;&#20154;&#32500;&#25345;&#26410;&#26469;&#31354;&#38388;&#26646;&#24687;&#22320;&#65292;&#22914;&#22826;&#31354;&#31449; Gateway&#12290;&#36890;&#36807;&#20462;&#25913; Expectation-Maximization &#39640;&#26031;&#28151;&#21512;&#27169;&#22411; (GMM) &#32858;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992; Earth Mover's Distance &#24230;&#37327;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#23545;&#27604;&#20004;&#20010;&#26102;&#31354;&#28857;&#30340;&#21464;&#21270;&#65292;&#20026;&#26410;&#26469;&#30340;&#31354;&#38388;&#33258;&#21160;&#21270;&#30417;&#25511;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02396v3 Announce Type: replace  Abstract: This work presents an algorithm for scene change detection from point clouds to enable autonomous robotic caretaking in future space habitats. Autonomous robotic systems will help maintain future deep-space habitats, such as the Gateway space station, which will be uncrewed for extended periods. Existing scene analysis software used on the International Space Station (ISS) relies on manually-labeled images for detecting changes. In contrast, the algorithm presented in this work uses raw, unlabeled point clouds as inputs. The algorithm first applies modified Expectation-Maximization Gaussian Mixture Model (GMM) clustering to two input point clouds. It then performs change detection by comparing the GMMs using the Earth Mover's Distance. The algorithm is validated quantitatively and qualitatively using a test dataset collected by an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth images taken directly by Astro
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APARATE&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#34917;&#19969;&#26469;&#25913;&#36827;CNN&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#20013;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#20854;&#22312;&#38754;&#20020;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#33021;&#22815;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.01351</link><description>&lt;p&gt;
APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APARATE&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#34917;&#19969;&#26469;&#25913;&#36827;CNN&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#20013;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#20854;&#22312;&#38754;&#20020;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#33021;&#22815;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01351v3 Announce Type: replace-cross  Abstract: In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to
&lt;/p&gt;</description></item></channel></rss>