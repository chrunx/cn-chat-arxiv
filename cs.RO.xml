<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20107;&#20214;&#21644;&#24815;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31383;&#21475;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21160;&#24577;&#21644;&#29031;&#26126;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#31934;&#30830;&#20107;&#20214;&#30456;&#26426;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#65292;&#22686;&#24378;&#20102;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01370</link><description>&lt;p&gt;
EVIT: &#22522;&#20110;&#20107;&#20214;&#30340;&#20107;&#20214;-&#24815;&#24615;&#21322;&#23494;&#38598;&#22320;&#22270;&#36319;&#36394;&#26041;&#27861;&#20351;&#29992;&#31383;&#38480;&#38750;&#32447;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20107;&#20214;&#21644;&#24815;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31383;&#21475;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21160;&#24577;&#21644;&#29031;&#26126;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#31934;&#30830;&#20107;&#20214;&#30456;&#26426;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#65292;&#22686;&#24378;&#20102;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01370v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20107;&#20214;&#25668;&#20687;&#22836;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#35270;&#35273;&#22806;&#24863;&#23448;&#22120;&#65292;&#23427;&#23545;&#20142;&#24230;&#21464;&#21270;&#20570;&#20986;&#21453;&#24212;&#65292;&#32780;&#19981;&#26159;&#25972;&#21512;&#32477;&#23545;&#22270;&#20687;&#24378;&#24230;&#12290;&#30001;&#20110;&#36825;&#31181;&#35774;&#35745;&#65292;&#20256;&#24863;&#22120;&#22312;&#21160;&#24577;&#21644;&#29031;&#26126;&#26465;&#20214;&#24694;&#21155;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20107;&#20214;&#39537;&#21160;&#30340;&#21516;&#27493;&#36319;&#36394;&#21644;&#26144;&#23556;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24050;&#32463;&#25351;&#20986;&#65292;&#20256;&#24863;&#22120;&#22312;&#22522;&#20110;&#20808;&#39564;&#21019;&#24314;&#30340;&#20256;&#32479;&#20256;&#24863;&#22120;&#22320;&#22270;&#30340;&#36319;&#36394;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#27880;&#20876;&#33539;&#24335;&#65292;&#30456;&#26426;&#30340;&#22823;&#33539;&#22260;&#29031;&#26126;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#33258;&#25105;&#36816;&#21160;&#21487;&#20197;&#22312;&#20934;&#30830;&#21019;&#24314;&#30340;&#20808;&#39564;&#22320;&#22270;&#19978;&#24471;&#21040;&#36319;&#36394;&#12290;&#26412;&#25991;&#24310;&#32493;&#20102;&#26368;&#36817;&#20171;&#32461;&#30340;&#20107;&#20214;&#39537;&#21160;&#20960;&#20309;&#21322;&#23494;&#38598;&#36319;&#36394;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#28155;&#21152;&#24815;&#24615;&#20449;&#21495;&#20197;&#22686;&#24378;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#28155;&#21152;&#30340;&#20449;&#21495;&#25552;&#20379;&#20102;&#26377;&#20851;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#36825;&#26679;&#21487;&#20197;&#22686;&#24378;&#23545;&#20107;&#20214;&#30456;&#26426;&#30340;&#31934;&#30830;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#20102;&#20107;&#20214;&#21644;&#24815;&#24615;&#27979;&#37327;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36319;&#36394;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01370v1 Announce Type: cross  Abstract: Event cameras are an interesting visual exteroceptive sensor that reacts to brightness changes rather than integrating absolute image intensities. Owing to this design, the sensor exhibits strong performance in situations of challenging dynamics and illumination conditions. While event-based simultaneous tracking and mapping remains a challenging problem, a number of recent works have pointed out the sensor's suitability for prior map-based tracking. By making use of cross-modal registration paradigms, the camera's ego-motion can be tracked across a large spectrum of illumination and dynamics conditions on top of accurate maps that have been created a priori by more traditional sensors. The present paper follows up on a recently introduced event-based geometric semi-dense tracking paradigm, and proposes the addition of inertial signals in order to robustify the estimation. More specifically, the added signals provide strong cues for po
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01334</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#38271;&#26399;&#20219;&#21153;&#29702;&#35299;&#30340;&#39592;&#24178;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Backbone for Long-Horizon Robot Task Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#31471;&#21040;&#31471;&#26426;&#22120;&#20154;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#20219;&#21153;&#39046;&#22495;&#65292;&#24120;&#24120;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#32467;&#26524;&#21644;&#19981;&#33391;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#21363;TBBF (Therblig-based Backbone Framework)&#65292;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;&#21644;&#36716;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;&#20351;&#29992;therbligs&#65288;&#22522;&#26412;&#21160;&#20316;&#20803;&#32032;&#65289;&#20316;&#20026;&#25903;&#25745;&#65292;&#24182;&#19982;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#29702;&#35299;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#12290;&#22312;&#31163;&#32447;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Meta-RGate SynerFusion (MGSF)&#32593;&#32476;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#21508;&#31181;&#20219;&#21153;&#30340;therbligs&#12290;&#22312;&#32447;&#27979;&#35797;&#38454;&#27573;&#65292;&#22312;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#28436;&#31034;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;MGSF&#32593;&#32476;&#25552;&#21462;&#39640;&#38454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;ActionREG&#65288;&#21160;&#20316;&#27880;&#20876;&#65289;&#23558;&#20854;&#32534;&#30721;&#25104;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;Meta-Learner&#65292;&#23427;&#21487;&#20197;&#20174;&#21333;&#20010;&#20219;&#21153;&#30340;&#34920;&#29616;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#27867;&#21270;&#21040;&#19981;&#21516;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#20013;&#36827;&#34892;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#25552;&#21319;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 Announce Type: new  Abstract: End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Additionally
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25511;&#21046;&#36755;&#20837;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#36830;&#32493;&#26426;&#22120;&#20154;&#24418;&#29366;&#30340;&#36712;&#36857;&#20272;&#31639;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#27979;&#37327;&#21644;&#20272;&#35745;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2408.01333</link><description>&lt;p&gt;
&#36830;&#32493;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#21644;&#36830;&#32493;&#26426;&#22120;&#20154;&#24418;&#29366;&#20272;&#35745;&#20013;&#32467;&#21512;&#25511;&#21046;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Incorporating Control Inputs in the Estimation of Continuous Mobile Robot Trajectories and Continuum Robot Shapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25511;&#21046;&#36755;&#20837;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#36830;&#32493;&#26426;&#22120;&#20154;&#24418;&#29366;&#30340;&#36712;&#36857;&#20272;&#31639;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#27979;&#37327;&#21644;&#20272;&#35745;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01333v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: &#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#25209;&#37327;&#29366;&#24577;&#20272;&#35745;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26426;&#22120;&#20154;&#36712;&#36857;&#12290;&#22312;&#36807;&#21435;&#65292;&#36825;&#31181;&#26041;&#27861;&#32771;&#34385;&#20102;&#30456;&#23545;&#31616;&#21333;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#20808;&#39564;&#65292;&#36825;&#20123;&#20808;&#39564;&#20351;&#29992;&#20102;&#24658;&#23450;&#36895;&#24230;&#25110;&#21152;&#36895;&#24230;&#36825;&#26679;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22806;&#37096;&#25511;&#21046;&#36755;&#20837;&#65292;&#22914;&#36895;&#24230;&#25110;&#21152;&#36895;&#24230;&#21629;&#20196;&#65292;&#32435;&#20837;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#20154;&#23398;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#20351;&#20854;&#19981;&#20165;&#36866;&#29992;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#36830;&#32493;&#26102;&#38388;&#36712;&#36857;&#30340;&#20272;&#35745;&#65292;&#20063;&#36866;&#29992;&#20110;&#36830;&#32493;&#26426;&#22120;&#20154;&#24418;&#29366;&#30340;&#20272;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#25511;&#21046;&#36755;&#20837;&#23548;&#33268;&#20102;&#26356;&#31934;&#30830;&#30340;&#20808;&#39564;&#65292;&#21487;&#33021;&#38656;&#35201;&#26356;&#23569;&#30340;&#27979;&#37327;&#21644;&#20272;&#35745;&#33410;&#28857;&#26469;&#33719;&#24471;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#36825;&#22312;&#20256;&#24863;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01333v1 Announce Type: new  Abstract: Continuous-time batch state estimation using Gaussian processes is an efficient approach to estimate the trajectories of robots over time. In the past, relatively simple physics-motivated priors have been considered for such approaches, using assumptions such as constant velocity or acceleration. This paper presents an approach to incorporating exogenous control inputs, such as velocity or acceleration commands, into the continuous Gaussian process state-estimation framework. It is shown that this approach generalizes across different domains in robotics, making it applicable to both the estimation of continuous-time trajectories for mobile robots and continuum-robot shapes. Results show that incorporating control inputs leads to more informed priors, potentially requiring less measurements and estimation nodes to obtain accurate estimates. This makes the approach particularly useful in situations in which limited sensing is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HeteroMorpheus&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#26500;&#22270;Transformer&#30340;&#24110;&#21161;&#19979;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154;&#32930;&#20307;&#30340;&#21151;&#33021;&#22810;&#26679;&#24615;&#36827;&#34892;&#20102;&#26377;&#25928;&#24314;&#27169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#36890;&#29992;&#25511;&#21046;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01230</link><description>&lt;p&gt;
&#24322;&#26500;&#39764;&#26041;&#26031;&#65306;&#22522;&#20110;&#24418;&#24577;&#24322;&#36136;&#24615;&#24314;&#27169;&#30340;&#36890;&#29992;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
HeteroMorpheus: Universal Control Based on Morphological Heterogeneity Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HeteroMorpheus&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#26500;&#22270;Transformer&#30340;&#24110;&#21161;&#19979;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154;&#32930;&#20307;&#30340;&#21151;&#33021;&#22810;&#26679;&#24615;&#36827;&#34892;&#20102;&#26377;&#25928;&#24314;&#27169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#36890;&#29992;&#25511;&#21046;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01230v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#39046;&#22495;&#65292;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#35774;&#35745;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#20250;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#36866;&#29992;&#20110;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#36890;&#29992;&#25511;&#21046;&#25919;&#31574;&#26377;&#26395;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25429;&#25417;&#26426;&#22120;&#20154;&#32930;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21160;&#24577;&#26041;&#38754;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Transformer&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36825;&#20123;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#21516;&#36136;&#30340;&#22270;&#32467;&#26500;&#65292;&#36825;&#24573;&#35270;&#20102;&#19981;&#21516;&#32930;&#20307;&#20043;&#38388;&#30340;&#21151;&#33021;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24322;&#26500;&#22270;Transformer&#30340;HeteroMorpheus&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#29420;&#29305;&#22320;&#35299;&#20915;&#20102;&#32930;&#20307;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#26426;&#22120;&#20154;&#24418;&#24577;&#21160;&#24577;&#30340;&#26356;&#22909;&#34920;&#31034;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;HeteroMorpheus&#22312;&#31574;&#30053;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01230v1 Announce Type: new  Abstract: In the field of robotic control, designing individual controllers for each robot leads to high computational costs. Universal control policies, applicable across diverse robot morphologies, promise to mitigate this challenge. Predominantly, models based on Graph Neural Networks (GNN) and Transformers are employed, owing to their effectiveness in capturing relational dynamics across a robot's limbs. However, these models typically employ homogeneous graph structures that overlook the functional diversity of different limbs. To bridge this gap, we introduce HeteroMorpheus, a novel method based on heterogeneous graph Transformer. This method uniquely addresses limb heterogeneity, fostering better representation of robot dynamics of various morphologies. Through extensive experiments we demonstrate the superiority of HeteroMorpheus against state-of-the-art methods in the capability of policy generalization, including zero-shot generalization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20960;&#20046;&#25152;&#26377;&#26631;&#20934;FDM&#25171;&#21360;&#26426;&#30340;&#26032;&#22411;3D&#25171;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#27424;&#25380;&#36825;&#19968;&#24120;&#35265;&#38382;&#39064;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36719;&#30828;&#26448;&#26009;&#20043;&#38388;&#30340;&#21487;&#38752;&#31896;&#21512;&#65292;&#35299;&#20915;&#20102;&#36719;&#30828;&#26448;&#26009;&#31896;&#21512;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01210</link><description>&lt;p&gt;
&#29983;&#29289;&#21551;&#21457;&#30340;3D&#25171;&#21360;&#65306;&#36890;&#36807;&#27424;&#25380;&#23454;&#29616;&#36719;&#30828;&#26448;&#26009;&#20043;&#38388;&#30340;&#29282;&#22266;&#31896;&#21512;
&lt;/p&gt;
&lt;p&gt;
From Problem to Solution: Bio-inspired 3D Printing for Bonding Soft and Rigid Materials via Underextrusions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20960;&#20046;&#25152;&#26377;&#26631;&#20934;FDM&#25171;&#21360;&#26426;&#30340;&#26032;&#22411;3D&#25171;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#27424;&#25380;&#36825;&#19968;&#24120;&#35265;&#38382;&#39064;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36719;&#30828;&#26448;&#26009;&#20043;&#38388;&#30340;&#21487;&#38752;&#31896;&#21512;&#65292;&#35299;&#20915;&#20102;&#36719;&#30828;&#26448;&#26009;&#31896;&#21512;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01210v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33034;&#26894;&#21160;&#29289;&#24471;&#30410;&#20110;&#20854;&#32467;&#26500;&#30340;&#21018;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#26580;&#36719;&#24615;&#12290;&#21516;&#26679;&#65292;&#23558;&#21018;&#24615;&#21644;&#26580;&#36719;&#24615;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#25552;&#39640;&#36719;&#20307;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#36719;&#30828;&#26448;&#26009;&#31896;&#21512;&#30028;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#28151;&#21512;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#29305;&#27530;&#30340;&#26426;&#22120;&#65292;&#20363;&#22914;&#19987;&#20026;&#36719;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;PolyJet 3D&#25171;&#21360;&#26426;&#65292;&#36825;&#20123;&#26426;&#22120;&#24182;&#19981;&#24120;&#35265;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20960;&#20046;&#25152;&#26377;&#21830;&#19994;&#21487;&#29992;&#30340;FDM&#25171;&#21360;&#26426;&#19978;&#20351;&#29992;&#30340;3D&#25171;&#21360;&#25216;&#26415;&#12290;&#36825;&#39033;&#25216;&#26415;&#21033;&#29992;&#20102;&#27424;&#25380;&#36825;&#19968;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#23618;&#34701;&#21512;&#20026;&#36719;&#30828;&#26448;&#26009;&#20043;&#38388;&#30340;&#29282;&#22266;&#31896;&#21512;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25509;&#21475;&#12290;&#27424;&#25380;&#29983;&#25104;&#19968;&#31181;&#22810;&#23380;&#32467;&#26500;&#65292;&#31867;&#20284;&#20110;&#32420;&#32500;&#29366;&#32467;&#32532;&#32452;&#32455;&#65292;&#23427;&#33021;&#19982;&#30828;&#24615;&#37096;&#20998;&#36890;&#36807;&#23618;&#34701;&#21512;&#25552;&#20379;&#22362;&#23454;&#30340;&#25509;&#21475;&#65292;&#32780;&#22810;&#23380;&#24615;&#21448;&#20801;&#35768;&#19982;&#26580;&#36719;&#26448;&#26009;&#20043;&#38388;&#30340;&#30456;&#20114;&#38145;&#23450;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#36890;&#36807;&#31616;&#21333;&#30340;&#35843;&#25972;&#25171;&#21360;&#21442;&#25968;&#26469;&#25511;&#21046;&#31896;&#21512;&#21147;&#30340;&#24378;&#24230;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#30340;&#32467;&#26500;&#22312;&#26080;&#27861;&#36731;&#26494;&#24212;&#29992;&#20256;&#32479;&#31896;&#21512;&#21058;&#30340;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23454;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01210v1 Announce Type: new  Abstract: Vertebrate animals benefit from a combination of rigidity for structural support and softness for adaptation. Similarly, integrating rigidity and softness can enhance the versatility of soft robotics. However, the challenges associated with creating durable bonding interfaces between soft and rigid materials have limited the development of hybrid robots. Existing solutions require specialized machinery, such as polyjet 3D printers, which are not commonly available. In response to these challenges, we have developed a 3D printing technique that can be used with almost all commercially available FDM printers. This technique leverages the common issue of underextrusion to create a strong bond between soft and rigid materials. Underextrusion generates a porous structure, similar to fibrous connective tissues, that provides a robust interface with the rigid part through layer fusion, while the porosity enables interlocking with the soft mater
&lt;/p&gt;</description></item><item><title>IG-SLAM&#26159;&#19968;&#31181;&#20165;&#20351;&#29992;RGB&#30340;&#23494;&#38598;SLAM&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20581;&#22766;&#30340;&#23494;&#38598;SLAM&#36319;&#36394;&#26041;&#27861;&#21644;&#39640;&#26031;&#26001;&#28857;&#25216;&#26415;&#65292;&#20197;&#26500;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#23039;&#24577;&#21644;&#23494;&#38598;&#28145;&#24230;&#65292;&#31995;&#32479;&#33021;&#22815;&#20248;&#21270;&#22320;&#22270;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#34928;&#20943;&#31574;&#30053;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#36816;&#34892;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01126</link><description>&lt;p&gt;
IG-SLAM: &#21363;&#26102;&#39640;&#26031;SLAM
&lt;/p&gt;
&lt;p&gt;
IG-SLAM: Instant Gaussian SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01126
&lt;/p&gt;
&lt;p&gt;
IG-SLAM&#26159;&#19968;&#31181;&#20165;&#20351;&#29992;RGB&#30340;&#23494;&#38598;SLAM&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20581;&#22766;&#30340;&#23494;&#38598;SLAM&#36319;&#36394;&#26041;&#27861;&#21644;&#39640;&#26031;&#26001;&#28857;&#25216;&#26415;&#65292;&#20197;&#26500;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#12290;&#36890;&#36807;&#20934;&#30830;&#30340;&#23039;&#24577;&#21644;&#23494;&#38598;&#28145;&#24230;&#65292;&#31995;&#32479;&#33021;&#22815;&#20248;&#21270;&#22320;&#22270;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#34928;&#20943;&#31574;&#30053;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#36816;&#34892;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01126v1 &#23459;&#35328;&#31867;&#22411;: &#20132;&#21449; Abstract: 3D&#39640;&#26031;&#26001;&#28857;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;SLAM&#31995;&#32479;&#20013;&#22330;&#26223;&#34920;&#31034;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#19982;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#29992;&#20110;&#25351;&#23548;&#26144;&#23556;&#36807;&#31243;&#30340;&#23494;&#38598;&#28145;&#24230;&#22270;&#65292;&#35201;&#20040;&#22312;&#32771;&#34385;&#29615;&#22659;&#22823;&#23567;&#26102;&#27809;&#26377;&#35814;&#23613;&#30340;&#35757;&#32451;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IG-SLAM&#65292;&#19968;&#20010;&#20165;&#20351;&#29992;RGB&#30340;&#23494;&#38598;SLAM&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#20102;&#20581;&#22766;&#30340;&#23494;&#38598;SLAM&#36319;&#36394;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#39640;&#26031;&#26001;&#28857;&#30456;&#32467;&#21512;&#12290;&#20351;&#29992;&#36319;&#36394;&#25552;&#20379;&#30340;&#31934;&#30830;&#23039;&#24577;&#21644;&#23494;&#38598;&#28145;&#24230;&#65292;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#34987;&#26500;&#24314;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#26469;&#20248;&#21270;&#22320;&#22270;&#65292;&#20197;&#25552;&#39640;3D&#37325;&#24314;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#22320;&#22270;&#20248;&#21270;&#20013;&#20351;&#29992;&#30340;&#34928;&#20943;&#31574;&#30053;&#25552;&#39640;&#20102;&#25910;&#25947;&#24615;&#65292;&#24182;&#20801;&#35768;&#31995;&#32479;&#22312;&#19968;&#36827;&#31243;&#20013;&#20197;10&#24103;&#27599;&#31186;&#30340;&#36895;&#24230;&#36816;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#19982;&#29616;&#26377;&#25216;&#26415;&#20808;&#36827;&#30340;&#32431;RGB SLAM&#31995;&#32479;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01126v1 Announce Type: cross  Abstract: 3D Gaussian Splatting has recently shown promising results as an alternative scene representation in SLAM systems to neural implicit representations. However, current methods either lack dense depth maps to supervise the mapping process or detailed training designs that consider the scale of the environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only SLAM system that employs robust Dense-SLAM methods for tracking and combines them with Gaussian Splatting. A 3D map of the environment is constructed using accurate pose and dense depth provided by tracking. Additionally, we utilize depth uncertainty in map optimization to improve 3D reconstruction. Our decay strategy in map optimization enhances convergence and allows the system to run at 10 fps in a single process. We demonstrate competitive performance with state-of-the-art RGB-only SLAM systems while achieving faster operation speeds. We present our experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#23545;&#26410;&#30693;&#24418;&#29366;&#30340;&#31354;&#38388;&#30862;&#29255;&#36827;&#34892;&#36816;&#21160;&#20272;&#35745;&#65292;&#24182;&#21516;&#26102;&#36755;&#20986;&#29289;&#20307;&#30340;&#19981;&#26126;&#24418;&#29366;&#21644;&#30456;&#20851;&#25668;&#20687;&#26426;&#30340;&#30456;&#23545;&#23039;&#24577;&#36712;&#36857;&#65292;&#36825;&#20123;&#20449;&#24687;&#29992;&#20110;&#31934;&#30830;&#20272;&#35745;&#30446;&#26631;&#30340;&#36816;&#21160;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2408.01035</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#20840;&#21160;&#24577;&#20272;&#35745;&#21644;&#26410;&#30693;&#24418;&#29366;&#31354;&#38388;&#30862;&#29255;&#30340;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Structure from Motion-based Motion Estimation and 3D Reconstruction of Unknown Shaped Space Debris
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#23545;&#26410;&#30693;&#24418;&#29366;&#30340;&#31354;&#38388;&#30862;&#29255;&#36827;&#34892;&#36816;&#21160;&#20272;&#35745;&#65292;&#24182;&#21516;&#26102;&#36755;&#20986;&#29289;&#20307;&#30340;&#19981;&#26126;&#24418;&#29366;&#21644;&#30456;&#20851;&#25668;&#20687;&#26426;&#30340;&#30456;&#23545;&#23039;&#24577;&#36712;&#36857;&#65292;&#36825;&#20123;&#20449;&#24687;&#29992;&#20110;&#31934;&#30830;&#20272;&#35745;&#30446;&#26631;&#30340;&#36816;&#21160;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01035v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#38543;&#30528;&#36817;&#24180;&#26469; spacecraft &#21457;&#23556;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#31354;&#38388;&#30862;&#29255;&#38382;&#39064;&#26085;&#30410;&#25104;&#20026;&#20154;&#31867;&#24517;&#39035;&#38754;&#23545;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31354;&#38388;&#21033;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#22312;&#36712;&#30340;&#30862;&#29255;&#28165;&#38500;&#20219;&#21153;&#21487;&#38752;&#24615;&#26159;&#20854;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#36712;&#28165;&#38500;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#65292;&#30446;&#26631;&#30340;&#39640;&#31934;&#24230;&#21160;&#37327;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#30862;&#29255;&#22833;&#25481;&#20102;&#23039;&#24577;&#21644;&#36712;&#36947;&#25511;&#21046;&#33021;&#21147;&#65292;&#20197;&#21450;&#24418;&#29366;&#22240;&#30772;&#30862;&#19981;&#26126;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#26377;&#38480;&#36164;&#28304;&#26469;&#25191;&#34892;&#19981;&#26126;&#24418;&#29366;&#31354;&#38388;&#30862;&#29255;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#20165;&#38656;&#35201;&#20108;&#32500;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36755;&#20986;&#29289;&#20307;&#30340;&#19981;&#26126;&#24418;&#29366;&#21644;&#30446;&#26631;&#19982;&#25668;&#20687;&#26426;&#20043;&#38388;&#30340;&#30456;&#23545;&#23039;&#24577;&#36712;&#36857;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#29992;&#26469;&#20272;&#35745;&#30446;&#26631;&#30340;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01035v1 Announce Type: new  Abstract: With the boost in the number of spacecraft launches in the current decades, the space debris problem is daily becoming significantly crucial. For sustainable space utilization, the continuous removal of space debris is the most severe problem for humanity. To maximize the reliability of the debris capture mission in orbit, accurate motion estimation of the target is essential. Space debris has lost its attitude and orbit control capabilities, and its shape is unknown due to the break. This paper proposes the Structure from Motion-based algorithm to perform unknown shaped space debris motion estimation with limited resources, where only 2D images are required as input. The method then outputs the reconstructed shape of the unknown object and the relative pose trajectory between the target and the camera simultaneously, which are exploited to estimate the target's motion. The method is quantitatively validated with the realistic image data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#28388;&#27874;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#36890;&#36807;&#32467;&#21512;&#35843;&#21644;&#25351;&#25968;&#20998;&#24067;&#30340;&#29305;&#28857;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20195;&#34920;&#22797;&#26434;&#20998;&#24067;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#22343;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22810;&#23792;&#20998;&#24067;&#26102;&#12290;</title><link>https://arxiv.org/abs/2408.00907</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#20272;&#35745;&#22312;&#36816;&#21160;&#32452;&#19978;&#30340;&#35843;&#21644;&#25351;&#25968;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Harmonic Exponential Filter for Nonparametric Estimation on Motion Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#28388;&#27874;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#36890;&#36807;&#32467;&#21512;&#35843;&#21644;&#25351;&#25968;&#20998;&#24067;&#30340;&#29305;&#28857;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20195;&#34920;&#22797;&#26434;&#20998;&#24067;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#22343;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22810;&#23792;&#20998;&#24067;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00907v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;: &#36125;&#21494;&#26031;&#20272;&#35745;&#26159;&#19968;&#20010;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#26159;&#20801;&#35768;&#31995;&#32479;&#20351;&#29992;&#26469;&#33258;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#20256;&#24863;&#22120;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#26469;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#20449;&#24565;&#12290;&#20026;&#20102;&#20351;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#65292;&#35768;&#22810;&#31995;&#32479;&#37117;&#20551;&#35774;&#36816;&#21160;&#21644;&#27979;&#37327;&#22122;&#22768;&#65292;&#20197;&#21450;&#29366;&#24577;&#20998;&#24067;&#65292;&#37117;&#26159;&#21333;&#23792;&#30340;&#65292;&#24182;&#19988;&#26159;&#39640;&#26031;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#22330;&#26223;&#21644;&#31995;&#32479;&#19981;&#36981;&#24490;&#36825;&#20123;&#20551;&#35774;&#12290;&#29616;&#26377;&#30340;&#38750;&#21442;&#25968;&#28388;&#27874;&#22120;&#29992;&#20110;&#24314;&#27169;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#20195;&#34920;&#22810;&#26679;&#21270;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#28388;&#27874;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#35843;&#21644;&#25351;&#25968;&#20998;&#24067;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35843;&#21644;&#25351;&#25968;&#20998;&#24067;&#30340;&#20004;&#20010;&#20851;&#38190;&#35265;&#35299;&#65306;a) &#20004;&#20010;&#20998;&#24067;&#30340;&#20056;&#31215;&#21487;&#20197;&#34920;&#31034;&#20026;&#20854;&#23545;&#25968;&#20284;&#28982;&#20613;&#37324;&#21494;&#31995;&#25968;&#20043;&#38388;&#30340;&#20803;&#32032;&#32423;&#21152;&#27861;&#65307;&#20197;&#21450;b) &#35843;&#21644;&#25351;&#25968;&#20998;&#24067;&#33021;&#22815;&#34987;&#20998;&#35299;&#25104;&#20302;&#32500;&#30340;&#35843;&#21644;&#31639;&#23376;&#21644;&#39640;&#32500;&#21442;&#25968;&#21270;&#30340;&#25351;&#25968;&#20989;&#25968;&#65292;&#21518;&#32773;&#25552;&#20379;&#20102;&#23545;&#20989;&#25968;&#31354;&#38388;&#20013;&#22797;&#26434;&#20998;&#24067;&#30340;&#28789;&#27963;&#25551;&#36848;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#26032;&#30340;&#28388;&#27874;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#24418;&#29366;&#22797;&#26434;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;&#22914;&#22810;&#23792;&#20998;&#24067;&#65289;&#29305;&#21035;&#26377;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22914;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#20248;&#21270;&#31639;&#27861;&#30340;&#34917;&#20805;&#65292;&#20197;&#20415;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00907v1 Announce Type: new  Abstract: Bayesian estimation is a vital tool in robotics as it allows systems to update the belief of the robot state using incomplete information from noisy sensors. To render the state estimation problem tractable, many systems assume that the motion and measurement noise, as well as the state distribution, are all unimodal and Gaussian. However, there are numerous scenarios and systems that do not comply with these assumptions. Existing non-parametric filters that are used to model multimodal distributions have drawbacks that limit their ability to represent a diverse set of distributions. In this paper, we introduce a novel approach to nonparametric Bayesian filtering to cope with multimodal distributions using harmonic exponential distributions. This approach leverages two key insights of harmonic exponential distributions: a) the product of two distributions can be expressed as the element-wise addition of their log-likelihood Fourier coeff
&lt;/p&gt;</description></item><item><title>&#25105;&#19981;&#26159;&#26426;&#22120;&#20154;&#12290;</title><link>https://arxiv.org/abs/2408.00853</link><description>&lt;p&gt;
&#23454;&#26102;&#28789;&#24039;&#36965;&#25805;&#20316;&#26041;&#27861;&#65306;&#22522;&#20110;&#31471;&#25928;&#23548;&#21521;&#30340;&#26426;&#22120;&#23398;&#20064; approach
&lt;/p&gt;
&lt;p&gt;
Real-time Dexterous Telemanipulation with an End-Effect-Oriented Learning-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00853
&lt;/p&gt;
&lt;p&gt;
&#25105;&#19981;&#26159;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00853v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#28789;&#24039;&#36965;&#25805;&#20316;&#23545;&#20110;&#25512;&#21160;&#20154;&#31867;&#19982;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#31934;&#30830;&#21644;&#23433;&#20840;&#25805;&#20316;&#30340;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#30001;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25163;&#20043;&#38388;&#30340;&#29289;&#29702;&#24046;&#24322;&#12289;&#19982;&#29289;&#20307;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#36828;&#31243;&#29615;&#22659;&#30340;&#38388;&#25509;&#25511;&#21046;&#21644;&#24863;&#30693;&#25152;&#24102;&#26469;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#20154;&#31867;&#25163;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#21516;&#20249;&#19978;&#26469;&#22797;&#21046;&#36816;&#21160;&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29359;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#38169;&#35823;&#65306;&#23427;&#32463;&#24120;&#24573;&#35270;&#19982;&#29289;&#20307;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20132;&#20114;&#36127;&#25285;&#23436;&#20840;&#36716;&#23233;&#32473;&#20154;&#31867;&#65292;&#35201;&#27714;&#20854;&#22312;&#38388;&#25509;&#21644;&#21453;&#30452;&#35273;&#30340;&#36828;&#31243;&#29615;&#22659;&#30340;&#35266;&#23519;&#19979;&#20316;&#20986;&#21171;&#20316;&#19978;&#30340;&#35843;&#25972;&#12290;&#26412;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31471;&#25928;&#23548;&#21521;&#30340;&#28789;&#24039;&#36965;&#25805;&#20316;&#65288;EFOLD&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36965;&#25805;&#20316;&#20219;&#21153;&#12290;EFOLD&#23558;&#36965;&#25805;&#20316;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#31471;&#25928;&#29305;&#24449;&#65288;end-effect features&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#25163;&#30340;&#21160;&#20316;&#21644;&#26426;&#22120;&#20154;&#25163;&#19982;&#29289;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#25552;&#39640;&#36965;&#25805;&#20316;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#36965;&#25805;&#20316;&#20219;&#21153;&#20013;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;EFOLD&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#25552;&#39640;&#20132;&#20114;&#25928;&#26524;&#30340;&#36136;&#37327;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#36828;&#31243;&#25805;&#20316;&#26694;&#26550;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#36341;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00853v1 Announce Type: new  Abstract: Dexterous telemanipulation is crucial in advancing human-robot systems, especially in tasks requiring precise and safe manipulation. However, it faces significant challenges due to the physical differences between human and robotic hands, the dynamic interaction with objects, and the indirect control and perception of the remote environment. Current approaches predominantly focus on mapping the human hand onto robotic counterparts to replicate motions, which exhibits a critical oversight: it often neglects the physical interaction with objects and relegates the interaction burden to the human to adapt and make laborious adjustments in response to the indirect and counter-intuitive observation of the remote environment. This work develops an End-Effects-Oriented Learning-based Dexterous Telemanipulation (EFOLD) framework to address telemanipulation tasks. EFOLD models telemanipulation as a Markov Game, introducing multiple end-effect feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$MADA$&#30340;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#30417;&#25511;&#20219;&#21153;&#26102;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#35813;&#35268;&#21010;&#22120;&#32771;&#34385;&#20102;&#28508;&#22312;&#38556;&#30861;&#29289;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#19982;&#20854;&#20182;&#30417;&#25511;&#20219;&#21153;&#30340;&#21327;&#21516;&#65292;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25104;&#21151;&#23436;&#25104;&#30417;&#25511;&#20219;&#21153;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.00846</link><description>&lt;p&gt;
&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#32844;&#19994;&#24863;&#30693;&#35268;&#21010;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#20154;&#30417;&#25511;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Occupation-aware planning method for robotic monitoring missions in dynamic environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$MADA$&#30340;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#30417;&#25511;&#20219;&#21153;&#26102;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#35813;&#35268;&#21010;&#22120;&#32771;&#34385;&#20102;&#28508;&#22312;&#38556;&#30861;&#29289;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#19982;&#20854;&#20182;&#30417;&#25511;&#20219;&#21153;&#30340;&#21327;&#21516;&#65292;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25104;&#21151;&#23436;&#25104;&#30417;&#25511;&#20219;&#21153;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00846v1 &#20844;&#21578;&#31867;&#22411;: &#26032; Abstract: &#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#23384;&#22312;&#31227;&#21160;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#30417;&#25511;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#34429;&#28982;&#22330;&#26223;&#22320;&#22270;&#26159;&#24050;&#30693;&#30340;&#65292;&#20294;&#26426;&#22120;&#20154;&#22312;&#20854;&#30417;&#25511;&#20219;&#21153;&#26399;&#38388;&#32570;&#20047;&#20851;&#20110;&#21160;&#24577;&#38556;&#30861;&#29289;&#31227;&#21160;&#30340;&#20449;&#24687;&#12290;&#36817;&#24180;&#26469;&#65292;&#20026;&#20102;&#22312;&#39640;&#24230;&#21160;&#24577;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#23616;&#37096;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29615;&#22659;&#32570;&#20047;&#20840;&#23616;&#35268;&#21010;&#22120;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#36991;&#20813;&#30340;&#30896;&#25758;&#25110;&#26080;&#27861;&#22312;&#20154;&#21475;&#23494;&#38598;&#21306;&#22495;&#65288;&#22914;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#30340;&#30417;&#25511;&#22330;&#26223;&#65289;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#20840;&#29699;&#35268;&#21010;&#22120;&#26041;&#38754;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#35813;&#35268;&#21010;&#22120;&#31216;&#20026;$MADA$&#65288;Monitoring Avoiding Dynamic Areas&#65289;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#26426;&#22120;&#20154;&#36890;&#36807;&#25552;&#20986;&#30340;&#20004;&#27493;&#26041;&#27861;&#35745;&#21010;&#24182;&#25191;&#34892;&#20219;&#21153;&#12290;&#31532;&#19968;&#27493;&#28041;&#21450;&#22522;&#20110;&#29615;&#22659;&#30340;&#20998;&#24067;&#21644;&#20272;&#35745;&#30340;&#30417;&#25511;&#38656;&#27714;&#36873;&#25321;&#35266;&#23519;&#30446;&#26631;&#12290;&#31532;&#20108;&#27493;&#26159;&#36890;&#36807;&#20351;&#29992;$MADA$&#35268;&#21010;&#22120;&#26469;&#20248;&#21270;&#36335;&#24452;&#65292;&#35813;&#35268;&#21010;&#22120;&#32771;&#34385;&#20102;&#28508;&#22312;&#38556;&#30861;&#29289;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#19982;&#20854;&#20182;&#27491;&#22312;&#36816;&#34892;&#30340;&#30417;&#25511;&#20219;&#21153;&#30340;&#21327;&#21516;&#12290;&#22312;&#23454;&#38469;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;$MADA$&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#25552;&#39640;&#20102;&#23436;&#25104;&#30417;&#25511;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00846v1 Announce Type: new  Abstract: This paper presents a method for robotic monitoring missions in the presence of moving obstacles. Although the scenario map is known, the robot lacks information about the movement of dynamic obstacles during the monitoring mission. Numerous local planners have been developed in recent years for navigating highly dynamic environments. However, the absence of a global planner for these environments can result in unavoidable collisions or the inability to successfully complete missions in densely populated areas, such as a scenario monitoring in our case. This work addresses the development and evaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas), aimed at enhancing the deployment of robots in such challenging conditions. The robot plans and executes the mission using the proposed two-step approach. The first step involves selecting the observation goal based on the environment's distribution and estimated monitoring 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSMA&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32508;&#21512;&#22810;&#28304;&#25968;&#25454;&#20197;&#39044;&#27979;&#22797;&#26434;&#20132;&#36890;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#65292;&#29305;&#21035;&#20851;&#27880;&#20114;&#32852;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAV&#65289;&#21608;&#22260;&#29615;&#22659;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#12289;&#20114;&#32852;&#36710;&#36742;&#65288;CVs&#65289;&#21644;&#20154;&#31867;&#39550;&#39542;&#36710;&#36742;&#65288;HDVs&#65289;&#12290;</title><link>https://arxiv.org/abs/2407.21310</link><description>&lt;p&gt;
MSMA&#65306;&#32508;&#21512;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#22312;&#20114;&#32852;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous Vehicle Environment with Multi-source Data Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSMA&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32508;&#21512;&#22810;&#28304;&#25968;&#25454;&#20197;&#39044;&#27979;&#22797;&#26434;&#20132;&#36890;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#65292;&#29305;&#21035;&#20851;&#27880;&#20114;&#32852;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAV&#65289;&#21608;&#22260;&#29615;&#22659;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#12289;&#20114;&#32852;&#36710;&#36742;&#65288;CVs&#65289;&#21644;&#20154;&#31867;&#39550;&#39542;&#36710;&#36742;&#65288;HDVs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21310v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#30896;&#25758;&#26080;&#36335;&#24452;&#35268;&#21010;&#30340;&#20851;&#38190;&#26159;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#26679;&#19968;&#20010;&#22330;&#26223;&#65306;&#19968;&#20010;&#20114;&#32852;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAV&#65289;&#20316;&#20026;&#20013;&#24515;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#21644;&#36890;&#20449;&#25216;&#26415;&#26469;&#24863;&#30693;&#20854;&#21608;&#22260;&#20132;&#36890;&#65292;&#36825;&#20123;&#20132;&#36890;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#12289;&#20114;&#32852;&#36710;&#36742;&#65288;CVs&#65289;&#21644;&#20154;&#31867;&#39550;&#39542;&#36710;&#36742;&#65288;HDVs&#65289;&#12290;&#25105;&#20204;&#30340;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#38024;&#23545;&#25152;&#26377;&#34987;&#26816;&#27979;&#21040;&#30340;&#21608;&#22260;&#36710;&#36742;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#25972;&#21512;&#26469;&#33258;&#20256;&#24863;&#22120;&#21644;&#36890;&#20449;&#25216;&#26415;&#30340;&#22810;&#28304;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MSMA&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#20010;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#12290;&#20351;&#29992;&#30690;&#37327;&#22320;&#22270;&#25968;&#25454;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36712;&#36857;&#25968;&#25454;&#26159;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#25910;&#38598;&#30340;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#21512;&#25104;&#25968;&#25454;&#38169;&#35823;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#28151;&#21512;&#20132;&#36890;&#27969;&#24773;&#20917;&#19979;&#65292;&#22810;&#28304;&#25968;&#25454;&#30340;&#38598;&#25104;&#23545;&#39044;&#27979;&#22797;&#26434;&#20132;&#36890;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21310v2 Announce Type: replace  Abstract: The prediction of surrounding vehicle trajectories is crucial for collision-free path planning. In this study, we focus on a scenario where a connected and autonomous vehicle (CAV) serves as the central agent, utilizing both sensors and communication technologies to perceive its surrounding traffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and human-driven vehicles (HDVs). Our trajectory prediction task is aimed at all the detected surrounding vehicles. To effectively integrate the multi-source data from both sensor and communication technologies, we propose a deep learning framework called MSMA utilizing a cross-attention module for multi-source data fusion. Vector map data is utilized to provide contextual information. The trajectory dataset is collected in CARLA simulator with synthesized data errors introduced. Numerical experiments demonstrate that in a mixed traffic flow scenario, the integration of da
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#33258;&#35780;&#20272;&#31995;&#32479;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#31185;&#23398;&#23478;&#20204;&#23547;&#27714;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.19631</link><description>&lt;p&gt;
"&#19968;&#20010;&#22909;&#30340;&#26426;&#22120;&#20154;&#24635;&#26159;&#30693;&#36947;&#20854;&#23616;&#38480;&#24615;": &#36890;&#36807;&#22240;&#23376;&#21270;&#30340;&#33258;&#25105;&#33258;&#20449;&#37327;&#24230;&#35780;&#20272;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
"A Good Bot Always Knows Its Limitations": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19631
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#33258;&#35780;&#20272;&#31995;&#32479;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#31185;&#23398;&#23478;&#20204;&#23547;&#27714;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19631v2 &#23459;&#24067;&#31867;&#22411;: &#26367;&#25442;&#36328;&#26639;&#25688;&#35201;: &#26234;&#33021;&#26426;&#22120;&#22914;&#20309;&#35780;&#20272;&#20854;&#22312;&#23436;&#25104;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65311;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#31639;&#27861;&#24615; reasoning &#30340;&#33258;&#20027;&#31995;&#32479;&#26469;&#35828;&#26159;&#28966;&#28857;&#12290;&#22312;&#36825;&#37324;&#65292;&#25552;&#20986;&#26234;&#33021;&#26426;&#22120;&#30340;&#33258;&#25105;&#33258;&#20449;&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#23545;&#19990;&#30028;&#30340;&#29366;&#24577;&#12289;&#33258;&#24049;&#30340;&#30693;&#35782;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#30340;&#33258;&#25105;&#35780;&#20272;&#65292;&#26159;&#19968;&#31181;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#26377;&#29992;&#33021;&#21147;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#31216;&#20026;&#22240;&#23376;&#21270;&#30340;&#33258;&#25105;&#33258;&#20449;&#65288;Factorized Machine Self-confidence, FaMSeC&#65289;&#65292;&#20026;&#31639;&#27861;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#12289;&#24037;&#31243;&#23548;&#21521;&#30340;&#25551;&#36848;&#65292;&#21253;&#25324;&#65306;&#32467;&#26524;&#35780;&#20272;&#12289;&#27714;&#35299;&#22120;&#36136;&#37327;&#12289;&#27169;&#22411;&#36136;&#37327;&#12289;&#23545;&#40784;&#36136;&#37327;&#21644;&#36807;&#21435;&#30340;&#32463;&#39564;&#12290;&#22312;FaMSeC&#20013;&#65292;&#33258;&#25105;&#33258;&#20449;&#25351;&#26631;&#26159;&#20174;&#23618;&#27425;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#19981;&#20165;&#33021;&#22815;&#21578;&#30693;&#31995;&#32479;&#20869;&#37096;&#30340;&#30693;&#35782;&#21644;&#33021;&#21147;&#65292;&#20063;&#33021;&#22312;&#36328;&#31995;&#32479;&#20043;&#38388;&#36827;&#34892;&#33258;&#25105;&#27807;&#36890;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#30340;&#27807;&#36890;&#21644;&#25945;&#32946;&#65292;&#24182;&#23545;&#20110;&#35774;&#35745;&#21644;&#26500;&#24314;&#26356;&#21152;&#21487;&#20449;&#21644;&#21487;&#38752;&#30340;&#33258;&#20027;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19631v2 Announce Type: replace-cross  Abstract: How can intelligent machines assess their competencies in completing tasks? This question has come into focus for autonomous systems that algorithmically reason and make decisions under uncertainty. It is argued here that machine self-confidence - a form of meta-reasoning based on self-assessments of an agent's knowledge about the state of the world and itself, as well as its ability to reason about and execute tasks - leads to many eminently computable and useful competency indicators for such agents. This paper presents a culmination of work on this concept in the form of a computational framework called Factorized Machine Self-confidence (FaMSeC), which provides a holistic engineering-focused description of factors driving an algorithmic decision-making process, including: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self confidence indicators are derived from hierarch
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; L-PR &#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#23545;&#22810;&#35270;&#22270;&#28857;&#20113;&#36827;&#34892;&#27880;&#20876;&#65292;&#24182;&#35299;&#20915;&#20102;&#20302;&#37325;&#21472;&#24773;&#20917;&#19979;&#30340;&#27880;&#20876;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2406.03298</link><description>&lt;p&gt;
L-PR: &#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#36827;&#34892;&#28857;&#20113;&#27880;&#20876;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.03298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; L-PR &#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#23545;&#22810;&#35270;&#22270;&#28857;&#20113;&#36827;&#34892;&#27880;&#20876;&#65292;&#24182;&#35299;&#20915;&#20102;&#20302;&#37325;&#21472;&#24773;&#20917;&#19979;&#30340;&#27880;&#20876;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
L-PR: &#21033;&#29992;&#28608;&#20809;&#38647;&#36798; fiducial&#26631;&#35760;&#36827;&#34892;&#20302;&#37325;&#21472;&#22810;&#35270;&#22270;&#28857;&#20113;&#27880;&#20876;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.03298v2 Announce Type: replace-cross  Abstract: Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework dubbed L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consis
&lt;/p&gt;</description></item><item><title>DASA&#31639;&#27861;&#26159;&#38024;&#23545;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#38382;&#39064;&#35774;&#35745;&#30340;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#36890;&#20449;&#30340;&#24310;&#36831;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.17247</link><description>&lt;p&gt;
DASA: &#33258;&#36866;&#24212;&#24310;&#36831;&#30340;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17247
&lt;/p&gt;
&lt;p&gt;
DASA&#31639;&#27861;&#26159;&#38024;&#23545;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#38382;&#39064;&#35774;&#35745;&#30340;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#36890;&#20449;&#30340;&#24310;&#36831;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v3 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#25105;&#20204;&#32771;&#34385;&#36825;&#26679;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;N&#20010;&#20195;&#29702;&#35797;&#22270;&#36890;&#36807;&#24182;&#34892;&#34892;&#21160;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#26469;&#21152;&#36895;&#19968;&#20010;&#20849;&#21516;&#30340;&#38543;&#26426;&#36924;&#36817;(SA)&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#21521;&#19978;&#28216;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#30340;&#26102;&#38388;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19981;&#21464;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#20943;&#36731;&#24310;&#36831;&#21644;&#33853;&#21518;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\texttt{DASA}&#65292;&#23427;&#26159;&#22522;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#12290;&#25105;&#20204;&#20026;\texttt{DASA}&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#20998;&#26512;&#65292;&#20551;&#35774;&#20195;&#29702;&#30340;&#38543;&#26426;&#35266;&#27979;&#36807;&#31243;&#26159;&#29420;&#31435;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#26174;&#33879;&#22320;&#25512;&#36827;&#20102;&#29616;&#26377;&#30340;&#32467;&#26524;&#65292;\texttt{DASA}&#26159;&#31532;&#19968;&#20010;&#19982;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#21462;&#20915;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30456;&#20851;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;Markovian&#25277;&#26679;&#19979;&#30340;N&#20493;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21508;&#31181;SA&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#24403;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#29615;&#22659;&#26102;&#65292;&#20854;&#31639;&#27861;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v3 Announce Type: replace-cross  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tau_{mix}$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#32423;&#27979;&#35797;&#26041;&#27861;&#65292;&#20026;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#30340;&#26377;&#25928;&#27979;&#35797;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15857</link><description>&lt;p&gt;
&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#31995;&#32479;&#32423;&#33258;&#21160;&#27979;&#35797;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated System-level Testing of Unmanned Aerial Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#32423;&#27979;&#35797;&#26041;&#27861;&#65292;&#20026;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#30340;&#26377;&#25928;&#27979;&#35797;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#25688;&#35201;&#65306;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#65288;UAS&#65289;&#20381;&#36182;&#20110;&#21508;&#31181;&#33322;&#31354;&#30005;&#23376;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#23545;&#20110;&#23433;&#20840;&#24615;&#21644;&#20219;&#21153;&#25191;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#38469;&#23433;&#20840;&#26631;&#20934;&#30340;&#37325;&#22823;&#35201;&#27714;&#26159;&#23545;&#33322;&#31354;&#30005;&#23376;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#20005;&#26684;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#12290;&#24403;&#21069;&#24037;&#19994;&#20570;&#27861;&#26159;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#22330;&#26223;&#65292;&#20351;&#29992;&#27169;&#25311;&#22120;&#25163;&#21160;/&#33258;&#21160;&#25191;&#34892;&#36825;&#20123;&#22330;&#26223;&#65292;&#24182;&#25163;&#21160;&#35780;&#20272;&#32467;&#26524;&#12290;&#27979;&#35797;&#22330;&#26223;&#36890;&#24120;&#21253;&#25324;&#35774;&#32622;&#29305;&#23450;&#39134;&#34892;&#25110;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#21463;&#27979;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#27492;&#30446;&#30340;&#30340;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#20063;&#35201;&#27714;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#31995;&#32479;&#32423;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;AITester&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#26469;&#33258;&#21160;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#35780;&#20272;&#21508;&#31181;&#27979;&#35797;&#22330;&#26223;&#12290;&#27979;&#35797;&#22330;&#26223;&#36890;&#36807;AI&#25216;&#26415;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#65292;&#24182;&#27169;&#25311;&#21644;&#35780;&#20272;&#31995;&#32479;&#20013;&#19981;&#21516;&#32452;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27979;&#35797;&#30340;&#35206;&#30422;&#29575;&#21644;&#21487;&#38752;&#24230;&#12290;&#27492;&#22806;&#65292;AITester&#36824;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#20998;&#26512;&#21644;&#20248;&#21270;&#27979;&#35797;&#26696;&#20363;&#65292;&#20174;&#32780;&#22312;&#30701;&#26102;&#38388;&#20869;&#21457;&#29616;&#24182;&#38548;&#31163;&#31995;&#32479;&#28508;&#22312;&#30340;&#38169;&#35823;&#21644;&#38382;&#39064;&#12290;AITester&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#25104;&#26412;&#24182;&#25552;&#39640;UAS&#31995;&#32479;&#27979;&#35797;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v2 Announce Type: replace-cross  Abstract: Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems. The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes. The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings. The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation. In this paper, we propose a novel approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#26469;&#25552;&#39640;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#30340;&#36710;&#27969;&#37327;&#21644;&#36895;&#24230;&#26465;&#20214;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#36890;&#36807;&#29575;&#24182;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.09436</link><description>&lt;p&gt;
&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#22312;&#31895;&#31890;&#24230;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Temporal Transfer Learning for Traffic Optimization with Coarse-grained Advisory Autonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#26469;&#25552;&#39640;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#30340;&#36710;&#27969;&#37327;&#21644;&#36895;&#24230;&#26465;&#20214;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#36890;&#36807;&#29575;&#24182;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09436v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09436v2 Announce Type: replace  Abstract: The recent development of connected and automated vehicle (CAV) technologies has spurred investigations to optimize dense urban traffic to maximize vehicle speed and throughput. This paper explores advisory autonomy, in which real-time driving advisories are issued to the human drivers, thus achieving near-term performance of automated vehicles. Due to the complexity of traffic systems, recent studies of coordinating CAVs have resorted to leveraging deep reinforcement learning (RL). Coarse-grained advisory is formalized as zero-order holds, and we consider a range of hold duration from 0.1 to 40 seconds. However, despite the similarity of the higher frequency tasks on CAVs, a direct application of deep RL fails to be generalized to advisory autonomy tasks. To overcome this, we utilize zero-shot transfer, training policies on a set of source tasks--specific traffic scenarios with designated hold durations--and then evaluating the effi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32039;&#20945;&#30340;SNN&#27169;&#22359;&#12289;&#22810;&#20010;&#20195;&#34920;&#30456;&#21516;&#22320;&#28857;&#30340;SNN&#32452;&#21512;&#65292;&#20197;&#21450;&#23545;&#36830;&#32493;&#22270;&#20687;&#24207;&#21015;&#21305;&#37197;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25216;&#26415;&#20026;&#26426;&#22120;&#20154;&#35270;&#35273;&#31995;&#32479;&#30340;&#31934;&#30830;&#21644;&#20302;&#21151;&#29575;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2311.13186</link><description>&lt;p&gt;
&#12298;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
Applications of Spiking Neural Networks in Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13186
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32039;&#20945;&#30340;SNN&#27169;&#22359;&#12289;&#22810;&#20010;&#20195;&#34920;&#30456;&#21516;&#22320;&#28857;&#30340;SNN&#32452;&#21512;&#65292;&#20197;&#21450;&#23545;&#36830;&#32493;&#22270;&#20687;&#24207;&#21015;&#21305;&#37197;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25216;&#26415;&#20026;&#26426;&#22120;&#20154;&#35270;&#35273;&#31995;&#32479;&#30340;&#31934;&#30830;&#21644;&#20302;&#21151;&#29575;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13186v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22240;&#20854;&#22312;&#22823;&#22823;&#26410;&#23454;&#29616;&#30340;&#28508;&#21147;&#33021;&#28304;&#25928;&#29575;&#21644;&#20302;&#24310;&#36831;&#65292;&#29305;&#21035;&#26159;&#22312;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#26102;&#65292;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#24378;&#35843;&#20102;SNNs&#22312;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#65288;VPR&#65289;&#26041;&#38754;&#30340;&#19977;&#39033;&#20808;&#36827;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#12298;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12299;&#65292;&#20854;&#20013;&#27599;&#20010;SNN&#20195;&#34920;&#19968;&#32452;&#19981;&#37325;&#21472;&#30340;&#22320;&#29702;&#19981;&#21516;&#22320;&#28857;&#65292;&#20351;&#32593;&#32476;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#12298;&#27169;&#22359;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#12299;&#65292;&#20854;&#20013;&#22810;&#20010;&#32593;&#32476;&#20195;&#34920;&#21516;&#19968;&#22320;&#28857;&#65292;&#19982;&#21333;&#19968;&#32593;&#32476;&#27169;&#22411;&#30340;&#31934;&#24230;&#30456;&#27604;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;SNN&#27169;&#22359;&#38750;&#24120;&#32039;&#20945;&#65292;&#20165;&#21253;&#21547;1500&#20010;&#31070;&#32463;&#20803;&#21644;474k&#20010;&#31361;&#35302;&#65292;&#20351;&#20854;&#22240;&#20026;&#23427;&#20204;&#30340;&#20307;&#31215;&#23567;&#32780;&#38750;&#24120;&#36866;&#21512;&#32452;&#32676;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SNN&#22522;&#30784;VPR&#20013;&#24207;&#21015;&#21305;&#37197;&#30340;&#20316;&#29992;&#65292;&#19968;&#31181;&#25216;&#26415;&#65292;&#20351;&#29992;&#36830;&#32493;&#22270;&#20687;&#26469;&#32454;&#21270;&#20301;&#32622;&#35782;&#21035;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#37325;&#22797;&#20351;&#29992;&#21516;&#19968;&#20010;SNN&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26435;&#37325;&#26356;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#20301;&#32622;&#35782;&#21035;&#31934;&#24230;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#27169;&#22359;&#21270;SNN&#30340;&#38598;&#21512;&#21644;&#24207;&#21015;&#21305;&#37197;&#65292;&#23427;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#35270;&#35273;&#31995;&#32479;&#20013;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#20302;&#21151;&#29575;&#30340;&#35270;&#35273;&#20301;&#32622;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13186v2 Announce Type: replace-cross  Abstract: In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for their largely-unrealized potential energy efficiency and low latency particularly when implemented on neuromorphic hardware. Our paper highlights three advancements for SNNs in Visual Place Recognition (VPR). Firstly, we propose Modular SNNs, where each SNN represents a set of non-overlapping geographically distinct places, enabling scalable networks for large environments. Secondly, we present Ensembles of Modular SNNs, where multiple networks represent the same place, significantly enhancing accuracy compared to single-network models. Each of our Modular SNN modules is compact, comprising only 1500 neurons and 474k synapses, making them ideally suited for ensembling due to their small size. Lastly, we investigate the role of sequence matching in SNN-based VPR, a technique where consecutive images are used to refine place recognition. We analyze the re
&lt;/p&gt;</description></item></channel></rss>