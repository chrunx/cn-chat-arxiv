<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.RO.xml</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAC-Net&#30340;&#32447;&#24615;&#34701;&#21512;&#27880;&#24847;&#21147;&#25351;&#23548;&#21367;&#31215;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;RGB&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#26469;&#33258;&#28145;&#24230;&#22270;&#20687;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23545;&#36974;&#25377;&#22330;&#26223;&#20013;&#30446;&#26631;&#23545;&#35937;&#30340;&#20934;&#30830;&#25235;&#21462;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.03238</link><description>&lt;p&gt;
LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03238
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAC-Net&#30340;&#32447;&#24615;&#34701;&#21512;&#27880;&#24847;&#21147;&#25351;&#23548;&#21367;&#31215;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;RGB&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#26469;&#33258;&#28145;&#24230;&#22270;&#20687;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23545;&#36974;&#25377;&#22330;&#26223;&#20013;&#30446;&#26631;&#23545;&#35937;&#30340;&#20934;&#30830;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03238v1 Announce Type: new  Abstract: This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-m
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFedSIS&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20808;&#39564;&#65292;&#32467;&#21512;&#20840;&#23616;&#20010;&#24615;&#21270; disentanglement&#12289;&#22806;&#35266;&#35843;&#33410;&#20010;&#24615;&#21270;&#22686;&#24378;&#21644;&#24418;&#29366;&#30456;&#20284;&#24615;&#20840;&#23616;&#22686;&#24378;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#22806;&#35266;&#22810;&#26679;&#24615;&#19982;&#22120;&#26800;&#24418;&#29366;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22836;&#26435;&#37325;&#20010;&#24615;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#35757;&#32451;&#31449;&#28857;&#29305;&#24449;&#30340;&#31934;&#30830;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#29420;&#31435;&#31449;&#28857;&#19978;&#30340;&#22120;&#26800;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03208</link><description>&lt;p&gt;
Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFedSIS&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20808;&#39564;&#65292;&#32467;&#21512;&#20840;&#23616;&#20010;&#24615;&#21270; disentanglement&#12289;&#22806;&#35266;&#35843;&#33410;&#20010;&#24615;&#21270;&#22686;&#24378;&#21644;&#24418;&#29366;&#30456;&#20284;&#24615;&#20840;&#23616;&#22686;&#24378;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#22806;&#35266;&#22810;&#26679;&#24615;&#19982;&#22120;&#26800;&#24418;&#29366;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22836;&#26435;&#37325;&#20010;&#24615;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#35757;&#32451;&#31449;&#28857;&#29305;&#24449;&#30340;&#31934;&#30830;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#29420;&#31435;&#31449;&#28857;&#19978;&#30340;&#22120;&#26800;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03208v1 Announce Type: cross  Abstract: Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;istic&#20154;&#31867;&#39550;&#39542;&#20808;&#39564;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#23545;&#25239;&#24615;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#29616;&#23454;&#21644;&#25361;&#25112;&#24615;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#31995;&#32479;&#35780;&#20272;&#20013;&#33719;&#21462;&#22823;&#35268;&#27169;&#27979;&#35797;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20132;&#36890;&#20132;&#20114;&#29615;&#22659;&#20197;&#21450;&#23454;&#26045;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65292;&#27169;&#25311;&#39550;&#39542;&#31574;&#30053;&#65292;&#36827;&#32780;&#29983;&#25104;&#26082;&#26377;&#30495;&#23454;&#24863;&#21448;&#20855;&#22791;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#21270;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2408.03200</link><description>&lt;p&gt;
Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;istic&#20154;&#31867;&#39550;&#39542;&#20808;&#39564;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#23545;&#25239;&#24615;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#29616;&#23454;&#21644;&#25361;&#25112;&#24615;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#31995;&#32479;&#35780;&#20272;&#20013;&#33719;&#21462;&#22823;&#35268;&#27169;&#27979;&#35797;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20132;&#36890;&#20132;&#20114;&#29615;&#22659;&#20197;&#21450;&#23454;&#26045;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65292;&#27169;&#25311;&#39550;&#39542;&#31574;&#30053;&#65292;&#36827;&#32780;&#29983;&#25104;&#26082;&#26377;&#30495;&#23454;&#24863;&#21448;&#20855;&#22791;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#21270;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03200v1 Announce Type: new  Abstract: Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques. By doing this, we can obtain large-scale test scenarios that are both diverse and realistic. Specifically, we build a simulation environment that mimics natural traffic interaction scenarios. Informed by this environment, we implement a two-stage procedure. The first stage incorporates conventional rule-based models, e.g., IDM~(Intelligent Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and ca
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39057;&#35889;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#23545;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#30340;&#39640;&#25928;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#39057;&#29575;&#25104;&#20998;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#36235;&#21183;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#24847;&#22270;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#23558;&#39044;&#27979;&#30340;&#24847;&#22270;&#35299;&#30721;&#25104;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24847;&#22270;&#39044;&#27979;&#27169;&#22359;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23433;&#20840;&#26377;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2408.03191</link><description>&lt;p&gt;
Integrated Intention Prediction and Decision-Making with Spectrum Attention Net and Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39057;&#35889;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#23545;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#30340;&#39640;&#25928;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#39057;&#29575;&#25104;&#20998;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#36235;&#21183;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#24847;&#22270;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#23558;&#39044;&#27979;&#30340;&#24847;&#22270;&#35299;&#30721;&#25104;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24847;&#22270;&#39044;&#27979;&#27169;&#22359;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23433;&#20840;&#26377;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03191v1 Announce Type: new  Abstract: For autonomous driving in highly dynamic environments, it is anticipated to predict the future behaviors of surrounding vehicles (SVs) and make safe and effective decisions. However, modeling the inherent coupling effect between the prediction and decision-making modules has been a long-standing challenge, especially when there is a need to maintain appropriate computational efficiency. To tackle these problems, we propose a novel integrated intention prediction and decision-making approach, which explicitly models the coupling relationship and achieves efficient computation. Specifically, a spectrum attention net is designed to predict the intentions of SVs by capturing the trends of each frequency component over time and their interrelations. Fast computation of the intention prediction module is attained as the predicted intentions are not decoded to trajectories in the executing process. Furthermore, the proximal policy optimization 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#32435;&#31859;&#26080;&#20154;&#26426;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22495;&#36801;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24863;&#30693;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03168</link><description>&lt;p&gt;
Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#32435;&#31859;&#26080;&#20154;&#26426;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22495;&#36801;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03168v1 Announce Type: new  Abstract: Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training label
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stochastic Trajectory Optimization for Demonstration Imitation (STODI)&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#24110;&#21161;&#26426;&#22120;&#20154;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#30340;&#28436;&#31034;&#36816;&#21160;&#36712;&#36857;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#25552;&#21319;&#26426;&#22120;&#20154;&#30340;&#21160;&#24577;&#24615;&#33021;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#35777;&#28436;&#31034;&#36712;&#36857;&#24418;&#29366;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#22122;&#22768;&#25552;&#39640;&#36712;&#36857;&#25506;&#32034;&#25928;&#29575;&#65292;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#26032;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.03131</link><description>&lt;p&gt;
Stochastic Trajectory Optimization for Demonstration Imitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stochastic Trajectory Optimization for Demonstration Imitation (STODI)&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#24110;&#21161;&#26426;&#22120;&#20154;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#30340;&#28436;&#31034;&#36816;&#21160;&#36712;&#36857;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#25552;&#21319;&#26426;&#22120;&#20154;&#30340;&#21160;&#24577;&#24615;&#33021;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#35777;&#28436;&#31034;&#36712;&#36857;&#24418;&#29366;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#22122;&#22768;&#25552;&#39640;&#36712;&#36857;&#25506;&#32034;&#25928;&#29575;&#65292;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#26032;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03131v1 Announce Type: new  Abstract: Humans often learn new skills by imitating the experts and gradually developing their proficiency. In this work, we introduce Stochastic Trajectory Optimization for Demonstration Imitation (STODI), a trajectory optimization framework for robots to imitate the shape of demonstration trajectories with improved dynamic performance. Consistent with the human learning process, demonstration imitation serves as an initial step, while trajectory optimization aims to enhance robot motion performance. By generating random noise and constructing proper cost functions, the STODI effectively explores and exploits generated noisy trajectories while preserving the demonstration shape characteristics. We employ three metrics to measure the similarity of trajectories in both the time and frequency domains to help with demonstration imitation. Theoretical analysis reveals relationships among these metrics, emphasizing the benefits of frequency-domain ana
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#19968;&#39033;&#38024;&#23545;&#20855;&#26377;&#19981;&#30830;&#23450;&#21160;&#24577;&#12289;&#25391;&#21160;&#24178;&#25200;&#21644;&#36127;&#36733;&#21464;&#21270;&#24178;&#25200;&#30340;&#26426;&#22120;&#20154;&#33218;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#30340;&#36866;&#24212;&#24615;&#28369;&#21160;&#27169;&#24335;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;MATLAB-SIMULINK&#36719;&#20214;&#36827;&#34892;&#27169;&#25311;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#35774;&#35745;&#30340;&#25511;&#21046;&#22120;&#33021;&#22312;&#25200;&#21160;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#23454;&#26045;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03102</link><description>&lt;p&gt;
Adaptive-Sliding Mode Trajectory Control of Robot Manipulators with Uncertainties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#19968;&#39033;&#38024;&#23545;&#20855;&#26377;&#19981;&#30830;&#23450;&#21160;&#24577;&#12289;&#25391;&#21160;&#24178;&#25200;&#21644;&#36127;&#36733;&#21464;&#21270;&#24178;&#25200;&#30340;&#26426;&#22120;&#20154;&#33218;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#30340;&#36866;&#24212;&#24615;&#28369;&#21160;&#27169;&#24335;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;MATLAB-SIMULINK&#36719;&#20214;&#36827;&#34892;&#27169;&#25311;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#35774;&#35745;&#30340;&#25511;&#21046;&#22120;&#33021;&#22312;&#25200;&#21160;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#23454;&#26045;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03102v1 Announce Type: cross  Abstract: In this paper, we propose and demonstrate an adaptive-sliding mode control for trajectory tracking control of robot manipulators subjected to uncertain dynamics, vibration disturbance, and payload variation disturbance. Throughout this work we seek a controller that is, robust to the uncertainty and disturbance, accurate, and implementable. To perform these requirements, we use a nonlinear Lyapunov-based approach for designing the controller and guaranteeing its stability. MATLAB-SIMULINK software is used to validate the approach and demonstrate the performance of the controller. Simulation results show that the derived controller is stable, robust to the disturbance and uncertainties, accurate, and implementable.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#38024;&#23545;&#26426;&#22120;&#20154;&#33218;&#22312;&#36973;&#36935;&#22806;&#37096;&#25391;&#21160;&#21644;&#36127;&#36733;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#38024;&#23545;&#24615;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31574;&#30053;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#31934;&#30830;&#25511;&#21046;&#36755;&#20837;&#23454;&#29616;&#20102;&#20851;&#33410;&#36712;&#36857;&#36319;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.03098</link><description>&lt;p&gt;
Dedicated Nonlinear Control of Robot Manipulators in the Presence of External Vibration and Uncertain Payload
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03098
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#38024;&#23545;&#26426;&#22120;&#20154;&#33218;&#22312;&#36973;&#36935;&#22806;&#37096;&#25391;&#21160;&#21644;&#36127;&#36733;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#38024;&#23545;&#24615;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31574;&#30053;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#31934;&#30830;&#25511;&#21046;&#36755;&#20837;&#23454;&#29616;&#20102;&#20851;&#33410;&#36712;&#36857;&#36319;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03098v1 Announce Type: cross  Abstract: Robot manipulators are often tasked with working in environments with vibrations and are subject to load uncertainty. Providing an accurate tracking control design with implementable torque input for these robots is a complex topic. This paper presents two approaches to solve this problem. The approaches consider joint space tracking control design in the presence of nonlinear uncertain torques caused by external vibration and payload variation. The properties of the uncertain torques are used in both approaches. The first approach is based on the boundedness property, while the second approach considers the differentiability and boundedness together. The controllers derived from each approach differ from the perspectives of accuracy, control effort, and disturbance properties. A Lyapunov-based analysis is utilized to guarantee the stability of the control design in each case. Simulation results validate the approaches and demonstrate 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25163;&#26415;&#24212;&#29992;&#30340;&#24320;&#25918;&#22495;&#21333;&#30446;&#35270;&#35273;SLAM&#26694;&#26550;BodySLAM&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21333;&#30446;&#25668;&#20687;&#22836;&#30340;&#36755;&#20837;&#65292;&#26080;&#38656;&#20219;&#20309;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25163;&#26415;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25805;&#32437;&#31934;&#20934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.03078</link><description>&lt;p&gt;
BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25163;&#26415;&#24212;&#29992;&#30340;&#24320;&#25918;&#22495;&#21333;&#30446;&#35270;&#35273;SLAM&#26694;&#26550;BodySLAM&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21333;&#30446;&#25668;&#20687;&#22836;&#30340;&#36755;&#20837;&#65292;&#26080;&#38656;&#20219;&#20309;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25163;&#26415;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25805;&#32437;&#31934;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03078v1 Announce Type: cross  Abstract: Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM) has emerged as a promising solution to address these limitations, its implementation in endoscopic procedures presents significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents a robust deep learning-based SLAM approach that combines state-of-the-art and newly developed models. It consists of three main parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method based on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the novel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous models to create a coherent surgical map. The performance of the procedure was rigorously eva
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;SYLPH&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22242;&#38431;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#20801;&#35768;&#20195;&#29702;&#23398;&#20064;&#19981;&#21516;&#30340;&#31038;&#20250;&#34892;&#20026;&#20197;&#24212;&#23545;&#20132;&#36890;&#22581;&#22622;&#31561;&#29305;&#23450;&#24773;&#20917;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36328;&#20195;&#29702;&#21442;&#25968;&#20849;&#20139;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SYLPH&#33021;&#22815;&#25552;&#39640;&#36335;&#24452;&#25628;&#32034;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#30896;&#25758;&#24182;&#38450;&#27490;&#20102;&#27515;&#38145;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2408.03063</link><description>&lt;p&gt;
Social Behavior as a Key to Learning-based Multi-Agent Pathfinding Dilemmas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;SYLPH&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22242;&#38431;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#20801;&#35768;&#20195;&#29702;&#23398;&#20064;&#19981;&#21516;&#30340;&#31038;&#20250;&#34892;&#20026;&#20197;&#24212;&#23545;&#20132;&#36890;&#22581;&#22622;&#31561;&#29305;&#23450;&#24773;&#20917;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36328;&#20195;&#29702;&#21442;&#25968;&#20849;&#20139;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SYLPH&#33021;&#22815;&#25552;&#39640;&#36335;&#24452;&#25628;&#32034;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#30896;&#25758;&#24182;&#38450;&#27490;&#20102;&#27515;&#38145;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03063v1 Announce Type: new  Abstract: The Multi-agent Path Finding (MAPF) problem involves finding collision-free paths for a team of agents in a known, static environment, with important applications in warehouse automation, logistics, or last-mile delivery. To meet the needs of these large-scale applications, current learning-based methods often deploy the same fully trained, decentralized network to all agents to improve scalability. However, such parameter sharing typically results in homogeneous behaviors among agents, which may prevent agents from breaking ties around symmetric conflict (e.g., bottlenecks) and might lead to live-/deadlocks. In this paper, we propose SYLPH, a novel learning-based MAPF framework aimed to mitigate the adverse effects of homogeneity by allowing agents to learn and dynamically select different social behaviors (akin to individual, dynamic roles), without affecting the scalability offered by parameter sharing. Specifically, SYLPH agents lear
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#25193;&#25955;&#25919;&#31574;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#26893;&#34987;&#33538;&#23494;&#30340;&#20892;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#36335;&#32447;&#36716;&#24367;&#65292;&#35299;&#20915;&#20102;GPS&#20449;&#21495;&#24369;&#12289;&#35270;&#35273;&#28151;&#28102;&#12289;&#36974;&#25377;&#21644;&#22797;&#26434;&#36710;&#36742;&#21160;&#24577;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.03059</link><description>&lt;p&gt;
Learning to Turn: Diffusion Imitation for Robust Row Turning in Under-Canopy Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#25193;&#25955;&#25919;&#31574;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#26893;&#34987;&#33538;&#23494;&#30340;&#20892;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#36335;&#32447;&#36716;&#24367;&#65292;&#35299;&#20915;&#20102;GPS&#20449;&#21495;&#24369;&#12289;&#35270;&#35273;&#28151;&#28102;&#12289;&#36974;&#25377;&#21644;&#22797;&#26434;&#36710;&#36742;&#21160;&#24577;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03059v1 Announce Type: new  Abstract: Under-canopy agricultural robots require robust navigation capabilities to enable full autonomy but struggle with tight row turning between crop rows due to degraded GPS reception, visual aliasing, occlusion, and complex vehicle dynamics. We propose an imitation learning approach using diffusion policies to learn row turning behaviors from demonstrations provided by human operators or privileged controllers. Simulation experiments in a corn field environment show potential in learning this task with only visual observations and velocity states. However, challenges remain in maintaining control within rows and handling varied initial conditions, highlighting areas for future improvement.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CSI&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#22810;&#31181;&#19981;&#21516;&#39118;&#26684;&#30340;&#36816;&#21160;&#25216;&#33021;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#25511;&#21046;&#22120;&#20013;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#12290;&#36825;&#20026; legged robots &#30340;&#22810;&#25216;&#33021;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#29615;&#22659;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.03018</link><description>&lt;p&gt;
Integrating Controllable Motion Skills from Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CSI&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#22810;&#31181;&#19981;&#21516;&#39118;&#26684;&#30340;&#36816;&#21160;&#25216;&#33021;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#25511;&#21046;&#22120;&#20013;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#12290;&#36825;&#20026; legged robots &#30340;&#22810;&#25216;&#33021;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03018v1 Announce Type: new  Abstract: The expanding applications of legged robots require their mastery of versatile motion skills. Correspondingly, researchers must address the challenge of integrating multiple diverse motion skills into controllers. While existing reinforcement learning (RL)-based approaches have achieved notable success in multi-skill integration for legged robots, these methods often require intricate reward engineering or are restricted to integrating a predefined set of motion skills constrained by specific task objectives, resulting in limited flexibility. In this work, we introduce a flexible multi-skill integration framework named Controllable Skills Integration (CSI). CSI enables the integration of a diverse set of motion skills with varying styles into a single policy without the need for complex reward tuning. Furthermore, in a hierarchical control manner, the trained low-level policy can be coupled with a high-level Natural Language Inference (N
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#26059;&#36716;&#30340;&#27704;&#20037;&#30913;&#38081;&#25152;&#35013;&#22791;&#30340;&#30913;&#24615;&#39537;&#21160;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23545;&#30913;&#24615;&#36719;&#36830;&#32493;&#20307;&#26426;&#22120;&#20154;&#65288;MSCRs&#65289;&#30340;&#38381;&#29615;&#20559;&#36716;&#25511;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#38750;&#22343;&#21248;&#30913;&#22330;&#20013;MSCR&#30340;&#19981;&#21516;&#24577;&#21160;&#24577;&#27169;&#22411;&#65292;&#25991;&#31456;&#25512;&#23548;&#20986;&#20102;&#26426;&#22120;&#20154;&#20960;&#20309;&#20301;&#32622;&#19982;&#38597;&#21487;&#27604;&#30697;&#38453;&#23384;&#22312;&#21807;&#19968;&#24615;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#38597;&#21487;&#27604;&#30697;&#38453;&#20934;&#30830;&#30340;&#25511;&#21046;&#26041;&#21521;&#22312;&#27169;&#25311;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#20934;&#38745;&#24577;&#25511;&#21046;&#65288;QSC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#20010;&#32447;&#24615;&#25193;&#23637;&#29366;&#24577;&#35266;&#27979;&#22120;&#20197;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#35752;&#35770;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2408.03017</link><description>&lt;p&gt;
Closed-Loop Magnetic Control of Medical Soft Continuum Robots for Deflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#26059;&#36716;&#30340;&#27704;&#20037;&#30913;&#38081;&#25152;&#35013;&#22791;&#30340;&#30913;&#24615;&#39537;&#21160;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23545;&#30913;&#24615;&#36719;&#36830;&#32493;&#20307;&#26426;&#22120;&#20154;&#65288;MSCRs&#65289;&#30340;&#38381;&#29615;&#20559;&#36716;&#25511;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#38750;&#22343;&#21248;&#30913;&#22330;&#20013;MSCR&#30340;&#19981;&#21516;&#24577;&#21160;&#24577;&#27169;&#22411;&#65292;&#25991;&#31456;&#25512;&#23548;&#20986;&#20102;&#26426;&#22120;&#20154;&#20960;&#20309;&#20301;&#32622;&#19982;&#38597;&#21487;&#27604;&#30697;&#38453;&#23384;&#22312;&#21807;&#19968;&#24615;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#38597;&#21487;&#27604;&#30697;&#38453;&#20934;&#30830;&#30340;&#25511;&#21046;&#26041;&#21521;&#22312;&#27169;&#25311;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#20934;&#38745;&#24577;&#25511;&#21046;&#65288;QSC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#20010;&#32447;&#24615;&#25193;&#23637;&#29366;&#24577;&#35266;&#27979;&#22120;&#20197;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#35752;&#35770;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03017v1 Announce Type: new  Abstract: Magnetic soft continuum robots (MSCRs) have emerged as powerful devices in endovascular interventions owing to their hyperelastic fibre matrix and enhanced magnetic manipulability. Effective closed-loop control of tethered magnetic devices contributes to the achievement of autonomous vascular robotic surgery. In this article, we employ a magnetic actuation system equipped with a single rotatable permanent magnet to achieve closed-loop deflection control of the MSCR. To this end, we establish a differential kinematic model of MSCRs exposed to non-uniform magnetic fields. The relationship between the existence and uniqueness of Jacobian and the geometric position between robots is deduced. The accurate control direction induced by Jacobian is demonstrated to be crucial in simulations. Then, the corresponding quasi-static control (QSC) framework integrates a linear extended state observer to estimate model uncertainties. Finally, the effect
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#26680;&#24863;&#30693;&#26041;&#27861;&#35757;&#32451;&#30340;&#26032;&#22411;&#20803;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#26497;&#31471;&#22495;&#20559;&#31227;&#19979;&#30340;&#33258;&#20027;&#26426;&#26800;&#33218;&#23569;&#26679;&#26412;&#37319;&#30719;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26368;&#22823;&#37096;&#32626;&#38388;&#38553;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19987;&#38376;&#35757;&#32451;&#27169;&#22411;&#20197;&#20811;&#26381;&#36825;&#20123;&#38388;&#38553;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#24040;&#22823;&#30340;&#22495;&#20559;&#31227;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#24207;&#36143;&#20915;&#31574;&#26694;&#26550;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#37319;&#30719;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36824;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#26679;&#26412;&#37319;&#25496;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.02949</link><description>&lt;p&gt;
Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#26680;&#24863;&#30693;&#26041;&#27861;&#35757;&#32451;&#30340;&#26032;&#22411;&#20803;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#26497;&#31471;&#22495;&#20559;&#31227;&#19979;&#30340;&#33258;&#20027;&#26426;&#26800;&#33218;&#23569;&#26679;&#26412;&#37319;&#30719;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26368;&#22823;&#37096;&#32626;&#38388;&#38553;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19987;&#38376;&#35757;&#32451;&#27169;&#22411;&#20197;&#20811;&#26381;&#36825;&#20123;&#38388;&#38553;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#24040;&#22823;&#30340;&#22495;&#20559;&#31227;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#24207;&#36143;&#20915;&#31574;&#26694;&#26550;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#37319;&#30719;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36824;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#26679;&#26412;&#37319;&#25496;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02949v1 Announce Type: new  Abstract: Autonomous lander missions on extraterrestrial bodies need to sample granular materials while coping with domain shifts, even when sampling strategies are extensively tuned on Earth. To tackle this challenge, this paper studies the few-shot scooping problem and proposes a vision-based adaptive scooping strategy that uses the deep kernel Gaussian process method trained with a novel meta-training strategy to learn online from very limited experience on out-of-distribution target terrains. Our Deep Kernel Calibration with Maximal Deployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt to large domain shifts by creating simulated maximal deployment gaps from an offline training dataset and training models to overcome these deployment gaps during training. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to perform high-quality scooping actions on out-of-dist
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;KOI&#30340;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20851;&#38190;&#29366;&#24577;&#25351;&#23548;&#26469;&#31934;&#30830;&#20272;&#35745;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#22312;&#32447;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2408.02912</link><description>&lt;p&gt;
KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02912
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;KOI&#30340;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20851;&#38190;&#29366;&#24577;&#25351;&#23548;&#26469;&#31934;&#30830;&#20272;&#35745;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#22312;&#32447;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02912v1 Announce Type: new  Abstract: Online Imitation Learning methods struggle with the gap between extensive online exploration space and limited expert trajectories, which hinder efficient exploration due to inaccurate task-aware reward estimation. Inspired by the findings from cognitive neuroscience that task decomposition could facilitate cognitive processing for efficient learning, we hypothesize that an agent could estimate precise task-aware imitation rewards for efficient online exploration by decomposing the target task into the objectives of "what to do" and the mechanisms of "how to do". In this work, we introduce the hybrid Key-state guided Online Imitation (KOI) learning approach, which leverages the integration of semantic and motion key states as guidance for task-aware reward estimation. Initially, we utilize the visual-language models to segment the expert trajectory into semantic key states, indicating the objectives of "what to do". Within the intervals 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;Larp&#65292;&#23427;&#36890;&#36807;&#23558;&#28508;&#22312;&#22330;&#20998;&#21106;&#25104;&#20855;&#26377;&#19981;&#21516;&#38556;&#30861;&#29289;&#36317;&#31163;&#38480;&#21046;&#21306;&#30340;&#39640;&#32423;&#23618;&#27425;&#32454;&#32990;&#26469;&#25552;&#39640;&#36335;&#24452;&#30340;&#23433;&#20840;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;Larp&#22312;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#20445;&#35777;&#36828;&#31163;&#38556;&#30861;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#26174;&#30528;&#25552;&#21319;&#20102;&#25968;&#21313;&#20159;&#39046;&#22495;&#20013;&#26080;&#20154;&#26426;&#36816;&#36755;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02786</link><description>&lt;p&gt;
Multi-Scale Cell Decomposition for Path Planning using Restrictive Routing Potential Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;Larp&#65292;&#23427;&#36890;&#36807;&#23558;&#28508;&#22312;&#22330;&#20998;&#21106;&#25104;&#20855;&#26377;&#19981;&#21516;&#38556;&#30861;&#29289;&#36317;&#31163;&#38480;&#21046;&#21306;&#30340;&#39640;&#32423;&#23618;&#27425;&#32454;&#32990;&#26469;&#25552;&#39640;&#36335;&#24452;&#30340;&#23433;&#20840;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;Larp&#22312;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#20445;&#35777;&#36828;&#31163;&#38556;&#30861;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#26174;&#30528;&#25552;&#21319;&#20102;&#25968;&#21313;&#20159;&#39046;&#22495;&#20013;&#26080;&#20154;&#26426;&#36816;&#36755;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02786v1 Announce Type: new  Abstract: In burgeoning domains, like urban goods distribution, the advent of aerial cargo transportation necessitates the development of routing solutions that prioritize safety. This paper introduces Larp, a novel path planning framework that leverages the concept of restrictive potential fields to forge routes demonstrably safer than those derived from existing methods. The algorithm achieves it by segmenting a potential field into a hierarchy of cells, each with a designated restriction zone determined by obstacle proximity. While the primary impetus behind Larp is to enhance the safety of aerial pathways for cargo-carrying Unmanned Aerial Vehicles (UAVs), its utility extends to a wide array of path planning scenarios. Comparative analyses with both established and contemporary potential field-based methods reveal Larp's proficiency in maintaining a safe distance from restrictions and its adeptness in circumventing local minima.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#30446;&#31435;&#20307;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#20165;&#26377;&#20960;&#27627;&#31859;&#30452;&#24452;&#30340;&#24102;&#29366;&#24494;&#23610;&#24230;&#36830;&#32493;&#26426;&#22120;&#20154;&#65288;NTCR&#65289;&#30340;&#19977;&#32500;&#24418;&#24577;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#21488;&#30456;&#23545;&#25918;&#32622;&#30340;&#38745;&#24577;&#31435;&#20307;&#25668;&#20687;&#22836;&#65292;&#21363;&#20351;&#21407;&#22987;&#28857;&#20113;&#37319;&#38598;&#36136;&#37327;&#19981;&#39640;&#65292;&#20063;&#33021;&#22815;&#36890;&#36807;&#39044;&#35774;&#20960;&#20309;&#21442;&#32771;&#19979;&#30340;KD&#26641;&#31639;&#27861;&#23545;&#37319;&#38598;&#30340;&#28857;&#20113;&#36827;&#34892;&#27491;&#30830;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;NTCR&#24418;&#24577;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01615</link><description>&lt;p&gt;
Three-dimensional Morphological Reconstruction of Millimeter-Scale Soft Continuum Robots based on Dual Stereo Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#30446;&#31435;&#20307;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#20165;&#26377;&#20960;&#27627;&#31859;&#30452;&#24452;&#30340;&#24102;&#29366;&#24494;&#23610;&#24230;&#36830;&#32493;&#26426;&#22120;&#20154;&#65288;NTCR&#65289;&#30340;&#19977;&#32500;&#24418;&#24577;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#21488;&#30456;&#23545;&#25918;&#32622;&#30340;&#38745;&#24577;&#31435;&#20307;&#25668;&#20687;&#22836;&#65292;&#21363;&#20351;&#21407;&#22987;&#28857;&#20113;&#37319;&#38598;&#36136;&#37327;&#19981;&#39640;&#65292;&#20063;&#33021;&#22815;&#36890;&#36807;&#39044;&#35774;&#20960;&#20309;&#21442;&#32771;&#19979;&#30340;KD&#26641;&#31639;&#27861;&#23545;&#37319;&#38598;&#30340;&#28857;&#20113;&#36827;&#34892;&#27491;&#30830;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;NTCR&#24418;&#24577;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01615v1 Announce Type: new  Abstract: Continuum robots can be miniaturized to just a few millimeters in diameter. Among these, notched tubular continuum robots (NTCR) show great potential in many delicate applications. Existing works in robotic modeling focus on kinematics and dynamics but still face challenges in reproducing the robot's morphology -- a significant factor that can expand the research landscape of continuum robots, especially for those with asymmetric continuum structures. This paper proposes a dual stereo vision-based method for the three-dimensional morphological reconstruction of millimeter-scale NTCRs. The method employs two oppositely located stationary binocular cameras to capture the point cloud of the NTCR, then utilizes predefined geometry as a reference for the KD tree method to relocate the capture point clouds, resulting in a morphologically correct NTCR despite the low-quality raw point cloud collection. The method has been proved feasible for an
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#25628;&#32034;&#21644;&#21487;&#35265;&#24615;&#22270;&#26500;&#24314;&#36335;&#24452;&#20197;&#21450;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20302;&#32423;&#36816;&#21160;&#21629;&#20196;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#26080;&#20154;&#26426;&#33258;&#20027;&#39134;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.00275</link><description>&lt;p&gt;
A Reinforcement Learning Based Motion Planner for Quadrotor Autonomous Flight in Dense Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#25628;&#32034;&#21644;&#21487;&#35265;&#24615;&#22270;&#26500;&#24314;&#36335;&#24452;&#20197;&#21450;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20302;&#32423;&#36816;&#21160;&#21629;&#20196;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#26080;&#20154;&#26426;&#33258;&#20027;&#39134;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00275v2 Announce Type: replace  Abstract: Quadrotor motion planning is critical for autonomous flight in complex environments, such as rescue operations. Traditional methods often employ trajectory generation optimization and passive time allocation strategies, which can limit the exploitation of the quadrotor's dynamic capabilities and introduce delays and inaccuracies. To address these challenges, we propose a novel motion planning framework that integrates visibility path searching and reinforcement learning (RL) motion generation. Our method constructs collision-free paths using heuristic search and visibility graphs, which are then refined by an RL policy to generate low-level motion commands. We validate our approach in simulated indoor environments, demonstrating better performance than traditional methods in terms of time span.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimEndoGS&#30340;&#26032;&#22411;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#30340;&#25163;&#26415;&#22330;&#26223;&#27169;&#25311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#29289;&#29702;&#23884;&#20837;&#24335;3D&#39640;&#26031;&#20998;&#24067;&#26469;&#27169;&#25311;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#19977;&#32500;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#20013;&#33258;&#21160;&#23398;&#20064;&#25163;&#26415;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#24182;&#22312;&#20445;&#35777;&#22330;&#26223;&#20960;&#20309;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#28145;&#24230;&#30417;&#30563;&#21644;&#21508;&#21521;&#24322;&#24615;&#32422;&#26463;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#30495;&#23454;&#21644;&#21487;&#25193;&#22823;&#30340;&#27169;&#25311;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2405.00956</link><description>&lt;p&gt;
SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00956
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimEndoGS&#30340;&#26032;&#22411;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#30340;&#25163;&#26415;&#22330;&#26223;&#27169;&#25311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#29289;&#29702;&#23884;&#20837;&#24335;3D&#39640;&#26031;&#20998;&#24067;&#26469;&#27169;&#25311;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#19977;&#32500;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#20013;&#33258;&#21160;&#23398;&#20064;&#25163;&#26415;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#24182;&#22312;&#20445;&#35777;&#22330;&#26223;&#20960;&#20309;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#28145;&#24230;&#30417;&#30563;&#21644;&#21508;&#21521;&#24322;&#24615;&#32422;&#26463;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#30495;&#23454;&#21644;&#21487;&#25193;&#22823;&#30340;&#27169;&#25311;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00956v3 Announce Type: replace  Abstract: Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy r
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MambaMOS&#30340;&#22522;&#20110;LiDAR&#30340;3D&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Time Clue Bootstrapping Embedding&#65288;TCBE&#65289;&#30340;&#26032;&#23884;&#20837;&#27169;&#22359;&#26469;&#22686;&#24378;&#28857;&#20113;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#32806;&#21512;&#65292;&#24182;&#32531;&#35299;&#20102;&#24573;&#30053;&#26102;&#31354;&#32447;&#32034;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;Motion-aware State Space Model&#65288;MSSM&#65289;&#65292;&#20197;&#35753;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#21516;&#19968;&#23545;&#35937;&#30340;&#31354;&#38388;&#29366;&#24577;&#65292;&#24378;&#35843;&#20102;&#31227;&#21160;&#23545;&#35937;&#30340;&#36816;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#22312;LiDAR&#28857;&#20113;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2404.12794</link><description>&lt;p&gt;
MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MambaMOS&#30340;&#22522;&#20110;LiDAR&#30340;3D&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Time Clue Bootstrapping Embedding&#65288;TCBE&#65289;&#30340;&#26032;&#23884;&#20837;&#27169;&#22359;&#26469;&#22686;&#24378;&#28857;&#20113;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#32806;&#21512;&#65292;&#24182;&#32531;&#35299;&#20102;&#24573;&#30053;&#26102;&#31354;&#32447;&#32034;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;Motion-aware State Space Model&#65288;MSSM&#65289;&#65292;&#20197;&#35753;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#21516;&#19968;&#23545;&#35937;&#30340;&#31354;&#38388;&#29366;&#24577;&#65292;&#24378;&#35843;&#20102;&#31227;&#21160;&#23545;&#35937;&#30340;&#36816;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#22312;LiDAR&#28857;&#20113;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#31227;&#21160;&#23545;&#35937;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12794v2 Announce Type: replace-cross  Abstract: LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment moving objects in point clouds of the current scan using motion information from previous scans. Despite the promising results achieved by previous MOS methods, several key issues, such as the weak coupling of temporal and spatial information, still need further study. In this paper, we propose a novel LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model, termed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue Bootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial information in point clouds and alleviate the issue of overlooked temporal clues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to endow the model with the capacity to understand the temporal correlations of the same object across different time steps. Specifically, MSSM emphasizes the motion states of the sa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26631;&#31614;&#21435;&#22122;&#31574;&#30053;&#26469;&#35299;&#20915;skeleton-based&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#31232;&#30095;&#39592;&#26550;&#25968;&#25454;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09975</link><description>&lt;p&gt;
Skeleton-Based Human Action Recognition with Noisy Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26631;&#31614;&#21435;&#22122;&#31574;&#30053;&#26469;&#35299;&#20915;skeleton-based&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#31232;&#30095;&#39592;&#26550;&#25968;&#25454;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09975v2 Announce Type: replace-cross  Abstract: Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model's training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodolo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EchoTrack&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#38899;&#39057;&#25351;&#20195;&#22810;&#23545;&#35937;&#36319;&#36394;&#65288;AR-MOT&#65289;&#12290;EchoTrack&#36890;&#36807;&#21452;&#21521;&#39057;&#29575;&#22495;&#36328;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65288;Bi-FCFM&#65289;&#23454;&#29616;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#29305;&#24449;&#30340;&#21452;&#27969;&#31471;&#21040;&#31471;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21644;&#25991;&#26412;&#20381;&#36182;&#30340;&#22810;&#23545;&#35937;&#36319;&#36394;&#22312;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18302</link><description>&lt;p&gt;
EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EchoTrack&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#38899;&#39057;&#25351;&#20195;&#22810;&#23545;&#35937;&#36319;&#36394;&#65288;AR-MOT&#65289;&#12290;EchoTrack&#36890;&#36807;&#21452;&#21521;&#39057;&#29575;&#22495;&#36328;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65288;Bi-FCFM&#65289;&#23454;&#29616;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#29305;&#24449;&#30340;&#21452;&#27969;&#31471;&#21040;&#31471;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21644;&#25991;&#26412;&#20381;&#36182;&#30340;&#22810;&#23545;&#35937;&#36319;&#36394;&#22312;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18302v2 Announce Type: replace-cross  Abstract: This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#19977;&#32500;&#29615;&#22659;&#37325;&#24314;&#21644;&#24494;&#37325;&#21147;&#26465;&#20214;&#19979;&#21464;&#21270;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30001;&#39134;&#34892;&#30340;&#26426;&#22120;&#20154;&#26469;&#32500;&#25252;&#22826;&#31354;&#22522;&#22320;&#12290;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#22270;&#20687;&#21644;&#28145;&#24230;&#20449;&#24687;&#37325;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#21478;&#19968;&#20010;&#26426;&#22120;&#20154;&#23450;&#26399;&#26816;&#26597;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#36866;&#29992;&#20110;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2311.02558</link><description>&lt;p&gt;
Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#19977;&#32500;&#29615;&#22659;&#37325;&#24314;&#21644;&#24494;&#37325;&#21147;&#26465;&#20214;&#19979;&#21464;&#21270;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30001;&#39134;&#34892;&#30340;&#26426;&#22120;&#20154;&#26469;&#32500;&#25252;&#22826;&#31354;&#22522;&#22320;&#12290;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#22270;&#20687;&#21644;&#28145;&#24230;&#20449;&#24687;&#37325;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#21478;&#19968;&#20010;&#26426;&#22120;&#20154;&#23450;&#26399;&#26816;&#26597;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#36866;&#29992;&#20110;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02558v3 Announce Type: replace  Abstract: Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction sys
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;Agent&#21327;&#20316;&#29615;&#22659;&#20013;&#35299;&#20915;&#30001;&#20110;&#35270;&#35282;&#21464;&#21270;&#36896;&#25104;&#30340;&#35270;&#35273;&#23450;&#20301;&#38382;&#39064;&#12290;&#30740;&#31350;&#24037;&#20316;&#36890;&#36807;&#23545;&#27604;&#29616;&#26377;&#26041;&#27861;&#21644;&#25552;&#20986;&#30340;&#22810;&#20010;&#22522;&#32447;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#26088;&#22312;&#36873;&#25321;&#22312;&#29305;&#23450;&#20301;&#32622;&#36827;&#34892;&#26368;&#20339;&#30340;&#35270;&#35282;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2310.02650</link><description>&lt;p&gt;
Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;Agent&#21327;&#20316;&#29615;&#22659;&#20013;&#35299;&#20915;&#30001;&#20110;&#35270;&#35282;&#21464;&#21270;&#36896;&#25104;&#30340;&#35270;&#35273;&#23450;&#20301;&#38382;&#39064;&#12290;&#30740;&#31350;&#24037;&#20316;&#36890;&#36807;&#23545;&#27604;&#29616;&#26377;&#26041;&#27861;&#21644;&#25552;&#20986;&#30340;&#22810;&#20010;&#22522;&#32447;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#26088;&#22312;&#36873;&#25321;&#22312;&#29305;&#23450;&#20301;&#32622;&#36827;&#34892;&#26368;&#20339;&#30340;&#35270;&#35282;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02650v3 Announce Type: replace  Abstract: Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#19979;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#20272;&#35745;&#65288;AMGE&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;MuscleMap&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#36816;&#21160;&#35270;&#39057;&#21644;&#22810;&#31181;&#29289;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#30340;&#26631;&#27880;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#19968;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#24037;&#20316;&#30340;&#35270;&#39057;&#20998;&#26512;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#20307;&#32946;&#21644;&#24247;&#22797;&#21307;&#30103;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2303.00952</link><description>&lt;p&gt;
Towards Activated Muscle Group Estimation in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#19979;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#20272;&#35745;&#65288;AMGE&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;MuscleMap&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#36816;&#21160;&#35270;&#39057;&#21644;&#22810;&#31181;&#29289;&#36816;&#21160;&#20013;&#28608;&#27963;&#32908;&#32905;&#32452;&#30340;&#26631;&#27880;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#19968;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#24037;&#20316;&#30340;&#35270;&#39057;&#20998;&#26512;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#20307;&#32946;&#21644;&#24247;&#22797;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00952v5 Announce Type: replace-cross  Abstract: In this paper, we tackle the new task of video-based Activated Muscle Group Estimation (AMGE) aiming at identifying active muscle regions during physical activity in the wild. To this intent, we provide the MuscleMap dataset featuring &gt;15K video clips with 135 different activities and 20 labeled muscle groups. This dataset opens the vistas to multiple video-based applications in sports and rehabilitation medicine under flexible environment constraints. The proposed MuscleMap dataset is constructed with YouTube videos, specifically targeting High-Intensity Interval Training (HIIT) physical exercise in the wild. To make the AMGE model applicable in real-life situations, it is crucial to ensure that the model can generalize well to numerous types of physical activities not present during training and involving new combinations of activated muscles. To achieve this, our benchmark also covers an evaluation setting where the model is
&lt;/p&gt;</description></item></channel></rss>