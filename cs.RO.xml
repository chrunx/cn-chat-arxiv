<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>cs.RO</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25191;&#34892;&#29615;&#22659;&#30417;&#27979;&#20219;&#21153;&#30340;&#20154;&#31867;-&#32676;&#20307;&#31995;&#32479;&#20013;&#38598;&#20013;&#24335;&#19982;&#20998;&#25955;&#24335;&#25511;&#21046;&#31574;&#30053;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#28151;&#21512;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#20943;&#36731;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#25968;&#25454;&#20256;&#36755;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2408.02605</link><description>&lt;p&gt;
Trade-offs of Dynamic Control Structure in Human-swarm Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25191;&#34892;&#29615;&#22659;&#30417;&#27979;&#20219;&#21153;&#30340;&#20154;&#31867;-&#32676;&#20307;&#31995;&#32479;&#20013;&#38598;&#20013;&#24335;&#19982;&#20998;&#25955;&#24335;&#25511;&#21046;&#31574;&#30053;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#28151;&#21512;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#20943;&#36731;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#25968;&#25454;&#20256;&#36755;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02605v1 Announce Type: new  Abstract: Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment. The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics. Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems. While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload. We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems. We find that a flexible hybrid system can outperform a centralised system (in our environmental mon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#24615;&#32908;&#32905;&#32593;&#32476;&#30340;&#25163;&#21183;&#24863;&#30693;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#39057;&#29305;&#24449;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02547</link><description>&lt;p&gt;
The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#24615;&#32908;&#32905;&#32593;&#32476;&#30340;&#25163;&#21183;&#24863;&#30693;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#39057;&#29305;&#24449;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02547v1 Announce Type: new  Abstract: Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots. Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems. The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions. This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation. We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#31181;&#25351;&#23574;&#25235;&#21462;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#21487;&#20197;&#20934;&#30830;&#26144;&#23556;&#21040;&#22810;&#25351;&#23574;&#25235;&#21462;&#31354;&#38388;&#65292;&#20165;&#38656;&#25968;&#30334;&#33267;&#25968;&#21315;&#20010;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25235;&#21462;&#20998;&#25968;&#12290;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#33021;&#22312;&#20165;500&#27425;&#30495;&#23454;&#22330;&#26223;&#25235;&#21462;&#35757;&#32451;&#21518;&#36798;&#21040;78.64%&#30340;&#25104;&#21151;&#29575;&#65292;&#22312;4500&#27425;&#35757;&#32451;&#21518;&#36798;&#21040;87%&#30340;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#22810;&#25351;&#23574;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#20114;&#25235;&#21462;&#22330;&#26223;&#20013;&#65292;&#20063;&#33021;&#36798;&#21040;84.51%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02455</link><description>&lt;p&gt;
A Surprisingly Efficient Representation for Multi-Finger Grasping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#31181;&#25351;&#23574;&#25235;&#21462;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#21487;&#20197;&#20934;&#30830;&#26144;&#23556;&#21040;&#22810;&#25351;&#23574;&#25235;&#21462;&#31354;&#38388;&#65292;&#20165;&#38656;&#25968;&#30334;&#33267;&#25968;&#21315;&#20010;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25235;&#21462;&#20998;&#25968;&#12290;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#33021;&#22312;&#20165;500&#27425;&#30495;&#23454;&#22330;&#26223;&#25235;&#21462;&#35757;&#32451;&#21518;&#36798;&#21040;78.64%&#30340;&#25104;&#21151;&#29575;&#65292;&#22312;4500&#27425;&#35757;&#32451;&#21518;&#36798;&#21040;87%&#30340;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#22810;&#25351;&#23574;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#20114;&#25235;&#21462;&#22330;&#26223;&#20013;&#65292;&#20063;&#33021;&#36798;&#21040;84.51%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02455v1 Announce Type: new  Abstract: The problem of grasping objects using a multi-finger hand has received significant attention in recent years. However, it remains challenging to handle a large number of unfamiliar objects in real and cluttered environments. In this work, we propose a representation that can be effectively mapped to the multi-finger grasp space. Based on this representation, we develop a simple decision model that generates accurate grasp quality scores for different multi-finger grasp poses using only hundreds to thousands of training samples. We demonstrate that our representation performs well on a real robot and achieves a success rate of 78.64% after training with only 500 real-world grasp attempts and 87% with 4500 grasp attempts. Additionally, we achieve a success rate of 84.51% in a dynamic human-robot handover scenario using a multi-finger hand.
&lt;/p&gt;</description></item><item><title>TGS&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;CVAE&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#26080;&#22270;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#36712;&#36857;&#29983;&#25104;&#21644;&#36873;&#25321;&#65292;&#24110;&#21161;&#36718;&#24335;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#36991;&#24320;&#38556;&#30861;&#65292;&#36866;&#24212;&#22797;&#26434;&#22320;&#38754;&#12290;</title><link>https://arxiv.org/abs/2408.02454</link><description>&lt;p&gt;
TGS: Trajectory Generation and Selection using Vision Language Models in Mapless Outdoor Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02454
&lt;/p&gt;
&lt;p&gt;
TGS&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;CVAE&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#26080;&#22270;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#36712;&#36857;&#29983;&#25104;&#21644;&#36873;&#25321;&#65292;&#24110;&#21161;&#36718;&#24335;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#36991;&#24320;&#38556;&#30861;&#65292;&#36866;&#24212;&#22797;&#26434;&#22320;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02454v1 Announce Type: new  Abstract: We present a multi-modal trajectory generation and selection algorithm for real-world mapless outdoor navigation in challenging scenarios with unstructured off-road features like buildings, grass, and curbs. Our goal is to compute suitable trajectories that (1) satisfy the environment-specific traversability constraints and (2) match human-like paths while navigating in crosswalks, sidewalks, etc. Our formulation uses a Conditional Variational Autoencoder (CVAE) generative model enhanced with traversability constraints to generate multiple candidate trajectories for global navigation. We use VLMs and a visual prompting approach with their zero-shot ability of semantic understanding and logical reasoning to choose the best trajectory given the contextual information about the task. We evaluate our methods in various outdoor scenes with wheeled robots and compare the performance with other global navigation algorithms. In practice, we obse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; RIs-Calib&#65292;&#19968;&#20010;&#22810;3D&#38647;&#36798;&#21644;IMU&#30340;&#31354;&#38388;-&#26102;&#38388;&#26657;&#20934;&#22120;&#65292;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#20272;&#35745;&#65292;&#26080;&#38656;&#39069;&#22806;&#22522;&#30784;&#35774;&#26045;&#23601;&#33021;&#23454;&#29616;&#31934;&#30830;&#26657;&#20934;&#12290;&#26041;&#27861;&#25552;&#39640;&#20102;&#23454;&#26102;&#31995;&#32479;&#24615;&#33021;&#65292;&#36890;&#36807;&#20248;&#21270;&#20943;&#23569;&#35823;&#24046;&#20256;&#25773;&#65292;&#26377;&#25928;&#31649;&#29702;&#20102;&#31354;&#38388;&#28418;&#31227;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25552;&#20379;&#20248;&#36234;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02444</link><description>&lt;p&gt;
RIs-Calib: An Open-Source Spatiotemporal Calibrator for Multiple 3D Radars and IMUs Based on Continuous-Time Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; RIs-Calib&#65292;&#19968;&#20010;&#22810;3D&#38647;&#36798;&#21644;IMU&#30340;&#31354;&#38388;-&#26102;&#38388;&#26657;&#20934;&#22120;&#65292;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#20272;&#35745;&#65292;&#26080;&#38656;&#39069;&#22806;&#22522;&#30784;&#35774;&#26045;&#23601;&#33021;&#23454;&#29616;&#31934;&#30830;&#26657;&#20934;&#12290;&#26041;&#27861;&#25552;&#39640;&#20102;&#23454;&#26102;&#31995;&#32479;&#24615;&#33021;&#65292;&#36890;&#36807;&#20248;&#21270;&#20943;&#23569;&#35823;&#24046;&#20256;&#25773;&#65292;&#26377;&#25928;&#31649;&#29702;&#20102;&#31354;&#38388;&#28418;&#31227;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25552;&#20379;&#20248;&#36234;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02444v1 Announce Type: new  Abstract: Aided inertial navigation system (INS), typically consisting of an inertial measurement unit (IMU) and an exteroceptive sensor, has been widely accepted as a feasible solution for navigation. Compared with vision-aided and LiDAR-aided INS, radar-aided INS could achieve better performance in adverse weather conditions since the radar utilizes low-frequency measuring signals with less attenuation effect in atmospheric gases and rain. For such a radar-aided INS, accurate spatiotemporal transformation is a fundamental prerequisite to achieving optimal information fusion. In this work, we present RIs-Calib: a spatiotemporal calibrator for multiple 3D radars and IMUs based on continuous-time estimation, which enables accurate spatiotemporal calibration and does not require any additional artificial infrastructure or prior knowledge. Our approach starts with a rigorous and robust procedure for state initialization, followed by batch optimizatio
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;CMR-Agent&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;2D-3D&#28151;&#21512;&#29366;&#24577;&#34920;&#31034;&#26469;&#20805;&#20998;&#21033;&#29992;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#25552;&#39640;&#27880;&#20876;&#31934;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02394</link><description>&lt;p&gt;
CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point Cloud Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02394
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;CMR-Agent&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;2D-3D&#28151;&#21512;&#29366;&#24577;&#34920;&#31034;&#26469;&#20805;&#20998;&#21033;&#29992;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#25552;&#39640;&#27880;&#20876;&#31934;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02394v1 Announce Type: cross  Abstract: Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMR-Agent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#26032;&#30340;&#24231;&#26885;&#33050;&#25511;&#22120;&#65292;&#19987;&#20026;&#25511;&#21046;&#36828;&#31243;&#21576;&#29616;&#26426;&#22120;&#20154;&#21450;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#31227;&#21160;&#35774;&#35745;&#65292;&#37319;&#29992;3 DOF&#65292;&#21487;&#23454;&#29616;&#33050;&#37096;&#30340;&#21069;&#21518;&#24038;&#21491;&#31227;&#21160;&#65292;&#24182;&#33021;&#22260;&#32469;&#22402;&#30452;&#36724;&#26059;&#36716;&#12290;&#25511;&#21046;&#22120;&#20855;&#26377;&#33258;&#26657;&#20934;&#21151;&#33021;&#65292;&#20351;&#29992;HTC Vive&#36319;&#36394;&#22120;&#26469;&#36716;&#25442;&#26041;&#21521;&#21629;&#20196;&#12290;&#35813;&#25511;&#22120;&#22312;&#29305;&#23450;&#27604;&#36187;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#20844;&#24320;&#20102;&#20854;3D&#25171;&#21360;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.02319</link><description>&lt;p&gt;
Self-centering 3-DOF feet controller for hands-free locomotion control in telepresence and virtual reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#26032;&#30340;&#24231;&#26885;&#33050;&#25511;&#22120;&#65292;&#19987;&#20026;&#25511;&#21046;&#36828;&#31243;&#21576;&#29616;&#26426;&#22120;&#20154;&#21450;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#31227;&#21160;&#35774;&#35745;&#65292;&#37319;&#29992;3 DOF&#65292;&#21487;&#23454;&#29616;&#33050;&#37096;&#30340;&#21069;&#21518;&#24038;&#21491;&#31227;&#21160;&#65292;&#24182;&#33021;&#22260;&#32469;&#22402;&#30452;&#36724;&#26059;&#36716;&#12290;&#25511;&#21046;&#22120;&#20855;&#26377;&#33258;&#26657;&#20934;&#21151;&#33021;&#65292;&#20351;&#29992;HTC Vive&#36319;&#36394;&#22120;&#26469;&#36716;&#25442;&#26041;&#21521;&#21629;&#20196;&#12290;&#35813;&#25511;&#22120;&#22312;&#29305;&#23450;&#27604;&#36187;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#20844;&#24320;&#20102;&#20854;3D&#25171;&#21360;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02319v1 Announce Type: new  Abstract: We present a novel seated foot controller for handling 3-DOF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering foot controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction.
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#36890;&#36807;&#26657;&#20934;&#24863;&#30693;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#25913;&#36827;&#20102;&#36523;&#20307;&#20154;&#24037;&#26234;&#33021;&#30340;&#25628;&#32034;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.02297</link><description>&lt;p&gt;
Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02297
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#36890;&#36807;&#26657;&#20934;&#24863;&#30693;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#25913;&#36827;&#20102;&#36523;&#20307;&#20154;&#24037;&#26234;&#33021;&#30340;&#25628;&#32034;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02297v1 Announce Type: new  Abstract: Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through calibrated perception probabilities and uncertainty across aggregation and found decisions, thereby adapting the models for sequential tasks. The resulting methods can be directly integrated with pretrained models across a wide family of existing search approaches at no additional training cost. We perform extensive evaluations of aggregation methods across both different semantic perception models and policies, confirming the importance of calib
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OPENGRASP-LITE&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#20154;&#24037;&#25163;&#39033;&#30446;&#65292;&#23427;&#20855;&#26377;&#36731;&#37327;&#32423;&#12289;&#39640;&#38598;&#25104;&#24230;&#12289;&#35302;&#35273;&#21151;&#33021;&#21644;&#22522;&#20110;&#21487;&#21464;&#24418;&#36830;&#26438;&#30340;&#25235;&#21462;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#35302;&#35273;&#20154;&#36896;&#25163;&#25216;&#26415;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2408.02293</link><description>&lt;p&gt;
OPENGRASP-LITE Version 1.0: A Tactile Artificial Hand with a Compliant Linkage Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02293
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OPENGRASP-LITE&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#20154;&#24037;&#25163;&#39033;&#30446;&#65292;&#23427;&#20855;&#26377;&#36731;&#37327;&#32423;&#12289;&#39640;&#38598;&#25104;&#24230;&#12289;&#35302;&#35273;&#21151;&#33021;&#21644;&#22522;&#20110;&#21487;&#21464;&#24418;&#36830;&#26438;&#30340;&#25235;&#21462;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#35302;&#35273;&#20154;&#36896;&#25163;&#25216;&#26415;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02293v1 Announce Type: new  Abstract: Recent research has seen notable progress in the development of linkage-based artificial hands. While previous designs have focused on adaptive grasping, dexterity and biomimetic artificial skin, only a few systems have proposed a lightweight, accessible solution integrating tactile sensing with a compliant linkage-based mechanism. This paper introduces OPENGRASP LITE, an open-source, highly integrated, tactile, and lightweight artificial hand. Leveraging compliant linkage systems and MEMS barometer-based tactile sensing, it offers versatile grasping capabilities with six degrees of actuation. By providing tactile sensors and enabling soft grasping, it serves as an accessible platform for further research in tactile artificial hands.
&lt;/p&gt;</description></item><item><title>&#38646;&#25490;&#25918;&#28023;&#36816;&#36755;&#24037;&#20855;&#65288;ZEST&#65289;&#39033;&#30446;&#21033;&#29992;&#25968;&#23383;&#21452;&#32990;&#32974;&#27010;&#24565;&#21644;&#30707;&#39318;&#40060;&#27169;&#25311;&#22120;&#25512;&#36827;&#20102;&#20302;&#30899;&#25490;&#25918;&#30340;&#22810;&#29992;&#36884;&#21452;&#20307;&#33337;&#30340;&#24320;&#21457;&#65292;&#20197;&#25903;&#25345;&#21487;&#25345;&#32493;&#30340;&#28023;&#19978;&#36816;&#36755;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.02277</link><description>&lt;p&gt;
Integrating a Digital Twin Concept in the Zero Emission Sea Transporter (ZEST) Project for Sustainable Maritime Transport using Stonefish Simulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02277
&lt;/p&gt;
&lt;p&gt;
&#38646;&#25490;&#25918;&#28023;&#36816;&#36755;&#24037;&#20855;&#65288;ZEST&#65289;&#39033;&#30446;&#21033;&#29992;&#25968;&#23383;&#21452;&#32990;&#32974;&#27010;&#24565;&#21644;&#30707;&#39318;&#40060;&#27169;&#25311;&#22120;&#25512;&#36827;&#20102;&#20302;&#30899;&#25490;&#25918;&#30340;&#22810;&#29992;&#36884;&#21452;&#20307;&#33337;&#30340;&#24320;&#21457;&#65292;&#20197;&#25903;&#25345;&#21487;&#25345;&#32493;&#30340;&#28023;&#19978;&#36816;&#36755;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02277v1 Announce Type: new  Abstract: In response to stringent emission reduction targets imposed by the International Maritime Organization (IMO) and the European Green Deal's Fit for 55 legislation package, the maritime industry has shifted its focus towards decarbonization. While significant attention has been placed on vessels exceeding 5,000 gross tons (GT), emissions from coastal and short sea shipping, amounting to approximately 13% of global shipping transportation and 15% within the European Union (EU), have not received adequate consideration. This abstract introduces the Zero Emission Sea Transporter (ZEST) project, designed to address this issue by developing a zero-emissions multi-purpose catamaran for short sea route
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#30340;&#21516;&#27493;&#22270;&#20687;&#37319;&#38598;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#31616;&#21333;&#38598;&#25104;&#21040;&#25163;&#25351;&#39592;&#30340;&#20851;&#38190;&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#65288;VBTSs&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#38646;&#26679;&#26412;&#26657;&#20934;&#26041;&#27861;&#20197;&#25552;&#39640;&#22810;VBTS&#21516;&#26102;&#26657;&#20934;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#22312;&#20026;&#19968;&#20010;&#23567;&#22411;&#19977;&#25351;&#26426;&#22120;&#20154;&#22841;&#29226;&#37197;&#22791;7&#20010;VBTS&#20043;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35206;&#30422;&#22841;&#29226;&#30340;&#25152;&#26377;&#25509;&#35302;&#34920;&#38754;&#65292;&#39564;&#35777;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35302;&#35273;&#24863;&#30693;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02206</link><description>&lt;p&gt;
Large-scale Deployment of Vision-based Tactile Sensors on Multi-fingered Grippers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#30340;&#21516;&#27493;&#22270;&#20687;&#37319;&#38598;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#31616;&#21333;&#38598;&#25104;&#21040;&#25163;&#25351;&#39592;&#30340;&#20851;&#38190;&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#65288;VBTSs&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#38646;&#26679;&#26412;&#26657;&#20934;&#26041;&#27861;&#20197;&#25552;&#39640;&#22810;VBTS&#21516;&#26102;&#26657;&#20934;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#22312;&#20026;&#19968;&#20010;&#23567;&#22411;&#19977;&#25351;&#26426;&#22120;&#20154;&#22841;&#29226;&#37197;&#22791;7&#20010;VBTS&#20043;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35206;&#30422;&#22841;&#29226;&#30340;&#25152;&#26377;&#25509;&#35302;&#34920;&#38754;&#65292;&#39564;&#35777;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35302;&#35273;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02206v1 Announce Type: new  Abstract: Vision-based Tactile Sensors (VBTSs) show significant promise in that they can leverage image measurements to provide high-spatial-resolution human-like performance. However, current VBTS designs, typically confined to the fingertips of robotic grippers, prove somewhat inadequate, as many grasping and manipulation tasks require multiple contact points with the object. With an end goal of enabling large-scale, multi-surface tactile sensing via VBTSs, our research (i) develops a synchronized image acquisition system with minimal latency,(ii) proposes a modularized VBTS design for easy integration into finger phalanges, and (iii) devises a zero-shot calibration approach to improve data efficiency in the simultaneous calibration of multiple VBTSs. In validating the system within a miniature 3-fingered robotic gripper equipped with 7 VBTSs we demonstrate improved tactile perception performance by covering the contact surfaces of both gripper 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoPotter&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#24863;&#30693;&#21644;&#38518;&#33402;&#25216;&#33021;&#23398;&#20064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38518;&#33402;&#36718;&#25345;&#32493;&#21464;&#24418;&#31896;&#22303;&#30340;&#26032;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#32467;&#26500;&#20808;&#39564;&#22312;&#26426;&#22120;&#20154;&#38518;&#33402;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.02184</link><description>&lt;p&gt;
RoPotter: Toward Robotic Pottery and Deformable Object Manipulation with Structural Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoPotter&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#24863;&#30693;&#21644;&#38518;&#33402;&#25216;&#33021;&#23398;&#20064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38518;&#33402;&#36718;&#25345;&#32493;&#21464;&#24418;&#31896;&#22303;&#30340;&#26032;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#32467;&#26500;&#20808;&#39564;&#22312;&#26426;&#22120;&#20154;&#38518;&#33402;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02184v1 Announce Type: new  Abstract: Humans are capable of continuously manipulating a wide variety of deformable objects into complex shapes. This is made possible by our intuitive understanding of material properties and mechanics of the object, for reasoning about object states even when visual perception is occluded. These capabilities allow us to perform diverse tasks ranging from cooking with dough to expressing ourselves with pottery-making. However, developing robotic systems to robustly perform similar tasks remains challenging, as current methods struggle to effectively model volumetric deformable objects and reason about the complex behavior they typically exhibit. To study the robotic systems and algorithms capable of deforming volumetric objects, we introduce a novel robotics task of continuously deforming clay on a pottery wheel. We propose a pipeline for perception and pottery skill-learning, called RoPotter, wherein we demonstrate that structural priors spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#22411;&#33258;&#20027;&#24494;&#22609;&#26009;&#25910;&#38598;&#21322;&#28508;&#24335;&#24179;&#21488;&#30340;&#35774;&#35745;&#19982;&#23454;&#26045;&#65292;&#26088;&#22312;&#25552;&#39640;&#29616;&#26377;&#24494;&#22609;&#26009;&#25910;&#38598;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02162</link><description>&lt;p&gt;
Improvement and Empirical Testing of a Novel Autonomous Microplastics-Collecting Semisubmersible
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#22411;&#33258;&#20027;&#24494;&#22609;&#26009;&#25910;&#38598;&#21322;&#28508;&#24335;&#24179;&#21488;&#30340;&#35774;&#35745;&#19982;&#23454;&#26045;&#65292;&#26088;&#22312;&#25552;&#39640;&#29616;&#26377;&#24494;&#22609;&#26009;&#25910;&#38598;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02162v1 Announce Type: new  Abstract: Since their invention, plastics have become ubiquitous in modern societies all around the world, and their impact on the environment has, in recent years, become nearly as well-known. Plastics produced by humans have reached nearly every corner of the world, and throughout their centuries-long lifetimes, plastics continually break down into smaller and smaller particles due to the physical stresses which they are subjected to. These stresses eventually, inevitably, break these plastics down into microplastics -pieces of plastic small enough to be consumed by organisms in bodies of water throughout the globe. These microplastics can very easily bioaccumulate, and have been found everywhere from the Great Lakes to the bloodstreams of humans. The effects of these plastics are poorly understood, however, they have been linked to infertility, halted growth, and a host of other maladies in aquatic organisms. Currently, removal of these plastic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#30452;&#25509;&#35268;&#21010;&#20986;&#20572;&#36710;&#36335;&#24452;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02061</link><description>&lt;p&gt;
ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02061
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#30452;&#25509;&#35268;&#21010;&#20986;&#20572;&#36710;&#36335;&#24452;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02061v1 Announce Type: cross  Abstract: Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conducted extensive experiments in real-world scenarios, and the resul
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;EqvAfford&#26694;&#26550;&#36890;&#36807;&#30830;&#20445;&#28857;&#32423;affordance&#23398;&#20064;&#30340;SE(3)&#31561;&#21464;&#24615;&#65292;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#35770;&#29289;&#20307;&#23039;&#24577;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2408.01953</link><description>&lt;p&gt;
EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01953
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;EqvAfford&#26694;&#26550;&#36890;&#36807;&#30830;&#20445;&#28857;&#32423;affordance&#23398;&#20064;&#30340;SE(3)&#31561;&#21464;&#24615;&#65292;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#35770;&#29289;&#20307;&#23039;&#24577;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01953v1 Announce Type: new  Abstract: Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;PnP&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#23039;&#21183;&#20272;&#35745;&#22330;&#26223;&#20013;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2408.01945</link><description>&lt;p&gt;
Generalized Maximum Likelihood Estimation for Perspective-n-Point Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;PnP&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#23039;&#21183;&#20272;&#35745;&#22330;&#26223;&#20013;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01945v1 Announce Type: cross  Abstract: The Perspective-n-Point (PnP) problem has been widely studied in the literature and applied in various vision-based pose estimation scenarios. However, existing methods ignore the anisotropy uncertainty of observations, as demonstrated in several real-world datasets in this paper. This oversight may lead to suboptimal and inaccurate estimation, particularly in the presence of noisy observations. To this end, we propose a generalized maximum likelihood PnP solver, named GMLPnP, that minimizes the determinant criterion by iterating the GLS procedure to estimate the pose and uncertainty simultaneously. Further, the proposed method is decoupled from the camera model. Results of synthetic and real experiments show that our method achieves better accuracy in common pose estimation scenarios, GMLPnP improves rotation/translation accuracy by 4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best baseline. It is more ac
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#21050;&#28608;&#25216;&#26415;&#65292;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#19968;&#31181;&#20154;&#24037;&#36171;&#33021;&#33258;&#28982;&#21160;&#29289;&#26234;&#33021;&#30340;&#36719;&#26426;&#22120;&#20154;&#65292;&#26377;&#26395;&#29992;&#20110;&#29615;&#22659;&#33258;&#20027;&#25506;&#32034;&#12289;&#23548;&#33322;&#20197;&#21450;&#23454;&#26102;&#34892;&#20026;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2408.01941</link><description>&lt;p&gt;
A Jellyfish Cyborg: Exploiting Natural Embodied Intelligence as Soft Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01941
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#21050;&#28608;&#25216;&#26415;&#65292;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#19968;&#31181;&#20154;&#24037;&#36171;&#33021;&#33258;&#28982;&#21160;&#29289;&#26234;&#33021;&#30340;&#36719;&#26426;&#22120;&#20154;&#65292;&#26377;&#26395;&#29992;&#20110;&#29615;&#22659;&#33258;&#20027;&#25506;&#32034;&#12289;&#23548;&#33322;&#20197;&#21450;&#23454;&#26102;&#34892;&#20026;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01941v1 Announce Type: new  Abstract: In the advanced field of bio-inspired robotics, the emergence of cyborgs represents the successful integration of engineering and biological systems. Building on previous research that showed how electrical stimuli could initiate and speed up a jellyfish's movement, this study presents a groundbreaking approach that explores how the natural embodied intelligence of the animal can be harnessed to address pivotal challenges such as spontaneous exploration, navigation in various environments, control of whole-body motion, and real-time predictions of behavior. We have developed a comprehensive data acquisition system and a unique setup for stimulating jellyfish, allowing for a detailed study of their movements. Through careful analysis of both spontaneous behaviors and behaviors induced by targeted stimulation, we have identified subtle differences between natural and induced motion patterns. By using a machine learning method called physic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;VFSTL&#65292;&#36890;&#36807;&#20248;&#21270;&#20215;&#20540;&#20989;&#25968;&#26469;&#25552;&#39640;&#20449;&#21495;&#26102;&#31354;&#36923;&#36753;&#65288;STL&#65289;&#35268;&#33539;&#30340;&#28385;&#36275;&#24230;&#65292;&#26080;&#38656;&#25163;&#21160;&#35859;&#35789;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#30340;&#38271;&#26399;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01923</link><description>&lt;p&gt;
Scalable Signal Temporal Logic Guided Reinforcement Learning via Value Function Space Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;VFSTL&#65292;&#36890;&#36807;&#20248;&#21270;&#20215;&#20540;&#20989;&#25968;&#26469;&#25552;&#39640;&#20449;&#21495;&#26102;&#31354;&#36923;&#36753;&#65288;STL&#65289;&#35268;&#33539;&#30340;&#28385;&#36275;&#24230;&#65292;&#26080;&#38656;&#25163;&#21160;&#35859;&#35789;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#30340;&#38271;&#26399;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01923v1 Announce Type: new  Abstract: The integration of reinforcement learning (RL) and formal methods has emerged as a promising framework for solving long-horizon planning problems. Conventional approaches typically involve abstraction of the state and action spaces and manually created labeling functions or predicates. However, the efficiency of these approaches deteriorates as the tasks become increasingly complex, which results in exponential growth in the size of labeling functions or predicates. To address these issues, we propose a scalable model-based RL framework, called VFSTL, which schedules pre-trained skills to follow unseen STL specifications without using hand-crafted predicates. Given a set of value functions obtained by goal-conditioned RL, we formulate an optimization problem to maximize the robustness value of Signal Temporal Logic (STL) defined specifications, which is computed using value functions as predicates. To further reduce the computation burde
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23454;&#20307;&#22320;&#38754;&#20195;&#29702;&#21644;&#31354;&#20013;&#20195;&#29702;&#20043;&#38388;&#23454;&#26045;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#36890;&#20449;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#30450;&#30446;&#25506;&#32034;&#30340;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2408.01877</link><description>&lt;p&gt;
Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23454;&#20307;&#22320;&#38754;&#20195;&#29702;&#21644;&#31354;&#20013;&#20195;&#29702;&#20043;&#38388;&#23454;&#26045;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#36890;&#20449;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#30450;&#30446;&#25506;&#32034;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01877v1 Announce Type: cross  Abstract: In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a target object specified by a natural language label without any environment-specific fine-tuning. This is challenging, given the limited view of a ground agent and its independent exploratory behavior. To address these issues, we consider an assistive overhead agent with a bounded global view alongside the ground agent and present two coordinated navigation schemes for judicious exploration. We establish the influence of the Generative Communication (GC) between the embodied agents equipped with Vision-Language Models (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in the ground agent's ability to find the target object in comparison with an unassisted setup in simulation. We further analyze the GC for unique traits quantifying the presence of hallucination and cooperation. In particular, we identify a unique trait of "preemptive hallucin
&lt;/p&gt;</description></item><item><title>BEVPlace++&#21033;&#29992;&#36731;&#37327;&#32423;CNNs&#20174;BEV&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#36890;&#36807;&#20301;&#32622;&#35782;&#21035;&#21644;3-DoF&#23450;&#20301;&#20272;&#35745;&#23454;&#29616;&#20102;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#28608;&#20809;&#38647;&#36798;&#20840;&#23616;&#23450;&#20301;&#30340;&#24555;&#36895;&#12289;&#40065;&#26834;&#19988;&#31934;&#20934;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01841</link><description>&lt;p&gt;
BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for Unmanned Ground Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01841
&lt;/p&gt;
&lt;p&gt;
BEVPlace++&#21033;&#29992;&#36731;&#37327;&#32423;CNNs&#20174;BEV&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#36890;&#36807;&#20301;&#32622;&#35782;&#21035;&#21644;3-DoF&#23450;&#20301;&#20272;&#35745;&#23454;&#29616;&#20102;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#28608;&#20809;&#38647;&#36798;&#20840;&#23616;&#23450;&#20301;&#30340;&#24555;&#36895;&#12289;&#40065;&#26834;&#19988;&#31934;&#20934;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01841v1 Announce Type: new  Abstract: This article introduces BEVPlace++, a novel, fast, and robust LiDAR global localization method for unmanned ground vehicles. It uses lightweight convolutional neural networks (CNNs) on Bird's Eye View (BEV) image-like representations of LiDAR data to achieve accurate global localization through place recognition followed by 3-DoF pose estimation. Our detailed analyses reveal an interesting fact that CNNs are inherently effective at extracting distinctive features from LiDAR BEV images. Remarkably, keypoints of two BEV images with large translations can be effectively matched using CNN-extracted features. Building on this insight, we design a rotation equivariant module (REM) to obtain distinctive features while enhancing robustness to rotational changes. A Rotation Equivariant and Invariant Network (REIN) is then developed by cascading REM and a descriptor generator, NetVLAD, to sequentially generate rotation equivariant local features a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;diS-Graph&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#24314;&#31569;&#34013;&#22270;&#20449;&#24687;&#21644;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#23450;&#20301;&#21644;&#22320;&#22270;&#26500;&#24314;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01737</link><description>&lt;p&gt;
Real-time Localization and Mapping in Architectural Plans with Deviations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;diS-Graph&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#24314;&#31569;&#34013;&#22270;&#20449;&#24687;&#21644;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#23450;&#20301;&#21644;&#22320;&#22270;&#26500;&#24314;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01737v1 Announce Type: new  Abstract: Having prior knowledge of an environment boosts the localization and mapping accuracy of robots. Several approaches in the literature have utilized architectural plans in this regard. However, almost all of them overlook the deviations between actual as-built environments and as-planned architectural designs, introducing bias in the estimations. To address this issue, we present a novel localization and mapping method denoted as deviations-informed Situational Graphs or diS-Graphs that integrates prior knowledge from architectural plans even in the presence of deviations. It is based on Situational Graphs (S-Graphs) that merge geometric models of the environment with 3D scene graphs into a multi-layered jointly optimizable factor graph. Our diS-Graph extracts information from architectural plans by first modeling them as a hierarchical factor graph, which we will call an Architectural Graph (A-Graph). While the robot explores the real en
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#20551;&#32930;&#39046;&#22495;&#65292;&#21253;&#25324;&#31070;&#32463;&#20551;&#32930;&#12289;&#36719;&#25191;&#34892;&#22120;&#21644;&#25511;&#21046;&#31574;&#30053;&#12290;&#23427;&#27010;&#36848;&#20102;&#26426;&#22120;&#20154;&#20551;&#32930;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#36719;&#26426;&#22120;&#20154;&#25191;&#34892;&#22120;&#22914;EAP&#12289;SMA&#21644;FFA&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#23457;&#26597;&#20102;&#21508;&#31181;&#25511;&#21046;&#31574;&#30053;&#21644;&#26410;&#26469;&#30340;&#28508;&#22312;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01729</link><description>&lt;p&gt;
A Survey on Robotic Prosthetics: Neuroprosthetics, Soft Actuators, and Control Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#20551;&#32930;&#39046;&#22495;&#65292;&#21253;&#25324;&#31070;&#32463;&#20551;&#32930;&#12289;&#36719;&#25191;&#34892;&#22120;&#21644;&#25511;&#21046;&#31574;&#30053;&#12290;&#23427;&#27010;&#36848;&#20102;&#26426;&#22120;&#20154;&#20551;&#32930;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#36719;&#26426;&#22120;&#20154;&#25191;&#34892;&#22120;&#22914;EAP&#12289;SMA&#21644;FFA&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#23457;&#26597;&#20102;&#21508;&#31181;&#25511;&#21046;&#31574;&#30053;&#21644;&#26410;&#26469;&#30340;&#28508;&#22312;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01729v1 Announce Type: new  Abstract: The field of robotics is a quickly evolving feat of technology that accepts contributions from various genres of science. Neuroscience, Physiology, Chemistry, Material science, Computer science, and the wide umbrella of mechatronics have all simultaneously contributed to many innovations in the prosthetic applications of robotics. This review begins with a discussion of the scope of the term robotic prosthetics and discusses the evolving domain of Neuroprosthetics. The discussion is then constrained to focus on various actuation and control strategies for robotic prosthetic limbs. This review discusses various soft robotic actuators such as EAP, SMA, FFA, etc., and the merits of such actuators over conventional hard robotic actuators. Options in control strategies for robotic prosthetics, that are in various states of research and development, are reviewed. This paper concludes the discussion with an analysis regarding the prospective di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22810;&#31181;&#35270;&#35273;-&#24815;&#24615;SLAM&#31995;&#32479;&#36827;&#34892;&#20102;&#20892;&#19994;&#26426;&#22120;&#20154;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#38381;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2408.01716</link><description>&lt;p&gt;
Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the Benefits and Computational Costs of Loop Closing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#31181;&#35270;&#35273;-&#24815;&#24615;SLAM&#31995;&#32479;&#36827;&#34892;&#20102;&#20892;&#19994;&#26426;&#22120;&#20154;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#38381;&#29615;&#38381;&#21512;&#23545;&#23450;&#20301;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01716v1 Announce Type: new  Abstract: Simultaneous Localization and Mapping (SLAM) is essential for mobile robotics, enabling autonomous navigation in dynamic, unstructured outdoor environments without relying on external positioning systems. In agricultural applications, where environmental conditions can be particularly challenging due to variable lighting or weather conditions, Visual-Inertial SLAM has emerged as a potential solution. This paper benchmarks several open-source Visual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS, Kimera, and SVO Pro, to evaluate their performance in agricultural settings. We focus on the impact of loop closing on localization accuracy and computational demands, providing a comprehensive analysis of these systems' effectiveness in real-world environments and especially their application to embedded systems in agricultural robotics. Our contributions further include an assessment of varying frame rates on localization acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#27454;&#22810;&#26059;&#32764;&#26080;&#20154;&#26426;&#21407;&#22411;&#65292;&#21363;&#20351;&#22312;&#26059;&#32764;&#21457;&#29983;&#25925;&#38556;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#36827;&#34892;&#31934;&#30830;&#30528;&#38470;&#65292;&#24182;&#36890;&#36807;&#20809;&#23398;&#23548;&#33322;&#31995;&#32479;&#30830;&#20445;&#20102;&#36895;&#24230;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01676</link><description>&lt;p&gt;
Prototyping of a multirotor UAV for precision landing under rotor failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#27454;&#22810;&#26059;&#32764;&#26080;&#20154;&#26426;&#21407;&#22411;&#65292;&#21363;&#20351;&#22312;&#26059;&#32764;&#21457;&#29983;&#25925;&#38556;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#36827;&#34892;&#31934;&#30830;&#30528;&#38470;&#65292;&#24182;&#36890;&#36807;&#20809;&#23398;&#23548;&#33322;&#31995;&#32479;&#30830;&#20445;&#20102;&#36895;&#24230;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01676v1 Announce Type: new  Abstract: This work presents a prototype of a multirotor aerial vehicle capable of precision landing, even under the effects of rotor failures. The manuscript presents the fault-tolerant techniques and mechanical designs to achieve a fault-tolerant multirotor, and a vision-based navigation system required to achieve a precision landing. Preliminary experimental results will be shown, to validate on one hand the fault-tolerant control vehicle and, on the other hand, the autonomous landing algorithm. Also, a prototype of the fault-tolerant UAV is presented, capable of precise autonomous landing, which will be used in future experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPORT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#19968;&#33324;&#23545;&#35937;&#37325;&#26032;&#25490;&#21015;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#26032;&#29615;&#22659;&#20013;&#20063;&#33021;&#25353;&#29031;&#20154;&#31867;&#30340;&#25351;&#31034;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307;&#12290;&#26694;&#26550;&#21253;&#21547;&#23545;&#35937;&#23450;&#20301;&#12289;&#30446;&#26631;&#24819;&#35937;&#21147;&#20197;&#21450;&#26426;&#22120;&#20154;&#25511;&#21046;&#19977;&#37096;&#20998;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#35821;&#20041;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;3D&#23039;&#24577;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#29289;&#29702;&#19978;&#21512;&#29702;&#30340;&#37325;&#26032;&#25490;&#21015;&#25928;&#26524;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#24320;&#25918;&#24335;&#23545;&#35937;&#23450;&#20301;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#26080;&#38656;&#29305;&#23450;&#20110;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#24494;&#35843;&#65292;&#24182;&#19988;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#20165;&#38656;&#22312;&#29289;&#20307;&#19982;&#21442;&#32771;&#29289;&#20307;&#30340;&#30456;&#23545;&#20301;&#32622;&#30830;&#23450;&#21518;&#36827;&#34892;&#24819;&#35937;&#12290;</title><link>https://arxiv.org/abs/2408.01655</link><description>&lt;p&gt;
Stimulating Imagination: Towards General-purpose Object Rearrangement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPORT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#19968;&#33324;&#23545;&#35937;&#37325;&#26032;&#25490;&#21015;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#26032;&#29615;&#22659;&#20013;&#20063;&#33021;&#25353;&#29031;&#20154;&#31867;&#30340;&#25351;&#31034;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307;&#12290;&#26694;&#26550;&#21253;&#21547;&#23545;&#35937;&#23450;&#20301;&#12289;&#30446;&#26631;&#24819;&#35937;&#21147;&#20197;&#21450;&#26426;&#22120;&#20154;&#25511;&#21046;&#19977;&#37096;&#20998;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#35821;&#20041;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;3D&#23039;&#24577;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#29289;&#29702;&#19978;&#21512;&#29702;&#30340;&#37325;&#26032;&#25490;&#21015;&#25928;&#26524;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#24320;&#25918;&#24335;&#23545;&#35937;&#23450;&#20301;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#26080;&#38656;&#29305;&#23450;&#20110;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#24494;&#35843;&#65292;&#24182;&#19988;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#20165;&#38656;&#22312;&#29289;&#20307;&#19982;&#21442;&#32771;&#29289;&#20307;&#30340;&#30456;&#23545;&#20301;&#32622;&#30830;&#23450;&#21518;&#36827;&#34892;&#24819;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01655v1 Announce Type: new  Abstract: General-purpose object placement is a fundamental capability of an intelligent generalist robot, i.e., being capable of rearranging objects following human instructions even in novel environments. To achieve this, we break the rearrangement down into three parts, including object localization, goal imagination and robot control, and propose a framework named SPORT. SPORT leverages pre-trained large vision models for broad semantic reasoning about objects, and learns a diffusion-based 3D pose estimator to ensure physically-realistic results. Only object types (to be moved or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object localization and recognition since no specific fine-tuning is needed for robotic scenarios. Furthermore, the diffusion-based estimator only need to "imagine" the poses of the moving and reference objects after the pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28040;&#36153;&#22411;&#36710;&#36742;&#19978;&#30340;GNSS&#25968;&#25454;&#21644;&#25668;&#20687;&#22836;&#22270;&#20687;&#36827;&#34892;&#36947;&#36335;&#22270;&#33258;&#21160;&#21019;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#21019;&#24314;&#36947;&#36335;&#22270;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.01640</link><description>&lt;p&gt;
Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for Robust Road Network Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28040;&#36153;&#22411;&#36710;&#36742;&#19978;&#30340;GNSS&#25968;&#25454;&#21644;&#25668;&#20687;&#22836;&#22270;&#20687;&#36827;&#34892;&#36947;&#36335;&#22270;&#33258;&#21160;&#21019;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#21019;&#24314;&#36947;&#36335;&#22270;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01640v1 Announce Type: cross  Abstract: Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today's advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data's time 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PUCL&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#26410;&#30693;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#32422;&#26463;&#21442;&#25968;&#21270;&#21644;&#29615;&#22659;&#27169;&#22411;&#12290;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#20174;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#36335;&#24452;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36880;&#27493;&#20248;&#21270;&#32422;&#26463;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2408.01622</link><description>&lt;p&gt;
Positive-Unlabeled Constraint Learning (PUCL) for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PUCL&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#26410;&#30693;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#32422;&#26463;&#21442;&#25968;&#21270;&#21644;&#29615;&#22659;&#27169;&#22411;&#12290;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#20174;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#36335;&#24452;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36880;&#27493;&#20248;&#21270;&#32422;&#26463;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01622v1 Announce Type: new  Abstract: Planning for a wide range of real-world robotic tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. This paper presents a novel Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a continuous arbitrary constraint function from demonstration, without requiring prior knowledge of the true constraint parameterization or environmental model as existing works. Within our framework, we treat all data in demonstrations as positive (feasible) data, and learn a control policy to generate potentially infeasible trajectories, which serve as unlabeled data. In each iteration, we first update the policy and then a two-step positive-unlabeled learning procedure is applied, where it first identifies reliable infeasible data using a distance metric, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#30446;&#31435;&#20307;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#27627;&#31859;&#32423;NTCR&#30340;&#19981;&#23545;&#31216;&#24418;&#24577;&#65292;&#21363;&#20351;&#22312;&#20302;&#36136;&#37327;&#28857;&#20113;&#25910;&#38598;&#20063;&#23384;&#22312;&#38590;&#24230;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2408.01615</link><description>&lt;p&gt;
Three-dimensional Morphological Reconstruction of Millimeter-Scale Soft Continuum Robots based on Dual Stereo Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#30446;&#31435;&#20307;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#27627;&#31859;&#32423;NTCR&#30340;&#19981;&#23545;&#31216;&#24418;&#24577;&#65292;&#21363;&#20351;&#22312;&#20302;&#36136;&#37327;&#28857;&#20113;&#25910;&#38598;&#20063;&#23384;&#22312;&#38590;&#24230;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01615v1 Announce Type: new  Abstract: Continuum robots can be miniaturized to just a few millimeters in diameter. Among these, notched tubular continuum robots (NTCR) show great potential in many delicate applications. Existing works in robotic modeling focus on kinematics and dynamics but still face challenges in reproducing the robot's morphology -- a significant factor that can expand the research landscape of continuum robots, especially for those with asymmetric continuum structures. This paper proposes a dual stereo vision-based method for the three-dimensional morphological reconstruction of millimeter-scale NTCRs. The method employs two oppositely located stationary binocular cameras to capture the point cloud of the NTCR, then utilizes predefined geometry as a reference for the KD tree method to relocate the capture point clouds, resulting in a morphologically correct NTCR despite the low-quality raw point cloud collection. The method has been proved feasible for an
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20381;&#36182;&#23436;&#20840;&#21487;&#35270;&#21270;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#23450;&#20301;&#22303;&#22756;&#21306;&#22495;&#20013;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01589</link><description>&lt;p&gt;
Soil Sample Search in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01589
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20381;&#36182;&#23436;&#20840;&#21487;&#35270;&#21270;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#23450;&#20301;&#22303;&#22756;&#21306;&#22495;&#20013;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01589v1 Announce Type: new  Abstract: To work in unknown outdoor environments, autonomous sampling machines need the ability to target samples despite limited visibility and robotic arm reach distance. We design a heuristic guided search method to speed up the search process and more efficiently localize the approximate center of soil regions. Through simulation experiments, we assess the effectiveness of the proposed algorithm and discover superior performance in terms of speed, distance traveled, and success rate compared to naive baselines.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#33258;&#21160;&#20351;&#29992;&#26700;&#38754;&#23454;&#39564;&#23460;&#35774;&#22791;&#65292;&#22914;&#31163;&#24515;&#26426;&#12290;</title><link>https://arxiv.org/abs/2408.01576</link><description>&lt;p&gt;
Autonomous Integration of Bench-Top Wet Lab Equipment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01576
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#33258;&#21160;&#20351;&#29992;&#26700;&#38754;&#23454;&#39564;&#23460;&#35774;&#22791;&#65292;&#22914;&#31163;&#24515;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01576v1 Announce Type: new  Abstract: Laboratory automation is an expensive and complicated endeavor with limited inflexible options for small-scale labs. We develop a prototype system for tending to a bench-top centrifuge using computer vision methods for color detection and circular Hough Transforms to detect and localize centrifuge buckets. Initial results show that the prototype is capable of automating the usage of regular bench-top lab equipment.
&lt;/p&gt;</description></item><item><title>TURTLMap &#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#23454;&#26102;&#23450;&#20301;&#21644;&#26144;&#23556;&#26041;&#27861;&#35299;&#20915;&#20302;&#32441;&#29702;&#27700;&#19979;&#29615;&#22659;&#30340;&#25361;&#25112;&#65292;&#25104;&#26412;&#20302;&#24265;&#65292;&#33021;&#22815;&#20934;&#30830;&#36319;&#36394;&#26426;&#22120;&#20154;&#65292;&#22312;&#20302;&#32441;&#29702;&#29615;&#22659;&#20013;&#23454;&#26102;&#26500;&#24314;&#23494;&#38598;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2408.01569</link><description>&lt;p&gt;
TURTLMap: Real-time Localization and Dense Mapping of Low-texture Underwater Environments with a Low-cost Unmanned Underwater Vehicle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01569
&lt;/p&gt;
&lt;p&gt;
TURTLMap &#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#23454;&#26102;&#23450;&#20301;&#21644;&#26144;&#23556;&#26041;&#27861;&#35299;&#20915;&#20302;&#32441;&#29702;&#27700;&#19979;&#29615;&#22659;&#30340;&#25361;&#25112;&#65292;&#25104;&#26412;&#20302;&#24265;&#65292;&#33021;&#22815;&#20934;&#30830;&#36319;&#36394;&#26426;&#22120;&#20154;&#65292;&#22312;&#20302;&#32441;&#29702;&#29615;&#22659;&#20013;&#23454;&#26102;&#26500;&#24314;&#23494;&#38598;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01569v1 Announce Type: new  Abstract: Significant work has been done on advancing localization and mapping in underwater environments. Still, state-of-the-art methods are challenged by low-texture environments, which is common for underwater settings. This makes it difficult to use existing methods in diverse, real-world scenes. In this paper, we present TURTLMap, a novel solution that focuses on textureless underwater environments through a real-time localization and mapping method. We show that this method is low-cost, and capable of tracking the robot accurately, while constructing a dense map of a low-textured environment in real-time. We evaluate the proposed method using real-world data collected in an indoor water tank with a motion capture system and ground truth reference map. Qualitative and quantitative results validate the proposed system achieves accurate and robust localization and precise dense mapping, even when subject to wave conditions. The project page fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26032;&#22411;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#20010;&#24615;&#21270;&#12289;&#22686;&#26448;&#21046;&#36896;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#36827;&#34892;&#32963;&#24687;&#32905;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2408.01554</link><description>&lt;p&gt;
Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26032;&#22411;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#20010;&#24615;&#21270;&#12289;&#22686;&#26448;&#21046;&#36896;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#36827;&#34892;&#32963;&#24687;&#32905;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01554v1 Announce Type: cross  Abstract: In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#20195;&#29702;&#21327;&#21516;&#25805;&#32437;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#21147;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#21508;&#20010;&#20195;&#29702;&#33021;&#22815;&#29420;&#31435;&#20110;&#22806;&#37096;&#30340;&#20449;&#24687;&#36827;&#34892;&#22797;&#26434;&#30340;&#20219;&#21153;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2408.01543</link><description>&lt;p&gt;
A Decomposition of Interaction Force for Multi-Agent Co-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01543
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#20195;&#29702;&#21327;&#21516;&#25805;&#32437;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#21147;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#21508;&#20010;&#20195;&#29702;&#33021;&#22815;&#29420;&#31435;&#20110;&#22806;&#37096;&#30340;&#20449;&#24687;&#36827;&#34892;&#22797;&#26434;&#30340;&#20219;&#21153;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01543v1 Announce Type: new  Abstract: Multi-agent human-robot co-manipulation is a poorly understood process with many inputs that potentially affect agent behavior. This paper explores one such input known as interaction force. Interaction force is potentially a primary component in communication that occurs during co-manipulation. There are, however, many different perspectives and definitions of interaction force in the literature. Therefore, a decomposition of interaction force is proposed that provides a consistent way of ascertaining the state of an agent relative to the group for multi-agent co-manipulation. This proposed method extends a current definition from one to four degrees of freedom, does not rely on a predefined object path, and is independent of the number of agents acting on the system and their locations and input wrenches (forces and torques). In addition, all of the necessary measures can be obtained by a self-contained robotic system, allowing for a m
&lt;/p&gt;</description></item><item><title>SceneMotion&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#23558;&#23616;&#37096;agent-centric&#23884;&#20837;&#36716;&#25442;&#20026;&#20840;&#26223;&#32423;&#39044;&#27979;&#65292;&#22312;Waymo Open Interaction Prediction Challenge&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2408.01537</link><description>&lt;p&gt;
SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01537
&lt;/p&gt;
&lt;p&gt;
SceneMotion&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#27169;&#22359;&#23558;&#23616;&#37096;agent-centric&#23884;&#20837;&#36716;&#25442;&#20026;&#20840;&#26223;&#32423;&#39044;&#27979;&#65292;&#22312;Waymo Open Interaction Prediction Challenge&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01537v1 Announce Type: cross  Abstract: Self-driving vehicles rely on multimodal motion forecasts to effectively interact with their environment and plan safe maneuvers. We introduce SceneMotion, an attention-based model for forecasting scene-wide motion modes of multiple traffic agents. Our model transforms local agent-centric embeddings into scene-wide forecasts using a novel latent context module. This module learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling. The competitive performance in the Waymo Open Interaction Prediction Challenge demonstrates the effectiveness of our approach. Moreover, we cluster future waypoints in time and space to quantify the interaction between agents. We merge all modes and analyze each mode independently to determine which clusters are resolved through interaction or result in conflict. Our implementation is available at: https://github.com/kit-mrt/future-motion
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#35268;&#21010;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#38271;&#26399;&#29366;&#24577;&#36712;&#36857;&#30340;&#33021;&#21147;&#65292;&#20248;&#21270;&#20102;&#36830;&#32493;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#35843;&#25972;&#35745;&#21010;&#25191;&#34892;&#21608;&#26399;&#65292;&#20197;&#38477;&#20302;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#25552;&#39640;&#29615;&#22659;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01510</link><description>&lt;p&gt;
Adaptive Planning with Generative Models under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01510
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#35268;&#21010;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#38271;&#26399;&#29366;&#24577;&#36712;&#36857;&#30340;&#33021;&#21147;&#65292;&#20248;&#21270;&#20102;&#36830;&#32493;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#35843;&#25972;&#35745;&#21010;&#25191;&#34892;&#21608;&#26399;&#65292;&#20197;&#38477;&#20302;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#25552;&#39640;&#29615;&#22659;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01510v1 Announce Type: new  Abstract: Planning with generative models has emerged as an effective decision-making paradigm across a wide range of domains, including reinforcement learning and autonomous navigation. While continuous replanning at each timestep might seem intuitive because it allows decisions to be made based on the most recent environmental observations, it results in substantial computational challenges, primarily due to the complexity of the generative model's underlying deep learning architecture. Our work addresses this challenge by introducing a simple adaptive planning policy that leverages the generative model's ability to predict long-horizon state trajectories, enabling the execution of multiple actions consecutively without the need for immediate replanning. We propose to use the predictive uncertainty derived from a Deep Ensemble of inverse dynamics models to dynamically adjust the intervals between planning sessions. In our experiments conducted o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#21046;&#22270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#26631;&#25968;&#25454;&#21644;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22312;&#32447;&#39640;&#28165;&#22320;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2408.01471</link><description>&lt;p&gt;
Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01471
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#21046;&#22270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#26631;&#25968;&#25454;&#21644;&#26631;&#20934;&#23450;&#20041;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22312;&#32447;&#39640;&#28165;&#22320;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01471v1 Announce Type: cross  Abstract: Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definition (SD) maps-in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20004;&#27493;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#35889;&#32858;&#31867;&#12289;&#23616;&#37096;&#28508;&#22312;&#22330;&#26041;&#27861;&#21644;&#36229;&#36951;&#20256;&#31639;&#27861;&#26469;&#20026;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.01435</link><description>&lt;p&gt;
A New Clustering-based View Planning Method for Building Inspection with Drone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20004;&#27493;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#35889;&#32858;&#31867;&#12289;&#23616;&#37096;&#28508;&#22312;&#22330;&#26041;&#27861;&#21644;&#36229;&#36951;&#20256;&#31639;&#27861;&#26469;&#20026;&#26080;&#20154;&#26426;&#24314;&#31569;&#26816;&#26597;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01435v1 Announce Type: cross  Abstract: With the rapid development of drone technology, the application of drones equipped with visual sensors for building inspection and surveillance has attracted much attention. View planning aims to find a set of near-optimal viewpoints for vision-related tasks to achieve the vision coverage goal. This paper proposes a new clustering-based two-step computational method using spectral clustering, local potential field method, and hyper-heuristic algorithm to find near-optimal views to cover the target building surface. In the first step, the proposed method generates candidate viewpoints based on spectral clustering and corrects the positions of candidate viewpoints based on our newly proposed local potential field method. In the second step, the optimization problem is converted into a Set Covering Problem (SCP), and the optimal viewpoint subset is solved using our proposed hyper-heuristic algorithm. Experimental results show that the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;UAV&#38388;&#21512;&#20316;&#24863;&#30693;&#25968;&#25454;&#38598;&#8220;U2UData&#8221;&#65292;&#29992;&#20110;&#33258;&#20027;&#39134;&#34892;&#65292;&#35206;&#30422;9&#24179;&#26041;&#20844;&#37324;&#39134;&#34892;&#21306;&#22495;&#65292;&#21253;&#21547;315K&#28608;&#20809;&#38647;&#36798;&#24103;&#12289;945KRGB&#21644;&#28145;&#24230;&#24103;&#20197;&#21450;2.41M&#20010;3D&#36793;&#30028;&#26694;&#26631;&#27880;&#12290;</title><link>https://arxiv.org/abs/2408.00606</link><description>&lt;p&gt;
U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;UAV&#38388;&#21512;&#20316;&#24863;&#30693;&#25968;&#25454;&#38598;&#8220;U2UData&#8221;&#65292;&#29992;&#20110;&#33258;&#20027;&#39134;&#34892;&#65292;&#35206;&#30422;9&#24179;&#26041;&#20844;&#37324;&#39134;&#34892;&#21306;&#22495;&#65292;&#21253;&#21547;315K&#28608;&#20809;&#38647;&#36798;&#24103;&#12289;945KRGB&#21644;&#28145;&#24230;&#24103;&#20197;&#21450;2.41M&#20010;3D&#36793;&#30028;&#26694;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00606v2 Announce Type: replace  Abstract: Modern perception systems for autonomous flight are sensitive to occlusion and have limited long-range capability, which is a key bottleneck in improving low-altitude economic task performance. Recent research has shown that the UAV-to-UAV (U2U) cooperative perception system has great potential to revolutionize the autonomous flight industry. However, the lack of a large-scale dataset is hindering progress in this area. This paper presents U2UData, the first large-scale cooperative perception dataset for swarm UAVs autonomous flight. The dataset was collected by three UAVs flying autonomously in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames, 945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes. It also includes brightness, temperature, humidity, smoke, and airflow values covering all flight routes. U2USim is the first real-world mapping swarm UAVs simulation environment. It take
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#21644;&#32416;&#27491;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24110;&#21161;&#25552;&#39640;&#22270;&#20687;&#24207;&#21015;&#20013;&#30456;&#26426;&#20301;&#32622;&#21644;&#26041;&#21521;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2407.20391</link><description>&lt;p&gt;
Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#21644;&#32416;&#27491;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24110;&#21161;&#25552;&#39640;&#22270;&#20687;&#24207;&#21015;&#20013;&#30456;&#26426;&#20301;&#32622;&#21644;&#26041;&#21521;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20391v2 Announce Type: replace-cross  Abstract: We propose three novel metrics for evaluating the accuracy of a set of estimated camera poses given the ground truth: Translation Alignment Score (TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS evaluates the translation accuracy independently of the rotations, and the RAS evaluates the rotation accuracy independently of the translations. The PAS is the average of the two scores, evaluating the combined accuracy of both translations and rotations. The TAS is computed in four steps: (1) Find the upper quartile of the closest-pair-distances, $d$. (2) Align the estimated trajectory to the ground truth using a robust registration method. (3) Collect all distance errors and obtain the cumulative frequencies for multiple thresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up these cumulative frequencies and normalize them such that the theoretical maximum is 1. The TAS has practical
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#37327;&#25968;&#25454;&#25903;&#25345;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2407.18569</link><description>&lt;p&gt;
PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#37327;&#25968;&#25454;&#25903;&#25345;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18569v3 Announce Type: replace  Abstract: Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a fundamental resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#21453;&#36716;&#29702;&#35770;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#19968;&#39537;&#21160;&#28304;&#25511;&#21046;&#22810;&#20010;&#36719;&#27668;&#21387;&#25191;&#34892;&#22120;&#65292;&#35299;&#20915;&#36719;&#25235;&#21462;&#22120;&#20013;&#22810;&#20010;&#33258;&#30001;&#24230;&#30340;&#21327;&#35843;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#19988;&#21327;&#35843;&#30340;&#25235;&#21462;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2406.04666</link><description>&lt;p&gt;
Underactuated Control of Multiple Soft Pneumatic Actuators via Stable Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#21453;&#36716;&#29702;&#35770;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#19968;&#39537;&#21160;&#28304;&#25511;&#21046;&#22810;&#20010;&#36719;&#27668;&#21387;&#25191;&#34892;&#22120;&#65292;&#35299;&#20915;&#36719;&#25235;&#21462;&#22120;&#20013;&#22810;&#20010;&#33258;&#30001;&#24230;&#30340;&#21327;&#35843;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#19988;&#21327;&#35843;&#30340;&#25235;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04666v2 Announce Type: replace  Abstract: Soft grippers, with their inherent compliance and adaptability, show advantages for delicate and versatile manipulation tasks in robotics. This paper presents a novel approach to underactuated control of multiple soft actuators, explicitly focusing on the coordination of soft fingers within a soft gripper. Utilizing a single syringe pump as the actuation mechanism, we address the challenge of coordinating multiple degrees of freedom of a compliant system. The theoretical framework applies concepts from stable inversion theory, adapting them to the unique dynamics of the underactuated soft gripper. Through meticulous mechatronic system design and controller synthesis, we demonstrate the efficacy and applicability of our approach in achieving precise and coordinated manipulation tasks in simulation and experimentation. Our findings not only contribute to the advancement of soft robot control but also offer practical insights into the d
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;VR&#29615;&#22659;&#20013;&#26469;&#25552;&#21319;&#29992;&#25143;&#20132;&#20114;&#21644;&#20219;&#21153;&#25928;&#29575;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;unity&#24341;&#25806;&#21644;&#33258;&#30740;&#30340;VLM&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#35270;&#35273;&#25991;&#26412;&#25351;&#20196;&#30340;&#23454;&#26102;&#12289;&#30452;&#35266;&#29992;&#25143;&#20132;&#20114;&#12290;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#33298;&#36866;&#24230;&#12290;</title><link>https://arxiv.org/abs/2405.11537</link><description>&lt;p&gt;
VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11537
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;VR&#29615;&#22659;&#20013;&#26469;&#25552;&#21319;&#29992;&#25143;&#20132;&#20114;&#21644;&#20219;&#21153;&#25928;&#29575;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;unity&#24341;&#25806;&#21644;&#33258;&#30740;&#30340;VLM&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#35270;&#35273;&#25991;&#26412;&#25351;&#20196;&#30340;&#23454;&#26102;&#12289;&#30452;&#35266;&#29992;&#25143;&#20132;&#20114;&#12290;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#33298;&#36866;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11537v3 Announce Type: replace  Abstract: The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.
&lt;/p&gt;</description></item><item><title>EVE &#26159;&#19968;&#20010; iOS &#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20351;&#29992;&#30452;&#35266;&#30340;&#22686;&#24378;&#29616;&#23454;&#35270;&#35273;&#25928;&#26524;&#28040;&#38500;&#20102;&#21482;&#26377;&#37027;&#20123;&#33021;&#22815;&#35775;&#38382;&#29289;&#29702;&#26426;&#22120;&#20154;&#30340;&#20154;&#25165;&#33021;&#22815;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#22521;&#35757;&#30340;&#38480;&#21046;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#25163;&#37096;&#31227;&#21160;&#30340;&#36335;&#24452;&#28857;&#12289;&#21487;&#35270;&#21270;&#26816;&#26597;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#12289;&#20462;&#25913;&#29616;&#26377;&#30340;&#36335;&#24452;&#28857;&#20197;&#21450;&#39564;&#35777;&#25910;&#38598;&#30340;&#36712;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#22312;&#21253;&#21547;&#19977;&#20010;&#24120;&#35265;&#26700;&#38754;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65288;N=14&#65292;D=30&#65289;&#65292;EVE &#22312;&#25104;&#21151;&#29575;&#21644;&#29289;&#29702;&#31227;&#21160;&#25945;&#20064;&#25216;&#26415;&#30456;&#27604;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2404.06089</link><description>&lt;p&gt;
EVE: Enabling Anyone to Train Robots using Augmented Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.06089
&lt;/p&gt;
&lt;p&gt;
EVE &#26159;&#19968;&#20010; iOS &#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20351;&#29992;&#30452;&#35266;&#30340;&#22686;&#24378;&#29616;&#23454;&#35270;&#35273;&#25928;&#26524;&#28040;&#38500;&#20102;&#21482;&#26377;&#37027;&#20123;&#33021;&#22815;&#35775;&#38382;&#29289;&#29702;&#26426;&#22120;&#20154;&#30340;&#20154;&#25165;&#33021;&#22815;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#22521;&#35757;&#30340;&#38480;&#21046;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#25163;&#37096;&#31227;&#21160;&#30340;&#36335;&#24452;&#28857;&#12289;&#21487;&#35270;&#21270;&#26816;&#26597;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#12289;&#20462;&#25913;&#29616;&#26377;&#30340;&#36335;&#24452;&#28857;&#20197;&#21450;&#39564;&#35777;&#25910;&#38598;&#30340;&#36712;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#22312;&#21253;&#21547;&#19977;&#20010;&#24120;&#35265;&#26700;&#38754;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65288;N=14&#65292;D=30&#65289;&#65292;EVE &#22312;&#25104;&#21151;&#29575;&#21644;&#29289;&#29702;&#31227;&#21160;&#25945;&#20064;&#25216;&#26415;&#30456;&#27604;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.06089v3 Announce Type: replace-cross  Abstract: The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task requires expensive trajectory data where a trained human annotator moves a physical robot to train it. Consequently, only those with access to robots produce demonstrations to train robots. In this work, we remove this restriction with EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations, without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study (N=14, D=30) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically movi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#21464;&#21270;&#33021;&#22815;&#36866;&#24212;&#30340;&#40065;&#26834;&#31070;&#32463;&#25511;&#21046;&#22120;&#21644;&#35777;&#20070;&#12290;&#36890;&#36807;&#30830;&#20445;Lyapunov&#23548;&#25968;&#30340;&#36882;&#20943;&#24615;&#26469;&#20445;&#35777;&#20102;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;Lyapunov&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#35299;&#20915;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20223;&#30495;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.03017</link><description>&lt;p&gt;
Distributionally Robust Policy and Lyapunov-Certificate Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#21464;&#21270;&#33021;&#22815;&#36866;&#24212;&#30340;&#40065;&#26834;&#31070;&#32463;&#25511;&#21046;&#22120;&#21644;&#35777;&#20070;&#12290;&#36890;&#36807;&#30830;&#20445;Lyapunov&#23548;&#25968;&#30340;&#36882;&#20943;&#24615;&#26469;&#20445;&#35777;&#20102;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;Lyapunov&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#35299;&#20915;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20223;&#30495;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.03017v2 Announce Type: replace-cross  Abstract: This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty. A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment. We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied. We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the gl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SSAP&#65288;Shape-Sensitive Adversarial Patch&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20013;&#20840;&#38754;&#30772;&#22351;&#21333;&#30446;Depth&#20272;&#35745;&#65288;MDE&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#24418;&#29366;&#24863;&#30693;&#29305;&#24615;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#25925;&#24847;&#35823;&#23548; MDE &#31995;&#32479;&#65292;&#20351;&#20854;&#22312;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#20013;&#20173;&#28982;&#26377;&#25928;&#12290;&#23454;&#39564;&#34920;&#26126; SSAP &#33021;&#22815;&#35825;&#23548;&#28145;&#24230;&#20272;&#35745;&#38169;&#35823;&#21644;&#34892;&#20026;&#20915;&#31574;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.11515</link><description>&lt;p&gt;
SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SSAP&#65288;Shape-Sensitive Adversarial Patch&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20013;&#20840;&#38754;&#30772;&#22351;&#21333;&#30446;Depth&#20272;&#35745;&#65288;MDE&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#24418;&#29366;&#24863;&#30693;&#29305;&#24615;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#25925;&#24847;&#35823;&#23548; MDE &#31995;&#32479;&#65292;&#20351;&#20854;&#22312;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#20013;&#20173;&#28982;&#26377;&#25928;&#12290;&#23454;&#39564;&#34920;&#26126; SSAP &#33021;&#22815;&#35825;&#23548;&#28145;&#24230;&#20272;&#35745;&#38169;&#35823;&#21644;&#34892;&#20026;&#20915;&#31574;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11515v2 Announce Type: replace-cross  Abstract: Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#21464;&#25442;&#21644;&#20984;&#20248;&#21270;&#31639;&#27861;&#25214;&#21040;&#26368;&#23567;&#21253;&#22260;&#30340;geodesic&#29699;&#65292;&#20174;&#32780;&#31934;&#30830;&#20869;&#36817;&#20284;&#20102;6D&#23039;&#21183;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;minium enclosing geodesic ball&#12290;</title><link>https://arxiv.org/abs/2403.09990</link><description>&lt;p&gt;
CLOSURE: Fast Quantification of Pose Uncertainty Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09990
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#21464;&#25442;&#21644;&#20984;&#20248;&#21270;&#31639;&#27861;&#25214;&#21040;&#26368;&#23567;&#21253;&#22260;&#30340;geodesic&#29699;&#65292;&#20174;&#32780;&#31934;&#30830;&#20869;&#36817;&#20284;&#20102;6D&#23039;&#21183;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;minium enclosing geodesic ball&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09990v3 Announce Type: replace  Abstract: We investigate uncertainty quantification of 6D pose estimation from learned noisy measurements (e.g. keypoints and pose hypotheses). Assuming unknown-but-bounded measurement noises, a pose uncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D poses compatible with the measurements. Despite being simple to formulate and its ability to embed uncertainty, the PURSE is difficult to manipulate and interpret due to the many abstract nonconvex polynomial constraints. An appealing simplification of PURSE is to find its minimum enclosing geodesic ball (MEGB), i.e., a point pose estimation with minimum worst-case error bound. We contribute (i) a geometric interpretation of the nonconvex PURSE, and (ii) a fast algorithm to inner approximate the MEGB. Particularly, we show the PURSE corresponds to the feasible set of a constrained dynamical system or the intersection of multiple geodesic balls, and this perspective allows u
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#23398;&#31185;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#31181;&#27169;&#22411;&#21644;&#25216;&#26415;&#24320;&#21457;&#26356;&#39640;&#25928;&#12289;&#29992;&#25143;&#20307;&#39564;&#26356;&#20248;&#30340;&#31038;&#20250;&#32463;&#27982;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.18294</link><description>&lt;p&gt;
Whole-body Humanoid Robot Locomotion with Human Reference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18294
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#23398;&#31185;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#31181;&#27169;&#22411;&#21644;&#25216;&#26415;&#24320;&#21457;&#26356;&#39640;&#25928;&#12289;&#29992;&#25143;&#20307;&#39564;&#26356;&#20248;&#30340;&#31038;&#20250;&#32463;&#27982;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18294v3 Announce Type: replace  Abstract: Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, "Adam", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the prop
&lt;/p&gt;</description></item><item><title>&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#29289;&#36965;&#25511;&#31574;&#30053;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#24041;&#33180;&#21147;&#25511;&#21046;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24041;&#33180;&#25509;&#35302;&#21147;&#65292;&#30830;&#20445;&#35270;&#32593;&#33180;&#38745;&#33033;&#31359;&#21050;&#25163;&#26415;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18088</link><description>&lt;p&gt;
Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera Force Control: Cooperative vs. Teleoperation Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18088
&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#29289;&#36965;&#25511;&#31574;&#30053;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#24041;&#33180;&#21147;&#25511;&#21046;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24041;&#33180;&#25509;&#35302;&#21147;&#65292;&#30830;&#20445;&#35270;&#32593;&#33180;&#38745;&#33033;&#31359;&#21050;&#25163;&#26415;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18088v2 Announce Type: replace  Abstract: Performing retinal vein cannulation (RVC) as a potential treatment for retinal vein occlusion (RVO) without the assistance of a surgical robotic system is very challenging to do safely. The main limitation is the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not safely minimize the contact force between the surgical instrument and the sclera to prevent tissue damage. Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. This work presents a bi
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#30340;&#19977;&#28857;&#37319;&#26679;&#21644;&#25490;&#24207;&#26426;&#21046;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#27604;&#20363;&#19979;&#23454;&#29616;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16598</link><description>&lt;p&gt;
PCR-99: A Practical Method for Point Cloud Registration with 99 Percent Outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#30340;&#19977;&#28857;&#37319;&#26679;&#21644;&#25490;&#24207;&#26426;&#21046;&#22312;99%&#30340;&#24322;&#24120;&#20540;&#27604;&#20363;&#19979;&#23454;&#29616;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16598v5 Announce Type: replace-cross  Abstract: We propose a robust method for point cloud registration that can handle both unknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a deterministic 3-point sampling approach with two novel mechanisms that significantly boost the speed: (1) an improved ordering of the samples based on pairwise scale consistency, prioritizing the point correspondences that are more likely to be inliers, and (2) an efficient outlier rejection scheme based on triplet scale consistency, prescreening bad samples and reducing the number of hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio, the proposed method achieves comparable performance to the state of the art. At 99% outlier ratio, however, it outperforms the state of the art for both known-scale and unknown-scale problems. Especially for the latter, we observe a clear superiority in terms of robustness and speed.
&lt;/p&gt;</description></item><item><title>&#20107;&#25925;&#34920;&#26126;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#23454;&#38469;&#20132;&#36890;&#20013;&#23384;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#32039;&#24613;&#24773;&#20917;&#30340;&#35782;&#21035;&#21644;&#21709;&#24212;&#23384;&#22312;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
&#20107;&#25925;&#34920;&#26126;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#23454;&#38469;&#20132;&#36890;&#20013;&#23384;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#32039;&#24613;&#24773;&#20917;&#30340;&#35782;&#21035;&#21644;&#21709;&#24212;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.06046v3 Announce Type: replace-cross  Abstract: An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. Is-sues stem not just from the loss events themselves, but also from how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. External investigation reports provide raw material describing the incident and critique the company's response from a regulatory point of view, but exclude safety engineering recommendations from scope. We highlight specific facts and relationships among events by tying together different pieces of the external report material. We then explore safety lessons that might be learned related to: recognizing and responding to nearby mishaps, building an accurate world model of a post-collision scenario, the in-adequa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#30340;&#33258;&#21161;&#26426;&#22120;&#20154;&#32500;&#25252;&#26410;&#26469;&#22826;&#31354;&#26646;&#24687;&#22320;&#30340;&#22330;&#26223;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#38590;&#20197;&#27880;&#24847;&#21040;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#21253;&#25324;&#21010;&#30165;&#12289;&#28784;&#23576;&#12289;&#20197;&#21450;&#32467;&#26500;&#37096;&#20214;&#30340;&#21464;&#21160;&#65292;&#29978;&#33267;&#21487;&#20197;&#26816;&#27979;&#21040;&#32454;&#24494;&#30340;&#32467;&#26500;&#31227;&#21160;&#21644;&#25439;&#22351;&#12290;</title><link>https://arxiv.org/abs/2312.02396</link><description>&lt;p&gt;
Unsupervised Change Detection for Space Habitats Using 3D Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02396
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#30340;&#33258;&#21161;&#26426;&#22120;&#20154;&#32500;&#25252;&#26410;&#26469;&#22826;&#31354;&#26646;&#24687;&#22320;&#30340;&#22330;&#26223;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#38590;&#20197;&#27880;&#24847;&#21040;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#21253;&#25324;&#21010;&#30165;&#12289;&#28784;&#23576;&#12289;&#20197;&#21450;&#32467;&#26500;&#37096;&#20214;&#30340;&#21464;&#21160;&#65292;&#29978;&#33267;&#21487;&#20197;&#26816;&#27979;&#21040;&#32454;&#24494;&#30340;&#32467;&#26500;&#31227;&#21160;&#21644;&#25439;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02396v3 Announce Type: replace  Abstract: This work presents an algorithm for scene change detection from point clouds to enable autonomous robotic caretaking in future space habitats. Autonomous robotic systems will help maintain future deep-space habitats, such as the Gateway space station, which will be uncrewed for extended periods. Existing scene analysis software used on the International Space Station (ISS) relies on manually-labeled images for detecting changes. In contrast, the algorithm presented in this work uses raw, unlabeled point clouds as inputs. The algorithm first applies modified Expectation-Maximization Gaussian Mixture Model (GMM) clustering to two input point clouds. It then performs change detection by comparing the GMMs using the Earth Mover's Distance. The algorithm is validated quantitatively and qualitatively using a test dataset collected by an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth images taken directly by Astro
&lt;/p&gt;</description></item></channel></rss>