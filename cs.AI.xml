<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#33258;&#25945;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#31867;&#27880;&#37322;&#65292;&#22823;&#24133;&#24230;&#25552;&#21319;&#20102;Llama3-70B-Instruct&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;RewardBench&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2408.02666</link><description>&lt;p&gt;
&#33258;&#25945;&#35009;&#21028;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#33258;&#25945;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#31867;&#27880;&#37322;&#65292;&#22823;&#24133;&#24230;&#25552;&#21319;&#20102;Llama3-70B-Instruct&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;RewardBench&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02666v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26159;&#25104;&#21151;&#27169;&#22411;&#24320;&#21457;&#30340;&#26680;&#24515;&#8212;&#8212;&#20316;&#20026;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#21450;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#30340;&#26367;&#20195;&#21697;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#35009;&#21028;&#65292;&#20256;&#32479;&#30340;&#20570;&#27861;&#26159;&#25910;&#38598;&#22823;&#37327;&#20154;&#31867;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#20559;&#22909;&#21028;&#26029;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#32791;&#21147;&#65292;&#22240;&#20026;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#25968;&#25454;&#20063;&#20250;&#21464;&#24471;&#36807;&#26102;&#12290;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25913;&#21892;&#35009;&#21028;&#30340;&#31574;&#30053;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#26410;&#26631;&#27880;&#30340;&#25351;&#20196;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#36845;&#20195;&#33258;&#25913;&#36827;&#26041;&#26696;&#29983;&#25104;&#23545;&#27604;&#24615;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#35009;&#21028;&#65292;&#20197;&#20415;&#29983;&#25104;&#25512;&#29702;&#36712;&#36857;&#21644;&#26368;&#32456;&#21028;&#26029;&#65292;&#22312;&#27599;&#20010;&#26032;&#36845;&#20195;&#20013;&#20351;&#29992;&#25913;&#36827;&#30340;&#39044;&#27979;&#36827;&#34892;&#37325;&#22797;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#33258;&#25945;&#35009;&#21028;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#27880;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#23601;&#21487;&#20197;&#23558;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65288;Llama3-70B-Instruct&#65289;&#30340;&#24615;&#33021;&#20174;75.4&#25552;&#39640;&#21040;88.3&#65288;&#21152;&#20837;&#22810;&#25968;&#25237;&#31080;&#65292;&#21017;&#20026;88.7&#65289;&#22312;RewardBench&#19978;&#12290;&#36825;&#36229;&#36807;&#20102;&#24120;&#29992;LLM&#35780;&#21028;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#26377;&#28508;&#21147;&#33410;&#30465;&#25104;&#26412;&#24182;&#24212;&#23545;&#27169;&#22411;&#19981;&#26029;&#36827;&#27493;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02666v1 Announce Type: cross  Abstract: Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM jud
&lt;/p&gt;</description></item><item><title>SEAS&#26694;&#26550;&#36890;&#36807;&#33258;&#25105;&#28436;&#21270;&#23545;&#25239;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32418;&#38431;&#28436;&#32451;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02632</link><description>&lt;p&gt;
SEAS&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#28436;&#21270;&#23545;&#25239;&#23433;&#20840;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02632
&lt;/p&gt;
&lt;p&gt;
SEAS&#26694;&#26550;&#36890;&#36807;&#33258;&#25105;&#28436;&#21270;&#23545;&#25239;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32418;&#38431;&#28436;&#32451;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02632v1 &#39044;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#35793;&#25991;&#25688;&#35201;&#65306;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33021;&#21147;&#21644;&#24433;&#21709;&#21147;&#26041;&#38754;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#38450;&#27490;&#26377;&#23475;&#36755;&#20986;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340; promising&#65288;&#26377;&#24076;&#26395;&#30340;&#65289;&#26041;&#27861;&#26159;&#35757;&#32451;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#28436;&#32451;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;LLMs&#28431;&#27934;&#30340;&#28436;&#21464;&#24494;&#22937;&#24615;&#25361;&#25112;&#20102;&#24403;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#19987;&#38376;&#38024;&#23545;&#24182;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#30340;&#24369;&#28857;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;SEAS&#65306;&#33258;&#25105;&#28436;&#21270;&#23545;&#25239;&#23433;&#20840;&#20248;&#21270;&#8221;&#65288;Self-Evolving Adversarial Safety Optimization&#65289;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#12290;SEAS&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#30340;&#38454;&#27573;&#65306;&#21021;&#22987;&#21270;&#12289;&#25915;&#20987;&#21644;&#23545;&#25239;&#24615;&#20248;&#21270;&#65292;&#26469;&#25913;&#36827;&#32418;&#38431;&#21644;&#30446;&#26631;&#27169;&#22411;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26412;&#26694;&#26550;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02632v1 Announce Type: cross  Abstract: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving }\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#21521;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20998;&#25968;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20026;RL&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#30340;&#38271;&#24207;&#21015;&#20132;&#20114;&#25552;&#20379;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2408.02606</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#35859;&#35789;&#30340;&#36870;&#21521;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Backward explanations via redefinition of predicates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#21521;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20998;&#25968;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20026;RL&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#30340;&#38271;&#24207;&#21015;&#20132;&#20114;&#25552;&#20379;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02606v1 &#36890;&#21578;&#31867;&#22411;&#65306; &#26032; &#25688;&#35201;&#65306; &#22522;&#20110;&#35859;&#35789;&#30340;&#21382;&#21490;&#35299;&#37322;&#65288;HXP&#65289;&#30740;&#31350;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20132;&#20114;&#34892;&#20026;&#65288;&#21382;&#21490;&#65289;&#65292;&#24182;&#36890;&#36807;&#20219;&#24847;&#35859;&#35789;&#26469;&#35266;&#23519;&#20854;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#20026;&#21382;&#21490;&#20013;&#30340;&#27599;&#20010;&#21160;&#20316;&#35745;&#31639;&#19968;&#20010;&#21160;&#20316;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#35299;&#37322;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#21521;&#29992;&#25143;&#26174;&#31034;&#26368;&#37325;&#35201;&#30340;&#21160;&#20316;&#12290;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#37325;&#35201;&#24615;&#20998;&#25968;&#26159;&#19968;&#32452;#W[1] hardness&#38382;&#39064;&#65292;&#23545;&#20110;&#38271;&#21382;&#21490;&#65292;&#25105;&#20204;&#38656;&#35201;&#36817;&#20284;&#20998;&#25968;&#65292;&#20197;&#29306;&#29298;&#20998;&#25968;&#36136;&#37327;&#20026;&#20195;&#20215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HXP&#26041;&#27861;&#65292;&#31216;&#20026;&#21453;&#21521;HXP&#65292;&#33021;&#22815;&#22312;&#19981;&#36817;&#20284;&#20998;&#25968;&#30340;&#24773;&#20917;&#19979;&#20026;&#36825;&#20123;&#21382;&#21490;&#25552;&#20379;&#35299;&#37322;&#12290;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;B-HXP&#26041;&#27861;&#25551;&#36848;&#38271;&#21382;&#21490;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02606v1 Announce Type: new  Abstract: History eXplanation based on Predicates (HXP), studies the behavior of a Reinforcement Learning (RL) agent in a sequence of agent's interactions with the environment (a history), through the prism of an arbitrary predicate. To this end, an action importance score is computed for each action in the history. The explanation consists in displaying the most important actions to the user. As the calculation of an action's importance is #W[1]-hard, it is necessary for long histories to approximate the scores, at the expense of their quality. We therefore propose a new HXP method, called Backward-HXP, to provide explanations for these histories without having to approximate scores. Experiments show the ability of B-HXP to summarise long histories.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#25991;&#25551;&#36848;&#26469;&#25429;&#25417;&#21644;&#26816;&#27979;&#35773;&#21050;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#25991;&#19981;&#21305;&#37197;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02595</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#25991;&#25551;&#36848;&#25552;&#21462;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#20013;&#22686;&#24378;&#30340;&#22810;&#23618;&#32423;&#36328;&#27169;&#24577;&#35821;&#20041;&#19981;&#21305;&#37197;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#35270;&#35273;&#35821;&#20041;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#25991;&#25551;&#36848;&#26469;&#25429;&#25417;&#21644;&#26816;&#27979;&#35773;&#21050;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#25991;&#19981;&#21305;&#37197;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02595v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#35773;&#21050;&#26159;&#19968;&#31181;&#35773;&#21050;&#65292;&#20197;&#20854;&#22266;&#26377;&#30340;&#23383;&#38754;&#35299;&#35835;&#19982;&#24847;&#22270;&#20869;&#28085;&#20043;&#38388;&#30340;&#24046;&#24322;&#20026;&#29305;&#24449;&#12290;&#23613;&#31649;&#25991;&#26412;&#35773;&#21050;&#26816;&#27979;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#35752;&#65292;&#20294;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#20165;&#20973;&#25991;&#26412;&#36755;&#20837;&#21487;&#33021;&#19981;&#36275;&#20197;&#35299;&#35835;&#35773;&#21050;&#12290;&#20026;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#26377;&#25928;&#22320;&#35782;&#21035;&#35773;&#21050;&#65292;&#24517;&#39035;&#21253;&#21547;&#39069;&#22806;&#30340;&#24773;&#22659;&#32447;&#32034;&#65292;&#22914;&#22270;&#29255;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#36755;&#20837;&#19977;&#20803;&#32452;&#12290;&#36825;&#20123;&#19977;&#20803;&#32452;&#20013;&#30340;&#20004;&#20010;&#32452;&#25104;&#20803;&#32032;&#26159;&#36755;&#20837;&#25991;&#26412;&#21450;&#20854;&#20851;&#32852;&#30340;&#22270;&#29255;&#65292;&#22914;&#25968;&#25454;&#38598;&#20013;&#25152;&#25552;&#20379;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34917;&#20805;&#27169;&#24577;&#65292;&#21363;&#25551;&#36848;&#24615;&#30340;&#22270;&#29255;&#25551;&#36848;&#12290;&#24341;&#20837;&#36825;&#31181;&#35270;&#35273;&#35821;&#20041;&#34920;&#31034;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#25429;&#33719;&#25991;&#26412;&#19982;&#35270;&#35273;&#20869;&#23481;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#36825;&#23545;&#20110;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#26469;&#35828;&#26159;&#26681;&#26412;&#30340;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#26356;&#21152;&#25935;&#24863;&#22320;&#25429;&#25417;&#22270;&#25991;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#24341;&#20154;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#25991;&#26412;&#27169;&#22359;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;Facebook&#21644;Instagram&#19978;&#20998;&#20139;&#30340;&#22270;&#29255;&#21644;&#22270;&#25991;&#24086;&#23376;&#20013;&#21152;&#20837;&#25551;&#36848;&#24615;&#22270;&#29255;&#25551;&#36848;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22810;&#23618;&#32423;&#35821;&#20041;&#34920;&#31034;&#26469;&#25429;&#25417;&#35773;&#21050;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20854;&#20013;&#30340;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02595v1 Announce Type: new  Abstract: Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#23558;&#22312;&#29305;&#23450;&#39046;&#22495;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#23454;&#38469;&#24615;&#33021;&#25552;&#21319;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2408.02584</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#65306;&#38024;&#23545;&#39640;&#36136;&#37327;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#30340;&#39640;&#36136;&#37327;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#23558;&#22312;&#29305;&#23450;&#39046;&#22495;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#23454;&#38469;&#24615;&#33021;&#25552;&#21319;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02584v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20449;&#24687;&#25968;&#23383;&#21270;&#26102;&#20195;&#30340;&#19981;&#26029;&#22686;&#38271;&#36843;&#20999;&#38656;&#35201;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#29992;&#25143;&#20174;&#38271;&#31687;&#25991;&#31456;&#20013;&#25552;&#21462;&#20851;&#38190;&#27934;&#23519;&#12290;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#37325;&#28857;&#20851;&#27880;&#29305;&#23450;&#26041;&#38754;&#22312;&#25991;&#20214;&#20869;&#30340;&#25688;&#35201;&#12290;&#23613;&#31649;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#30740;&#31350;&#22312;&#24635;&#32467;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#27493;&#65292;&#20294;&#25105;&#20204;&#20173;&#22312;&#19981;&#26029;&#23547;&#27714;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19981;&#21516;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#38761;&#21629;&#24615;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#24635;&#32467;&#38382;&#39064;&#20013;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24494;&#35843;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20844;&#24320;&#28304;&#22522;&#30784;LLMs&#65292;&#22914;Llama2&#12289;Mistral&#12289;Gemma&#21644;Aya&#30340;&#24494;&#35843;&#23545;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#22522;&#20110;&#23646;&#24615;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#36825;&#31181;&#26041;&#27861;&#23558;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#19982;&#23646;&#24615;&#30340;&#30456;&#20851;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02584v1 Announce Type: cross  Abstract: The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38590;&#20197;&#28041;&#36275;&#30340;&#21475;&#38899;&#35821;&#38899;&#35782;&#21035;&#30340;&#20844;&#24179;&#24615;&#31995;&#32479;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#38750;&#20856;&#22411;&#21475;&#38899;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02582</link><description>&lt;p&gt;
&#24102;&#26377;&#25276;&#38901;&#30340;&#35821;&#38899;&#32858;&#31867;&#21644;&#25366;&#25496;&#23545;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#35821;&#38899;&#35782;&#21035;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38590;&#20197;&#28041;&#36275;&#30340;&#21475;&#38899;&#35821;&#38899;&#35782;&#21035;&#30340;&#20844;&#24179;&#24615;&#31995;&#32479;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#38750;&#20856;&#22411;&#21475;&#38899;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02582v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185;&#25688;&#35201;: &#29616;&#20195;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#36890;&#24120;&#26159;&#22312;&#36229;&#36807;&#25968;&#19975;&#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#21462;&#24471;&#30340;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#20998;&#24067;&#36890;&#24120;&#20542;&#21521;&#20110;&#24179;&#34913;&#25110;&#32773;&#20856;&#22411;&#35821;&#38899;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#20856;&#22411;&#21475;&#38899;&#20197;&#22806;&#30340;&#38750;&#20856;&#22411;&#21475;&#38899;&#30340;&#35821;&#38899;&#35782;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#20542;&#26012;&#35821;&#38899;&#32858;&#31867;&#21644;&#25366;&#25496;&#26041;&#27861;&#26469;&#25171;&#36896;&#38024;&#23545;&#38590;&#20197;&#28041;&#36275;&#30340;&#21475;&#38899;&#35821;&#38899;&#35782;&#21035;&#30340;&#20844;&#24179;&#31995;&#32479;&#12290;&#23545;&#20110;&#21475;&#38899;&#35782;&#21035;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#37327;&#23567;&#25110;&#19981;&#24179;&#34913;&#30340;&#30417;&#30563;&#21475;&#38899;&#25968;&#25454;&#65306;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#30340;&#25968;&#25454;&#39044;&#35757;&#32451;&#12289;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;(DRO)&#21450;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#23545;&#20110;&#26657;&#27491;&#23567;&#35268;&#27169;&#21644;&#19981;&#24179;&#34913;&#21475;&#38899;&#35821;&#38899;&#30340;&#21475;&#38899;&#35782;&#21035;&#27169;&#22411;&#34920;&#29616;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21360;&#24230;&#21475;&#38899;&#35821;&#38899;&#19978;&#23545;ASR&#36827;&#34892;&#24494;&#35843;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#36825;&#34920;&#26126;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;&#35782;&#21035;&#27867;&#21270;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02582v1 Announce Type: cross  Abstract: Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical speech patterns. As a result, those systems often poorly perform on atypical accented speech. In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech. For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech. Fine-tuning ASR on the mined Indian accent speech using the proposed superv
&lt;/p&gt;</description></item><item><title>&#22312;&#8220;&#25346;&#25119;&#20025;&#8221;&#22810;&#29609;&#23478;&#21512;&#20316;&#28216;&#25103;&#20013;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#20195;&#29702;&#22914;&#20309;&#22522;&#20110;&#24515;&#26234;&#29702;&#35770;&#36866;&#24212;&#24212;&#23545;&#19981;&#21516;&#23545;&#25163;&#31574;&#30053;&#65292;&#23613;&#31649;&#20419;&#36827;&#20102;&#33391;&#22909;&#21512;&#20316;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#29305;&#23450;&#23545;&#25163;&#31574;&#30053;&#21644;&#28216;&#25103;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#23637;&#29616;&#20986;&#20102;&#25913;&#36827;LLM&#25512;&#29702;&#33021;&#21147;&#20197;&#35299;&#20915;&#20449;&#24687;&#19981;&#23436;&#32654;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02559</link><description>&lt;p&gt;
&#22522;&#20110;&#24515;&#26234;&#29702;&#35770;&#30340;LLM&#20195;&#29702;&#35780;&#20272;&#19982;&#22686;&#24378;&#65306;&#22312;&#8220;&#25346;&#25119;&#20025;&#8221;&#22810;&#29609;&#23478;&#21512;&#20316;&#28216;&#25103;&#20013;&#23545;&#19981;&#23436;&#32654;&#20449;&#24687;&#30340;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02559
&lt;/p&gt;
&lt;p&gt;
&#22312;&#8220;&#25346;&#25119;&#20025;&#8221;&#22810;&#29609;&#23478;&#21512;&#20316;&#28216;&#25103;&#20013;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#20195;&#29702;&#22914;&#20309;&#22522;&#20110;&#24515;&#26234;&#29702;&#35770;&#36866;&#24212;&#24212;&#23545;&#19981;&#21516;&#23545;&#25163;&#31574;&#30053;&#65292;&#23613;&#31649;&#20419;&#36827;&#20102;&#33391;&#22909;&#21512;&#20316;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#29305;&#23450;&#23545;&#25163;&#31574;&#30053;&#21644;&#28216;&#25103;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#23637;&#29616;&#20986;&#20102;&#25913;&#36827;LLM&#25512;&#29702;&#33021;&#21147;&#20197;&#35299;&#20915;&#20449;&#24687;&#19981;&#23436;&#32654;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02559v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#24102;&#26377;&#19981;&#23436;&#32654;&#20449;&#24687;&#30340;&#31616;&#21333;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#12289;&#24102;&#26377;&#19981;&#23436;&#32654;&#20449;&#24687;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#20195;&#29702;&#21327;&#35843;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#19982;&#20182;&#20154;&#21512;&#20316;&#23545;&#25239;&#20195;&#29702;&#30340;&#33021;&#21147;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#28304;&#21644;&#22522;&#20110;API&#30340;LLMs&#22312;&#22797;&#26434;&#30340;&#12289;&#38656;&#35201;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#29615;&#22659;&#20013;&#36827;&#34892;&#21512;&#20316;&#30340;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#20351;&#29992;&#20854;&#20182;&#31867;&#22411;&#20195;&#29702;&#30340;&#29616;&#26377;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#35268;&#21010;&#25216;&#26415;&#65292;&#20801;&#35768;LLM&#20195;&#29702;&#36890;&#36807;&#20165;&#20351;&#29992;&#28216;&#25103;&#35268;&#21017;&#12289;&#24403;&#21069;&#29366;&#24577;&#21644;&#21382;&#21490;&#19978;&#19979;&#25991;&#20316;&#20026;&#36755;&#20837;&#26469;&#36866;&#24212;&#23545;&#25239;&#21508;&#31181;&#23545;&#25163;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22806;&#37096;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#31181;&#25169;&#20811;&#28216;&#25103;&#20013;&#21160;&#24577;&#21644;&#24191;&#27867;&#30340;&#34892;&#21160;&#31354;&#38388;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LLM&#20195;&#29702;&#33021;&#22815;&#20165;&#20973;&#24120;&#35782;&#32780;&#25104;&#21151;&#65292;&#20294;&#20182;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#20381;&#36182;&#20110;&#20855;&#20307;&#30340;&#23545;&#25163;&#31574;&#30053;&#21644;&#28216;&#25103; dynamics&#65292;&#36825;&#34920;&#26126;&#20102;&#25913;&#36827;LLM&#25512;&#29702;&#21644;&#24212;&#23545;&#19981;&#23436;&#32654;&#20449;&#24687;&#25361;&#25112;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36825;&#26159;&#30740;&#31350;&#20154;&#26426;&#21512;&#20316;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#30740;&#31350;&#65292;&#26410;&#26469;&#24037;&#20316;&#23558;&#38598;&#20013;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#21644;&#25193;&#23637;&#21040;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#30340;&#22810;&#29609;&#23478;&#25991;&#26412;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02559v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that alth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#24615;&#32908;&#32905;&#32593;&#32476;&#30340;&#25163;&#21183;&#24863;&#30693;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#39057;&#29305;&#24449;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02547</link><description>&lt;p&gt;
&#21151;&#33021;&#24615;&#32908;&#32905;&#32593;&#32476;&#22312;&#25552;&#21319;&#25163;&#21183;&#24863;&#30693;&#31934;&#24230;&#20013;&#30340;&#20316;&#29992;&#23545;&#20154;&#31867;&#26426;&#22120;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#24615;&#32908;&#32905;&#32593;&#32476;&#30340;&#25163;&#21183;&#24863;&#30693;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#22320;&#25552;&#21462;&#26102;&#39057;&#29305;&#24449;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02547v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20934;&#30830;&#30340;&#25163;&#21183;&#24863;&#30693;&#27169;&#22411;&#23545;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#30452;&#25509;&#24433;&#21709;&#21040;&#31070;&#32463;&#26426;&#22120;&#20154;&#23398;&#21644;&#20114;&#21160;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#22240;&#20854;&#20016;&#23500;&#30340;&#20449;&#24687;&#19978;&#19979;&#25991;&#21644;&#19982;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21450;&#21487;&#31359;&#25140;&#31995;&#32479;&#30340;&#20860;&#23481;&#24615;&#32780;&#34987;&#25506;&#32034;&#65292;&#29992;&#20197;&#25552;&#39640;&#21508;&#31181;&#22522;&#20110;sEMG&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#24863;&#30693;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#23545;&#20110;&#20351;&#29992;sEMG&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#30340;&#40065;&#26834;&#24615;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#39640;&#25928;&#24615;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#38656;&#35201;&#36739;&#24378;&#30340;&#35745;&#31639;&#33021;&#21147;&#12289;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#20302;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#20174;&#32908;&#32905;&#21516;&#27493;&#32780;&#38750;&#32908;&#32905;&#20010;&#20307;&#28608;&#27963;&#20013;&#35299;&#30721;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#21151;&#33021;&#24615;&#32908;&#32905;&#32593;&#32476;&#32972;&#21518;&#30340;&#32908;&#32905;&#21516;&#27493;&#36827;&#34892;&#20102;&#19968;&#23450;&#31243;&#24230;&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#21151;&#33021;&#32908;&#32905;&#32593;&#32476;&#30340;&#25163;&#21183;&#24863;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32908;&#32905;&#38388;&#30340;&#21516;&#27493;&#21644;&#32908;&#32905;&#36830;&#25509;&#20114;&#32852;&#30340;&#22270;&#32593;&#32476;&#20316;&#20026;&#25163;&#37096;&#31070;&#32463;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39640;&#25928;&#30340;&#26102;&#39057;&#29305;&#24449;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25216;&#26415;&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#20102;&#31361;&#26174;&#30340;&#25552;&#21319;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#26174;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02547v1 Announce Type: new  Abstract: Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots. Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems. The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions. This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation. We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle con
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.02529</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Shapley Values for Explaining Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02529
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02529v1 Announce Type: new  Abstract: This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the ``Counterfactual Difference Characteristic Value" and the ``Average Counterfactual Difference Characteristic Value." These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;PredicTaps&#65292;&#21487;&#20197;&#22312;&#21333;&#20987;&#25110;&#21452;&#20987;&#20986;&#29616;&#20043;&#21069;&#39044;&#27979;&#26816;&#27979;&#21040;&#30340;&#28857;&#20987;&#26159;&#21333;&#20987;&#36824;&#26159;&#21452;&#20987;&#30340;&#31532;&#19968;&#25509;&#35302;&#28857;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#22312;&#35302;&#25720;&#23631;&#35774;&#22791;&#19978;&#30340;&#21333;&#20987;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2408.02525</link><description>&lt;p&gt;
&#21333;&#20987;&#24310;&#36831;&#20943;&#23569;&#19982;&#21333;&#20987;&#25110;&#21452;&#20987;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Single-tap Latency Reduction with Single- or Double- tap Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;PredicTaps&#65292;&#21487;&#20197;&#22312;&#21333;&#20987;&#25110;&#21452;&#20987;&#20986;&#29616;&#20043;&#21069;&#39044;&#27979;&#26816;&#27979;&#21040;&#30340;&#28857;&#20987;&#26159;&#21333;&#20987;&#36824;&#26159;&#21452;&#20987;&#30340;&#31532;&#19968;&#25509;&#35302;&#28857;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#22312;&#35302;&#25720;&#23631;&#35774;&#22791;&#19978;&#30340;&#21333;&#20987;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#26234;&#33021;&#25163;&#26426;&#12289;&#24179;&#26495;&#30005;&#33041;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#65288;&#35302;&#25511;&#26495;&#65289;&#19978;&#24191;&#27867;&#20351;&#29992;&#35302;&#25720;&#34920;&#38754;&#65292;&#21333;&#20987;&#21644;&#21452;&#20987;&#26159;&#26368;&#22522;&#26412;&#30340;&#20063;&#26159;&#26368;&#24120;&#29992;&#30340;&#25805;&#20316;&#12290;&#21333;&#25110;&#21452;&#20987;&#30340;&#26816;&#27979;&#23548;&#33268;&#21333;&#20987;&#24310;&#36831;&#38382;&#39064;&#65292;&#36825;&#22312;&#35302;&#25511;&#36755;&#20837;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#24418;&#25104;&#20102;&#19968;&#20010;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#21333;&#20987;&#24310;&#36831;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28857;&#20987;&#39044;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;PredicTaps&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#31561;&#24453;&#20256;&#32479;&#30340;&#25968;&#30334;&#27627;&#31186;&#65292;&#23601;&#33021;&#39044;&#27979;&#26816;&#27979;&#21040;&#30340;&#28857;&#20987;&#26159;&#21333;&#20987;&#36824;&#26159;&#21452;&#20987;&#30340;&#31532;&#19968;&#25509;&#35302;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#35780;&#20272;&#21644;&#19968;&#20010;&#29992;&#25143;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#28857;&#20987;&#24773;&#20917;&#19979;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#22312;&#19981;&#21516;&#24418;&#24335;&#22240;&#23376;&#30340;&#35302;&#25720;&#26495;&#21644;&#24179;&#26495;&#30005;&#33041;&#19978;&#30340;&#21487;&#29992;&#24615;&#65288;&#35302;&#25720;&#26495;&#21644;&#24179;&#26495;&#30005;&#33041;&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PredicTaps&#22312;&#19981;&#38477;&#20302;&#21487;&#29992;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#31508;&#35760;&#26412;&#30005;&#33041;&#30340;&#21333;&#20987;&#24310;&#36831;&#20174;150-500&#27627;&#31186;&#38477;&#20302;&#21040;12&#27627;&#31186;&#65292;&#23558;&#26234;&#33021;&#25163;&#26426;&#30340;&#21333;&#20987;&#24310;&#36831;&#38477;&#20302;&#21040;17.6&#27627;&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02525v1 Announce Type: cross  Abstract: Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops (touchpad), and single and double taps are the most basic and common operations on them. The detection of single or double taps causes the single-tap latency problem, which creates a bottleneck in terms of the sensitivity of touch inputs. To reduce the single-tap latency, we propose a novel machine-learning-based tap prediction method called PredicTaps. Our method predicts whether a detected tap is a single tap or the first contact of a double tap without having to wait for the hundreds of milliseconds conventionally required. We present three evaluations and one user evaluation that demonstrate its broad applicability and usability for various tap situations on two form factors (touchpad and smartphone). The results showed PredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops and to 17.6 ms on smartphones without reducing usability.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21463;&#29256;&#26435;&#20445;&#25252;&#20195;&#30721;&#29983;&#25104;&#30340;&#35768;&#21487;&#35777;&#21512;&#35268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#22522;&#20934;LiCoEval&#12290;</title><link>https://arxiv.org/abs/2408.02487</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#30721;&#29983;&#25104;&#21151;&#33021;&#30340;&#35768;&#21487;&#35777;&#21512;&#35268;&#24615;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at License Compliance Capability of LLMs in Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21463;&#29256;&#26435;&#20445;&#25252;&#20195;&#30721;&#29983;&#25104;&#30340;&#35768;&#21487;&#35777;&#21512;&#35268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#22522;&#20934;LiCoEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02487v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02487v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. To establish this benchmark, we conduct an empirical study to identify a reasonable standard for "striking similarity" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code. Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#65288;&#21487;&#33021;&#20855;&#26377;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#28508;&#21147;&#65289;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.02479</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#20214;&#24037;&#31243;&#20195;&#29702;&#65306;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#65288;&#21487;&#33021;&#20855;&#26377;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#28508;&#21147;&#65289;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02479v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#25506;&#32034;&#20854;&#22312;&#21508;&#20010;&#22402;&#30452;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#36719;&#20214;&#24037;&#31243;&#12290;LLMs&#24050;&#22312;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#21644;&#28431;&#27934;&#26816;&#27979;&#22312;&#20869;&#30340;&#39046;&#22495;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#34920;&#29616;&#20986;&#35768;&#22810;&#38480;&#21046;&#21644;&#19981;&#36275;&#12290;LLM-based&#20195;&#29702;&#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#20855;&#26377;&#25104;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#21147;&#65292;&#23427;&#23558;LLM&#20316;&#20026;&#26680;&#24515;&#36827;&#34892;&#20915;&#31574;&#21644;&#34892;&#21160;&#65292;&#35299;&#20915;&#20102;LLM&#32570;&#20047;&#33258;&#20027;&#24615;&#21644;&#33258;&#25105;&#25913;&#36827;&#30340;&#19968;&#20123;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#21644;&#23567;&#32467;&#25506;&#35752;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#20351;&#29992;LLM&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;LLM&#21644;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20043;&#38388;&#21306;&#21035;&#30340;&#26126;&#30830;&#21306;&#20998;&#12290;&#35813;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#20854;&#32479;&#19968;&#26631;&#20934;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#36825;&#20123;&#26631;&#20934;&#21644;&#27979;&#35797;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;LLM&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#21487;&#20197;&#34987;&#35748;&#21487;&#20026;&#19968;&#20010;&#22312;&#23427;&#30340;&#39046;&#22495;&#20013;&#30340;LLM-based&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#24191;&#27867;&#22320;&#30740;&#31350;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#20197;&#21450;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;LLM-based&#20195;&#29702;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02479v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;AI&#22312;&#22522;&#20110;&#30005;&#24433;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20998;&#21106;&#20013;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#21407;&#22240;&#65292;&#20197;&#20415;&#22312;&#26410;&#26469;&#25913;&#36827;&#27169;&#22411;&#20197;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2408.02462</link><description>&lt;p&gt;
&#20851;&#20110;AI&#22522;&#20110;&#30005;&#24433;CMR&#20998;&#21106;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An investigation into the causes of race bias in AI-based cine CMR segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;AI&#22312;&#22522;&#20110;&#30005;&#24433;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20998;&#21106;&#20013;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#21407;&#22240;&#65292;&#20197;&#20415;&#22312;&#26410;&#26469;&#25913;&#36827;&#27169;&#22411;&#20197;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02462v1 Announce Type: cross &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#33258;&#21160;&#20998;&#21106;&#30005;&#24433;&#24515;&#33039;&#30913;&#20849;&#25391;&#65288;CMR&#65289;&#25104;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#65292;&#21363;&#23427;&#20204;&#26681;&#25454;&#29992;&#20110;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#25968;&#25454;&#30340;&#24179;&#34913;&#65288;&#19981;&#24179;&#34913;&#65289;&#27700;&#24179;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#26469;&#28304;&#65292;&#23547;&#27714;&#29702;&#35299;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#21152;&#20197;&#32531;&#35299;&#12290;&#25105;&#20204;&#22312;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#25910;&#38598;&#30340;&#40657;&#20154;&#21644;&#30333;&#20154;&#21463;&#32773;&#30340;&#30701;&#36724;&#30005;&#24433;CMR&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#21106;&#23454;&#39564;&#65292;&#24182;&#24212;&#29992;&#20102;AI&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#29702;&#35299;&#32467;&#26524;&#12290;&#22312;&#20998;&#31867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20174;&#22270;&#20687;&#20013;&#29420;&#33258;&#39044;&#27979;&#31181;&#26063;&#65292;&#21457;&#29616;&#20934;&#30830;&#29575;&#24456;&#39640;&#65292;&#20294;&#20174; ground truth &#20998;&#21106;&#20013;&#39044;&#27979;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#31181;&#26063;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#36825;&#36890;&#24120;&#26159;&#23548;&#33268;AI&#27169;&#22411;&#24615;&#33021;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;ent 10.17632/ngpcgbvrb.2.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02462v1 Announce Type: cross  Abstract: Artificial intelligence (AI) methods are being used increasingly for the automated segmentation of cine cardiac magnetic resonance (CMR) imaging. However, these methods have been shown to be subject to race bias, i.e. they exhibit different levels of performance for different races depending on the (im)balance of the data used to train the AI model. In this paper we investigate the source of this bias, seeking to understand its root cause(s) so that it can be effectively mitigated. We perform a series of classification and segmentation experiments on short-axis cine CMR images acquired from Black and White subjects from the UK Biobank and apply AI interpretability methods to understand the results. In the classification experiments, we found that race can be predicted with high accuracy from the images alone, but less accurately from ground truth segmentations, suggesting that the distributional shift between races, which is often the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GATH&#65292;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#24322;&#26500;&#30693;&#35782;&#22270;&#23436;&#22791;&#24615;&#30340;&#26032;&#22411;&#22522;&#20110;GAT&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20004;&#20010;&#29420;&#31435;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#39044;&#27979;&#32570;&#22833;&#23454;&#20307;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02456</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#39062;&#30340;GAT&#65288;Graph Attention Network&#65289;&#26041;&#27861;&#22686;&#24378;&#24322;&#26500;&#30693;&#35782;&#22270; completion
&lt;/p&gt;
&lt;p&gt;
Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GATH&#65292;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#24322;&#26500;&#30693;&#35782;&#22270;&#23436;&#22791;&#24615;&#30340;&#26032;&#22411;&#22522;&#20110;GAT&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20004;&#20010;&#29420;&#31435;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#39044;&#27979;&#32570;&#22833;&#23454;&#20307;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#22312;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#29575;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#38543;&#30528;KG&#22823;&#23567;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#23427;&#20204;&#21464;&#24471;&#19981;&#20934;&#30830;&#19988;&#19981;&#23436;&#25972;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30693;&#35782;&#22270; completion&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#22522;&#20110;GAT&#30340;&#26041;&#27861;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAT&#30340;&#30693;&#35782;&#22270; completion&#26041;&#27861;&#22312;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#22270;&#26102;&#32463;&#24120;&#36973;&#21463;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#26679;&#26412;&#25968;&#37327;&#19981;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#39044;&#27979;&#20849;&#20139;&#21516;&#19968;&#36335;&#24452;&#21644;&#22836;&#37096;&#65288;&#23614;&#37096;&#65289;&#23454;&#20307;&#30340;&#23614;&#65288;&#22836;&#65289;&#23454;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GATH&#65288;Graph Attention Transformation for Heterogeneous KGs&#65289;&#65292;&#19968;&#31181;&#19987;&#20026;&#24322;&#26500;KG&#35774;&#35745;&#30340;&#26032;&#39062;&#22522;&#20110;GAT&#30340;&#26041;&#27861;&#12290;GATH&#21253;&#21547;&#20004;&#20010;&#21333;&#29420;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#23427;&#20204;&#21327;&#21516;&#24037;&#20316;&#26469;&#39044;&#27979;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#30340;&#23454;&#20307;&#20013;&#33258;&#21160;&#35843;&#25972;&#38590;&#26131;&#31243;&#24230;&#65292;&#20197;&#20415;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#24322;&#26500;KG&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GATH&#22312;&#25552;&#39640;&#24322;&#26500;KG&#30340;&#23436;&#22791;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02456v1 Announce Type: cross  Abstract: Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of the KGs, they are becoming inaccuracy and incomplete. This problem can be solved by the knowledge graph completion methods, of which graph attention network (GAT)-based methods stand out since their superior performance. However, existing GAT-based knowledge graph completion methods often suffer from overfitting issues when dealing with heterogeneous knowledge graphs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduc
&lt;/p&gt;</description></item><item><title>LIBRA&#26159;&#19968;&#22871;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;LLM&#23545;&#20420;&#32599;&#26031;&#38271;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;21&#20010;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#31561;&#32423;&#12290;</title><link>https://arxiv.org/abs/2408.02439</link><description>&lt;p&gt;
&#38271;&#36755;&#20837;&#22522;&#20934;&#20998;&#26512;&#20420;&#32599;&#26031;&#35821;&#35328;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Long Input Benchmark for Russian Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02439
&lt;/p&gt;
&lt;p&gt;
LIBRA&#26159;&#19968;&#22871;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;LLM&#23545;&#20420;&#32599;&#26031;&#38271;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;21&#20010;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#31561;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02439v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#22823;&#37327;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#24212;&#29992;&#30340;&#20851;&#38190;&#26041;&#38754;&#20043;&#19968;&#26159;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#26631;&#35760;&#12290;&#36825;&#21019;&#36896;&#20102;&#23545;&#38271;&#24773;&#22659;&#29702;&#35299;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#28385;&#36275;&#20420;&#32599;&#26031;&#35821;&#35328;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#38271;&#36755;&#20837;&#22522;&#20934;&#65288;LIBRA&#65289;&#65292;&#23427;&#21253;&#25324;21&#20010;&#36866;&#24212;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#28145;&#20837;&#30740;&#31350;LLM&#23545;&#38271;&#25991;&#26412;&#30340;&#29702;&#35299;&#12290;&#27979;&#35797;&#34987;&#20998;&#20026;&#22235;&#20010;&#22797;&#26434;&#24230;&#32452;&#65292;&#24182;&#20801;&#35768;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#38271;&#24230;&#19978;&#35780;&#20272;&#27169;&#22411;&#65292;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;4k&#30452;&#21040;128k&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#20026;LIBRA&#25552;&#20379;&#20102;&#24320;&#28304;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#24211;&#21644;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#20197;&#25351;&#24341;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02439v1 Announce Type: cross  Abstract: Recent advancements in Natural Language Processing (NLP) have fostered the development of Large Language Models (LLMs) that can solve an immense variety of tasks. One of the key aspects of their application is their ability to work with long text documents and to process long sequences of tokens. This has created a demand for proper evaluation of long-context understanding. To address this need for the Russian language, we propose LIBRA (Long Input Benchmark for Russian Analysis), which comprises 21 adapted datasets to study the LLM's abilities to understand long texts thoroughly. The tests are divided into four complexity groups and allow the evaluation of models across various context lengths ranging from 4k up to 128k tokens. We provide the open-source datasets, codebase, and public leaderboard for LIBRA to guide forthcoming research.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23558;&#36890;&#29992;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#20026;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02412</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;PENDRAM&#65306;&#36890;&#36807;&#27867;&#21270;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
PENDRAM: Enabling High-Performance and Energy-Efficient Processing of Deep Neural Networks through a Generalized DRAM Data Mapping Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02412
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23558;&#36890;&#29992;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#20026;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#31867;&#22411;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#25552;&#39640;CNN&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;CNN&#21152;&#36895;&#22120;&#20173;&#28982;&#38754;&#20020;&#24615;&#33021;&#21644;&#33021;&#25928;&#38382;&#39064;&#65292;&#22240;&#20026;&#31163;&#25955;&#20869;&#23384;&#65288;DRAM&#65289;&#30340;&#35775;&#38382;&#24310;&#36831;&#21644;&#33021;&#32791;&#24456;&#39640;&#65292;&#36825;&#23545;&#20110;&#37027;&#20123;&#23545;&#24310;&#36831;&#21644;&#33021;&#28304;&#26377;&#20005;&#26684;&#38480;&#21046;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#31243;&#24207;&#23588;&#20854;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;DRAM&#26550;&#26500;&#26377;&#19981;&#21516;&#30340;&#35775;&#38382;&#24310;&#36831;&#21644;&#33021;&#32791;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#20026;&#39640;&#33021;&#25928;&#21644;&#39640;&#24615;&#33021;CNN&#21152;&#36895;&#22120;&#20248;&#21270;&#23427;&#20204;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PENDRAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#27867;&#21270;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#20026;CNN&#21152;&#36895;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#12290;&#23588;&#20854;&#26159;&#65292;&#23427;&#36890;&#36807;&#27867;&#21270;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#20026;CNN&#21152;&#36895;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;DRAM&#30340;&#39640;&#25928;&#35775;&#38382;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#31867;&#20284;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02412v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs), a prominent type of Deep Neural Networks (DNNs), have emerged as a state-of-the-art solution for solving machine learning tasks. To improve the performance and energy efficiency of CNN inference, the employment of specialized hardware accelerators is prevalent. However, CNN accelerators still face performance- and energy-efficiency challenges due to high off-chip memory (DRAM) access latency and energy, which are especially crucial for latency- and energy-constrained embedded applications. Moreover, different DRAM architectures have different profiles of access latency and energy, thus making it challenging to optimize them for high performance and energy-efficient CNN accelerators. To address this, we present PENDRAM, a novel design space exploration methodology that enables high-performance and energy-efficient CNN acceleration through a generalized DRAM data mapping policy. Specifically, it expl
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986; MCGF &#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21160;&#24577;&#36866;&#24212;&#26410;&#30693;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#25552;&#21319;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02408</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02408
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986; MCGF &#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21160;&#24577;&#36866;&#24212;&#26410;&#30693;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#25552;&#21319;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02408v1 &#20844;&#21578;&#31867;&#22411;: &#26032; Abstract: &#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#22312;&#26080;GNSS&#20449;&#21495;&#30340;&#29615;&#22659;&#20013;&#26088;&#22312;&#36890;&#36807;&#21305;&#37197;&#26080;&#20154;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#19982;&#22823;&#37327;&#24102;&#26377;&#22320;&#29702;&#26631;&#31614;&#30340;&#21355;&#26143;&#22270;&#20687;&#26469;&#30830;&#23450;&#26410;&#30693;&#20301;&#32622;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#22825;&#27668;&#26465;&#20214;&#19979;&#23398;&#20064;&#21306;&#20998;&#24615;&#22270;&#20687;&#34920;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#24120;&#35265;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#30340;&#39057;&#32321;&#20986;&#29616;&#38459;&#30861;&#20102;&#36827;&#27493;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MCGF&#30340;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#23427;&#26088;&#22312;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21160;&#24577;&#36866;&#24212;&#26410;&#30693;&#22825;&#27668;&#26465;&#20214;&#12290;MCGF&#20026;&#22270;&#20687;&#24674;&#22797;&#21644;&#22320;&#29702;&#23450;&#20301;&#24314;&#31435;&#20102;&#32852;&#21512;&#20248;&#21270;&#12290;&#20026;&#20102;&#22270;&#20687;&#24674;&#22797;&#65292;MCGF&#24341;&#20837;&#20102;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;&#21435;&#22122;&#27169;&#22359;&#26469;&#24110;&#21161;&#20027;&#24178;&#28040;&#38500;&#19982;&#29305;&#23450;&#22825;&#27668;&#26377;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#22320;&#29702;&#23450;&#20301;&#65292;MCGF&#20351;&#29992;EVA-02&#20316;&#20026;&#20027;&#24178;&#26469;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#23450;&#20301;&#21644;&#22825;&#27668;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02408v1 Announce Type: new  Abstract: Cross-view geo-localization in GNSS-denied environments aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery. Recent research shows that learning discriminative image representations under specific weather conditions can significantly enhance performance. However, the frequent occurrence of unseen extreme weather conditions hinders progress. This paper introduces MCGF, a Multi-weather Cross-view Geo-localization Framework designed to dynamically adapt to unseen weather conditions. MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. For image restoration, MCGF incorporates a shared encoder and a lightweight restoration module to help the backbone eliminate weather-specific information. For geo-localization, MCGF uses EVA-02 as a backbone for feature extraction, with cross-entropy loss for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#26694;&#26550;&#24212;&#29992;&#20110;&#20449;&#24687;&#27969;&#65292;&#20197;&#25351;&#23548;&#20449;&#24687;&#20849;&#20139;&#21161;&#25163;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#30340;&#21512;&#35268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02373</link><description>&lt;p&gt;
&#12298;&#22312;&#38544;&#31169;&#24847;&#35782;&#39537;&#21160;&#30340;&#21161;&#25163;&#25805;&#20316;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Operationalizing Contextual Integrity in Privacy-Conscious Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#26694;&#26550;&#24212;&#29992;&#20110;&#20449;&#24687;&#27969;&#65292;&#20197;&#25351;&#23548;&#20449;&#24687;&#20849;&#20139;&#21161;&#25163;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#30340;&#21512;&#35268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02373v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#21457;&#24067; &#27010;&#35201;&#65306;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#32467;&#21512;&#20102;&#21069;&#27839;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#24037;&#20855;&#35775;&#38382;&#26435;&#38480;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29992;&#25143;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#25191;&#34892;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21161;&#25163;&#26377;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65288;&#22914;&#30005;&#23376;&#37038;&#20214;&#21644;&#25991;&#26723;&#65289;&#30340;&#33021;&#21147;&#65292;&#36825;&#25552;&#39640;&#20102;&#38544;&#31169;&#25285;&#24551;&#12290;&#20026;&#20102;&#24341;&#23548;&#20449;&#24687;&#20849;&#20139;&#21161;&#25163;&#30340;&#34892;&#20026;&#31526;&#21512;&#38544;&#31169;&#26399;&#26395;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#8220;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#8221;&#65288;CI&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#20449;&#24687;&#27969;&#65292;&#35813;&#26694;&#26550;&#35748;&#20026;&#38544;&#31169;&#26159;&#26681;&#25454;&#29305;&#23450;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#25351;&#23548;&#21161;&#25163;&#20449;&#24687;&#20849;&#20139;&#34892;&#20026;&#30340;CI&#21512;&#35268;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;&#19968;&#20010;&#30001;&#21512;&#25104;&#25968;&#25454;&#21644;&#20154;&#24037;&#27880;&#37322;&#32452;&#25104;&#30340;&#26032;&#30340;&#34920;&#21333;&#22635;&#20889;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#31034;&#21069;&#27839;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;CI&#22522;&#30784;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#35268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02373v1 Announce Type: new  Abstract: Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks on behalf of users. While the helpfulness of such assistants can increase dramatically with access to user information including emails and documents, this raises privacy concerns about assistants sharing inappropriate information with third parties without user supervision. To steer information-sharing assistants to behave in accordance with privacy expectations, we propose to operationalize $\textit{contextual integrity}$ (CI), a framework that equates privacy with the appropriate flow of information in a given context. In particular, we design and evaluate a number of strategies to steer assistants' information-sharing actions to be CI compliant. Our evaluation is based on a novel form filling benchmark composed of synthetic data and human annotations, and it reveals that prompting frontier LLMs to perform CI-based reasoning yields stron
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#32422;&#26463;&#30340;&#24605;&#32771;&#38142;&#35299;&#30721;&#25216;&#26415;&#26469;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#20307;&#30340;&#20449;&#24687;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26500;&#24314;&#26412;&#20307;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2408.02361</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#32422;&#26463;&#30340;&#24605;&#32771;&#38142;&#35299;&#30721;&#36827;&#34892;&#20250;&#35805;&#26412;&#20307;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#32422;&#26463;&#30340;&#24605;&#32771;&#38142;&#35299;&#30721;&#25216;&#26415;&#26469;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#20307;&#30340;&#20449;&#24687;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26500;&#24314;&#26412;&#20307;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02361v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#39046;&#22495;&#39046;&#20808;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#22823;&#22810;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26412;&#20307;&#20197;&#28385;&#36275;&#29992;&#25143;&#26597;&#35810;&#12290;&#20687;&#23458;&#25143;&#26381;&#21153;&#24405;&#38899;&#36825;&#26679;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#30340;&#22823;&#22810;&#25968;&#65292;&#37117;&#27809;&#26377;&#26412;&#20307;&#21644;&#26631;&#27880;&#12290;&#36825;&#26679;&#30340;&#26412;&#20307;&#36890;&#24120;&#26159;&#36890;&#36807;&#25163;&#21160;&#26500;&#24314;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#19987;&#29992;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#23545;&#35805;&#26412;&#20307;&#26500;&#36896;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#21253;&#25324;&#20004;&#27493;&#65306;&#26415;&#35821;&#25552;&#21462;&#21644;&#20851;&#31995;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36328;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#12290;&#20026;&#20102;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26426;&#21046;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#37319;&#32435;&#20102;&#26368;&#36817;&#20026;&#25512;&#29702;&#38382;&#39064;&#24320;&#21457;&#30340;&#24605;&#24819;&#38142;(CoT)&#35299;&#30721;&#25216;&#26415;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20851;&#31995;&#25552;&#21462;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#35299;&#30721;&#31354;&#38388;&#20013;&#29983;&#25104;&#22810;&#20010;&#20998;&#25903;&#65292;&#24182;&#26681;&#25454;&#20449;&#24515;&#38408;&#20540;&#36873;&#25321;&#20851;&#31995;&#12290;&#36890;&#36807;&#32422;&#26463;&#35299;&#30721;&#22120;&#33267;&#26412;&#20307;&#26415;&#35821;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#20307;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#36127;&#26679;&#30340;&#29983;&#25104;&#39044;&#38450;&#21040;&#20102;&#26368;&#20248;&#35299;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#21462;&#23545;&#35805;&#21644;&#30456;&#20851;&#20851;&#31995;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#25163;&#21160;&#26500;&#24314;&#26412;&#20307;&#30340;&#38656;&#27714;&#65292;&#23545;&#26500;&#24314;&#21160;&#24577;&#35843;&#25972;&#30340;&#29992;&#25143;&#26381;&#21153;&#31995;&#32479;&#26377;&#30528;&#31215;&#26497;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02361v1 Announce Type: cross  Abstract: State-of-the-art task-oriented dialogue systems typically rely on task-specific ontologies for fulfilling user queries. The majority of task-oriented dialogue data, such as customer service recordings, comes without ontology and annotation. Such ontologies are normally built manually, limiting the application of specialised systems. Dialogue ontology construction is an approach for automating that process and typically consists of two steps: term extraction and relation extraction. In this work, we focus on relation extraction in a transfer learning set-up. To improve the generalisation, we propose an extension to the decoding mechanism of large language models. We adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning problems, to generative relation extraction. Here, we generate multiple branches in the decoding space and select the relations based on a confidence threshold. By constraining the decoding to ontology t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;KBQA&#12289;MRC&#21644;IR&#25968;&#25454;&#38598;&#65292;&#24182;&#25512;&#20986;&#20102;&#39318;&#20010;&#27874;&#20848;&#35821;KBQA&#25968;&#25454;&#38598;PUGG&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#27874;&#20848;&#35821;&#30693;&#35782;&#22270;&#35889;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02337</link><description>&lt;p&gt;
&#24320;&#21457; PUGG &#23545;&#20110;&#27874;&#20848;&#35821;&#65306;&#38024;&#23545; KBQA&#12289;MRC &#21644; IR &#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#29616;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;KBQA&#12289;MRC&#21644;IR&#25968;&#25454;&#38598;&#65292;&#24182;&#25512;&#20986;&#20102;&#39318;&#20010;&#27874;&#20848;&#35821;KBQA&#25968;&#25454;&#38598;PUGG&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#27874;&#20848;&#35821;&#30693;&#35782;&#22270;&#35889;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02337v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#27493;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#35821;&#35328;&#20114;&#21160;&#65292;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#25198;&#28436;&#20102;&#26680;&#24515;&#35282;&#33394;&#12290;&#30693;&#35782;&#22522;&#36136;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#65292;&#20801;&#35768;&#22788;&#29702;&#22823;&#37327;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;KBQA&#25968;&#25454;&#38598;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24046;&#36317;&#12290;&#35768;&#22810;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#31649;&#36947;&#36807;&#26102;&#19988;&#22312;&#20154;&#21147;&#36164;&#28304;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19981;&#21033;&#29992;&#29616;&#20195;&#36741;&#21161;&#24037;&#20855;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20943;&#23569;&#24037;&#20316;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19987;&#38376;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#29616;&#20195;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;KBQA&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#25191;&#34892;&#20102;&#36825;&#20010;&#31649;&#36947;&#65292;&#24182;&#25512;&#20986;&#20102;PUGG&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#39318;&#20010;&#27874;&#20848;&#35821;KBQA&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32467;&#21512;LLM&#25216;&#26415;&#21644;&#27874;&#20848;&#35821;&#30693;&#35782;&#22270;&#35889;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#20026;&#27874;&#20848;&#35821;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#24102;&#26469;&#20102;&#26399;&#26395;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02337v1 Announce Type: new  Abstract: Advancements in AI and natural language processing have revolutionized machine-human language interactions, with question answering (QA) systems playing a pivotal role. The knowledge base question answering (KBQA) task, utilizing structured knowledge graphs (KG), allows for handling extensive knowledge-intensive questions. However, a significant gap exists in KBQA datasets, especially for low-resource languages. Many existing construction pipelines for these datasets are outdated and inefficient in human labor, and modern assisting tools like Large Language Models (LLM) are not utilized to reduce the workload. To address this, we have designed and implemented a modern, semi-automated approach for creating datasets, encompassing tasks such as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR), tailored explicitly for low-resource environments. We executed this pipeline and introduced the PUGG dataset, the first Poli
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24191;&#20041;&#39640;&#26031;&#35823;&#24046;&#27169;&#22411;&#25913;&#36827;&#20102;&#25968;&#25454;&#30340;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2408.02295</link><description>&lt;p&gt;
&#20851;&#20110;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#24191;&#20041;&#39640;&#26031;TD&#35823;&#24046;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Gaussian Temporal Difference Error For Uncertainty-aware Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02295
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24191;&#20041;&#39640;&#26031;&#35823;&#24046;&#27169;&#22411;&#25913;&#36827;&#20102;&#25968;&#25454;&#30340;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#24191;&#20041;&#39640;&#26031;&#35823;&#24046;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#25511;&#21046;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25913;&#36827;&#20102;&#23545;&#35823;&#24046;&#20998;&#24067;&#30340;&#28789;&#27963;&#24314;&#27169;&#65292;&#36890;&#36807;&#21253;&#21547;&#26356;&#39640;&#38454;&#30340;&#32479;&#35745;&#37327;&#65288;&#29305;&#21035;&#26159;&#23792;&#24230;&#65289;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#20272;&#35745;&#65292;&#21363;&#65292;&#35823;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#30340;&#24418;&#29366;&#21442;&#25968;&#23545;&#35823;&#24046;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#38381;&#21512;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#65292;&#36825;&#34920;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#19982;&#24418;&#29366;&#21442;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02295v1 Announce Type: cross  Abstract: Conventional uncertainty-aware temporal difference (TD) learning methods often rely on simplistic assumptions, typically including a zero-mean Gaussian distribution for TD errors. Such oversimplification can lead to inaccurate error representations and compromised uncertainty estimation. In this paper, we introduce a novel framework for generalized Gaussian error modeling in deep reinforcement learning, applicable to both discrete and continuous control settings. Our framework enhances the flexibility of error distribution modeling by incorporating higher-order moments, particularly kurtosis, thereby improving the estimation and mitigation of data-dependent noise, i.e., aleatoric uncertainty. We examine the influence of the shape parameter of the generalized Gaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form expression that demonstrates an inverse relationship between uncertainty and the shape parameter. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#30340;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#25512;&#29702;&#26102;&#38388;&#25972;&#21512;&#21040;&#21518;&#39564;&#38598;&#25104;&#20013;&#65292;&#20197;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#25552;&#39640;&#38598;&#25104;&#21151;&#29575;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#25110;&#25104;&#26412;&#25935;&#24863;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.02280</link><description>&lt;p&gt;
&#30828;&#20214;&#33258;&#36866;&#24212;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#20197;&#24179;&#34913;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and Cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#30340;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#25512;&#29702;&#26102;&#38388;&#25972;&#21512;&#21040;&#21518;&#39564;&#38598;&#25104;&#20013;&#65292;&#20197;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#25552;&#39640;&#38598;&#25104;&#21151;&#29575;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#25110;&#25104;&#26412;&#25935;&#24863;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02280v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#33258;&#21160;&#26426;&#22120;&#23398;&#20064; (AutoML) &#22312;&#31616;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#23427;&#33258;&#21160;&#21270;&#20102;&#20174;&#25968;&#25454;&#39044;&#22788;&#29702;&#21040;&#27169;&#22411;&#36873;&#25321;&#20877;&#21040;&#38598;&#25104;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;AutoML&#31995;&#32479;&#36890;&#24120;&#37319;&#29992;&#21518;&#39564;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#38271;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#36825;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#26159;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#30340;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#25512;&#29702;&#26102;&#38388;&#25972;&#21512;&#21040;&#21518;&#39564;&#38598;&#25104;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#38598;&#25104;&#36873;&#25321;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20505;&#36873;&#38598;&#21512;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#31181;&#21452;&#37325;&#28966;&#28857;&#20351;&#20154;&#20204;&#33021;&#22815;&#20174;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20351;&#29992;&#21512;&#38598;&#33021;&#22815;&#26681;&#25454;&#26381;&#21153;&#22120;&#30828;&#20214;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#32454;&#33410;&#65292;&#22312;&#39044;&#27979;&#31934;&#24230;&#12289;&#38598;&#25104;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#20043;&#38388;&#21462;&#24471;&#26435;&#34913;&#12290;&#36825;&#31181;&#26041;&#27861;&#39564;&#35777;&#20102;&#22312;&#20445;&#25345;&#39640;&#39044;&#27979;&#31934;&#24230;&#21516;&#26102;&#25552;&#39640;&#38598;&#25104;&#21151;&#29575;&#30340;&#28508;&#21147;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#25110;&#25104;&#26412;&#25935;&#24863;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02280v1 Announce Type: cross  Abstract: Automated Machine Learning (AutoML) significantly simplifies the deployment of machine learning models by automating tasks from data preprocessing to model selection to ensembling. AutoML systems for tabular data often employ post hoc ensembling, where multiple models are combined to improve predictive accuracy. This typically results in longer inference times, a major limitation in practical deployments. Addressing this, we introduce a hardware-aware ensemble selection approach that integrates inference time into post hoc ensembling. By leveraging an existing framework for ensemble selection with quality diversity optimization, our method evaluates ensemble candidates for their predictive accuracy and hardware efficiency. This dual focus allows for a balanced consideration of accuracy and operational efficiency. Thus, our approach enables practitioners to choose from a Pareto front of accurate and efficient ensembles. Our evaluation u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DRFormer &#30340;&#22810;&#23610;&#24230; Transformer &#27169;&#22411;&#65292;&#21019;&#26032;&#24615;&#22320;&#20351;&#29992;&#21160;&#24577; Tokenizer &#21644;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#31639;&#27861;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#26679; receptive field &#21644;&#31232;&#30095;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#26500;&#24314;&#23618;&#32423;&#21270;&#30340; receptive field&#65292;&#24182;&#26377;&#25928;&#25277;&#21462;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02279</link><description>&lt;p&gt;
DRFormer&#65306;&#21033;&#29992;&#22810;&#26679; receptive field &#30340;&#22810;&#23610;&#24230; Transformer &#38271;&#26102;&#24207;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DRFormer &#30340;&#22810;&#23610;&#24230; Transformer &#27169;&#22411;&#65292;&#21019;&#26032;&#24615;&#22320;&#20351;&#29992;&#21160;&#24577; Tokenizer &#21644;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#31639;&#27861;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#26679; receptive field &#21644;&#31232;&#30095;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#26500;&#24314;&#23618;&#32423;&#21270;&#30340; receptive field&#65292;&#24182;&#26377;&#25928;&#25277;&#21462;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02279v1 Announce Type: cross  &#25688;&#35201;&#65306;&#38271;&#26102;&#24207;&#39044;&#27979;&#65288;LTSF&#65289;&#22312;&#37329;&#34701;&#12289;&#20132;&#36890;&#39044;&#27979;&#31561;&#35832;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#22522;&#20110;&#29255;&#27573;&#30340; Transformer &#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#23558;&#25968;&#25454;&#20998;&#20026;&#24213;&#23618;&#29255;&#27573;&#65292;&#20316;&#20026;&#36755;&#20837;&#30340;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39044;&#20808;&#35774;&#23450;&#30340;&#29255;&#27573;&#38271;&#24230;&#65292;&#36825;&#38656;&#35201;&#34892;&#19994;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#25429;&#25417;&#19981;&#21516;&#23610;&#24230;&#30340;&#25968;&#25454;&#29305;&#24449;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#21464;&#21270;&#21644;&#27874;&#21160;&#27169;&#24335;&#65292;&#36825;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#26377;&#25928;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21160;&#24577;&#23398;&#20064;&#31639;&#27861;&#20026;&#25351;&#23548;&#30340;&#21160;&#24577; Tokenizer&#65292;&#29992;&#20110;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679; receptive field &#21644;&#31232;&#30095;&#27169;&#24335;&#12290;&#20026;&#20102;&#24314;&#31435;&#23618;&#32423;&#21270;&#30340; receptive field&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230; Transformer &#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#24207;&#21015;&#25277;&#21462;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25277;&#21462;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02279v1 Announce Type: cross  Abstract: Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution feat
&lt;/p&gt;</description></item><item><title>&#26412;&#31995;&#32479;&#21033;&#29992;&#20960;&#20309;&#20195;&#25968;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;3D&#22330;&#26223;&#20013;&#23545;&#35937;&#31934;&#30830;&#37325;&#26032;&#23450;&#20301;&#65292;&#26080;&#38656;&#19987;&#19994;&#35757;&#32451;&#25968;&#25454;&#65292;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21363;&#21487;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2408.02275</link><description>&lt;p&gt;
&#20960;&#20309;&#20195;&#25968;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#65306;3D&#20132;&#20114;&#22411;&#21487;&#25511;&#22330;&#26223;&#20013;&#20998;&#31163;&#32593;&#26684;&#30340;&#25351;&#20196;&#39537;&#21160;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31995;&#32479;&#21033;&#29992;&#20960;&#20309;&#20195;&#25968;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;3D&#22330;&#26223;&#20013;&#23545;&#35937;&#31934;&#30830;&#37325;&#26032;&#23450;&#20301;&#65292;&#26080;&#38656;&#19987;&#19994;&#35757;&#32451;&#25968;&#25454;&#65292;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21363;&#21487;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#24247;&#24335;&#20960;&#20309;&#20195;&#25968;(CGA)&#30340;&#38598;&#25104;&#65292;&#23427;&#26088;&#22312;&#38761;&#26032;&#21487;&#25511;3D&#22330;&#26223;&#32534;&#36753;&#65292;&#29305;&#21035;&#26159;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#25163;&#21160;&#27969;&#31243;&#21644;&#19987;&#19994;&#25216;&#33021;&#12290;&#36825;&#20123;&#20256;&#32479;&#30340;&#25216;&#33402;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#32570;&#20047;&#23545;&#31934;&#30830;&#32534;&#36753;&#30340;&#27491;&#24335;&#35821;&#35328;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#8220;shenlong&#8221;&#20351;&#29992;CGA&#20316;&#20026;&#24378;&#22823;&#30340;&#27491;&#24335;&#35821;&#35328;&#65292;&#31934;&#30830;&#22320;&#24314;&#27169;&#20102;&#36827;&#34892;&#31934;&#30830;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#25152;&#38656;&#30340;&#31354;&#38388;&#21464;&#25442;&#12290;&#20511;&#21161;&#39044;&#35757;&#32451;LLMs&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#65292;&#8220;shenlong&#8221;&#21487;&#20197;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#32763;&#35793;&#25104;CGA&#25805;&#20316;&#65292;&#24182;&#23558;&#36825;&#20123;&#25805;&#20316;&#24212;&#29992;&#20110;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#21464;&#25442;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#25968;&#25454;&#39044;&#35757;&#32451;&#12290;&#22312;&#29616;&#23454;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#23454;&#26045;&#21518;&#65292;&#8220;shenlong&#8221;&#30830;&#20445;&#20102;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02275v1 Announce Type: new  Abstract: This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, shenlong ensures compatibility with existing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#22810;&#28304;&#24322;&#26500;&#22806;&#28304;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27861;&#24459;&#25351;&#25511;&#30340;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#27861;&#24459;&#30693;&#35782;&#24211;&#12289;&#23545;&#35805;&#24335;LLM&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27861;&#24459;&#25991;&#29486;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.02233</link><description>&lt;p&gt;
&#22810;&#28304;&#24322;&#26500;&#30693;&#35782;&#27880;&#20837;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#27861;&#24459;&#25351;&#25511;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method for Legal Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#22810;&#28304;&#24322;&#26500;&#22806;&#28304;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27861;&#24459;&#25351;&#25511;&#30340;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#27861;&#24459;&#30693;&#35782;&#24211;&#12289;&#23545;&#35805;&#24335;LLM&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27861;&#24459;&#25991;&#29486;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#25351;&#25511;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#26088;&#22312;&#20026;&#26696;&#20214;&#25551;&#36848;&#20934;&#30830;&#22320;&#20998;&#37197;&#25351;&#25511;&#26631;&#31614;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340; recent &#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30452;&#25509;&#24314;&#27169;&#26696;&#20214;&#25551;&#36848;&#65292;&#26410;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#28304;&#22806;&#37096;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#21033;&#29992;&#20102;&#27861;&#24459;&#30693;&#35782;&#24211;&#12289;&#23545;&#35805;&#24335;LLM&#21644;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#30340;&#22810;&#28304;&#24322;&#26500;&#22806;&#37096;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#27861;&#24459;&#30693;&#35782;&#24211;&#19982;&#26696;&#20214;&#25551;&#36848;&#20013;&#30340;&#30693;&#35782;&#29255;&#27573;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#30828;&#25552;&#31034;&#27169;&#26495;&#23558;&#23427;&#20204;&#23553;&#35013;&#21040;&#36755;&#20837;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26816;&#32034;&#19982;&#32473;&#23450;&#26696;&#20214;&#25551;&#36848;&#30456;&#20851;&#32852;&#30340;&#27861;&#24459;&#25991;&#31456;&#65292;&#24182;&#36890;&#36807;&#23545;&#35805;&#24335;LLM&#33719;&#24471;&#26696;&#20214;&#25551;&#36848;&#20013;&#30340;&#20107;&#23454;&#20803;&#32032;&#12290;&#25105;&#20204;&#36890;&#36807;&#36719;&#25552;&#31034;&#35789;&#27719;&#30340;&#23884;&#20837;&#21521;&#37327;&#34701;&#21512;&#26469;&#23436;&#25104;&#36825;&#39033;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02233v1 Announce Type: cross  Abstract: Legal charge prediction, an essential task in legal AI, seeks to assign accurate charge labels to case descriptions, attracting significant recent interest. Existing methods primarily employ diverse neural network structures for modeling case descriptions directly, failing to effectively leverage multi-source external knowledge. We propose a prompt learning framework-based method that simultaneously leverages multi-source heterogeneous external knowledge from a legal knowledge base, a conversational LLM, and related legal articles. Specifically, we match knowledge snippets in case descriptions via the legal knowledge base and encapsulate them into the input through a hard prompt template. Additionally, we retrieve legal articles related to a given case description through contrastive learning, and then obtain factual elements within the case description through a conversational LLM. We fuse the embedding vectors of soft prompt tokens w
&lt;/p&gt;</description></item><item><title>&#27492;&#22788;&#26159;&#20013;&#25991;&#25688;&#35201;&#30340;&#23567;&#32467;</title><link>https://arxiv.org/abs/2408.02232</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SpecRover: Code Intent Extraction via LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02232
&lt;/p&gt;
&lt;p&gt;
&#27492;&#22788;&#26159;&#20013;&#25991;&#25688;&#35201;&#30340;&#23567;&#32467;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Paper abstract translated
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02232v1 Announce Type: cross  Abstract: Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our app
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#21327;&#21161;Database&#31649;&#29702;&#21592;&#23545;&#25968;&#25454;&#24211;&#24615;&#33021;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.02213</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#26377;&#25928;&#22320;&#36827;&#34892;&#25968;&#25454;&#24211;&#25671;&#26438;&#35843;&#25972;&#65311;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Large Language Model Good at Database Knob Tuning? A Comprehensive Experimental Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02213
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#21327;&#21161;Database&#31649;&#29702;&#21592;&#23545;&#25968;&#25454;&#24211;&#24615;&#33021;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02213v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#25968;&#25454;&#24211;&#20248;&#21270;&#36890;&#36807;&#35843;&#25972;&#25671;&#26438;&#26469;&#22686;&#24378;&#25968;&#25454;&#24211;&#24615;&#33021;&#65292;&#25671;&#26438;&#35843;&#25972;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35843;&#25972;&#26041;&#27861;&#22823;&#22810;&#36981;&#24490;&#8220;&#23581;&#35797;-&#25910;&#38598;-&#35843;&#25972;&#8221;&#30340;&#36807;&#31243;&#65292;&#35777;&#26126;&#20102;&#20854;&#20302;&#25928;&#19988;&#29305;&#23450;&#20110;&#25968;&#25454;&#24211;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#19981;&#36879;&#26126;&#65292;&#20351;&#24471;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#38590;&#20197;&#29702;&#35299;&#32972;&#21518;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;  &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;GPT-4&#21644;Claude-3&#65292;&#24050;&#32463;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#25968;&#25454;&#24211;&#25671;&#26438;&#35843;&#25972;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;LLM&#20316;&#20026;&#26377;&#32463;&#39564;&#30340;DBA&#36827;&#34892;&#25671;&#26438;&#35843;&#25972;&#20219;&#21153;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#35782;&#21035;&#20986;&#31995;&#32479;&#35843;&#35856;&#30340;&#19977;&#39033;&#20851;&#38190;&#23376;&#20219;&#21153;&#65306;&#25671;&#26438;&#20462;&#21098;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#25671;&#26438;&#24314;&#35758;&#65292;&#24182;&#23545;&#27599;&#39033;&#23376;&#20219;&#21153;&#25552;&#20986;&#20102;LLM&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#21462;&#20195;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;  &#25105;&#20204;&#24320;&#23637;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23558;LLM&#39537;&#21160;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#24211;&#31995;&#32479;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#24212;&#29992;&#39564;&#35777;&#20102;LLM&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;LLM&#22312;&#25968;&#25454;&#24211;&#35843;&#20248;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02213v1 Announce Type: cross  Abstract: Knob tuning plays a crucial role in optimizing databases by adjusting knobs to enhance database performance. However, traditional tuning methods often follow a Try-Collect-Adjust approach, proving inefficient and database-specific. Moreover, these methods are often opaque, making it challenging for DBAs to grasp the underlying decision-making process.   The emergence of large language models (LLMs) like GPT-4 and Claude-3 has excelled in complex natural language tasks, yet their potential in database knob tuning remains largely unexplored. This study harnesses LLMs as experienced DBAs for knob-tuning tasks with carefully designed prompts. We identify three key subtasks in the tuning system: knob pruning, model initialization, and knob recommendation, proposing LLM-driven solutions to replace conventional methods for each subtask.   We conduct extensive experiments to compare LLM-driven approaches against traditional methods across the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#25252;&#26639;&#20998;&#31867;&#31995;&#32479;&#26088;&#22312;&#30830;&#20445;&#22522;&#30784;&#27169;&#22411;&#31995;&#32479;&#22312;&#36816;&#34892;&#26102;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02205</link><description>&lt;p&gt;
&#26397;&#21521;&#23433;&#20840;&#35774;&#35745;&#30340;AI&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#30340;&#36816;&#34892;&#26102;&#25252;&#26639;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards AI-Safety-by-Design: A Taxonomy of Runtime Guardrails in Foundation Model based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#25252;&#26639;&#20998;&#31867;&#31995;&#32479;&#26088;&#22312;&#30830;&#20445;&#22522;&#30784;&#27169;&#22411;&#31995;&#32479;&#22312;&#36816;&#34892;&#26102;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#22312;&#29616;&#20195;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#31995;&#32479;&#65292;&#31995;&#32479;&#22320;&#23558;&#36816;&#34892;&#26102;&#25252;&#26639;&#20316;&#20026;&#19968;&#31181;&#25216;&#26415;&#25163;&#27573;&#26469;&#30830;&#20445;FM&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35774;&#35745;&#25252;&#26639;&#26102;&#24212;&#32771;&#34385;&#30340;&#36719;&#20214;&#29305;&#24615;&#21644;&#20174;&#36719;&#20214;&#26550;&#26500;&#35282;&#24230;&#30830;&#20445;&#36825;&#20123;&#29305;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02205v1 Announce Type: cross  Abstract: The rapid advancement and widespread deployment of foundation model (FM) based systems have revolutionized numerous applications across various domains. However, the fast-growing capabilities and autonomy have also raised significant concerns about responsible AI and AI safety. Recently, there have been increasing attention toward implementing guardrails to ensure the runtime behavior of FM-based systems is safe and responsible. Given the early stage of FMs and their applications (such as agents), the design of guardrails have not yet been systematically studied. It remains underexplored which software qualities should be considered when designing guardrails and how these qualities can be ensured from a software architecture perspective. Therefore, in this paper, we present a taxonomy for guardrails to classify and compare the characteristics and design options of guardrails. Our taxonomy is organized into three main categories: the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SelfBC&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38598;&#25104;&#33258;&#25105;&#32422;&#26463;&#26426;&#21046;&#21040;off-policy&#26041;&#27861;&#20013;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#38750;&#20445;&#23432;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#31574;&#30053;&#23849;&#28291;&#65292;&#24182;&#19988;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02165</link><description>&lt;p&gt;
SelfBC: &#33258;&#25105;&#34892;&#20026;&#20811;&#38534;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SelfBC: Self Behavior Cloning for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SelfBC&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38598;&#25104;&#33258;&#25105;&#32422;&#26463;&#26426;&#21046;&#21040;off-policy&#26041;&#27861;&#20013;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#38750;&#20445;&#23432;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#31574;&#30053;&#23849;&#28291;&#65292;&#24182;&#19988;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02165v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31574;&#30053;&#32422;&#26463;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#38480;&#21046;&#23398;&#20064;&#31574;&#30053;&#19982;&#31163;&#32447;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#23548;&#33268;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#65292;&#27169;&#20223;&#34892;&#20026;&#31574;&#30053;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#24182;&#23558;&#36825;&#19968;&#23616;&#38480;&#24615;&#24402;&#22240;&#20110;&#20256;&#32479;&#32422;&#26463;&#30340;&#38745;&#24577;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#31574;&#30053;&#32422;&#26463;&#65292;&#23427;&#38480;&#21046;&#20102;&#20197;&#21069;&#23398;&#20064;&#31574;&#30053;&#29983;&#25104;&#25351;&#25968;&#24179;&#28369;&#24179;&#22343;samples&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#33258;&#25105;&#32422;&#26463;&#26426;&#21046;&#38598;&#25104;&#21040;off-policy&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#20419;&#36827;&#20102;&#38750;&#20445;&#23432;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#31574;&#30053;&#23849;&#28291;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#20010;&#20960;&#20046;&#21333;&#35843;&#25913;&#36827;&#30340;&#21442;&#32771;&#31574;&#30053;&#12290;&#22312;D4RL MuJoCo&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SelfBC&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02165v1 Announce Type: cross  Abstract: Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain d
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23569;&#26679;&#26412;&#32034;&#24341;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#25552;&#31034;LLM&#21019;&#24314;&#25991;&#26723;&#26631;&#35782;&#31526;&#38134;&#34892;&#65292;&#24182;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#38480;&#21046;&#27169;&#22411;&#20135;&#29983;&#30340;docid&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02152</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#32034;&#24341;&#30340;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generative Retrieval with Few-shot Indexing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02152
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23569;&#26679;&#26412;&#32034;&#24341;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#25552;&#31034;LLM&#21019;&#24314;&#25991;&#26723;&#26631;&#35782;&#31526;&#38134;&#34892;&#65292;&#24182;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#38480;&#21046;&#27169;&#22411;&#20135;&#29983;&#30340;docid&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02152v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#29616;&#26377;&#30340;&#29983;&#25104;&#26816;&#32034;&#65288;GR&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#24211;&#32034;&#24341;&#26041;&#27861;&#65292;&#21363;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35760;&#20303;&#26597;&#35810;&#19982;&#20854;&#30456;&#20851;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;docid&#65289;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#32034;&#24341;&#26377;&#19977;&#20010;&#23616;&#38480;&#24615;&#65306;&#35757;&#32451;&#25104;&#26412;&#39640;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#21033;&#29992;&#29575;&#19981;&#36275;&#20197;&#21450;&#21160;&#24577;&#25991;&#26723;&#38598;&#21512;&#19978;&#30340;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23569;&#26679;&#26412;&#32034;&#24341;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65288;Few-Shot GR&#65289;&#12290;&#23427;&#20855;&#26377;&#19968;&#31181;&#26032;&#30340;&#23569;&#26679;&#26412;&#32034;&#24341;&#36807;&#31243;&#65292;&#22312;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#25972;&#20010;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#25152;&#26377;&#25991;&#26723;&#25552;&#20379;&#32473;LLM&#36827;&#34892;&#25552;&#31034;&#65292;&#26368;&#32456;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#25991;&#26723;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#38134;&#34892;&#12290;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#25552;&#20379;&#32473;&#30456;&#21516;&#30340;LLM&#65292;&#24182;&#38480;&#21046;&#20854;&#20135;&#29983;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#20301;&#20110;&#22312;&#32034;&#24341;&#26399;&#38388;&#21019;&#24314;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#38134;&#34892;&#20043;&#20869;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;docid&#26144;&#23556;&#22238;&#20854;&#23545;&#24212;&#30340;&#25991;&#26723;&#12290;Few-Shot GR&#20381;&#36182;&#20110;&#23545;LLM&#36827;&#34892;&#25552;&#31034;&#65292;&#32780;&#26080;&#38656;&#35201;&#27714;&#20854;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02152v1 Announce Type: cross  Abstract: Existing generative retrieval (GR) approaches rely on training-based indexing, i.e., fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has three limitations: high training overhead, under-utilization of the pre-trained knowledge of large language models (LLMs), and challenges in adapting to a dynamic document corpus. To address the above issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR). It has a novel few-shot indexing process, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Few-Shot GR relies solely on prompting an LLM without req
&lt;/p&gt;</description></item><item><title>VidModEx&#26041;&#27861;&#20351;&#29992;SHAP&#22686;&#24378;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02140</link><description>&lt;p&gt;
VidModEx: &#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#39640;&#25928;&#30340;&#40657;&#33394;&#30418;&#23376;&#27169;&#22411;&#25552;&#21462;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02140
&lt;/p&gt;
&lt;p&gt;
VidModEx&#26041;&#27861;&#20351;&#29992;SHAP&#22686;&#24378;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02140v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22312;&#40657;&#31665;&#27169;&#22411;&#25552;&#21462;&#39046;&#22495;&#65292;&#20381;&#36182;&#20110;&#36719;&#26631;&#31614;&#25110;&#26367;&#20195;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#21644;&#22788;&#29702;&#22823;&#37327;&#30456;&#20851;&#32852;&#30340;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;SHAP&#37327;&#21270;&#20102;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#36755;&#20986;&#25152;&#20570;&#30340;&#20010;&#20307;&#36129;&#29486;&#65292;&#36825;&#26377;&#21161;&#20110;&#20248;&#21270;&#19968;&#20010;&#22522;&#20110;&#33021;&#37327;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#36798;&#21040;&#19968;&#20010;&#29702;&#24819;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;16.45%&#65292;&#24182;&#19988;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#65292;&#22312;UCF11&#12289;UCF101&#12289;Kinetics 400&#12289;Kinetics 600&#21644;Something-Something V2&#31561;&#38590;&#20197;&#25361;&#25112;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;26.11%&#65292;&#26368;&#39640;&#25552;&#21319;&#20102;33.36%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#23454;&#38469;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#32771;&#34385;&#20215;&#20540;&#24182;&#20135;&#29983;&#27491;&#24403;&#29702;&#30001;&#30340;&#20195;&#29702;&#33021;&#22815;&#25552;&#39640;&#20914;&#31361;&#35299;&#20915;&#29575;&#12289;&#31038;&#20132;&#20307;&#39564;&#12289;&#38544;&#31169;&#20445;&#25252;&#20197;&#21450;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02117</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30340;&#27491;&#24403;&#29702;&#30001;&#25913;&#21892;&#31038;&#20132;&#20307;&#39564;&#65306;&#19968;&#39033;&#22810;Agent&#27169;&#25311;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02117
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#32771;&#34385;&#20215;&#20540;&#24182;&#20135;&#29983;&#27491;&#24403;&#29702;&#30001;&#30340;&#20195;&#29702;&#33021;&#22815;&#25552;&#39640;&#20914;&#31361;&#35299;&#20915;&#29575;&#12289;&#31038;&#20132;&#20307;&#39564;&#12289;&#38544;&#31169;&#20445;&#25252;&#20197;&#21450;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Exanna&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20195;&#29702;&#22312;&#20854;&#20915;&#31574;&#36807;&#31243;&#20013;&#32771;&#34385;&#33258;&#36523;&#30340;&#20215;&#20540;&#20197;&#21450;&#20182;&#20154;&#30340;&#20215;&#20540;&#12290;&#22312;&#25552;&#20379;&#34892;&#20026;&#27491;&#24403;&#29702;&#30001;&#24182;&#35780;&#20272;&#20182;&#20154;&#27491;&#24403;&#29702;&#30001;&#30340;&#36807;&#31243;&#20013;&#65292;Exanna&#20195;&#29702;&#20250;&#32771;&#34385;&#21040;&#20215;&#20540;&#22240;&#32032;&#12290;&#36890;&#36807;&#22810;Agent&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32771;&#34385;&#20215;&#20540;&#24182;&#20135;&#29983;&#27491;&#24403;&#29702;&#30001;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36829;&#21453;&#24120;&#35268;&#30340;&#34892;&#20026;&#65292;&#33021;&#22815;&#23548;&#33268;&#65288;1&#65289;&#26356;&#39640;&#30340;&#20914;&#31361;&#35299;&#20915;&#29575;&#65292;&#65288;2&#65289;&#26356;&#22909;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#65288;3&#65289;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#20197;&#21450;&#65288;4&#65289;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02117v1 Announce Type: cross  Abstract: We propose Exanna, a framework to realize agents that incorporate values in decision making. An Exannaagent considers the values of itself and others when providing rationales for its actions and evaluating the rationales provided by others. Via multiagent simulation, we demonstrate that considering values in decision making and producing rationales, especially for norm-deviating actions, leads to (1) higher conflict resolution, (2) better social experience, (3) higher privacy, and (4) higher flexibility.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#21019;&#26032;&#25945;&#23398;&#65292;&#25506;&#32034;&#20102;&#38899;&#39057;&#35270;&#35273;&#20316;&#21697;&#22768;&#38899;&#35774;&#35745;&#30340;&#25945;&#23398;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#39033;&#30446;&#24335;&#23398;&#20064;&#22312;&#25552;&#21319;&#23398;&#29983;&#38899;&#39057;&#25216;&#26415;&#23454;&#36341;&#33021;&#21147;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#31361;&#20986;&#20102;&#21019;&#24314;&#21253;&#23481;&#24615;&#21644;&#20114;&#21160;&#24615;&#30340;&#23398;&#20064;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#25945;&#24072;&#23545;&#23398;&#29983;&#20010;&#24615;&#21270;&#25903;&#25345;&#19982;&#25351;&#23548;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02113</link><description>&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#20316;&#21697;&#22768;&#38899;&#35774;&#35745;&#30340;&#25945;&#23398;&#35774;&#35745;&#25506;&#32034;&#65306;&#36890;&#36807;&#26234;&#33021;&#24037;&#20855;&#23454;&#29616;&#21019;&#26032;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Dise\~no de sonido para producciones audiovisuales e historias sonoras en el aula. Hacia una docencia creativa mediante el uso de herramientas inteligentes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#21019;&#26032;&#25945;&#23398;&#65292;&#25506;&#32034;&#20102;&#38899;&#39057;&#35270;&#35273;&#20316;&#21697;&#22768;&#38899;&#35774;&#35745;&#30340;&#25945;&#23398;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#39033;&#30446;&#24335;&#23398;&#20064;&#22312;&#25552;&#21319;&#23398;&#29983;&#38899;&#39057;&#25216;&#26415;&#23454;&#36341;&#33021;&#21147;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#31361;&#20986;&#20102;&#21019;&#24314;&#21253;&#23481;&#24615;&#21644;&#20114;&#21160;&#24615;&#30340;&#23398;&#20064;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#25945;&#24072;&#23545;&#23398;&#29983;&#20010;&#24615;&#21270;&#25903;&#25345;&#19982;&#25351;&#23548;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02113v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; Abstract: &#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#20139;&#20256;&#25480;&#38899;&#39057;&#35270;&#35273;&#20316;&#21697;&#22768;&#38899;&#35774;&#35745;&#30340;&#25945;&#23398;&#32463;&#39564;&#65292;&#24182;&#19982;&#19981;&#21516;&#39033;&#30446;&#19979;&#30340;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#24182;&#38750;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25945;&#23398;&#36827;&#34892;&#27604;&#36739;&#65292;&#32780;&#26356;&#20391;&#37325;&#20110;&#20998;&#26512;&#22312;&#19981;&#21516;&#24180;&#32423;&#23398;&#20064;&#35813;&#35838;&#31243;&#30340;&#19981;&#21516;&#23398;&#29983;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#23545;&#20110;&#24456;&#22823;&#19968;&#37096;&#20998;&#23398;&#29983;&#26469;&#35828;&#65292;&#38899;&#39057;&#19990;&#30028;&#21487;&#33021;&#38750;&#24120;&#26377;&#36259;&#65292;&#26080;&#35770;&#26159;&#37027;&#20123;&#20855;&#26377;&#21019;&#36896;&#24615;&#21644;&#25216;&#26415;&#20542;&#21521;&#30340;&#23398;&#29983;&#12290;&#38899;&#20048;&#21019;&#20316;&#21644;&#21046;&#20316;&#12289;&#38899;&#39057;&#19982;&#22270;&#20687;&#30340;&#21516;&#27493;&#12289;&#37197;&#38899;&#31561;&#23398;&#31185;&#36890;&#24120;&#23545;&#23398;&#29983;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#26497;&#39640;&#30340;&#25216;&#26415;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#26377;&#26102;&#21487;&#33021;&#25104;&#20026;&#20837;&#38376;&#30340;&#19968;&#20010;&#38750;&#24120;&#39640;&#30340;&#38376;&#27099;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19981;&#29087;&#24713;&#38899;&#39057;&#32534;&#36753;&#36719;&#20214;&#30340;&#23398;&#29983;&#26469;&#35828;&#65292;&#20182;&#20204;&#21487;&#33021;&#38656;&#35201;&#20960;&#21608;&#29978;&#33267;&#20960;&#20010;&#26376;&#30340;&#26102;&#38388;&#26469;&#29087;&#32451;&#20351;&#29992;&#36825;&#20123;&#36719;&#20214;&#65292;&#32780;&#36825;&#20123;&#36719;&#20214;&#24182;&#38750;&#24635;&#23545;&#23398;&#29983;&#29992;&#25143;&#29305;&#21035;&#30452;&#35266;&#12290;&#36890;&#36807;&#37319;&#29992;&#39033;&#30446;&#24335;&#23398;&#20064;&#65288;PBL&#65289;&#26041;&#27861;&#35770;&#30340;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#30340;&#19987;&#19994;&#23454;&#36341;&#33021;&#21147;&#65292;&#26377;&#25928;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#36164;&#28304;&#33021;&#22815;&#20351;&#23398;&#29983;&#22312;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#25484;&#25569;&#19987;&#19994;&#30693;&#35782;&#65292;&#20174;&#32780;&#25512;&#21160;&#23398;&#20064;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#19968;&#20010;&#21253;&#23481;&#21644;&#20114;&#21160;&#30340;&#35838;&#22530;&#29615;&#22659;&#23545;&#20110;&#40723;&#21169;&#23398;&#29983;&#22312;&#23454;&#36341;&#20013;&#21191;&#20110;&#23581;&#35797;&#21644;&#29359;&#38169;&#33267;&#20851;&#37325;&#35201;&#65292;&#25945;&#24072;&#38656;&#35201;&#26681;&#25454;&#23398;&#29983;&#30340;&#23454;&#38469;&#24773;&#20917;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25903;&#25345;&#21644;&#25351;&#23548;&#12290;&#23545;&#20110;&#39640;&#26657;&#25945;&#24072;&#26469;&#35828;&#65292;&#20102;&#35299;&#23398;&#29983;&#30340;&#23398;&#20064;&#20559;&#22909;&#21644;&#23398;&#20064;&#39118;&#26684;&#23545;&#20110;&#35774;&#35745;&#26377;&#25928;&#25945;&#23398;&#26041;&#27861;&#21644;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02113v1 Announce Type: cross  Abstract: This study aims to share a teaching experience teaching sound design for audiovisual productions and compares different projects tackled by students. It is not intended to be a comparative analysis of different types of teaching but rather an analysis of different problems observed in different profiles of students of the subject who study it in different grades. The world of audio can be very interesting for a large part of the students, both those with creative and technical inclinations. Musical creation and production, synchronization with images, dubbing, etc. They are disciplines that are generally interesting but can have a very high barrier to entry due to their great technical complexity. Sometimes it can take weeks or even months for the uninitiated to begin to use audio editing programs with the necessary ease, which are not always particularly intuitive for students. Learning through the use of PBL methodologies generates, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RCBEV-KAN&#31639;&#27861;&#65292;&#19968;&#31181;&#34701;&#21512;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#20840;&#26032;&#31639;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02088</link><description>&lt;p&gt;
KAN-RCBEV&#28145;&#24230;&#65306;&#22522;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for autonomous driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RCBEV-KAN&#31639;&#27861;&#65292;&#19968;&#31181;&#34701;&#21512;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#20840;&#26032;&#31639;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02088v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20934;&#30830;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22240;&#20026;&#36974;&#25377;&#12289;&#29289;&#20307;&#35268;&#27169;&#30340;&#21464;&#21270;&#20197;&#21450;&#22797;&#26434;&#30340;&#22478;&#24066;&#29615;&#22659;&#32780;&#26497;&#20855;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RCBEV-KAN&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#40479;&#30640;&#35270;&#22270;(BEV)&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02088v1 Announce Type: cross  Abstract: Accurate 3D object detection in autonomous driving is critical yet challenging due to occlusions, varying object scales, and complex urban environments. This paper introduces the RCBEV-KAN algorithm, a pioneering method designed to enhance 3D object detection by fusing multimodal sensor data from cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View (BEV)-based approach, utilizing a Transformer architecture, significantly boosts detection precision and efficiency by seamlessly integrating diverse data sources, improving spatial relationship handling, and optimizing computational processes. Experimental results show that the RCBEV-KAN model demonstrates superior performance across most detection categories, achieving higher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score (0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8% faster). These results indicate that RCBEV-KAN i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#35780;&#20272;&#19982;&#36873;&#25321;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35757;&#32451;&#30340;&#25968;&#25454;&#26041;&#27861;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#65292;&#26088;&#22312;&#20026;&#26368;&#20248;&#30340;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
&#26631;&#39064;&#65306;&#37322;&#25918;&#25968;&#25454;&#24040;&#28010;&#30340;&#21147;&#37327;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35757;&#32451;&#30340;&#25968;&#25454;&#35780;&#20215;&#19982;&#36873;&#25321;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#35780;&#20272;&#19982;&#36873;&#25321;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35757;&#32451;&#30340;&#25968;&#25454;&#26041;&#27861;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#65292;&#26088;&#22312;&#20026;&#26368;&#20248;&#30340;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;arXiv:2408.02085v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25991;&#25688;&#35201;&#65306;&#25351;&#20196;&#35757;&#32451;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#21916;&#22909;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#24320;&#25918;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20294;&#30450;&#30446;&#22320;&#22312;&#25152;&#26377;&#29616;&#26377;&#25351;&#20196;&#19978;&#35757;&#32451;&#19968;&#20010;LLM&#21487;&#33021;&#24182;&#19981;&#29702;&#24819;&#19988;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#26377;&#21033;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#25351;&#20196;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#30693;&#35782;&#24046;&#36317;&#65292;&#21363;&#21738;&#20123;&#25968;&#25454;&#35780;&#20272;&#25351;&#26631;&#21487;&#20197;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#26159;&#22914;&#20309;&#34701;&#20837;&#36873;&#25321;&#26426;&#21046;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#29992;&#20110;&#25351;&#20196;&#35757;&#32451;&#30340;LLMs&#30340;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#29616;&#26377;&#25991;&#29486;&#30340;&#20840;&#38754;&#22238;&#39038;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#25152;&#26377;&#36866;&#29992;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22522;&#20110;&#36136;&#37327;&#30340;&#12289;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#21644;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#19977;&#31867;&#65292;&#20854;&#20013;&#32454;&#21270;&#20102;&#31934;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#23545;&#20110;&#27599;&#31181;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#26410;&#26469;&#30740;&#31350;&#21487;&#33021;&#30340;&#31354;&#30333;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#21644;&#26032;&#20852;&#25216;&#26415;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#25105;&#20204;&#30456;&#20449;&#21487;&#20197;&#20026;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#20799;&#31461;&#21457;&#23637;&#36831;&#32531;&#35786;&#26029;&#31579;&#26597;&#26041;&#27861;&#65292;&#26088;&#22312;&#26089;&#21457;&#29616;&#12289;&#26089;&#24178;&#39044;&#65292;&#20943;&#23569;&#21307;&#30103;&#21644;&#31038;&#20250;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02073</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#20799;&#31461;&#21457;&#23637;&#36831;&#32531;&#35786;&#26029;&#31579;&#26597;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Case-based reasoning approach for diagnostic screening of children with developmental delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#20799;&#31461;&#21457;&#23637;&#36831;&#32531;&#35786;&#26029;&#31579;&#26597;&#26041;&#27861;&#65292;&#26088;&#22312;&#26089;&#21457;&#29616;&#12289;&#26089;&#24178;&#39044;&#65292;&#20943;&#23569;&#21307;&#30103;&#21644;&#31038;&#20250;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#30340;&#25968;&#25454;&#65292;&#20840;&#29699;&#32422;&#26377;6%&#33267;9%&#30340;&#20154;&#21475;&#24739;&#26377;&#21457;&#23637;&#36831;&#32531;&#12290;&#22522;&#20110;&#20013;&#22269;&#23433;&#24509;&#30465;&#28142;&#21271;&#24066;2023&#24180;&#26032;&#29983;&#20799;&#30340;&#25968;&#37327;&#65288;94,420&#65289;&#65292;&#25105;&#20204;&#20272;&#35745;&#27599;&#24180;&#26377;&#22823;&#32422;7,500&#20363;&#65288;&#30097;&#20284;&#21457;&#23637;&#36831;&#32531;&#30149;&#20363;&#65289;&#30340;&#30097;&#34385;&#30149;&#20363;&#12290;&#23545;&#36825;&#20123;&#20799;&#31461;&#36827;&#34892;&#26089;&#26399;&#35782;&#21035;&#24182;&#36827;&#34892;&#36866;&#24403;&#30340;&#26089;&#26399;&#24178;&#39044;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21307;&#30103;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#31038;&#20250;&#25104;&#26412;&#12290;&#22269;&#38469;&#30740;&#31350;&#25351;&#20986;&#65292;&#21457;&#23637;&#36831;&#32531;&#20799;&#31461;&#30340;&#26368;&#20339;&#24178;&#39044;&#26399;&#26159;&#20845;&#23681;&#21069;&#65292;&#26368;&#22909;&#26159;&#22312;&#19977;&#23681;&#21322;&#20043;&#21069;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25509;&#21463;&#26089;&#26399;&#24178;&#39044;&#30340;&#21457;&#23637;&#36831;&#32531;&#20799;&#31461;&#30151;&#29366;&#26126;&#26174;&#25913;&#21892;&#65292;&#29978;&#33267;&#26377;&#20123;&#20799;&#31461;&#21487;&#33021;&#23436;&#20840;&#24247;&#22797;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#32467;&#21512;CNN-Trans&#30340;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02073v1 Announce Type: new  Abstract: According to the World Health Organization, the population of children with developmental delays constitutes approximately 6% to 9% of the total population. Based on the number of newborns in Huaibei, Anhui Province, China, in 2023 (94,420), it is estimated that there are about 7,500 cases (suspected cases of developmental delays) of suspicious cases annually. Early identification and appropriate early intervention for these children can significantly reduce the wastage of medical resources and societal costs. International research indicates that the optimal period for intervention in children with developmental delays is before the age of six, with the golden treatment period being before three and a half years of age. Studies have shown that children with developmental delays who receive early intervention exhibit significant improvement in symptoms; some may even fully recover. This research adopts a hybrid model combining a CNN-Tran
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#30452;&#25509;&#35268;&#21010;&#20986;&#20572;&#36710;&#36335;&#24452;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02061</link><description>&lt;p&gt;
&#20572;&#36710;&#22330;&#32593;&#32476;&#65306;&#30456;&#26426;&#20026;&#22522;&#30784;&#30340;&#31471;&#21040;&#31471;&#27850;&#36710;&#65292;&#20174;&#22270;&#20687;&#21040;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02061
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#30452;&#25509;&#35268;&#21010;&#20986;&#20572;&#36710;&#36335;&#24452;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02061v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#33258;&#21160;&#39550;&#39542;&#27850;&#36710;&#26159;&#26234;&#33021;&#39550;&#39542;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#27850;&#36710;&#31639;&#27861;&#36890;&#24120;&#22522;&#20110;&#35268;&#21017;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31639;&#27861;&#35774;&#35745;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#20572;&#36710;&#22330;&#26223;&#20013;&#19981;&#22826;&#26377;&#25928;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02061v1 Announce Type: cross  Abstract: Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conducted extensive experiments in real-world scenarios, and the resul
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;HVTrack&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#20013;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#38382;&#39064;&#65292;&#36890;&#36807;&#30456;&#23545;&#23039;&#24577;&#24863;&#30693;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#24352;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.02049</link><description>&lt;p&gt;
&#28857;&#20113;&#20013;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
3D Single-object Tracking in Point Clouds with High Temporal Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02049
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HVTrack&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#20013;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#38382;&#39064;&#65292;&#36890;&#36807;&#30456;&#23545;&#23039;&#24577;&#24863;&#30693;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#24352;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02049v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#65288;3D SOT&#65289;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#28857;&#20113;&#30340;&#39640;&#26102;&#24207;&#21464;&#21270;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#28857;&#20113;&#30340;&#24418;&#29366;&#21464;&#21270;&#21644;&#23545;&#35937;&#22312;&#30456;&#37051;&#24103;&#20043;&#38388;&#30340;&#36816;&#21160;&#26159;&#24179;&#28369;&#30340;&#65292;&#26080;&#27861;&#24212;&#23545;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#20013;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#19977;&#32500;&#21333;&#23545;&#35937;&#36319;&#36394;&#38382;&#39064;&#65292;&#31216;&#20026;HVTrack&#12290;HVTrack&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#32452;&#20214;&#26469;&#22788;&#29702;&#39640;&#26102;&#24207;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#65306;1&#65289;&#30456;&#23545;&#23039;&#24577;&#24863;&#30693;&#35760;&#24518;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#24207;&#21464;&#21270;&#30340;&#28857;&#20113;&#24418;&#29366;&#65307;2&#65289;&#22522;&#25193;&#24352;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#25193;&#23637;&#30340;&#25628;&#32034;&#21306;&#22495;&#20869;&#19982;&#23545;&#35937;&#30456;&#20284;&#30340;&#24178;&#25200;&#65307;3&#65289;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;KITTI-HV&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#19981;&#21516;&#30340;&#24103;&#38388;&#38548;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#26102;&#24207;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02049v1 Announce Type: new  Abstract: The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#36793;&#32536;&#29983;&#25104;&#35745;&#31639;&#31995;&#32479;&#20013;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#36164;&#28304;&#30340;&#32852;&#21512;&#20998;&#37197;&#38382;&#39064;&#65292;&#20197;&#38477;&#20302;&#29992;&#25143;&#30340;&#26381;&#21153;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2408.02047</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#31227;&#21160;&#36793;&#32536;&#29983;&#25104;&#35745;&#31639;&#30340;&#24863;&#30693;&#24615;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Latency-Aware Resource Allocation for Mobile Edge Generation and Computing via Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#36793;&#32536;&#29983;&#25104;&#35745;&#31639;&#31995;&#32479;&#20013;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#36164;&#28304;&#30340;&#32852;&#21512;&#20998;&#37197;&#38382;&#39064;&#65292;&#20197;&#38477;&#20302;&#29992;&#25143;&#30340;&#26381;&#21153;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02047v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26368;&#36817;&#65292;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25216;&#26415;&#30340;&#32467;&#21512;&#20652;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#65292;&#21363;&#31227;&#21160;&#36793;&#32536;&#29983;&#25104;&#35745;&#31639;&#65288;MEGC&#65289;&#65292;&#23427;&#20026;&#31227;&#21160;&#29992;&#25143;&#25552;&#20379;&#24322;&#26500;&#26381;&#21153;&#65292;&#22914;&#20219;&#21153;&#35745;&#31639;&#21644;&#20869;&#23481;&#29983;&#25104;&#12290;&#22312;&#36825;&#31687;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MEGC&#31995;&#32479;&#20013;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#31227;&#21160;&#29992;&#25143;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#30001;&#20110;&#20248;&#21270;&#21464;&#37327;&#30340;&#24378;&#28872;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20004;&#20010;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02047v1 Announce Type: cross  Abstract: Recently, the integration of mobile edge computing (MEC) and generative artificial intelligence (GAI) technology has given rise to a new area called mobile edge generation and computing (MEGC), which offers mobile users heterogeneous services such as task computing and content generation. In this letter, we investigate the joint communication, computation, and the AIGC resource allocation problem in an MEGC system. A latency minimization problem is first formulated to enhance the quality of service for mobile users. Due to the strong coupling of the optimization variables, we propose a new deep reinforcement learning-based algorithm to solve it efficiently. Numerical results demonstrate that the proposed algorithm can achieve lower latency than two baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24494;&#35843;&#20102;&#20960;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;Twitter/X&#19978;&#30340;&#19996;&#27431;V4&#22269;&#23478;&#35821;&#35328;&#20013;&#26377;&#20851;&#20420;&#32599;&#26031;&#21644;&#20044;&#20811;&#20848;&#20891;&#20107;&#20914;&#31361;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02044</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;Twitter/X&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24494;&#35843;&#65306;&#23545;V4&#22269;&#23478;&#19996;&#27431;&#22235;&#22269;&#22806;&#35821;&#30340;&#24773;&#24863;&#20998;&#26512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24494;&#35843;&#20102;&#20960;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;Twitter/X&#19978;&#30340;&#19996;&#27431;V4&#22269;&#23478;&#35821;&#35328;&#20013;&#26377;&#20851;&#20420;&#32599;&#26031;&#21644;&#20044;&#20811;&#20848;&#20891;&#20107;&#20914;&#31361;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02044v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02044v1 Announce Type: cross  Abstract: The aspect-based sentiment analysis (ABSA) is a standard NLP task with numerous approaches and benchmarks, where large language models (LLM) represent the current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data in underrepresented languages. On such narrow tasks, small tuned language models can often outperform universal large ones, providing available and cheap solutions.   We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for classification of sentiment towards Russia and Ukraine in the context of the ongoing military conflict. The training/testing dataset was obtained from the academic API from Twitter/X during 2023, narrowed to the languages of the V4 countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their performance under a variety of settings including translations, sentiment targets, in-context learning and more, using GPT4 as a reference model. We document several inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#30340;&#23646;&#24615;&#22270;&#20013;&#24102;&#26631;&#31614;&#30340;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;PIONEER&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#36335;&#24452;&#30340;&#25239;&#21333;&#35843;&#24615;&#36827;&#34892;&#26377;&#25928;&#21098;&#26525;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#25216;&#26415;&#21644;&#24182;&#34892;&#21270;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#12290;</title><link>https://arxiv.org/abs/2408.02029</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#22823;&#22411;&#30340;&#23646;&#24615;&#22270;&#20013;&#25366;&#25496;&#24102;&#26631;&#31614;&#30340;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Mining Path Association Rules in Large Property Graphs (with Appendix)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#30340;&#23646;&#24615;&#22270;&#20013;&#24102;&#26631;&#31614;&#30340;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;PIONEER&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#36335;&#24452;&#30340;&#25239;&#21333;&#35843;&#24615;&#36827;&#34892;&#26377;&#25928;&#21098;&#26525;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#25216;&#26415;&#21644;&#24182;&#34892;&#21270;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;arXiv:2408.02029v1&#30340;&#20844;&#21578;&#31867;&#22411;&#20026;&#20132;&#21449;&#21442;&#32771;&#30340;&#25991;&#29486;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#23567;&#25968;&#25454;&#38598;&#21040;&#22823;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#12290;&#36890;&#36807;&#29616;&#22330;&#32463;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#26159;&#21487;&#34892;&#30340;&#12289;&#25104;&#26412;&#25928;&#30410;&#24456;&#39640;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#39044;&#27979;&#26469;&#35828;&#26159;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#26469;&#25506;&#32034;&#20351;&#29992;&#22810;&#25968;&#25454;&#28304;&#25972;&#21512;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;DINEOF-2.0&#36719;&#20214;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;30&#21517;&#24739;&#32773;&#30340;&#32437;&#21521;&#21307;&#30103;&#24433;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#25104;&#21151;&#22320;&#26816;&#27979;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30456;&#20851;&#30340;&#20307;&#31215;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22836;&#39592;&#22788;&#26041;&#23545;&#25239;&#27874;&#30340;&#26657;&#27491;&#26159;&#26377;&#30410;&#30340;&#65292;&#24182;&#19988;&#26410;&#26469;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02029v1 Announce Type: cross  Abstract: How can we mine frequent path regularities from a graph with edge labels and vertex attributes? The task of association rule mining successfully discovers regular patterns in item sets and substructures. Still, to our best knowledge, this concept has not yet been extended to path patterns in large property graphs. In this paper, we introduce the problem of path association rule mining (PARM). Applied to any \emph{reachability path} between two vertices within a large graph, PARM discovers regular ways in which path patterns, identified by vertex attributes and edge labels, co-occur with each other. We develop an efficient and scalable algorithm PIONEER that exploits an anti-monotonicity property to effectively prune the search space. Further, we devise approximation techniques and employ parallelization to achieve scalable path association rule mining. Our experimental study using real-world graph data verifies the significance of path
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#38754;&#37096;&#19982;&#22768;&#38899;&#35782;&#21035;&#38754;&#20020;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#27861;&#38754;&#23545;&#38754;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2408.02025</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#20110;&#22810;&#35821;&#35328;&#30340;&#22768;&#38899;&#19982;&#38754;&#37096;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning-based Chaining-Cluster for Multilingual Voice-Face Association
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#38754;&#37096;&#19982;&#22768;&#38899;&#35782;&#21035;&#38754;&#20020;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#27861;&#38754;&#23545;&#38754;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02025v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#26368;&#36817;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30740;&#31350;&#19968;&#20010;&#20154;&#33080;&#19982;&#22768;&#38899;&#30340;&#22266;&#26377;&#32852;&#31995;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#26159;&#25105;&#20204;&#20026;2024&#24180;FAME&#65288;Face-Voice Association in Multilingual Environments&#65289;&#25361;&#25112;&#36187;&#25552;&#20986;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#38754;&#37096;&#19982;&#22768;&#38899;&#30340;&#20851;&#32852;&#12290;&#36825;&#39033;&#20219;&#21153;&#28041;&#21450;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#19979;&#24314;&#31435;&#35270;&#35273;&#21644;&#21548;&#35273;&#27169;&#24577;&#20449;&#21495;&#30340;&#29983;&#29289;&#35782;&#21035;&#20851;&#31995;&#65292;&#20197;&#21450;&#24314;&#27169;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#33410;&#22863;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#21487;&#21464;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#19981;&#31616;&#21333;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#30417;&#30563;&#30340;&#20132;&#21449;&#23545;&#27604;&#23398;&#20064;&#65288;SCC&#65289;&#26469;&#24314;&#31435;&#22810;&#35821;&#35328;&#24773;&#26223;&#20013;&#30340;&#22768;&#38899;&#19982;&#38754;&#23380;&#30340;&#22362;&#22266;&#20851;&#32852;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#38142;&#24335;&#38598;&#32676;&#30340;&#21518;&#32493;&#22788;&#29702;&#27493;&#39588;&#65292;&#20197;&#20943;&#36731;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02025v1 Announce Type: cross  Abstract: The innate correlation between a person's face and voice has recently emerged as a compelling area of study, especially within the context of multilingual environments. This paper introduces our novel solution to the Face-Voice Association in Multilingual Environments (FAME) 2024 challenge, focusing on a contrastive learning-based chaining-cluster method to enhance face-voice association. This task involves the challenges of building biometric relations between auditory and visual modality cues and modelling the prosody interdependence between different languages while addressing both intrinsic and extrinsic variability present in the data. To handle these non-trivial challenges, our method employs supervised cross-contrastive (SCC) learning to establish robust associations between voices and faces in multi-language scenarios. Following this, we have specifically designed a chaining-cluster-based post-processing step to mitigate the im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;AutoEncoder&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22312;&#20010;&#20307;&#22522;&#30784;&#19978;&#36827;&#34892;MRI&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#29305;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02018</link><description>&lt;p&gt;
&#20010;&#20307;&#21270;&#22810;&#26102;&#38388;&#32447;MRI&#36712;&#36857;&#39044;&#27979;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;AutoEncoder&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22312;&#20010;&#20307;&#22522;&#30784;&#19978;&#36827;&#34892;MRI&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36864;&#34892;&#24615;&#25913;&#21464;&#36890;&#36807;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26469;&#27979;&#37327;&#65292;&#36825;&#26159;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#19968;&#31181;&#28508;&#22312;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20294;&#20854;&#26222;&#36941;&#35748;&#20026;&#19981;&#22914;&#28096;&#31881;&#26679;&#34507;&#30333;&#25110;tau&#29983;&#29289;&#26631;&#24535;&#29289;&#29305;&#24322;&#24615;&#12290;&#30001;&#20110;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#33041;&#35299;&#21078;&#32467;&#26500;&#30340;&#24046;&#24322;&#24456;&#22823;&#65292;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;MRI&#26102;&#38388;&#24207;&#21015;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#29305;&#24322;&#24615;&#65292;&#21363;&#23558;&#27599;&#20010;&#24739;&#32773;&#35270;&#20026;&#20854;&#33258;&#36523;&#30340;&#22522;&#32447;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36716;&#21521;&#26465;&#20214;&#21464;&#20998; Autoencoder &#26469;&#29983;&#25104;&#20010;&#20307;&#21270;&#30340;MRI&#39044;&#27979;&#65292;&#36825;&#20123;&#39044;&#27979;&#22522;&#20110;&#24739;&#32773;&#30340;&#24180;&#40836;&#12289;&#30142;&#30149;&#29366;&#24577;&#21644;&#20043;&#21069;&#30340;&#25195;&#25551;&#12290;&#20351;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#20513;&#35758;&#65288;ADNI&#65289;&#30340;&#36830;&#32493;&#25104;&#20687;&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#26550;&#26500;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#65292;&#21487;&#20197;&#20174;&#35813;&#20998;&#24067;&#20013;&#25277;&#26679;&#29983;&#25104;&#35299;&#21078;&#32467;&#26500;&#21464;&#21270;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#36229;&#20986;&#25968;&#25454;&#38598;&#30340;&#33539;&#22260;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;MRI&#36827;&#34892;&#39044;&#27979;&#65292;&#26368;&#22810;&#21487;&#36798;10&#24180;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20445;&#30041;&#30340;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#20174;&#26410;&#34987;&#29992;&#26469;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02018v1 Announce Type: new  Abstract: Neurodegeneration as measured through magnetic resonance imaging (MRI) is recognized as a potential biomarker for diagnosing Alzheimer's disease (AD), but is generally considered less specific than amyloid or tau based biomarkers. Due to a large amount of variability in brain anatomy between different individuals, we hypothesize that leveraging MRI time series can help improve specificity, by treating each patient as their own baseline. Here we turn to conditional variational autoencoders to generate individualized MRI predictions given the subject's age, disease status and one previous scan. Using serial imaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a novel architecture to build a latent space distribution which can be sampled from to generate future predictions of changing anatomy. This enables us to extrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated the model on a held-out set fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;MetaWearS&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20943;&#23569;&#20415;&#25658;&#24335;&#31995;&#32479;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;&#29992;&#21407;&#22411;&#26356;&#26032;&#26426;&#21046;&#31616;&#21270;&#26356;&#26032;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2408.01988</link><description>&lt;p&gt;
MetaWearS: &#19968;&#31181;&#20165;&#38656;&#23569;&#37327;&#24555;&#29031;&#30340;&#20415;&#25658;&#24335;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few Shots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;MetaWearS&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20943;&#23569;&#20415;&#25658;&#24335;&#31995;&#32479;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;&#29992;&#21407;&#22411;&#26356;&#26032;&#26426;&#21046;&#31616;&#21270;&#26356;&#26032;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20415;&#25658;&#24335;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36830;&#32493;&#30340;&#20581;&#24247;&#30417;&#27979;&#21151;&#33021;&#65292;&#24182;&#26377;&#21487;&#33021;&#22312;&#26089;&#26399;&#21457;&#29616;&#28508;&#22312;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20415;&#25658;&#24335;&#31995;&#32479;&#30340;&#29983;&#21629;&#21608;&#26399;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#26032;&#30340;&#20415;&#25658;&#24335;&#35774;&#22791;&#65292;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#38656;&#35201;&#26469;&#33258;&#21508;&#31181;&#20027;&#39064;&#30340;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#30452;&#25509;&#30001;&#20415;&#25658;&#24335;&#35774;&#22791;&#25910;&#38598;&#12290;&#20854;&#27425;&#65292;&#21518;&#32487;&#30340;&#27169;&#22411;&#26356;&#26032;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#37325;&#26032;&#22521;&#35757;&#12290;&#26368;&#21518;&#65292;&#39057;&#32321;&#22320;&#22312;&#20415;&#25658;&#24335;&#35774;&#22791;&#19978;&#26356;&#26032;&#27169;&#22411;&#21487;&#33021;&#20250;&#20943;&#23569;&#22312;&#38271;&#26399;&#25968;&#25454;&#30417;&#25511;&#20013;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;MetaWearS&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#21407;&#22411;&#26356;&#26032;&#26426;&#21046;&#65292;&#36890;&#36807;&#20462;&#25913;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#37325;&#26032;&#22521;&#35757;&#25972;&#20010;&#27169;&#22411;&#65292;&#31616;&#21270;&#20102;&#26356;&#26032;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#20013;&#25506;&#35752;&#20102;MetaWearS&#30340;&#24615;&#33021;&#65292;&#21363;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#21644;&#26234;&#33021;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01988v1 Announce Type: cross  Abstract: Wearable systems provide continuous health monitoring and can lead to early detection of potential health issues. However, the lifecycle of wearable systems faces several challenges. First, effective model training for new wearable devices requires substantial labeled data from various subjects collected directly by the wearable. Second, subsequent model updates require further extensive labeled data for retraining. Finally, frequent model updating on the wearable device can decrease the battery life in long-term data monitoring. Addressing these challenges, in this paper, we propose MetaWearS, a meta-learning method to reduce the amount of initial data collection required. Moreover, our approach incorporates a prototypical updating mechanism, simplifying the update process by modifying the class prototype rather than retraining the entire model. We explore the performance of MetaWearS in two case studies, namely, the detection of epil
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#34987;&#24635;&#32467;&#20986;&#30340;&#20013;&#25991;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.01986</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#34987;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
DeMansia: Mamba Never Forgets Any Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#34987;&#24635;&#32467;&#20986;&#30340;&#20013;&#25991;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#34987;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01986v1 Announce Type: cross  Abstract: This paper examines the mathematical foundations of transformer architectures, highlighting their limitations particularly in handling long sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM), and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia integrates state space models with token labeling techniques to enhance performance in image classification tasks, efficiently addressing the computational challenges posed by traditional transformers. The architecture, benchmark, and comparisons with contemporary models demonstrate DeMansia's effectiveness. The implementation of this paper is available on GitHub at https://github.com/catalpaaa/DeMansia
&lt;/p&gt;</description></item><item><title>SR-CIS&#36890;&#36807;&#32467;&#21512;&#23567;&#27169;&#22411;&#24555;&#36895;&#25512;&#29702;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;CA-OAD&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#23384;&#19982;&#25512;&#29702;&#35299;&#32806;&#30340;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#23398;&#20064;&#26426;&#21046;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.01970</link><description>&lt;p&gt;
SR-CIS: &#33258;&#25105;&#21453;&#24605;&#22686;&#37327;&#31995;&#32479;&#19982;&#35299;&#32806;&#35760;&#24518;&#19982;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01970
&lt;/p&gt;
&lt;p&gt;
SR-CIS&#36890;&#36807;&#32467;&#21512;&#23567;&#27169;&#22411;&#24555;&#36895;&#25512;&#29702;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;CA-OAD&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#23384;&#19982;&#25512;&#29702;&#35299;&#32806;&#30340;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#23398;&#20064;&#26426;&#21046;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01970v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20154;&#31867;&#24555;&#36895;&#23398;&#20064;&#26032;&#30693;&#35782;&#21516;&#26102;&#20445;&#30041;&#26087;&#35760;&#24518;&#30340;&#33021;&#21147;&#20026;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35760;&#24518;&#21644;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24335;&#20114;&#34917;&#22686;&#37327;&#31995;&#32479;&#65288;SR-CIS&#65289;&#12290;SR-CIS&#30001;&#35299;&#26500;&#21270;&#30340;&#20114;&#34917;&#25512;&#26029;&#27169;&#22359;&#65288;CIM&#65289;&#21644;&#20114;&#34917;&#35760;&#24518;&#27169;&#22359;&#65288;CMM&#65289;&#32452;&#25104;&#65292;&#20854;&#20013;CIM&#36890;&#36807;&#32622;&#20449;&#24230;&#33258;&#36866;&#24212;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65288;CA-OAD&#65289;&#26426;&#21046;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#23567;&#27169;&#22411;&#65292;&#32780;CIM&#21017;&#30001;Confidence-Aware Online Anomaly Detection&#65288;CA-OAD&#65289;&#26426;&#21046;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#21644;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#27169;&#22411;&#12290;CMM&#30001;&#20219;&#21153;&#29305;&#23450;&#30340;&#30701;&#26399;&#35760;&#24518;&#65288;STM&#65289;&#21306;&#22495;&#21644;&#36890;&#29992;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#21306;&#22495;&#32452;&#25104;&#12290;&#36890;&#36807;&#35774;&#23450;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#21644;&#30456;&#20851;&#21407;&#22411;&#26435;&#37325;&#21644;&#20559;&#24046;&#65292;&#23427;&#20026;&#21442;&#25968;&#21644;&#34920;&#31034;&#35760;&#24518;&#24314;&#31435;&#20102;&#22806;&#37096;&#23384;&#20648;&#23454;&#20363;&#65292;&#20174;&#32780;&#35299;&#26500;&#20102;&#35760;&#24518;&#27169;&#22359;&#19982;&#25512;&#29702;&#27169;&#22359;&#65292;&#24182;&#23454;&#29616;&#20102;&#35760;&#24518;&#30340;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere
&lt;/p&gt;</description></item><item><title>ML-EAT&#26159;&#19968;&#31181;&#37327;&#24230;&#35821;&#35328;&#25216;&#26415;&#22266;&#26377;&#20559;&#35265;&#30340;&#24037;&#20855;&#65292;&#37319;&#29992;&#22810;&#23618;&#27425;&#26041;&#27861;&#36827;&#34892;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2408.01966</link><description>&lt;p&gt;
ML-EAT&#65306;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#22810;&#23618;&#27425;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65292;&#20197;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#35821;&#35328;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01966
&lt;/p&gt;
&lt;p&gt;
ML-EAT&#26159;&#19968;&#31181;&#37327;&#24230;&#35821;&#35328;&#25216;&#26415;&#22266;&#26377;&#20559;&#35265;&#30340;&#24037;&#20855;&#65292;&#37319;&#29992;&#22810;&#23618;&#27425;&#26041;&#27861;&#36827;&#34892;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65288;ML-EAT&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#35821;&#35328;&#25216;&#26415;&#30340;&#22266;&#26377;&#20559;&#35265;&#36827;&#34892;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#24230;&#37327;&#12290;ML-EAT&#36890;&#36807;&#37327;&#21270;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#20559;&#24046;&#8212;&#8212;&#20004;&#20010;&#30446;&#26631;&#27010;&#24565;&#19982;&#20004;&#20010;&#23646;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#20851;&#32852;&#65307;&#20004;&#20010;&#23646;&#24615;&#27010;&#24565;&#20013;&#27599;&#20010;&#30446;&#26631;&#27010;&#24565;&#30340;&#20010;&#20307;&#25928;&#24212;&#22823;&#23567;&#65307;&#20197;&#21450;&#27599;&#20010;&#20010;&#20307;&#30446;&#26631;&#27010;&#24565;&#19982;&#27599;&#20010;&#20010;&#20307;&#23646;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#32852;&#8212;&#8212;&#35299;&#20915;&#20102;&#20256;&#32479;EAT&#27979;&#37327;&#30340;&#27169;&#31946;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;ML-EAT&#65292;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;EAT&#27169;&#24335; taxonomy&#65292;&#25551;&#36848;&#20102;&#19968;&#20010;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#30340;&#20061;&#31181;&#21487;&#33021;&#32467;&#26524;&#65292;&#27599;&#31181;&#32467;&#26524;&#37117;&#19982;&#19968;&#20010;&#29420;&#29305;&#30340;EAT-Map&#30456;&#20851;&#32852;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22235;&#35937;&#38480;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;ML-EAT&#12290;&#23545;&#38745;&#24577;&#21644;&#21382;&#26102;&#21333;&#35789;&#23884;&#20837;&#20197;&#21450;GPT-2&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;ML-EAT&#33021;&#22815;&#25581;&#31034;&#23884;&#20837;&#32454;&#33410;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01966v1 Announce Type: cross  Abstract: This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#31995;&#32479;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2408.01964</link><description>&lt;p&gt;
&#20013;&#25991;&#25688;&#35201;&#65306;&#20160;&#20040;&#26159;&#22522;&#20110;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#32593;&#32476;&#23433;&#20840;&#31574;&#30053;&#65311;
&lt;/p&gt;
&lt;p&gt;
Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#31995;&#32479;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01964v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20998;&#24067;&#24335;&#31995;&#32479;&#22240;&#20854;&#20855;&#26377;&#30340;&#39640;&#24230;&#21487;&#38752;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#23433;&#20840;&#31574;&#30053;&#20013;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#21644;&#20998;&#26512;&#20998;&#24067;&#24335;&#31995;&#32479;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#23433;&#20840;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#19981;&#21463;&#25915;&#20987;&#21644;&#20445;&#25252;&#25968;&#25454;&#23433;&#20840;&#12290;&#20316;&#32773;&#23558;&#39318;&#20808;&#20171;&#32461;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#29305;&#28857;&#65292;&#28982;&#21518;&#35814;&#32454;&#25506;&#35752;&#20854;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#23454;&#20363;&#65292;&#24182;&#25552;&#20986;&#19968;&#20123;&#21487;&#33021;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#38450;&#33539;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01964v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have attracted substantial interest due to their exceptional performance on graph-based data. However, their robustness, especially on heterogeneous graphs, remains underexplored, particularly against adversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion black-box attack method for heterogeneous graphs. By integrating reinforcement learning with a Top-K algorithm to reduce the action space, our method efficiently identifies effective attack strategies to disrupt node classification tasks. We validate the effectiveness of HeteroKRLAttack through experiments on multiple heterogeneous graph datasets, showing significant reductions in classification accuracy compared to baseline methods. An ablation study underscores the critical role of the Top-K algorithm in enhancing attack performance. Our findings highlight potential vulnerabilities in current models and provide guidance for future d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#25918;&#29983;&#25104;&#27169;&#22411;&#23545;&#20197;&#20154;&#20026;&#26412;&#30340;&#25968;&#25454;&#31185;&#23398;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#30340;&#20855;&#20307;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#32452;&#32455;&#22312;&#25968;&#25454;&#31185;&#23398;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#25918;&#27169;&#22411;&#30340;&#21160;&#26426;&#21450;&#20854;&#23545;AI&#29983;&#25104;&#31038;&#20250;&#24433;&#21709;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2408.01962</link><description>&lt;p&gt;
&#24320;&#25918;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#20154;&#20026;&#26412;&#30340;&#25968;&#25454;&#31185;&#23398;&#24037;&#20316;&#20013;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#25918;&#29983;&#25104;&#27169;&#22411;&#23545;&#20197;&#20154;&#20026;&#26412;&#30340;&#25968;&#25454;&#31185;&#23398;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#30340;&#20855;&#20307;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#32452;&#32455;&#22312;&#25968;&#25454;&#31185;&#23398;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#25918;&#27169;&#22411;&#30340;&#21160;&#26426;&#21450;&#20854;&#23545;AI&#29983;&#25104;&#31038;&#20250;&#24433;&#21709;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01962v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#21628;&#21505;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#20351;&#29992;&#24320;&#25918;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#31185;&#23398;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24615;AI&#30340;&#24433;&#21709;&#36828;&#19981;&#27490;&#20110;&#23398;&#26415;&#30028;&#65292;&#22240;&#20026;&#20225;&#19994;&#21644;&#20844;&#20849;&#21033;&#30410;&#32452;&#32455;&#20063;&#24320;&#22987;&#23558;&#36825;&#20123;&#27169;&#22411;&#34701;&#20837;&#21040;&#20182;&#20204;&#30340;&#25968;&#25454;&#31185;&#23398;&#27969;&#20013;&#12290;&#25105;&#20204;&#25193;&#22823;&#20102;&#36825;&#20010;&#35270;&#35282;&#65292;&#21253;&#25324;&#24320;&#25918;&#27169;&#22411;&#23545;&#32452;&#32455;&#30340;&#24433;&lt;/p&gt;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01962v1 Announce Type: cross  Abstract: Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an int
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#21457;&#29616;&#33521;&#23612;&#20004;&#31181;&#35821;&#35328;&#30340;AI&#20542;&#21521;&#23558;&#38738;&#23569;&#24180;&#25551;&#32472;&#25104;&#31038;&#20250;&#38382;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#32780;&#38738;&#23569;&#24180;&#33258;&#24049;&#26356;&#24076;&#26395;&#34987;&#23637;&#31034;&#20986;&#20316;&#20026;&#25104;&#38271;&#20013;&#20010;&#20307;&#25152;&#38754;&#20020;&#30340;&#27491;&#24120;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.01961</link><description>&lt;p&gt;
&#12298;&#20154;&#24037;&#26234;&#33021;&#20013;&#38738;&#23569;&#24180;&#20195;&#34920;&#24615;&#30340;&#20559;&#24046;&#65306;&#36328;&#35821;&#35328;&#36328;&#25991;&#21270;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01961
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#21457;&#29616;&#33521;&#23612;&#20004;&#31181;&#35821;&#35328;&#30340;AI&#20542;&#21521;&#23558;&#38738;&#23569;&#24180;&#25551;&#32472;&#25104;&#31038;&#20250;&#38382;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#32780;&#38738;&#23569;&#24180;&#33258;&#24049;&#26356;&#24076;&#26395;&#34987;&#23637;&#31034;&#20986;&#20316;&#20026;&#25104;&#38271;&#20013;&#20010;&#20307;&#25152;&#38754;&#20020;&#30340;&#27491;&#24120;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#38395;&#23186;&#20307;&#23545;&#38738;&#23569;&#24180;&#30340;&#24120;&#35265;&#25551;&#32472;&#65292;&#36825;&#20123;&#25551;&#32472;&#24448;&#24448;&#24102;&#26377;&#22840;&#22823;&#21644;&#36127;&#38754;&#33394;&#24425;&#65292;&#23558;&#38738;&#23569;&#24180;&#35270;&#20316;&#23545;&#31038;&#20250;&#26500;&#25104;&#39118;&#38505;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#31038;&#20250;&#20445;&#25252;&#30340;&#32676;&#20307;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#24320;&#22987;&#25215;&#25285;&#37096;&#20998;&#20256;&#32479;&#23186;&#20307;&#30340;&#21151;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#25216;&#26415;&#26159;&#22914;&#20309;&#21453;&#26144;&#23545;&#38738;&#23569;&#24180;&#30340;&#25551;&#32472;&#20197;&#21450;&#23545;&#20182;&#20204;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#35770;&#25991;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#20110;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#20013;&#30340;&#38738;&#23569;&#24180;&#32676;&#20307;&#65306;&#32654;&#22269;&#21644;&#23612;&#27850;&#23572;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#38745;&#24577;&#35789;&#23884;&#20837;&#65288;SWEs&#65289;&#21644;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#22312;&#25551;&#36848;&#38738;&#23569;&#24180;&#26102;&#23384;&#22312;&#30528;&#20559;&#35265;&#12290;&#22312;&#33521;&#35821;&#35821;&#35328;&#30340;SWEs&#20013;&#65292;&#38738;&#23569;&#24180;&#19982;&#31038;&#20250;&#19978;&#30340;&#19968;&#20123;&#38382;&#39064;&#30456;&#25346;&#38057;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;GloVe SWEs&#20013;&#65292;&#19982;&#38738;&#23569;&#24180;&#26368;&#20026;&#30456;&#20851;&#30340;1000&#20010;&#21333;&#35789;&#20013;&#65292;&#36229;&#36807;50%&#19982;&#36825;&#20123;&#31038;&#20250;&#38382;&#39064;&#30456;&#20851;&#12290;&#22312;&#20351;&#29992;GPT2-XL&#21644;LLaMA-2-7B GLMs&#36827;&#34892;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36229;&#36807;30%&#30340;&#36755;&#20986;&#28041;&#21450;&#21040;&#31038;&#20250;&#38382;&#39064;&#65292;&#20363;&#22914;&#26292;&#21147;&#12289;&#33647;&#29289;&#28389;&#29992;&#12289;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#20197;&#21450;&#24615;&#31105;&#24524;&#31561;&#12290;&#23612;&#27850;&#23572;&#35821;&#30340;&#27169;&#22411;&#21516;&#26679;&#26174;&#31034;&#20986;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#38738;&#23569;&#24180;&#32676;&#20307;&#33258;&#36523;&#30340;&#35270;&#35282;&#36827;&#34892;&#20998;&#26512;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#38738;&#23569;&#24180;&#24076;&#26395;&#24471;&#21040;&#26356;&#31215;&#26497;&#12289;&#24179;&#34913;&#30340;&#25551;&#32472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01961v1 Announce Type: cross  Abstract: Popular and news media often portray teenagers with sensationalism, as both a risk to society and at risk from society. As AI begins to absorb some of the epistemic functions of traditional media, we study how teenagers in two countries speaking two languages: 1) are depicted by AI, and 2) how they would prefer to be depicted. Specifically, we study the biases about teenagers learned by static word embeddings (SWEs) and generative language models (GLMs), comparing these with the perspectives of adolescents living in the U.S. and Nepal. We find English-language SWEs associate teenagers with societal problems, and more than 50% of the 1,000 words most associated with teenagers in the pretrained GloVe SWE reflect such problems. Given prompts about teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss societal problems, most commonly violence, but also drug use, mental illness, and sexual taboo. Nepali models, while n
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#26041;&#38754;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#31038;&#20250;&#19968;&#33268;&#24615;&#20250;&#20013;&#20171;&#27169;&#22411;&#21453;&#26144;&#30340;&#20154;&#31867;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38598;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20934;&#30830;&#24418;&#25104;&#37027;&#20123;&#20381;&#36182;&#20110;&#19981;&#21487;&#35265;&#23646;&#24615;&#30340;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2408.01959</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#31038;&#20250;&#19968;&#33268;&#24615;&#20013;&#20171;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#30340;&#35270;&#35273;&#35821;&#35328;AI
&lt;/p&gt;
&lt;p&gt;
Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01959
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#26041;&#38754;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#31038;&#20250;&#19968;&#33268;&#24615;&#20250;&#20013;&#20171;&#27169;&#22411;&#21453;&#26144;&#30340;&#20154;&#31867;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38598;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20934;&#30830;&#24418;&#25104;&#37027;&#20123;&#20381;&#36182;&#20110;&#19981;&#21487;&#35265;&#23646;&#24615;&#30340;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01959v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#22312;&#21253;&#25324;&#33258;&#21160;&#22270;&#20687;&#23383;&#24149;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#29978;&#33267;&#23545;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#29992;&#25143;&#30340;&#23433;&#20840;&#35775;&#38382;&#24212;&#29992;&#31243;&#24207;&#20063;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#23545;&#20559;&#35265;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#21644;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#38754;&#37096;&#21360;&#35937;&#20559;&#35265;&#65292;&#24182;&#19988;&#25105;&#20204;&#39318;&#27425;&#21457;&#29616;&#65292;&#36825;&#20123;&#20559;&#35265;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;CLIP&#27169;&#22411;&#23478;&#26063;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;&#25105;&#20204;&#36824;&#39318;&#27425;&#23637;&#31034;&#20102;&#65292;&#19968;&#20010;&#20559;&#35265;&#30340;&#31243;&#24230;&#22312;&#31038;&#20250;&#20013;&#20849;&#20139;&#65292;&#39044;&#27979;&#20102;&#23427;&#22312;CLIP&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#31243;&#24230;&#30340;&#31243;&#24230;&#12290;&#22312;&#20165;&#23545;&#26368;&#22823;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#20154;&#31867;&#20284;&#30340;&#38754;&#37096;&#21360;&#35937;&#25165;&#20250;&#20986;&#29616;&#65292;&#36825;&#34920;&#26126;&#19982;&#26410;&#32463;&#23457;&#26680;&#30340;&#25991;&#21270;&#30340;&#26356;&#22909;&#21305;&#37197;&#23545;&#20110;&#22797;&#21046;&#36234;&#26469;&#36234;&#24494;&#22937;&#30340;&#31038;&#20132;&#20559;&#35265;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#23545;&#22270;&#20687;&#30340;&#26576;&#20123;&#19981;&#21487;&#35265;&#23646;&#24615;&#65288;&#22914;&#21487;&#20449;&#36182;&#24615;&#21644;&#24615;&#21462;&#21521;&#65289;&#24418;&#25104;&#21360;&#35937;&#26102;&#65292;&#21482;&#26377;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#19979;&#25991;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#25165;&#26356;&#21152;&#20934;&#30830;&#12290;&#36825;&#21487;&#33021;&#24847;&#21619;&#30528;&#22312;&#29983;&#25104;&#19982;&#31038;&#20250;&#20559;&#22909;&#39640;&#24230;&#19968;&#33268;&#30340;&#25991;&#26412;&#25551;&#36848;&#26041;&#38754;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#36828;&#31163;&#23545;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#26144;&#23556;&#65292;&#24182;&#24320;&#22987;&#23398;&#20064;&#22914;&#20309;&#20102;&#35299;&#21644;&#27169;&#20223;&#20154;&#31867;&#30340;&#31038;&#20250;&#24418;&#24577;&#30693;&#35273;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#25581;&#31034;&#36825;&#20123;&#20559;&#35265;&#23545;&#20915;&#31574;&#21046;&#23450;&#30340;&#28508;&#22312;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35270;&#35273;&#23545;&#40784;&#21644;&#22522;&#20110;VLM&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#21319;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#23548;&#20195;&#29702;&#36827;&#34892;&#26410;&#35265;&#36807;&#26032;&#23545;&#35937;&#30340;&#38646;&#26679;&#26412;&#27010;&#25324;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01942</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#35937;&#32423;&#21035;&#27010;&#25324;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Visual Grounding for Object-Level Generalization in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01942
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35270;&#35273;&#23545;&#40784;&#21644;&#22522;&#20110;VLM&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#21319;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#23548;&#20195;&#29702;&#36827;&#34892;&#26410;&#35265;&#36807;&#26032;&#23545;&#35937;&#30340;&#38646;&#26679;&#26412;&#27010;&#25324;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#23548;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#20195;&#29702;&#36827;&#34892;&#27010;&#25324;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#26469;&#36827;&#34892;&#35270;&#35273;&#23545;&#40784;&#65292;&#24182;&#23558;&#36825;&#31181;&#35270;&#35273;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21040;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#22312;&#26410;&#35265;&#36807;&#30340;&#26032;&#23545;&#35937;&#21644;&#25351;&#20196;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#27010;&#25324;&#12290;&#36890;&#36807;&#35270;&#35273;&#23545;&#40784;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25351;&#31034;&#25351;&#20196;&#20013;&#30446;&#26631;&#23545;&#35937;&#30340;&#20855;&#20307;&#32622;&#20449;&#22270;&#12290;&#22522;&#20110;&#35813;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;VLM&#30693;&#35782;&#36716;&#31227;&#21040;RL&#20013;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#22270;&#30340;&#23545;&#35937;&#23545;&#40784;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#20195;&#29702;&#25509;&#36817;&#30446;&#26631;&#23545;&#35937;&#12290;&#20854;&#27425;&#65292;&#32622;&#20449;&#22270;&#20026;&#20195;&#29702;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#21152;&#32479;&#19968;&#21644;&#21487;&#35775;&#38382;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#30456;&#23545;&#20110;&#35821;&#35328;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#33021;&#21147;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#26356;&#26377;&#25928;&#30340;&#36884;&#24452;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#26032;&#23545;&#35937;&#21644;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01942v1 Announce Type: new  Abstract: Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26102;&#30340;&#20915;&#31574;&#21644;&#22797;&#21512;&#39118;&#38505;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#26694;&#26550;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#39118;&#38505;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#39057;&#32321;&#38169;&#35823;&#25253;&#21578;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2408.01935</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#23450;&#20041;&#21644;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#34987;&#24212;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#21644;&#22797;&#21512;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26102;&#30340;&#20915;&#31574;&#21644;&#22797;&#21512;&#39118;&#38505;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#26694;&#26550;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#39118;&#38505;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#39057;&#32321;&#38169;&#35823;&#25253;&#21578;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;: &#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20063;&#34987;&#35748;&#20026;&#23384;&#22312;&#37325;&#35201;&#30340;&#39118;&#38505;&#12290;&#20854;&#20013;&#19968;&#31181;&#39118;&#38505;&#26469;&#33258;&#20110;&#27169;&#26495;&#30340;&#20449;&#24515;&#38169;&#35823;&#25918;&#32622;&#65292;&#26080;&#35770;&#26159;&#36807;&#20110;&#33258;&#20449;&#36824;&#26159;&#32570;&#20047;&#33258;&#20449;&#65292;&#36825;&#22312;&#27169;&#22411;&#23545;&#35821;&#20041;&#25512;&#29702;&#30340;&#21028;&#26029;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#34429;&#28982;&#21069;&#32773;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#21518;&#32773;&#32570;&#20047;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#23545;&#22522;&#20110;&#20449;&#24515;&#38169;&#35823;&#25918;&#32622;&#30340;&#27169;&#22411;&#20840;&#38754;&#39118;&#38505;&#30340;&#29702;&#35299;&#23384;&#22312;&#19981;&#23545;&#31216;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;&#20004;&#31181;&#39118;&#38505;&#31867;&#22411;&#65288;&#20915;&#31574;&#39118;&#38505;&#21644;&#22797;&#21512;&#39118;&#38505;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#32423;&#25512;&#29702;&#26550;&#26500;&#21644;&#30456;&#24212;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#27979;&#37327;&#22312;&#20998;&#31867;&#21644;&#29983;&#25104;&#24615;LLMs&#20013;&#30340;&#39118;&#38505;&#12290;&#31532;&#19968;&#32423;&#20381;&#36182;&#20110;&#19968;&#20010;&#20915;&#31574;&#35268;&#21017;&#65292;&#23427;&#20915;&#23450;&#20102;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#24212;&#35813;&#36991;&#20813;&#20570;&#20986;&#25512;&#29702;&#12290;&#22914;&#26524;&#27169;&#22411;&#19981;&#22238;&#36991;&#65292;&#31532;&#20108;&#32423;(&#23427;&#23558;&#24212;&#29992;&#20110;&#27169;&#22411;&#22238;&#36991;&#30340;&#24773;&#20917;)&#23601;&#26159;&#27169;&#22411;&#30340;&#25512;&#29702;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#30340;&#35814;&#32454;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#39057;&#32321;&#38169;&#35823;&#25253;&#21578;&#26041;&#38754;&#30340;&#23616;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#25512;&#29702;&#26102;&#20943;&#23569;&#20107;&#25925;&#39118;&#38505;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01935v1 Announce Type: cross  Abstract: Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FULORA&#30340;&#20195;&#29702;&#21452;&#20154;&#30693;&#35782;&#22270;&#25512;&#29702;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39640;&#25928;&#25351;&#24341;-&#25506;&#32034;&#31574;&#30053;&#26469;&#20811;&#26381;&#22810;&#36339;&#25512;&#29702;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#21644;&#35757;&#32451;&#38590;&#39064;&#12290;&#36890;&#36807;&#39640;&#32423;&#20195;&#29702;&#25552;&#20379;&#38454;&#27573;&#25351;&#24341;&#65292;&#20302;&#32423;&#20195;&#29702;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01880</link><description>&lt;p&gt;
&#12298;&#21452;&#20154;&#36187;&#36305;&#22312;&#22270;&#34920;&#19978;&#65306;&#36890;&#36807;&#26377;&#25928;&#25351;&#24341;-&#25506;&#32034;&#30340;&#20195;&#29702;&#21452;&#20154;&#30693;&#35782;&#22270;&#25512;&#29702;&#12299;
&lt;/p&gt;
&lt;p&gt;
Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FULORA&#30340;&#20195;&#29702;&#21452;&#20154;&#30693;&#35782;&#22270;&#25512;&#29702;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39640;&#25928;&#25351;&#24341;-&#25506;&#32034;&#31574;&#30053;&#26469;&#20811;&#26381;&#22810;&#36339;&#25512;&#29702;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#21644;&#35757;&#32451;&#38590;&#39064;&#12290;&#36890;&#36807;&#39640;&#32423;&#20195;&#29702;&#25552;&#20379;&#38454;&#27573;&#25351;&#24341;&#65292;&#20302;&#32423;&#20195;&#29702;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22810;&#36339;&#25512;&#29702;&#22312;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25512;&#29702;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#21407;&#22240;&#22312;&#20110;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#31232;&#30095;&#65292;&#20808;&#21069;&#30340;&#22823;&#22810;&#36339;&#25512;&#29702;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#20195;&#29702;&#22312;&#26089;&#26399;&#38454;&#27573;&#38590;&#20197;&#23398;&#20250;&#26377;&#25928;&#30340;&#12289;&#31283;&#20581;&#30340;&#25919;&#31574;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22914;&#31232;&#30095;&#30693;&#35782;&#22270;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#36208;&#36807;&#38271;&#36335;&#24452;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#20195;&#29702;&#21452;&#20154;&#30693;&#35782;&#22270;&#25512;&#29702;&#27169;&#22411;&#65292;&#21517;&#20026;FULORA&#12290;FULORA&#36890;&#36807;&#39640;&#25928;&#25351;&#24341;-&#25506;&#32034;&#30340;&#21452;&#20154;&#20195;&#29702;&#26469;&#24212;&#23545;&#19978;&#36848;&#25512;&#29702;&#25361;&#25112;&#12290;&#39640;&#32423;&#20195;&#29702;&#22312;&#31616;&#21270;&#21518;&#30340;&#30693;&#35782;&#22270;&#19978;&#34892;&#36208;&#65292;&#20026;&#20302;&#32423;&#20195;&#29702;&#22312;&#21407;&#22987;&#30693;&#35782;&#22270;&#19978;&#30340;&#34892;&#36208;&#25552;&#20379;&#38454;&#27573;&#24341;&#23548;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20302;&#32423;&#20195;&#29702;&#20248;&#21270;&#19968;&#20010;&#20215;&#20540;&#65292;&#20197;&#20415;&#36873;&#25321;&#36866;&#24403;&#30340;&#36793;&#32536;&#26469;&#23548;&#33322;&#30693;&#35782;&#22270;&#20197;&#21040;&#36798;&#30446;&#26631;&#23454;&#20307;&#12290;&#22312;&#36825;&#31181;&#26041;&#24335;&#20013;&#65292;&#39640;&#32423;&#20195;&#29702;&#36890;&#36807;&#25552;&#20379;&#38454;&#27573;&#24615;&#25351;&#24341;&#26469;&#22686;&#24378;&#20302;&#32423;&#20195;&#29702;&#30340;&#32463;&#39564;&#31215;&#32047;&#65292;&#20351;&#24471;&#20302;&#32423;&#20195;&#29702;&#21487;&#20197;&#20174;&#39640;&#32423;&#20195;&#29702;&#30340;&#25351;&#23548;&#19979;&#24555;&#36895;&#25910;&#25947;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20195;&#29702;&#20174;&#22836;&#24320;&#22987;&#22312;&#21407;&#22987;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#19968;&#36807;&#31243;&#26377;&#25928;&#22320;&#21152;&#36895;&#20102;&#25512;&#29702;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#20195;&#29702;&#22312;&#35768;&#22810;&#19981;&#21516;&#31867;&#22411;&#30693;&#35782;&#22270;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#35797;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;FULORA&#22312;&#38169;&#35823;&#29575;&#21644;&#20854;&#20182;&#25928;&#26524;&#25351;&#26631;&#19978;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31232;&#30095;&#21644;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01880v1 Announce Type: new  Abstract: Recent years, multi-hop reasoning has been widely studied for knowledge graph (KG) reasoning due to its efficacy and interpretability. However, previous multi-hop reasoning approaches are subject to two primary shortcomings. First, agents struggle to learn effective and robust policies at the early phase due to sparse rewards. Second, these approaches often falter on specific datasets like sparse knowledge graphs, where agents are required to traverse lengthy reasoning paths. To address these problems, we propose a multi-hop reasoning model with dual agents based on hierarchical reinforcement learning (HRL), which is named FULORA. FULORA tackles the above reasoning challenges by eFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks on the simplified knowledge graph to provide stage-wise hints for the low-level agent walking on the original knowledge graph. In this framework, the low-level agent optimizes a value 
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2408.01872</link><description>&lt;p&gt;
&#20351;&#29992;&#19982;&#27491;&#26679;&#26412;&#30456;&#21516;&#30340;&#20869;&#20998;&#24067;&#25968;&#25454;&#23454;&#29616;&#23433;&#20840;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01872
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01872v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#24403;&#21482;&#26377;&#23569;&#37327;&#30340;&#26631;&#31614;&#21487;&#29992;&#26102;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#26631;&#27880;&#21644;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#31867;&#20998;&#24067;&#30456;&#31561;&#65307;&#28982;&#32780;&#65292;&#24403;&#26410;&#26631;&#27880;&#25968;&#25454;&#20013;&#23384;&#22312;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20197;&#24448;&#30340;&#23433;&#20840;&#21322;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#24050;&#32463;&#36890;&#36807;&#22522;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31574;&#30053;&#26377;&#25928;&#38477;&#20302;&#20102;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36825;&#20123;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#22320;&#31579;&#36873;&#20986;&#19981;&#24517;&#35201;&#30340;&#20986;&#20998;&#24067;&#25968;&#25454;&#65292;&#20182;&#20204;&#20063;&#21487;&#33021;&#20250;&#22833;&#21435;&#25968;&#25454;&#20043;&#38388;&#20849;&#20139;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#32780;&#19981;&#31649;&#20854;&#31867;&#21035;&#22914;&#20309;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#31995;&#25968;&#35843;&#24230;&#27604;&#20363;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#32858;&#21512;&#20316;&#20026;&#38170;&#23450;&#30340;&#26631;&#27880;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#30830;&#20445;&#25152;&#26377;&#25968;&#25454;&#22312;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#36991;&#20813;&#22240;&#36807;&#28388;&#20986;&#20998;&#24067;&#25968;&#25454;&#32780;&#25439;&#22833;&#22522;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110; linguistic feature &#30340;&#40657;&#30418;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#25104;&#26412;&#20302;&#19988;&#33021;&#25269;&#25239;&#27169;&#22411;&#26356;&#26032;&#30340;&#25932;&#24847;&#38899;&#39057;&#26679;&#26412;&#65292;&#20197;&#25104;&#21151;&#27450;&#39575; ASR &#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2408.01808</link><description>&lt;p&gt;
ALIF: &#20302;&#25104;&#26412;&#22522;&#20110; adversarial audio &#25915;&#20987;&#30340;&#40657;&#30418;&#35821;&#38899;&#24179;&#21488;&#20351;&#29992; linguistics features
&lt;/p&gt;
&lt;p&gt;
ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110; linguistic feature &#30340;&#40657;&#30418;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#25104;&#26412;&#20302;&#19988;&#33021;&#25269;&#25239;&#27169;&#22411;&#26356;&#26032;&#30340;&#25932;&#24847;&#38899;&#39057;&#26679;&#26412;&#65292;&#20197;&#25104;&#21151;&#27450;&#39575; ASR &#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01808v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#36825;&#26159; arXiv &#19978;&#19968;&#20010;&#39044;&#21360;&#26412;&#30340;&#25688;&#35201;&#32763;&#35793;&#12290;&#21407;&#25991;&#26159;&#30740;&#31350;&#20013;&#21457;&#29616; adversarial examples (AE) &#23545;&#22768;&#38899;&#25511;&#21046;&#26234;&#33021;&#35774;&#22791;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20165;&#38656;&#35201;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#31995;&#32479;&#26368;&#32456;&#36716;&#24405;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#23545; ASR &#36827;&#34892;&#24456;&#22810;&#26597;&#35810;&#65292;&#36825;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110; AE &#30340;&#25932;&#24847;&#38899;&#39057;&#26679;&#26412;&#23481;&#26131;&#21463;&#21040; ASR &#26356;&#26032;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#21363;&#26080;&#27861;&#30452;&#25509;&#22312;&#28145;&#24230;&#23398;&#20064; (DL) &#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#19978;&#26500;&#36896; AE &#25915;&#20987;&#26679;&#26412;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ALIF&#65292;&#21363;&#31532;&#19968;&#20010;&#22522;&#20110;&#40657;&#30418; adversarial linguistic feature &#30340;&#25915;&#20987;&#31649;&#36947;&#12290;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#21040;&#35821;&#38899; (TTS) &#21644; ASR &#27169;&#22411;&#20043;&#38388;&#30340;&#20114;&#36870;&#36807;&#31243;&#22312; linguistic embedding &#31354;&#38388;&#20013;&#29983;&#25104;&#25200;&#21160;&#65292;&#20854;&#20013;&#20915;&#31574;&#36793;&#30028;&#23384;&#22312;&#12290;&#22522;&#20110; ALIF &#31649;&#36947;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20165;&#38656;&#35201;&#20960;&#27493;&#26597;&#35810;&#20415;&#33021;&#27450;&#39575; ASR &#30340;&#20302;&#25104;&#26412;&#38899;&#39057;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#23558;&#25915;&#20987;&#22312; linguistic feature &#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#36825;&#20123;&#26679;&#26412;&#33021;&#22815;&#24456;&#22909;&#22320;&#25269;&#25239; ASR &#26356;&#26032;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01808v1 Announce Type: cross  Abstract: Extensive research has revealed that adversarial examples (AE) pose a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic speech recognition (ASR) system. However, these attacks typically involve many queries to the ASR, resulting in substantial costs. Moreover, AE-based adversarial audio samples are susceptible to ASR updates. In this paper, we identify the root cause of these limitations, namely the inability to construct AE attack samples directly around the decision boundary of deep learning (DL) models. Building on this observation, we propose ALIF, the first black-box adversarial linguistic feature-based attack pipeline. We leverage the reciprocal process of text-to-speech (TTS) and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides. Based on the ALIF pi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26234;&#33021;&#21046;&#36896;&#19994;&#20013;&#20113;&#26381;&#21153;&#20248;&#21270;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#20248;&#21270;&#25351;&#26631;&#20307;&#31995;&#65292;&#26088;&#22312;&#25903;&#25345;&#26234;&#33021;&#21046;&#36896;&#24179;&#21488;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01795</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#19994;&#30340;&#20113;&#26381;&#21153;&#32452;&#21512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of Cloud Service Composition for Intelligent Manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26234;&#33021;&#21046;&#36896;&#19994;&#20013;&#20113;&#26381;&#21153;&#20248;&#21270;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#20248;&#21270;&#25351;&#26631;&#20307;&#31995;&#65292;&#26088;&#22312;&#25903;&#25345;&#26234;&#33021;&#21046;&#36896;&#24179;&#21488;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01795v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; Abstract: &#26234;&#33021;&#21046;&#36896;&#19994;&#26159;&#19968;&#31181;&#20351;&#29992;&#20808;&#36827;&#25216;&#26415;&#65292;&#22914;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21046;&#36896;&#29983;&#20135;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#20316;&#20026;&#25512;&#21160;&#21046;&#36896;&#19994;&#36716;&#22411;&#21644;&#21319;&#32423;&#30340;&#37325;&#35201;&#25903;&#25345;&#65292;&#20113;&#26381;&#21153;&#20248;&#21270;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#20026;&#20102;&#26234;&#33021;&#21046;&#36896;&#24179;&#21488;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#26234;&#33021;&#21046;&#36896;&#19994;&#30340;&#20113;&#26381;&#21153;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#20998;&#25955;&#30340;&#20248;&#21270;&#25351;&#26631;&#21644;&#38750;&#32479;&#19968;/&#38750;&#26631;&#20934;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#26234;&#33021;&#21046;&#36896;&#24179;&#21488;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#36843;&#20999;&#38656;&#27714;&#20986;&#21457;&#65292;&#23450;&#20041;&#20102;11&#20010;&#32771;&#34385;&#19977;&#26041;&#21442;&#19982;&#32773;&#20027;&#20307;&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#65292;&#23454;&#29616;&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01795v1 Announce Type: new  Abstract: Intelligent manufacturing is a new model that uses advanced technologies such as the Internet of Things, big data, and artificial intelligence to improve the efficiency and quality of manufacturing production. As an important support to promote the transformation and upgrading of the manufacturing industry, cloud service optimization has received the attention of researchers. In recent years, remarkable research results have been achieved in this field. For the sustainability of intelligent manufacturing platforms, in this paper we summarize the process of cloud service optimization for intelligent manufacturing. Further, to address the problems of dispersed optimization indicators and nonuniform/unstandardized definitions in the existing research, 11 optimization indicators that take into account three-party participant subjects are defined from the urgent requirements of the sustainable development of intelligent manufacturing platform
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#30740;&#31350;&#19987;&#27880;&#20110;&#24314;&#31435;&#36731;&#37327;&#32423;CNN&#27169;&#22411;&#65292;&#22914;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#30340;&#39640;&#25928;&#21644;&#20934;&#30830;&#35782;&#21035;&#12290;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#32477;&#23545;&#24046;&#24322;&#25805;&#20316;&#12289;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#21644;&#25913;&#36827;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30149;&#23475;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01752</link><description>&lt;p&gt;
&#25552;&#21319;&#32511;&#33394;AI: &#20026;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#35782;&#21035;&#35774;&#35745;&#30340;&#26377;&#25928;&#21644;&#39640;&#31934;&#24230;&#30340;&#36731;&#37327;&#32423;CNN
&lt;/p&gt;
&lt;p&gt;
Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#30740;&#31350;&#19987;&#27880;&#20110;&#24314;&#31435;&#36731;&#37327;&#32423;CNN&#27169;&#22411;&#65292;&#22914;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#30340;&#39640;&#25928;&#21644;&#20934;&#30830;&#35782;&#21035;&#12290;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#32477;&#23545;&#24046;&#24322;&#25805;&#20316;&#12289;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#21644;&#25913;&#36827;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30149;&#23475;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#27700;&#31291;&#26159;&#20840;&#29699;&#19968;&#21322;&#20197;&#19978;&#20154;&#21475;&#30340;&#20027;&#35201;&#39135;&#29289;&#26469;&#28304;&#65292;&#20854;&#20135;&#37327;&#23545;&#20110;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27700;&#31291;&#26685;&#22521;&#32463;&#24120;&#21463;&#21040;&#22810;&#31181;&#30142;&#30149;&#30340;&#20405;&#25200;&#65292;&#36825;&#20123;&#30142;&#30149;&#21487;&#20197;&#20005;&#37325;&#38477;&#20302;&#20135;&#37327;&#21644;&#21697;&#36136;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#21644;&#20934;&#30830;&#22320;&#26816;&#27979;&#27700;&#31291;&#30142;&#30149;&#23545;&#20110;&#38450;&#27490;&#30142;&#30149;&#34067;&#24310;&#21644;&#20943;&#36731;&#20316;&#29289;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#27425;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#65292;&#29992;&#20110;&#27700;&#31291;&#21494;&#29255;&#30149;&#23475;&#20998;&#31867;&#12290;&#36873;&#25321;&#36825;&#20123;&#27169;&#22411;&#26159;&#22240;&#20026;&#23427;&#20204;&#19982;&#31227;&#21160;&#35774;&#22791;&#20860;&#23481;&#65292;&#19982;&#20854;&#20182;CNN&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#38656;&#35201;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20869;&#23384;&#36739;&#23569;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#19977;&#27454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#28155;&#21152;&#20102;&#20004;&#20010;&#20840;&#36830;&#25509;&#23618;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#20010;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#30340;dropout&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#26089;&#20572;&#26041;&#27861;&#26469;&#38450;&#27490;&#27169;&#22411;&#21457;&#29983;&#36807;&#25311;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#24615;&#33021;&#26469;&#33258;&#8230;&#8230;&#65288;&#30001;&#20110;&#20869;&#23481;&#25972;&#21512;&#20013;&#33521;&#25991;&#25688;&#35201;&#36739;&#38271;&#65292;&#24314;&#35758;&#22312;&#29983;&#25104;&#25688;&#35201;&#26102;&#36827;&#34892;&#36866;&#24403;&#30340;&#31934;&#31616;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01752v1 Announce Type: new  Abstract: Rice plays a vital role as a primary food source for over half of the world's population, and its production is critical for global food security. Nevertheless, rice cultivation is frequently affected by various diseases that can severely decrease yield and quality. Therefore, early and accurate detection of rice diseases is necessary to prevent their spread and minimize crop losses. In this research, we explore three mobile-compatible CNN architectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice leaf disease classification. These models are selected due to their compatibility with mobile devices, as they demand less computational power and memory compared to other CNN models. To enhance the performance of the three models, we added two fully connected layers separated by a dropout layer. We used early stop creation to prevent the model from being overfiting. The results of the study showed that the best performance wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LAM3D &#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22312; Pyramid Vision Transformer v2 &#30340;&#22522;&#30784;&#19978;&#22686;&#21152;2D/3D&#26816;&#27979;&#26426;&#21046;&#65292;&#26377;&#25928;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#22312;KITTI 3D&#23545;&#35937;&#26816;&#27979;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01739</link><description>&lt;p&gt;
LAM3D&#65306;&#21033;&#29992;&#27880;&#24847;&#21147;&#30340;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LAM3D: Leveraging Attention for Monocular 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LAM3D &#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22312; Pyramid Vision Transformer v2 &#30340;&#22522;&#30784;&#19978;&#22686;&#21152;2D/3D&#26816;&#27979;&#26426;&#21046;&#65292;&#26377;&#25928;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#22312;KITTI 3D&#23545;&#35937;&#26816;&#27979;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01739v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#33258;&#20174;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#21644;Transformer&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#37319;&#32435;&#20197;&#26469;&#65292;&#22522;&#20110; Vision Transformer &#30340;&#26550;&#26500;&#22312;&#35813;&#39046;&#22495;&#33719;&#24471;&#20102;&#22823;&#37327;&#30340;&#20851;&#27880;&#65292;&#34987;&#29992;&#20110;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#39640;&#25928;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24212;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LAM3D&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312; Pyramid Vision Transformer v2 (PVTv2) &#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#39592;&#24178;&#19978;&#65292;&#20197;&#21450;&#23545;2D/3D&#26816;&#27979;&#26426;&#22120;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;KITTI 3D &#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36229;&#36234;&#20102;&#21442;&#32771;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20351;&#29992;&#20102; s
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01739v1 Announce Type: cross  Abstract: Since the introduction of the self-attention mechanism and the adoption of the Transformer architecture for Computer Vision tasks, the Vision Transformer-based architectures gained a lot of popularity in the field, being used for tasks such as image classification, object detection and image segmentation. However, efficiently leveraging the attention mechanism in vision transformers for the Monocular 3D Object Detection task remains an open question. In this paper, we present LAM3D, a framework that Leverages self-Attention mechanism for Monocular 3D object Detection. To do so, the proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as feature extraction backbone and 2D/3D detection machinery. We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods. Moreover, due to the usage of s
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01736</link><description>&lt;p&gt;
&#33021;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs predict the convergence of Stochastic Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01736
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01736v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#38395;&#21517;&#12290;&#20854;&#20196;&#20154;&#24778;&#35766;&#30340;&#19968;&#20010;&#20363;&#23376;&#26159;&#26368;&#36817;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28385;&#36275;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#21160;&#21147;&#31995;&#32479; governing &#21407;&#21017;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#21160;&#21147;&#23398;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#19968;&#26041;&#21521;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#36215;&#22987;&#28857;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979; SGD &#25910;&#25947;&#21040;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#38382;&#35810;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26377;&#28508;&#21147;&#29992;&#20110;&#25191;&#34892;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#26356;&#22823;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#38543;&#26426;&#35797;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01736v1 Announce Type: cross  Abstract: Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;&#38899;&#39057;&#21516;&#27493;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#35270;&#39057;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21767;&#24418;&#21516;&#27493;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2408.01732</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#35828;&#35805;&#20154;&#22836;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01732
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;&#38899;&#39057;&#21516;&#27493;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#35270;&#39057;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21767;&#24418;&#21516;&#27493;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01732v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#29983;&#25104;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#34394;&#25311;&#22836;&#20687;&#12289;&#30005;&#24433;&#21046;&#20316;&#21644;&#22312;&#32447;&#20250;&#35758;&#22312;&#20869;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#36807;&#20998;&#24378;&#35843;&#29983;&#25104;&#19982;&#35821;&#38899;&#21516;&#27493;&#30340;&#21767;&#24418;&#65292;&#32780;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;&#24103;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#32780;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#21017;&#36807;&#20998;&#24378;&#35843;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24103;&#65292;&#24573;&#35270;&#20102;&#21767;&#24418;&#21305;&#37197;&#65292;&#23548;&#33268;&#22068;&#37096;&#36816;&#21160;&#20986;&#29616;&#25238;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;&#31532;&#19968;&#38454;&#27573;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#35821;&#38899;&#29983;&#25104;&#21516;&#27493;&#30340;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#29305;&#24449;&#28857;&#20316;&#20026;&#26465;&#20214;&#22312;&#21435;&#22122;&#36807;&#31243;&#20013;&#20351;&#29992;&#65292;&#26088;&#22312;&#20248;&#21270;&#22068;&#37096;&#25238;&#21160;&#38382;&#39064;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#21516;&#27493;&#21644;&#22312;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#35828;&#35805;&#20154;&#22836;&#20687;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#25506;&#35752;&#26159;&#21542;&#23384;&#22312;&#33021;&#22815;&#34987;&#33391;&#24615;&#26679;&#26412;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23545;&#25239;&#24615;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2408.01715</link><description>&lt;p&gt;
&#32852;&#21512;&#36890;&#29992;&#23545;&#25239;&#24615;&#25200;&#21160;&#21450;&#20854;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Joint Universal Adversarial Perturbations with Interpretations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#25506;&#35752;&#26159;&#21542;&#23384;&#22312;&#33021;&#22815;&#34987;&#33391;&#24615;&#26679;&#26412;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23545;&#25239;&#24615;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01715v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01715v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "Downstream Transfer Attack (DTA) &#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;DTA&#33021;&#22815;&#22312;&#22810;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#36816;&#20316;&#12290;</title><link>https://arxiv.org/abs/2408.01705</link><description>&lt;p&gt;
&#19979;&#28216;&#36716;&#31227;&#25915;&#20987;&#65306;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "Downstream Transfer Attack (DTA) &#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;DTA&#33021;&#22815;&#22312;&#22810;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01705v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#38543;&#30528;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;ViT&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#23601;&#20687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19968;&#26679;&#65292;ViT&#20063;&#23481;&#26131;&#21463;&#21040; adversarial &#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#36755;&#20837;&#30340;&#23567;&#24178;&#25200;&#27450;&#39575;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#36716;&#31227;&#26131;&#24863;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110; \emph{&#26679;&#26412;-wise} &#36716;&#31227;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; \emph{Downstream Transfer Attack (DTA)}  &#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#27979;&#35797;&#22270;&#20687;&#65292;DTA&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#26469;&#21046;&#20316; adversarial &#31034;&#20363;&#65292;&#28982;&#21518;&#23558;&#35813; adversarial &#31034;&#20363;&#24212;&#29992;&#20110;&#25915;&#20987;&#19968;&#20010;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#36807;&#30340;&#27169;&#22411;&#29256;&#26412;&#12290;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#65292;DTA&#35782;&#21035;&#24182;&#21033;&#29992;&#27169;&#22411;&#20013;&#26368;&#33030;&#24369;&#30340;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#25915;&#20987;&#30340;&#20010;&#24615;&#21270;&#23450;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DTA &#33021;&#22815;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#26377;&#25928;&#22320;&#36716;&#31227;&#25915;&#20987;&#65292;&#21253;&#25324;&#30446;&#26631;&#26816;&#27979;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DTA &#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#22522;&#20110; adversar &#25915;&#20987;&#30456;&#27604;&#65292;&#33021;&#22815;&#25552;&#39640;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#25152;&#38656;&#30340; adversarial &#31034;&#20363;&#25968;&#37327;&#12290;&#27492;&#24037;&#20316;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#33021;&#20250;&#25581;&#31034;&#22522;&#20110; ViT &#30340;&#27169;&#22411;&#22312;&#26032;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#23433;&#20840;&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01705v1 Announce Type: cross  Abstract: With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on \emph{sample-wise} transfer attacks and propose a novel attack method termed \emph{Downstream Transfer Attack (DTA)}. For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of t
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#39564;&#35777;&#33322;&#31354;&#21046;&#36896;&#25991;&#26723;&#20013;&#30340;&#39640;&#24230;&#22797;&#26434;&#21644;&#20302;&#20135;&#37327;&#20135;&#21697;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2408.01700</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#12298;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#39564;&#35777;&#25991;&#26412;&#27979;&#35797;&#25968;&#25454;&#12299;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01700
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#39564;&#35777;&#33322;&#31354;&#21046;&#36896;&#25991;&#26723;&#20013;&#30340;&#39640;&#24230;&#22797;&#26434;&#21644;&#20302;&#20135;&#37327;&#20135;&#21697;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;: arXiv:2408.01700v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: &#20687;Thales Alenia Space&#36825;&#26679;&#30340;&#33322;&#31354;&#21046;&#36896;&#20844;&#21496;&#65292;&#35774;&#35745;&#12289;&#24320;&#21457;&#12289;&#38598;&#25104;&#12289;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#20135;&#21697;&#20855;&#26377;&#39640;&#24230;&#22797;&#26434;&#24615;&#21644;&#20302;&#20135;&#37327;&#12290;&#23427;&#20204;&#20180;&#32454;&#22320;&#35760;&#24405;&#20102;&#27599;&#20010;&#20135;&#21697;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#20294;&#36328;&#20135;&#21697;&#30340;&#20998;&#26512;&#30001;&#20110;&#25991;&#26723;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#24615;&#36136;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#35770;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;(KGs)&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#39564;&#35777;&#36825;&#20123;&#25991;&#26723;&#20013;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#38024;&#23545;&#19982;&#21355;&#26143;&#30005;&#23376;&#26495;&#30456;&#20851;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Semantic Sensor Network&#26412;&#20307;&#12290;&#25105;&#20204;&#23558;&#25253;&#21578;&#30340;&#20803;&#25968;&#25454;&#23384;&#20648;&#22312;&#19968;&#20010;KG&#20013;&#65292;&#32780;&#23454;&#38469;&#27979;&#35797;&#32467;&#26524;&#21017;&#23384;&#20648;&#22312;&#21487;&#30001;&#34394;&#25311;&#30693;&#35782;&#22270;&#35889;&#35775;&#38382;&#30340;parquet&#26684;&#24335;&#20013;&#12290;&#39564;&#35777;&#36807;&#31243;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26469;&#31649;&#29702;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#27425;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01700v1 Announce Type: new  Abstract: Aerospace manufacturing companies, such as Thales Alenia Space, design, develop, integrate, verify, and validate products characterized by high complexity and low volume. They carefully document all phases for each product but analyses across products are challenging due to the heterogeneity and unstructured nature of the data in documents. In this paper, we propose a hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with Large Language Models (LLMs) to extract and validate data contained in these documents. We consider a case study focused on test data related to electronic boards for satellites. To do so, we extend the Semantic Sensor Network ontology. We store the metadata of the reports in a KG, while the actual test results are stored in parquet accessible via a Virtual Knowledge Graph. The validation process is managed using an LLM-based approach. We also conduct a benchmarking study to evaluate the performanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#19981;&#21464;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#25552;&#21462;&#19981;&#21464;&#29305;&#24449;&#65292;&#20248;&#21270;&#22270;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#21407;&#20998;&#24067;&#25968;&#25454;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01697</link><description>&lt;p&gt;
&#19981;&#21464;&#22270;&#23398;&#20064;&#19982;&#20449;&#24687;&#29942;&#39048;&#29992;&#20110;&#36807;&#20998;&#24067;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#19981;&#21464;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#25552;&#21462;&#19981;&#21464;&#29305;&#24449;&#65292;&#20248;&#21270;&#22270;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#21407;&#20998;&#24067;&#25968;&#25454;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01697v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22270;&#36807;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#27867;&#21270;&#20173;&#28982;&#26159;&#22270;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#24120;&#24120;&#36973;&#21463;&#20005;&#37325;&#30340;&#34920;&#29616;&#24230;&#19979;&#38477;&#12290;&#19981;&#21464;&#23398;&#20064;&#65292;&#26088;&#22312;&#25552;&#21462;&#36328;&#22810;&#31181;&#20998;&#24067;&#30340;&#24658;&#23450;&#29305;&#24449;&#65292;&#22312;OOD&#27867;&#21270;&#38382;&#39064;&#19978;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#23450;&#25104;&#21151;&#12290;&#23613;&#31649;&#19981;&#21464;&#23398;&#20064;&#22312;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#19978;&#30340;OOD&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#22270;&#25968;&#25454;&#30340;&#30740;&#31350;&#20173;&#28982;&#21463;&#21040;&#20102;&#22270;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#30740;&#31350;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#25110;&#22240;&#26524;&#24178;&#39044;&#65292;&#22312;&#22788;&#29702;&#22270;&#30340;&#36807;&#31243;&#20013;&#32463;&#24120;&#30772;&#22351;&#20102;&#19981;&#21464;&#24615;&#65292;&#25110;&#32773;&#30001;&#20110;&#32570;&#20047;&#23545;&#22240;&#26524;&#37096;&#20998;&#30340;&#30417;&#30563;&#20449;&#21495;&#32780;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#19981;&#21464;&#22270;&#23398;&#20064;&#65288;InfoIGL&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21462;&#22270;&#30340;&#24658;&#23450;&#29305;&#24449;&#24182;&#22686;&#24378;&#20854;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#22312;&#25968;&#25454;&#19982;&#29305;&#24449;&#20043;&#38388;&#24314;&#31435;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#27969;&#65292;&#30830;&#20445;&#20102;&#19981;&#21464;&#24615;&#30340;&#25552;&#21462;&#21644;&#26368;&#20248;&#29305;&#24449;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;InfoIGL&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;GNN&#22312;OOD&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#21407;&#22987;&#20998;&#24067;&#25968;&#25454;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290; This framework not only ensures the effective extraction of invariant features but also optimizes the feature representations for graph learning, which is crucial for OOD generalization. Multiple experiments on various graph datasets demonstrate that InfoIGL significantly enhances the performance of GNNs in OOD tasks while maintaining good generalization abilities for the original distribution data.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01697v1 Announce Type: cross  Abstract: Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#30340;&#27450;&#35784;&#26816;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2408.01690</link><description>&lt;p&gt;
IDNet: &#19968;&#20010;&#29992;&#20110;&#36523;&#20221;&#25991;&#26723;&#20998;&#26512;&#21644;&#27450;&#35784;&#26816;&#27979;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#30340;&#27450;&#35784;&#26816;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01690v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#25387;&#36133;&#36523;&#20221;&#30423;&#31363;&#24182;&#21152;&#24378;&#23433;&#20840;&#65292;&#26377;&#25928;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#21644;&#23545;&#25919;&#24220;&#21457;&#34892;&#30340;&#36523;&#20221;&#25991;&#26723;(&#22914;&#25252;&#29031;&#12289;&#39550;&#39542;&#25191;&#29031;&#21644;&#36523;&#20221;&#35777;)&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#35757;&#32451;&#20934;&#30830;&#30340;&#36523;&#20221;&#25991;&#26723;&#27450;&#35784;&#26816;&#27979;&#21644;&#20998;&#26512;&#24037;&#20855;&#20381;&#36182;&#20110;&#22823;&#37327;&#36523;&#20221;&#25991;&#26723;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#29992;&#20110;&#36523;&#20221;&#25991;&#26723;&#20998;&#26512;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;MIDV-500&#12289;MIDV-2020&#21644;FMIDV&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65306;&#23427;&#20204;&#25552;&#20379;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#28041;&#21450;&#30340;&#27450;&#35784;&#27169;&#24335;&#19981;&#36275;&#65292;&#32780;&#19988;&#24456;&#23569;&#21253;&#25324;&#23545;&#20851;&#38190;&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#23383;&#27573;(&#22914;&#32918;&#20687;&#22270;&#20687;)&#30340;&#26356;&#25913;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#30495;&#23454;&#29983;&#27963;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;  &#20026;&#20102;&#22238;&#24212;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#8212;&#8212;IDNet&#65292;&#26088;&#22312;&#25512;&#36827;&#38544;&#31169;&#20445;&#25252;&#30340;&#27450;&#35784;&#26816;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01690v1 Announce Type: new  Abstract: Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy.   In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#949;&#25511;&#21046;&#22240;&#23376;&#35843;&#33410;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21435;&#23398;&#20064;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2408.01689</link><description>&lt;p&gt;
&#20351;&#29992;&#949;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#25511;&#21046;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#949;&#25511;&#21046;&#22240;&#23376;&#35843;&#33410;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21435;&#23398;&#20064;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29983;&#25104;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#27844;&#38706;&#21644;&#20559;&#35265;&#31561;&#38382;&#39064;&#30340;&#25285;&#24551;&#12290;&#26426;&#22120;&#21435;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20363;&#22914;&#21547;&#26377;&#31169;&#20154;&#20449;&#24687;&#21644;&#20559;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Image-to-Image&#65288;I2I&#65289;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20165;&#25552;&#20379;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#24573;&#35270;&#20102;&#29992;&#25143;&#23545;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#25928;&#29992;&#20043;&#38388;&#30340;&#22810;&#26679;&#21270;&#26399;&#24453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#21046;&#21435;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25511;&#21046;&#31995;&#25968;&#949;&#26469;&#25511;&#21046;&#36825;&#31181;&#21462;&#33293;&#12290;&#25105;&#20204;&#23558;I2I&#29983;&#25104;&#27169;&#22411;&#21435;&#23398;&#20064;&#30340;&#21839;&#38988;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#949;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#25214;&#21040;&#38024;&#23545;&#36951;&#24536;&#30340;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#30340;&#26032;&#40092;&#24230;&#21644;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#30340;&#22330;&#26223;&#65292;&#22914;&#22270;&#24418;&#35774;&#35745;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#31561;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#25968;&#25454;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01689v1 Announce Type: cross  Abstract: While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\varepsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\varepsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#22312;3D&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#35821;&#20041;&#23646;&#24615;&#32534;&#36753;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01664</link><description>&lt;p&gt;
SAT3D:&#22522;&#20110;&#22270;&#20687;&#30340;3D&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
SAT3D: Image-driven Semantic Attribute Transfer in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#22312;3D&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#35821;&#20041;&#23646;&#24615;&#32534;&#36753;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01664v1 Announce Type: &#26032;&#30340;&#25688;&#35201;: &#22522;&#20110;GAN&#30340;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#26088;&#22312;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#22270;&#20687;&#23646;&#24615;&#36827;&#34892;&#25805;&#20316;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;2D&#21644;3D&#24863;&#30693;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#38590;&#20197;&#20998;&#36776;&#30340;&#35821;&#20041;&#25110;&#21306;&#22495;&#32534;&#36753;&#23646;&#24615;&#65292;&#36825;&#26080;&#27861;&#23454;&#29616;&#25668;&#24433;&#39118;&#26684;&#30340;&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#65292;&#22914;&#20174;&#19968;&#20010;&#30007;&#20154;&#30340;&#29031;&#29255;&#20013;&#36716;&#31227;&#32993;&#39035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#35821;&#20041;&#23646;&#24615;&#36716;&#31227;&#26041;&#27861;(SAT3D)&#65292;&#36890;&#36807;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#32534;&#36753;&#35821;&#20041;&#23646;&#24615;&#12290;&#23545;&#20110;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#26159;&#22312;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;3D&#24863;&#30693;StyleGAN&#22522;generator&#30340;&#26679;&#24335;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#36890;&#36807;&#23398;&#20064;&#19982;&#26679;&#24335;&#20195;&#30721;&#36890;&#36947;&#30456;&#20851;&#30340;&#35821;&#20041;&#23646;&#24615;&#21644;&#26679;&#24335;&#30721;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#25351;&#23548;&#65292;&#25105;&#20204;&#19982;&#19968;&#32452;&#22522;&#20110;&#30701;&#35821;&#30340;&#25551;&#36848;&#31526;&#32452;&#20851;&#32852;&#27599;&#20010;&#23646;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#27979;&#37327;&#27169;&#22359;(QMM)&#65292;&#20197;&#22522;&#20110;&#25551;&#36848;&#31526;&#32452;&#22312;&#22270;&#20687;&#20013;&#23450;&#37327;&#25551;&#36848;&#23646;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01664v1 Announce Type: new  Abstract: GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor group
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PUCL&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#26410;&#30693;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#32422;&#26463;&#21442;&#25968;&#21270;&#21644;&#29615;&#22659;&#27169;&#22411;&#12290;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#20174;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#36335;&#24452;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36880;&#27493;&#20248;&#21270;&#32422;&#26463;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2408.01622</link><description>&lt;p&gt;
&#22522;&#20110;&#19987;&#23478;&#28436;&#31034;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#32422;&#26463;&#23398;&#20064;&#31639;&#27861; (PUCL)
&lt;/p&gt;
&lt;p&gt;
Positive-Unlabeled Constraint Learning (PUCL) for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PUCL&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#26410;&#30693;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#32422;&#26463;&#21442;&#25968;&#21270;&#21644;&#29615;&#22659;&#27169;&#22411;&#12290;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#20174;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#36335;&#24452;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36880;&#27493;&#20248;&#21270;&#32422;&#26463;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#8220;&#27491;&#36127;&#26410;&#26631;&#27880;&#32422;&#26463;&#23398;&#20064;&#8221;(PUCL)&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#38750;&#32447;&#24615;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#20808;&#30693;&#36947;&#30495;&#23454;&#30340;&#32422;&#26463;&#21442;&#25968;&#21270;&#25110;&#32773;&#29615;&#22659;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25226;&#25152;&#26377;&#30340;&#28436;&#31034;&#25968;&#25454;&#37117;&#35270;&#20026;&#27491;&#26679;&#20363;&#65288;&#21487;&#34892;&#30340;&#65289;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#31574;&#30053;&#26102;&#21019;&#36896;&#28508;&#22312;&#30340;&#19981;&#21487;&#34892;&#36712;&#36857;&#20316;&#20026;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#39318;&#20808;&#26356;&#26032;&#31574;&#30053;&#65292;&#28982;&#21518;&#24212;&#29992;&#19968;&#20010;&#20004;&#27493;&#30340;&#27491;&#36127;&#26410;&#26631;&#27880;&#23398;&#20064;&#36807;&#31243;&#65292;&#20854;&#20013;&#39318;&#20808;&#20351;&#29992;&#36317;&#31163;&#24230;&#37327;&#26469;&#30830;&#23450;&#21487;&#38752;&#30340;&#19981;&#21487;&#34892;&#25968;&#25454;&#65292;&#38543;&#21518;&#20351;&#29992;&#27491;&#36127;&#26410;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#19968;&#20010;&#30830;&#23450;&#24615;&#32422;&#26463;&#20989;&#25968;&#65292;&#23427;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#30028;&#38598;&#19978;&#30340;&#36817;&#20284;&#65292;&#24182;&#19988;&#22312;&#21487;&#34892;&#22495;&#30340;&#30456;&#37051;&#21306;&#22495;&#20869;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20248;&#21270;&#31574;&#30053;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24182;&#20943;&#24369;&#31574;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26368;&#32456;&#23398;&#20064;&#21040;&#26356;&#31934;&#30830;&#30340;&#32422;&#26463;&#20989;&#25968;&#12290;&#26412;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35268;&#21010;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#65292;&#37027;&#37324;&#23436;&#25972;&#30340;&#32422;&#26463;&#21487;&#33021;&#26082;&#26410;&#30693;&#20063;&#19981;&#26131;&#20934;&#30830;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01622v1 Announce Type: new  Abstract: Planning for a wide range of real-world robotic tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. This paper presents a novel Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a continuous arbitrary constraint function from demonstration, without requiring prior knowledge of the true constraint parameterization or environmental model as existing works. Within our framework, we treat all data in demonstrations as positive (feasible) data, and learn a control policy to generate potentially infeasible trajectories, which serve as unlabeled data. In each iteration, we first update the policy and then a two-step positive-unlabeled learning procedure is applied, where it first identifies reliable infeasible data using a distance metric, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24515;&#29702;&#20998;&#26512;&#24072;&#8221;&#65292;&#19968;&#31181;&#22522;&#20110;OpenAI GPT-4&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#20248;&#21270;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#30340;&#39044;&#31579;&#26597;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;DSM-5&#12289;PHQ-8&#35814;&#32454;&#25968;&#25454;&#25551;&#36848;&#21644;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#22312;&#35782;&#21035;&#24515;&#29702;&#22256;&#25200;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#39044;&#27979;PHQ-8&#35780;&#20998;&#26041;&#38754;&#30340;&#31934;&#30830;&#24230;&#12290;&#36890;&#36807;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#22312;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#21644;&#26381;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01614</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#29702;&#20581;&#24247;&#39044;&#31579;&#26597;&#65306;&#19968;&#31181;&#26032;&#22411;&#24515;&#29702;&#22256;&#25200;&#35780;&#20272;GPT
&lt;/p&gt;
&lt;p&gt;
Advancing Mental Health Pre-Screening: A New Custom GPT for Psychological Distress Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24515;&#29702;&#20998;&#26512;&#24072;&#8221;&#65292;&#19968;&#31181;&#22522;&#20110;OpenAI GPT-4&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#20248;&#21270;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#30340;&#39044;&#31579;&#26597;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;DSM-5&#12289;PHQ-8&#35814;&#32454;&#25968;&#25454;&#25551;&#36848;&#21644;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#22312;&#35782;&#21035;&#24515;&#29702;&#22256;&#25200;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#39044;&#27979;PHQ-8&#35780;&#20998;&#26041;&#38754;&#30340;&#31934;&#30830;&#24230;&#12290;&#36890;&#36807;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#22312;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#21644;&#26381;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24515;&#29702;&#20998;&#26512;&#24072;&#8221;&#30340;&#23450;&#21046;GPT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;OpenAI&#30340;GPT-4&#65292;&#20248;&#21270;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#30340;&#39044;&#31579;&#26597;&#12290;&#35813;&#27169;&#22411;&#20197;DSM-5&#12289;PHQ-8&#35814;&#32454;&#25968;&#25454;&#25551;&#36848;&#21644;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#20026;&#20248;&#21270;&#22522;&#30784;&#65292;&#25797;&#38271;&#35299;&#30721;&#24515;&#29702;&#22256;&#25200;&#30340;&#24494;&#22937;&#35821;&#35328;&#25351;&#31034;&#12290;&#27169;&#22411;&#37319;&#29992;&#21452;&#20219;&#21153;&#26694;&#26550;&#65292;&#21253;&#25324;&#20108;&#20803;&#20998;&#31867;&#21644;PHQ-8&#35780;&#20998;&#30340;&#19977;&#38454;&#27573;&#35745;&#31639;&#65292;&#21253;&#25324;&#21021;&#27493;&#35780;&#20272;&#12289;&#35814;&#32454;&#20998;&#26512;&#21644;&#29420;&#31435;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#31934;&#32454;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;&#20351;&#29992;DAIC-WOZ&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#39564;&#35777;&#26174;&#31034;&#20102;F1&#21644;&#23439;&#35266;F1&#20998;&#25968;&#20998;&#21035;&#20026;0.929&#21644;0.949&#65292;&#24182;&#19988;&#22312;PHQ-8&#35780;&#20998;&#26041;&#38754;&#30340;&#26368;&#20302;MAE&#21644;RMSE&#20998;&#21035;&#20026;2.89&#21644;3.69&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#35813;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#22312;&#25552;&#39640;&#20844;&#20247;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#12289;&#25552;&#21319;&#21487;&#21450;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20316;&#20026;&#19987;&#19994;&#20154;&#22763;&#31532;&#20108;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01614v1 Announce Type: cross  Abstract: This study introduces 'Psycho Analyst', a custom GPT model based on OpenAI's GPT-4, optimized for pre-screening mental health disorders. Enhanced with DSM-5, PHQ-8, detailed data descriptions, and extensive training data, the model adeptly decodes nuanced linguistic indicators of mental health disorders. It utilizes a dual-task framework that includes binary classification and a three-stage PHQ-8 score computation involving initial assessment, detailed breakdown, and independent assessment, showcasing refined analytic capabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1 scores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of 2.89 and 3.69 in PHQ-8 scoring. These results highlight the model's precision and transformative potential in enhancing public mental health support, improving accessibility, cost-effectiveness, and serving as a second opinion for professionals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#24433;&#21709;&#19979;&#22914;&#20309;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#20173;&#28982;&#26159;&#21487;&#20449;&#36182;&#30340;&#65292;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2408.01596</link><description>&lt;p&gt;
&#26631;&#39064;&#65306;&#31038;&#20250;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#19979;&#30340;&#21487;&#20449;&#36182;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Machine Learning under Social and Adversarial Data Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#24433;&#21709;&#19979;&#22914;&#20309;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#20173;&#28982;&#26159;&#21487;&#20449;&#36182;&#30340;&#65292;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20010;&#20154;&#21644;&#32452;&#32455;&#36234;&#26469;&#36234;&#39057;&#32321;&#22320;&#19982;&#36825;&#20123;&#31995;&#32479;&#20114;&#21160;&#65292;&#34920;&#29616;&#20986;&#21508;&#31181;&#31038;&#20250;&#21644;&#23545;&#25239;&#24615;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#34892;&#20026;&#21487;&#33021;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#20123;&#20114;&#21160;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#21487;&#33021;&#26159;&#30001;&#25112;&#30053;&#24615;&#30340;&#20010;&#20307;&#20135;&#29983;&#30340;&#65292;&#30001;&#33258;&#31169;&#30340;&#25968;&#25454;&#25910;&#38598;&#32773;&#25910;&#38598;&#65292;&#21487;&#33021;&#34987;&#25932;&#23545;&#30340;&#25915;&#20987;&#32773;&#31713;&#25913;&#65292;&#24182;&#19988;&#34987;&#29992;&#26469;&#24314;&#31435;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#39044;&#27979;&#22120;&#12289;&#27169;&#22411;&#21644;&#25919;&#31574;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#21487;&#33021;&#20250;&#24694;&#21270;&#65292;&#27604;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26131;&#24863;&#24615;&#65288;Shafahi et al., 2018; Szegedy et al., 2013&#65289;&#20197;&#21450;&#22312;&#23384;&#22312;&#25112;&#30053;&#20010;&#20307;&#30340;&#24773;&#22659;&#19979;&#32463;&#20856;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38477;&#65288;Ahmadi et al., 2021&#65289;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#35201;&#27714;&#24320;&#21457;&#20840;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#65292;&#36825;&#20123;&#26694;&#26550;&#21644;&#25216;&#26415;&#33021;&#22815;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#22312;&#38754;&#23545;&#36947;&#24503;&#21644;&#25112;&#30053;&#24615;&#30340;&#25968;&#25454;&#28304;&#26102;&#20173;&#28982;&#26159;&#21487;&#20449;&#36182;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01596v1 Announce Type: cross  Abstract: Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;Conformal&#26041;&#27861;&#65292;&#23545;&#22823;&#35268;&#27169;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#36827;&#34892;&#39640;&#25928;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#39044;&#27979;&#25968;&#25454;&#30340;&#27169;&#24335;&#21644;&#29702;&#35770;&#20248;&#21183;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#19981;&#21516;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#24182;&#23545;&#27835;&#30103;&#25928;&#24212;&#36827;&#34892;&#31934;&#30830;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2408.01582</link><description>&lt;p&gt;
&#22522;&#20110;Conformal&#26041;&#27861;&#30340;&#39640;&#26031;&#25193;&#25955;&#27169;&#22411;&#22312;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conformal Diffusion Models for Individual Treatment Effect Estimation and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;Conformal&#26041;&#27861;&#65292;&#23545;&#22823;&#35268;&#27169;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#36827;&#34892;&#39640;&#25928;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#39044;&#27979;&#25968;&#25454;&#30340;&#27169;&#24335;&#21644;&#29702;&#35770;&#20248;&#21183;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#19981;&#21516;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#24182;&#23545;&#27835;&#30103;&#25928;&#24212;&#36827;&#34892;&#31934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformal&#26041;&#27861;&#30340;&#39640;&#26031;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#12290;&#36890;&#36807;&#23545;&#20010;&#20307;&#25509;&#21463;&#24178;&#39044;&#21069;&#21518;&#30340;&#39044;&#21518;&#32467;&#26524;&#36827;&#34892;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#65292;&#27169;&#22411;&#25512;&#26029;&#20986;&#20102;&#19968;&#20010;&#24352;&#37327;&#25968;&#25454;&#32467;&#26500;&#65292;&#23558;&#30740;&#31350;&#38382;&#39064;&#36716;&#21270;&#20026;&#21327;&#21464;&#37327;&#20013;&#38388;&#36807;&#31243;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#24182;&#36890;&#36807;&#25506;&#32034;&#36807;&#24448;&#39044;&#27979;&#25968;&#25454;&#30340;&#27169;&#24335;&#21644;&#29702;&#35770;&#20248;&#21183;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;&#20986;&#27835;&#30103;&#25928;&#24212;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#19981;&#21516;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#36824;&#33021;&#22815;&#36890;&#36807;&#21512;&#29702;&#30340;&#20551;&#35774;&#24471;&#21040;&#21512;&#36866;&#30340;&#32479;&#35745;&#21028;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31934;&#24230;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#65292;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#19982;&#25968;&#25454;&#30456;&#20851;&#30340;&#20559;&#24046;&#21644;&#28151;&#26434;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01582v1 Announce Type: cross  Abstract: Estimating treatment effects from observational data is of central interest across numerous application domains. Individual treatment effect offers the most granular measure of treatment effect on an individual level, and is the most useful to facilitate personalized care. However, its estimation and inference remain underdeveloped due to several challenges. In this article, we propose a novel conformal diffusion model-based approach that addresses those intricate challenges. We integrate the highly flexible diffusion modeling, the model-free statistical inference paradigm of conformal inference, along with propensity score and covariate local approximation that tackle distributional shifts. We unbiasedly estimate the distributions of potential outcomes for individual treatment effect, construct an informative confidence interval, and establish rigorous theoretical guarantees. We demonstrate the competitive performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26032;&#22411;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#20010;&#24615;&#21270;&#12289;&#22686;&#26448;&#21046;&#36896;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#36827;&#34892;&#32963;&#24687;&#32905;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2408.01554</link><description>&lt;p&gt;
&#20351;&#29992;&#37096;&#20998;&#34920;&#38754;&#35302;&#35273;&#25104;&#20687;&#30340;&#26426;&#22120;&#20154;&#36171;&#33021;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35786;&#26029;&#32963;&#24687;&#32905;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26032;&#22411;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#20010;&#24615;&#21270;&#12289;&#22686;&#26448;&#21046;&#36896;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#36827;&#34892;&#32963;&#24687;&#32905;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20849;&#21516;&#35299;&#20915;&#39640;&#32423;&#32963;&#30284;&#32959;&#30244;&#30340;&#28040;&#21270;&#36947;&#35786;&#26029;&#20013;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290;&#39318;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#65288;i&#65289;&#20351;&#29992;&#24182;&#35780;&#20272;&#25105;&#20204;&#26368;&#36817;&#24320;&#21457;&#30340;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#65288;VTS&#65289;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#29992;&#23427;&#20204;&#30340;&#32441;&#29702;&#29305;&#24449;&#30340;&#34917;&#20805;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21033;&#29992;&#19968;&#20010;&#19971;&#33258;&#30001;&#24230;&#65288;DoF&#65289;&#30340;&#26426;&#22120;&#20154;&#26426;&#26800;&#25163;&#21644;&#29420;&#29305;&#30340;&#33258;&#23450;&#20041;&#35774;&#35745;&#24182;&#36890;&#36807;&#22686;&#26448;&#21046;&#36896;&#30340;&#26041;&#27861;&#21046;&#25104;&#30340;&#20551;&#32959;&#30244;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;VTS&#36827;&#34892;&#25968;&#25454;&#33258;&#21160;&#25910;&#38598;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#20013;&#36935;&#21040;&#30340;&#31232;&#32570;&#25968;&#25454;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;ML&#27169;&#22411;&#22312;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#19979;&#30340;&#35780;&#20272;&#20013;&#20063;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#19982;&#20351;&#29992;&#21508;&#31181;&#32479;&#35745;&#25351;&#26631;&#30340;&#20256;&#32479;ML&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01554v1 Announce Type: cross  Abstract: In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#24182;&#23450;&#20301;&#20266;&#36896;&#22270;&#20687;&#30340;&#19981;&#33258;&#28982;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2408.01532</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#24182;&#23450;&#20301;&#20266;&#36896;&#22270;&#20687;&#30340;&#19981;&#33258;&#28982;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#28145;&#24230;&#20266;&#36896;&#21644;&#21512;&#25104;&#23186;&#20307;&#30340;&#20986;&#29616;&#23545;&#31038;&#20250;&#25919;&#27835;&#30340;&#23436;&#25972;&#24615;&#21644;&#35802;&#20449;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#22522;&#20110;&#22810;&#27169;&#24577;&#25805;&#32437;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#22914;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#65292;&#26356;&#21152;&#36924;&#30495;&#65292;&#23041;&#32961;&#26356;&#22823;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36890;&#24120;&#22522;&#20110;&#36328;&#27169;&#24577;&#25968;&#25454;&#30340;&#24322;&#26500;&#27969;&#34701;&#21512;&#65292;&#22914;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#22312;&#26377;&#25928;&#34701;&#21512;&#21644;&#22240;&#27492;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#21019;&#36896;&#20102;&#20998;&#24067;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#30340;&#26816;&#27979;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#27880;&#24847;&#21147;&#21040;&#22810;&#27169;&#24577;&#22810;&#24207;&#21015;&#34920;&#31034;&#19978;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#36129;&#29486;&#24615;&#29305;&#24449;&#65292;&#29992;&#20110;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#20301;&#32622;&#35782;&#21035;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#23545;&#20266;&#36896;&#22270;&#20687;&#20013;&#30340;&#19981;&#33258;&#28982;&#21306;&#22495;&#36827;&#34892;&#23450;&#20301;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#25688;&#35201;&#30340;&#24635;&#32467;</title><link>https://arxiv.org/abs/2408.01527</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#22269;&#35770;&#25991;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of Software Desirability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#25688;&#35201;&#30340;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#23398;&#26415;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01527v1 Announce Type: cross  Abstract: This study explores the use of several LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability expressed by users. The study provides scaled numerical sentiment analysis unlike other methods that simply classify sentiment as positive, neutral, or negative. Numerical analysis provides deeper insights into the magnitude of sentiment, to drive better decisions regarding product desirability.   Data is collected through the use of the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of ZORQ, a gamification system used in undergraduate computer science education. The PDT data collected was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment (TRBS), and through Vader, a leading sentiment analysis 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#21442;&#25968;&#31354;&#38388;&#30340;&#26799;&#24230;&#27969;&#21487;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#31561;&#25928;&#20110;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#19988;&#22312;&#21442;&#25968;&#38597;&#21487;&#27604;&#30697;&#38453;&#28385;&#31209;&#30340;&#26465;&#20214;&#19979;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#21487;&#36798;&#12290;</title><link>https://arxiv.org/abs/2408.01517</link><description>&lt;p&gt;
&#21442;&#25968;&#31354;&#38388;&#26799;&#24230;&#27969;&#19982;&#36755;&#20986;&#31354;&#38388;&#32447;&#24615;&#25554;&#20540;&#31561;&#25928;
&lt;/p&gt;
&lt;p&gt;
Gradient flow in parameter space is equivalent to linear interpolation in output space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#21442;&#25968;&#31354;&#38388;&#30340;&#26799;&#24230;&#27969;&#21487;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#31561;&#25928;&#20110;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#19988;&#22312;&#21442;&#25968;&#38597;&#21487;&#27604;&#30697;&#38453;&#28385;&#31209;&#30340;&#26465;&#20214;&#19979;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#21487;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01517v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#23398;&#20064;&#20013;&#35768;&#22810;&#35757;&#32451;&#31639;&#27861;&#32972;&#21518;&#30340;&#26631;&#20934;&#21442;&#25968;&#31354;&#38388;&#26799;&#24230;&#27969;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#34987;&#36830;&#32493;&#21464;&#24418;&#20026;&#36866;&#24212;&#24615;&#26799;&#24230;&#27969;&#65292;&#35813;&#26799;&#24230;&#27969;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#23548;&#33268;&#65288;&#21463;&#38480;&#65289;&#27431;&#27663;&#26799;&#24230;&#27969;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#20851;&#20110;&#21442;&#25968;&#30340;&#36755;&#20986;&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#20110;&#22266;&#23450;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#28385;&#31209;&#65292;&#21017;&#21487;&#20197;&#37325;&#26032;&#21442;&#25968;&#21270;&#26102;&#38388;&#21464;&#37327;&#65292;&#20351;&#24471; resulting flow &#21482;&#26159;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01517v1 Announce Type: cross  Abstract: We prove that the usual gradient flow in parameter space that underlies many training algorithms for neural networks in deep learning can be continuously deformed into an adapted gradient flow which yields (constrained) Euclidean gradient flow in output space. Moreover, if the Jacobian of the outputs with respect to the parameters is full rank (for fixed training data), then the time variable can be reparametrized so that the resulting flow is simply linear interpolation, and a global minimum can be achieved.
&lt;/p&gt;</description></item><item><title>&#26412;&#22320;&#20215;&#20540;&#22522;&#20934;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20854;&#19982;&#28595;&#22823;&#21033;&#20122;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#30340;&#21487;&#25193;&#23637;&#22411;&#26694;&#26550;&#65292;&#24110;&#21161;&#19990;&#30028;&#21508;&#22320;&#30340;&#30417;&#31649;&#26426;&#26500;&#21046;&#23450;&#36866;&#21512;&#33258;&#24049;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2408.01460</link><description>&lt;p&gt;
&#26412;&#22320;&#20215;&#20540;&#22522;&#20934;&#65306;&#19968;&#20010;&#21512;&#20316;&#26500;&#24314;&#30340;&#21487;&#25193;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20215;&#20540;&#23545;&#40784;&#21644;&#20262;&#29702;&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#20215;&#20540;&#22522;&#20934;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20854;&#19982;&#28595;&#22823;&#21033;&#20122;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#30340;&#21487;&#25193;&#23637;&#22411;&#26694;&#26550;&#65292;&#24110;&#21161;&#19990;&#30028;&#21508;&#22320;&#30340;&#30417;&#31649;&#26426;&#26500;&#21046;&#23450;&#36866;&#21512;&#33258;&#24049;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01460v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#23545;&#23427;&#20204;&#19982;&#24403;&#22320;&#20215;&#20540;&#35266;&#21644;&#20262;&#29702;&#26631;&#20934;&#30340;&#23545;&#40784;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#24448;&#24448;&#21453;&#26144;&#20854;&#21019;&#24314;&#32773;&#30340;&#25991;&#21270;&#12289;&#27861;&#24459;&#21644;&#24847;&#35782;&#24418;&#24577;&#20215;&#20540;&#35266;&#30340;&#32972;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#20171;&#32461;&#30340;\textsc{LocalValueBench}&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28595;&#22823;&#21033;&#20122;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#19990;&#30028;&#21508;&#22320;&#30340;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#20102;&#26681;&#25454;&#26412;&#22320;&#20215;&#20540;&#35266;&#23545;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#20934;&#21046;&#23450;&#33258;&#24049;&#30340;&#25552;&#26696;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#20262;&#29702;&#25512;&#29702;&#31867;&#22411;&#23398;&#21644;&#19968;&#31181;&#36136;&#30097;&#26041;&#27861;&#65292;&#25105;&#20204;&#31934;&#24515;&#32534;&#25490;&#20102;&#20840;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20102;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#26469;&#25506;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#22320;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26631;&#20934;&#37327;&#21270;&#20102;&#19982;&#26412;&#22320;&#20215;&#20540;&#35266;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#20102;&#35780;&#20272;&#36807;&#31243;&#30340;&#20005;&#26684;&#24615;&#12290;&#23545;&#32654;&#22269;&#20379;&#24212;&#21830;&#30340;&#19977;&#27454;&#21830;&#19994;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#23454;&#29616;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#26174;&#33879;&#27934;&#23519;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#19968;&#25209;&#35780;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01460v1 Announce Type: cross  Abstract: The proliferation of large language models (LLMs) requires robust evaluation of their alignment with local values and ethical standards, especially as existing benchmarks often reflect the cultural, legal, and ideological values of their creators. \textsc{LocalValueBench}, introduced in this paper, is an extensible benchmark designed to assess LLMs' adherence to Australian values, and provides a framework for regulators worldwide to develop their own LLM benchmarks for local value alignment. Employing a novel typology for ethical reasoning and an interrogation approach, we curated comprehensive questions and utilized prompt engineering strategies to probe LLMs' value alignment. Our evaluation criteria quantified deviations from local values, ensuring a rigorous assessment process. Comparative analysis of three commercial LLMs by USA vendors revealed significant insights into their effectiveness and limitations, demonstrating the critic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ChatGPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#20195;&#29702;&#24335;AI&#25216;&#26415;&#21487;&#33021;&#26377;&#21161;&#20110;&#36776;&#21035;&#26657;&#22253;&#27450;&#20940;&#19982;&#29609;&#31505;&#65292;&#20026;&#23398;&#29983;&#30340;&#24515;&#29702;&#23433;&#20840;&#25552;&#20379;&#26377;&#25928;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2408.01459</link><description>&lt;p&gt;
AgentPeerTalk&#65306;&#36890;&#36807;&#24102;&#26377;&#20195;&#29702;AI&#30340;&#36776;&#21035;&#33021;&#21147;&#22686;&#24378;&#23398;&#26657;&#23398;&#29983;&#23545;&#25239;&#26657;&#22253;&#27450;&#20940;&#21644;&#29609;&#31505;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ChatGPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#20195;&#29702;&#24335;AI&#25216;&#26415;&#21487;&#33021;&#26377;&#21161;&#20110;&#36776;&#21035;&#26657;&#22253;&#27450;&#20940;&#19982;&#29609;&#31505;&#65292;&#20026;&#23398;&#29983;&#30340;&#24515;&#29702;&#23433;&#20840;&#25552;&#20379;&#26377;&#25928;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#36776;&#21035;&#23398;&#26657;&#20013;&#21516;&#20276;&#20114;&#21160;&#20013;&#30340;&#27450;&#20940;&#19982;&#29609;&#31505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#30340;&#24515;&#29702;&#20581;&#24247;&#25552;&#20379;&#26377;&#25928;&#21644;&#21450;&#26102;&#30340;&#38450;&#25252;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;ChatGPT-4&#12289;Gemini 1.5 Pro&#21644;Claude 3 Opus&#65292;&#24182;&#32463;&#36807;&#20154;&#24037;&#23457;&#26597;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;LLM&#37117;&#36866;&#21512;&#37319;&#29992;&#20195;&#29702;&#24335;&#26041;&#27861;&#12290;ChatGPT-4&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#26368;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#19981;&#21516;LLM&#36755;&#20986;&#30340;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#21463;&#21040;&#20102;&#25919;&#27835;&#19978;&#30340;&#36807;&#24230;&#32416;&#27491;&#12289;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#39044;&#20808;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22312;&#23454;&#26045;&#20102;&#20195;&#29702;&#24335;&#26041;&#27861;&#21518;&#65292;ChatGPT-4&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#30340;&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#20984;&#26174;&#20102;&#23427;&#33021;&#20026;&#26131;&#21463;&#20260;&#23475;&#30340;&#23398;&#29983;&#25552;&#20379;&#25345;&#32493;&#12289;&#23454;&#26102;&#25903;&#25345;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#65292;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#20351;&#29992;&#20195;&#29702;AI&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01459v1 Announce Type: cross  Abstract: Addressing school bullying effectively and promptly is crucial for the mental health of students. This study examined the potential of large language models (LLMs) to empower students by discerning between bullying and joking in school peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus, evaluating their effectiveness through human review. Our results revealed that not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the most promise. We observed variations in LLM outputs, possibly influenced by political overcorrectness, context window limitations, and pre-existing bias in their training data. ChatGPT-4 excelled in context-specific accuracy after implementing the agentic approach, highlighting its potential to provide continuous, real-time support to vulnerable students. This study underlines the significant social impact of using agentic AI in educational settings, offering a new avenue f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20844;&#20247;&#35843;&#26597;&#65292;&#25351;&#20986;&#36825;&#20123;&#35843;&#26597;&#23481;&#26131;&#21463;&#21040;&#35199;&#26041;&#30693;&#35782;&#21644;&#20215;&#20540;&#35266;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#35758;&#22312;&#20840;&#29699;&#21270;&#30340;&#32972;&#26223;&#19979;&#35774;&#35745;&#26356;&#25935;&#24863;&#30340;&#35843;&#26597;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01458</link><description>&lt;p&gt;
&#35843;&#26597;&#26377;&#23475;&#21527;&#65311;&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#12289;&#21457;&#23637;&#21644;&#27835;&#29702;&#20013;&#30340;&#35843;&#26597;&#20351;&#29992;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20844;&#20247;&#35843;&#26597;&#65292;&#25351;&#20986;&#36825;&#20123;&#35843;&#26597;&#23481;&#26131;&#21463;&#21040;&#35199;&#26041;&#30693;&#35782;&#21644;&#20215;&#20540;&#35266;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#35758;&#22312;&#20840;&#29699;&#21270;&#30340;&#32972;&#26223;&#19979;&#35774;&#35745;&#26356;&#25935;&#24863;&#30340;&#35843;&#26597;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01458v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#21628;&#21505;&#19982;&#20844;&#20247;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#12289;&#21457;&#23637;&#21644;&#27835;&#29702;&#20013;&#30340;&#25509;&#35302;&#65292;&#23548;&#33268;&#20351;&#29992;&#35843;&#26597;&#26469;&#25429;&#33719;&#19982;AI&#30456;&#20851;&#30340;&#20154;&#20204;&#23545;&#20854;&#20215;&#20540;&#12289;&#35266;&#24565;&#21644;&#32463;&#21382;&#30340;&#30475;&#27861;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19982;&#36825;&#20123;&#35805;&#39064;&#30456;&#20851;&#30340;&#20844;&#20247;&#35843;&#26597;&#29366;&#20917;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23457;&#35270;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#24320;&#23637;&#30340;&#19968;&#20010;&#35206;&#30422;&#20845;&#20010;&#22269;&#23478;&#30340;&#35843;&#26597;&#35797;&#28857;&#30340;&#33258;&#25105;&#21453;&#24605;&#20998;&#26512;&#20197;&#21450;&#23545;&#25105;&#20204;&#30830;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;44&#31687;&#35770;&#25991;&#20013;&#21253;&#21547;&#30340;&#35843;&#26597;&#30340;&#31995;&#32479;&#25991;&#29486;&#22238;&#39038;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20851;&#20110;AI&#30340;&#35843;&#26597;&#21040;&#30446;&#21069;&#20026;&#27490;&#25152;&#28041;&#21450;&#30340;&#26222;&#36941;&#35266;&#28857;&#21644;&#30740;&#31350;&#26041;&#27861;&#23398;&#19978;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#39046;&#22495;&#30340;&#20844;&#20247;&#35843;&#26597;&#22312;&#35774;&#35745;&#19978;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#20110;&#35199;&#26041;&#30340;&#30693;&#35782;&#12289;&#20215;&#20540;&#35266;&#21644;&#20551;&#35774;&#30340;&#25439;&#23475;&#65292;&#21253;&#25324;&#22312;&#23427;&#20204;&#23545;&#20262;&#29702;&#27010;&#24565;&#21644;&#31038;&#20250;&#20215;&#20540;&#30340;&#23450;&#20301;&#20013;&#65292;&#20197;&#21450;&#22312;&#23427;&#20204;&#23545;&#37096;&#32626;&#31574;&#30053;&#30340;&#30456;&#20851;&#36127;&#38754;&#35752;&#35770;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#25209;&#21028;&#24615;&#23545;&#35805;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25253;&#21578;&#20013;&#30340;&#36879;&#26126;&#24230;&#19981;&#19968;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20840;&#29699;&#21270;&#21644;&#22810;&#20803;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#35266;&#24565;&#30340;&#32972;&#26223;&#19979;&#65292;&#35774;&#35745;&#35843;&#26597;&#21644;&#25991;&#21270;&#26377;&#21035;&#30340;&#21463;&#20247;&#26102;&#38656;&#35201;&#26356;&#21152;&#25935;&#24863;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01458v1 Announce Type: cross  Abstract: Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our finding
&lt;/p&gt;</description></item><item><title>&#23433;&#26364;&#26234;&#24935;&#22478;&#24066;&#39033;&#30446;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#21019;&#26032;&#25216;&#26415;&#25913;&#21892;&#24066;&#27665;&#29983;&#27963;&#36136;&#37327;&#21644;&#22478;&#24066;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2408.01454</link><description>&lt;p&gt;
&#23433;&#26364;&#22478;&#24066;&#65292;&#32422;&#26086;&#65306;&#33258;&#19979;&#32780;&#19978;&#24314;&#35774;&#21487;&#25345;&#32493;&#22478;&#24066;
&lt;/p&gt;
&lt;p&gt;
Amman City, Jordan: Toward a Sustainable City from the Ground Up
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01454
&lt;/p&gt;
&lt;p&gt;
&#23433;&#26364;&#26234;&#24935;&#22478;&#24066;&#39033;&#30446;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#21019;&#26032;&#25216;&#26415;&#25913;&#21892;&#24066;&#27665;&#29983;&#27963;&#36136;&#37327;&#21644;&#22478;&#24066;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01454v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#26234;&#33021;&#22478;&#24066;&#65288;SCs&#65289;&#30340;&#29702;&#24565;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;SCs&#30340;&#29702;&#24565;&#26088;&#22312;&#25552;&#39640;&#24066;&#27665;&#30340;&#29983;&#27963;&#36136;&#37327;&#24182;&#20445;&#25252;&#22478;&#24066;&#29615;&#22659;&#12290;&#38543;&#30528;&#25105;&#20204;&#27493;&#20837;&#19979;&#19968;&#20195;&#26234;&#33021;&#22478;&#24066;&#26102;&#20195;&#65292;&#25506;&#32034;SC&#29702;&#24565;&#30340;&#25152;&#26377;&#30456;&#20851;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#20449;&#24687;&#25216;&#26415;&#30340;&#21457;&#23637;&#20419;&#36827;&#20102;&#26085;&#24120;&#29983;&#27963;&#20013;&#29289;&#21697;&#26234;&#33021;&#21270;&#30340;&#36235;&#21183;&#65292;&#26088;&#22312;&#20351;&#20154;&#31867;&#29983;&#27963;&#26356;&#21152;&#20415;&#25463;&#33298;&#36866;&#12290;SCs&#30340;&#27169;&#24335;&#20316;&#20026;&#19968;&#20010;&#22238;&#24212;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#29305;&#24449;&#30340;&#26410;&#26469;&#22478;&#24066;&#12290;&#23613;&#31649;SCs&#30340;&#23454;&#26045;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36825;&#19968;&#39046;&#22495;&#12290;&#22914;&#20170;&#65292;&#19981;&#21516;&#22478;&#24066;&#27491;&#22312;&#37319;&#32435;SCs&#30340;&#29305;&#28857;&#26469;&#25552;&#21319;&#26381;&#21153;&#21644;&#23621;&#27665;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#24037;&#20316;&#20026;&#35835;&#32773;&#25552;&#20379;&#20102;&#20851;&#20110;&#23433;&#26364;&#26234;&#24935;&#22478;&#24066;&#30340;&#26377;&#29992;&#21644;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01454v1 Announce Type: cross  Abstract: The idea of smart cities (SCs) has gained substantial attention in recent years. The SC paradigm aims to improve citizens' quality of life and protect the city's environment. As we enter the age of next-generation SCs, it is important to explore all relevant aspects of the SC paradigm. In recent years, the advancement of Information and Communication Technologies (ICT) has produced a trend of supporting daily objects with smartness, targeting to make human life easier and more comfortable. The paradigm of SCs appears as a response to the purpose of building the city of the future with advanced features. SCs still face many challenges in their implementation, but increasingly more studies regarding SCs are implemented. Nowadays, different cities are employing SC features to enhance services or the residents quality of life. This work provides readers with useful and important information about Amman Smart City.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#38382;&#31572;&#20013;&#24212;&#29992;&#30340;&#29615;&#22659;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#20943;&#23569;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#23545;&#29615;&#22659;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01453</link><description>&lt;p&gt;
&#29615;&#22659;&#24433;&#21709;&#25253;&#21578;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#20998;&#26512; - &#20197;&#22806;&#37096;&#30693;&#35782;&#20026;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
Reporting and Analysing the Environmental Impact of Language Models on the Example of Commonsense Question Answering with External Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#38382;&#31572;&#20013;&#24212;&#29992;&#30340;&#29615;&#22659;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#20943;&#23569;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#23545;&#29615;&#22659;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#23545;&#29615;&#22659;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20840;&#29699;&#30899;&#25490;&#25918;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#22686;&#38271;&#65292;&#36896;&#25104;&#20102;&#27668;&#20505;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#12290;&#20840;&#29699;&#25968;&#25454;&#20013;&#24515;&#30340;&#30899;&#25490;&#25918;&#20272;&#35745;&#21344;&#32654;&#22269;2021&#24180;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;0.5%&#12290;2022&#24180;&#24213;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#19990;&#24341;&#36215;&#20102;&#31038;&#20250;&#23545;&#36825;&#31867;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#20844;&#21496;&#24050;&#25512;&#20986;&#21253;&#21547;&#19981;&#21516;LLM&#30340;&#20135;&#21697;&#65292;&#32780;&#26356;&#22810;&#30340;&#27169;&#22411;&#27491;&#22312;&#24320;&#21457;&#20013;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#21482;&#26377;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#25165;&#20250;&#33719;&#24471;&#20851;&#27880;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#39640;&#25928;&#21644;&#30740;&#31350;&#30340;&#29615;&#22659;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#21644;&#20998;&#26512;&#26500;&#24314;&#21644;&#21457;&#23637;&#27492;&#31867;&#27169;&#22411;&#23545;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#20197;&#32531;&#35299;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#26696;&#20363;&#30340;&#35814;&#32454;&#35843;&#26597;&#65292;&#26412;&#25991;&#35745;&#21010;&#23637;&#31034;&#20102;&#25191;&#34892;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#24182;&#23545;&#22914;&#20309;&#20943;&#23569;&#36825;&#20123;&#20219;&#21153;&#30340;&#29615;&#22659;&#36275;&#36857;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20005;&#26684;&#35201;&#27714;&#30340;&#29615;&#22659;&#35780;&#20272;&#26631;&#20934;&#65292;&#24182;&#33268;&#21147;&#20110;&#20943;&#23569;&#22312;&#24320;&#21457;&#21644;&#20351;&#29992;LLMs&#30340;&#36807;&#31243;&#20013;&#23545;&#29615;&#22659;&#30340;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01453v1 Announce Type: cross  Abstract: Human-produced emissions are growing at an alarming rate, causing already observable changes in the climate and environment in general. Each year global carbon dioxide emissions hit a new record, and it is reported that 0.5% of total US greenhouse gas emissions are attributed to data centres as of 2021. The release of ChatGPT in late 2022 sparked social interest in Large Language Models (LLMs), the new generation of Language Models with a large number of parameters and trained on massive amounts of data. Currently, numerous companies are releasing products featuring various LLMs, with many more models in development and awaiting release. Deep Learning research is a competitive field, with only models that reach top performance attracting attention and being utilized. Hence, achieving better accuracy and results is often the first priority, while the model's efficiency and the environmental impact of the study are neglected. However, LL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25351;&#25968;PreIndex&#65292;&#29992;&#26469;&#20272;&#35745;&#22312;&#27169;&#22411;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#30340;&#20877;&#22521;&#35757;&#36807;&#31243;&#20013;&#65292;&#20174;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#20013;&#65292;&#25152;&#28041;&#21450;&#30340;&#29615;&#22659;&#25104;&#26412;&#65288;&#22914;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#28040;&#32791;&#65289;&#12290;</title><link>https://arxiv.org/abs/2408.01446</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#25104;&#26412;&#20272;&#35745;&#27169;&#22411;&#21450;&#20854;&#22312;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Estimating Environmental Cost Throughout Model's Adaptive Life Cycle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25351;&#25968;PreIndex&#65292;&#29992;&#26469;&#20272;&#35745;&#22312;&#27169;&#22411;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#30340;&#20877;&#22521;&#35757;&#36807;&#31243;&#20013;&#65292;&#20174;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#20013;&#65292;&#25152;&#28041;&#21450;&#30340;&#29615;&#22659;&#25104;&#26412;&#65288;&#22914;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#28040;&#32791;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01446v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#22312;&#24403;&#21069;&#26102;&#20195;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#33021;&#28304;&#26469;&#35757;&#32451;&#21644;&#20351;&#29992;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20276;&#38543;&#30528;&#30899;&#25490;&#25918;&#21040;&#29615;&#22659;&#20013;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#24037;&#26234;&#33021;/&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#30340;&#30899;&#36275;&#36857;&#20197;&#21450;&#30456;&#20851;&#30340;&#33021;&#28304;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20877;&#22521;&#35757;&#30340;&#39044;&#27979;&#25351;&#25968;&#65292;PreIndex&#65292;&#20197;&#20272;&#35745;&#19982;&#27169;&#22411;&#20877;&#22521;&#35757;&#30456;&#20851;&#30340;&#29615;&#22659;&#25104;&#26412;&#65292;&#22914;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;&#27169;&#22411;&#20877;&#22521;&#35757;&#26159;&#20026;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25110;&#22312;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#21464;&#21270;/&#24046;&#24322;&#12290;PreIndex&#25351;&#25968;&#21487;&#20197;&#29992;&#26469;&#20272;&#31639;&#20174;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#26032;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#23427;&#20063;&#19982;Model&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20026;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#30340;&#35774;&#35745;&#25552;&#20379;&#25903;&#25345;&#65292;&#20174;&#32780;&#24110;&#21161;&#38477;&#20302;&#20877;&#22521;&#35757;&#30340;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01446v1 Announce Type: cross  Abstract: With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with
&lt;/p&gt;</description></item><item><title>&#28145;&#22323;&#31185;&#25216;&#22823;&#23398;&#25552;&#20986;&#30340;GAN&#27169;&#22411;&#22686;&#24378;&#20102;&#22312;&#19981;&#21033;&#26465;&#20214;&#19979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#35937;&#35782;&#21035;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01430</link><description>&lt;p&gt;
&#28145;&#22323;&#31185;&#25216;&#22823;&#23398;GAN: &#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#22312;&#19981;&#21033;&#26465;&#20214;&#19979;&#30340;&#23545;&#35937;&#35782;&#21035;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SUSTechGAN: Image Generation for Object Recognition in Adverse Conditions of Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01430
&lt;/p&gt;
&lt;p&gt;
&#28145;&#22323;&#31185;&#25216;&#22823;&#23398;&#25552;&#20986;&#30340;GAN&#27169;&#22411;&#22686;&#24378;&#20102;&#22312;&#19981;&#21033;&#26465;&#20214;&#19979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#35937;&#35782;&#21035;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01430v1 &#23459;&#24067;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#33258;&#39550;&#39542;&#36710;&#26174;&#33879;&#21463;&#30410;&#20110;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#30340;&#25968;&#25454;&#36890;&#24120;&#31526;&#21512;&#38271;&#23614;&#20998;&#24067;&#65292;&#20854;&#20013;&#20851;&#38190;&#30340;&#39550;&#39542;&#25968;&#25454;&#22312;&#19981;&#21033;&#26465;&#20214;&#19979;&#38590;&#20197;&#25910;&#38598;&#12290;&#23613;&#31649;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#28155;&#21152;&#25968;&#25454;&#65292;&#20294;&#29983;&#25104;&#19981;&#21033;&#26465;&#20214;&#19979;&#30340;&#39550;&#39542;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#22323;&#31185;&#25216;&#22823;&#23398;GAN&#65292;&#23427;&#20855;&#26377;&#21452;&#37325;&#27880;&#24847;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#20197;&#25913;&#21892;&#22312;&#19981;&#21033;&#26465;&#20214;&#19979;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#35937;&#35782;&#21035;&#33021;&#21147;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#28145;&#22323;&#31185;&#25216;&#22823;&#23398;GAN&#21644;&#29616;&#26377;&#30340;&#30693;&#21517;GAN&#65292;&#20197;&#30830;&#20445;&#22312;&#38632;&#22825;&#21644;&#22812;&#26202;&#31561;&#19981;&#21033;&#26465;&#20214;&#19979;&#29983;&#25104;&#39550;&#39542;&#22270;&#20687;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#24212;&#29992;&#21040;&#37325;&#26032;&#35757;&#32451;&#23545;&#35937;&#35782;&#21035;&#32593;&#32476;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#21152;&#20837;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#33879;&#21517;&#30340;YOLOv5&#65292;&#24182;&#35780;&#20272;&#20102;&#22270;&#20687;&#25913;&#36827;&#23545;&#35937;&#35782;&#21035;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01430v1 Announce Type: cross  Abstract: Autonomous driving significantly benefits from data-driven deep neural networks. However, the data in autonomous driving typically fits the long-tailed distribution, in which the critical driving data in adverse conditions is hard to collect. Although generative adversarial networks (GANs) have been applied to augment data for autonomous driving, generating driving images in adverse conditions is still challenging. In this work, we propose a novel SUSTechGAN with dual attention modules and multi-scale generators to generate driving images for improving object recognition of autonomous driving in adverse conditions. We test the SUSTechGAN and the existing well-known GANs to generate driving images in adverse conditions of rain and night and apply the generated images to retrain object recognition networks. Specifically, we add generated images into the training datasets to retrain the well-known YOLOv5 and evaluate the improvement of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#33021;&#22312;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#22918;&#23481;&#20449;&#24687;&#31561;&#19968;&#31181;&#29305;&#24449;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2408.01428</link><description>&lt;p&gt;
&#36801;&#31227;&#24335;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#23545;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Transferable Adversarial Facial Images for Privacy Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#33021;&#22312;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#22918;&#23481;&#20449;&#24687;&#31561;&#19968;&#31181;&#29305;&#24449;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01428v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#30001;&#20110;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#65288;FR&#65289;&#31995;&#32479;&#30340;&#25104;&#21151;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#36861;&#36394;&#29992;&#25143;&#30340;&#33021;&#21147;&#32473;&#20104;&#20102;&#39640;&#24230;&#37325;&#35270;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#38754;&#37096;&#22270;&#20687;&#20013;&#24341;&#20837;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#26469;&#35823;&#23548;&#36825;&#20123;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#29992;&#25143;&#36873;&#25321;&#30340;&#21442;&#32771;&#26469;&#25351;&#23548;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#29983;&#25104;&#65292;&#24182;&#19988;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#26080;&#27861;&#21516;&#26102;&#21019;&#24314;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#38754;&#37096;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#36716;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#35758;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#21033;&#29992;&#31867;&#20284;&#20110;&#22918;&#23481;&#20449;&#24687;&#30340;&#19968;&#31181;&#38754;&#37096;&#29305;&#24449;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#20840;&#23616;&#23545;&#25239;&#24615;&#28508;&#21464;&#37327;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01428v1 Announce Type: new  Abstract: The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent sear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Siamese Transformer&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;Euclidean distance measure&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.01427</link><description>&lt;p&gt;
Siamese Transformer &#32593;&#32476;&#23545;&#20110;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Siamese Transformer Networks for Few-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Siamese Transformer&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;Euclidean distance measure&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01427v1 &#23459;&#24067;&#31867;&#22411;: &#26032; &#25688;&#35201;: &#20154;&#31867;&#22312;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#25928;&#29575;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21644;&#20998;&#31867;&#26032;&#22270;&#20687;&#65292;&#38656;&#35201;&#30340;&#31034;&#20363;&#24456;&#23569;&#12290;&#36825;&#31181;&#33021;&#21147;&#24402;&#21151;&#20110;&#20182;&#20204;&#33021;&#22815;&#19987;&#27880;&#20110;&#32454;&#33410;&#24182;&#35782;&#21035;&#20043;&#21069;&#30475;&#21040;&#30340;&#21644;&#26032;&#22270;&#20687;&#20043;&#38388;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#24448;&#24448;&#20391;&#37325;&#20110;&#20840;&#23616;&#29305;&#24449;&#25110;&#23616;&#37096;&#29305;&#24449;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#36825;&#20004;&#31181;&#29305;&#24449;&#30340;&#25972;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Siamese Transformer &#32593;&#32476;(STN)&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#32593;&#32476;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer(ViT)&#26550;&#26500;&#26469;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#20351;&#29992;ViT-Small&#32593;&#32476;&#26550;&#26500;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#23545;&#20998;&#25903;&#32593;&#32476;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26469;&#23545;&#20840;&#23616;&#29305;&#24449;&#21644;&#23616;&#37096;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#23569;&#25968;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#24212;&#23569;&#26679;&#26412;&#21644;&#21333;&#19968;&#26679;&#26412;&#26465;&#20214;&#19979;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01427v1 Announce Type: new  Abstract: Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global featur
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.01091</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01091
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01091v1 Announce Type: new  Abstract: Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. The dataset and code are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
&lt;/p&gt;</description></item><item><title>&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#19982;&#31070;&#32463;&#28210;&#26579;&#32467;&#21512;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D&#36229;&#22768;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00860</link><description>&lt;p&gt;
UlRe-NeRF: &#20351;&#29992;&#31070;&#32463;&#28210;&#26579;&#30340;3D&#36229;&#22768;&#25104;&#20687;&#65292;&#36890;&#36807;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#19982;&#31070;&#32463;&#28210;&#26579;&#32467;&#21512;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D&#36229;&#22768;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25552;&#20132;  &#25688;&#35201;&#65306;&#19977;&#32500;&#36229;&#22768;&#25104;&#20687;&#22312;&#21307;&#30103;&#35786;&#26029;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#19977;&#32500;&#36229;&#22768;&#25104;&#20687;&#26041;&#27861;&#23384;&#22312;&#22266;&#23450;&#20998;&#36776;&#29575;&#12289;&#23384;&#20648;&#25928;&#29575;&#20302;&#12289;&#19978;&#19979;&#25991;&#36830;&#25509;&#19981;&#36275;&#31561;&#38382;&#39064;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#24322;&#24120;&#21644;&#21453;&#23556;&#29305;&#24615;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;NeRF&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#30340;&#25216;&#26415;&#22312;&#35270;&#35282;&#21512;&#25104;&#21644;&#19977;&#32500;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#65292;&#20294;&#22312;&#39640;&#28165;&#26224;&#24230;&#36229;&#22768;&#25104;&#20687;&#26041;&#38754;&#20173;&#26377;&#30740;&#31350;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;UlRe-NeRF&#65292;&#23427;&#23558;&#38544;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26174;&#24335;&#30340;&#36229;&#22768;&#20307;&#32472;&#21046;&#21151;&#33021;&#38598;&#25104;&#21040;&#19968;&#20010;&#36229;&#22768;&#31070;&#32463;&#28210;&#26579;&#26550;&#26500;&#20013;&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#20102;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#65292;&#20351;&#29992;&#26041;&#21521;&#24615;MLP&#27169;&#22359;&#29983;&#25104;&#35270;&#35282;&#20381;&#36182;&#30340;&#39640;&#39057;&#21453;&#23556;&#24378;&#24230;&#20272;&#35745;&#65292;&#20197;&#21450;&#19968;&#20010;&#31354;&#38388;MLP&#27169;&#22359;&#26469;&#20272;&#35745;&#25972;&#20010;&#31354;&#38388;&#20869;&#30340;&#21453;&#23556;&#24378;&#24230;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#25216;&#26415;&#65292;UlRe-NeRF&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D ultrasound&#28210;&#26579;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#23384;&#20648;&#21644;&#26356;&#20248;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#21644;&#36879;&#35270;&#38382;&#39064;&#26102;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UlRe-NeRF&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#36229;&#22768;&#25104;&#20687;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v1 Announce Type: new  Abstract: Three-dimensional ultrasound imaging is a critical technology widely used in medical diagnostics. However, traditional 3D ultrasound imaging methods have limitations such as fixed resolution, low storage efficiency, and insufficient contextual connectivity, leading to poor performance in handling complex artifacts and reflection characteristics. Recently, techniques based on NeRF (Neural Radiance Fields) have made significant progress in view synthesis and 3D reconstruction, but there remains a research gap in high-quality ultrasound imaging. To address these issues, we propose a new model, UlRe-NeRF, which combines implicit neural networks and explicit ultrasound volume rendering into an ultrasound neural rendering architecture. This model incorporates reflection direction parameterization and harmonic encoding, using a directional MLP module to generate view-dependent high-frequency reflection intensity estimates, and a spatial MLP mod
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22788;&#29702;&#25991;&#23383;&#19982;&#22270;&#20687;&#27010;&#24565;&#26144;&#23556;&#26102;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#25991;&#23383;&#25552;&#31034;&#30340;&#21709;&#24212;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00230</link><description>&lt;p&gt;
&#36855;&#22833;&#22312;&#32763;&#35793;&#20013;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27010;&#24565;&#19981;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00230
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22788;&#29702;&#25991;&#23383;&#19982;&#22270;&#20687;&#27010;&#24565;&#26144;&#23556;&#26102;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#25991;&#23383;&#25552;&#31034;&#30340;&#21709;&#24212;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#27010;&#24565;&#26144;&#23556;&#26041;&#38754;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#65292;&#24403;&#36755;&#20837;&#21253;&#25324;&#20004;&#20010;&#28508;&#22312;&#27010;&#24565;&#30340;&#32452;&#21512;&#26102;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#26479;&#20912;&#21487;&#20048;&#8221;&#36825;&#20010;&#35789;&#32452;&#65289;&#65292;&#27169;&#22411;&#24120;&#24120;&#29983;&#25104;&#20912;&#21487;&#20048;&#19982;&#29627;&#29827;&#26479;&#30340;&#32452;&#21512;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#26159;&#22240;&#20026;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#28508;&#22312;&#27010;&#24565;&#26144;&#23556;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#30340;&#35780;&#20272;&#30830;&#35748;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00230v2 Announce Type: replace  Abstract: Advancements in text-to-image diffusion models have broadened extensive downstream practical applications, but such models often encounter misalignment issues between text and image. Taking the generation of a combination of two disentangled concepts as an example, say given the prompt "a tea cup of iced coke", existing models usually generate a glass cup of iced coke because the iced coke usually co-occurs with the glass cup instead of the tea one during model training. The root of such misalignment is attributed to the confusion in the latent semantic space of text-to-image diffusion models, and hence we refer to the "a tea cup of iced coke" phenomenon as Latent Concept Misalignment (LC-Mis). We leverage large language models (LLMs) to thoroughly investigate the scope of LC-Mis, and develop an automated pipeline for aligning the latent semantics of diffusion models to text prompts. Empirical assessments confirm the effectiveness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AA-CBR-P&#65288;&#22522;&#20110;&#20559;&#22909;&#30340;&#25277;&#35937;&#35770;&#35777;&#26696;&#20363;&#22522;&#20110;&#25512;&#29702;&#65289;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#29305;&#23450;&#20559;&#22909;&#23545;&#26696;&#20363;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#36981;&#24490;&#36825;&#20123;&#20559;&#22909;&#26102;&#36827;&#34892;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;&#22836;&#39045;&#32959;&#30244;&#24739;&#32773;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#19978;&#24471;&#21040;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.00108</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#25277;&#35937;&#35770;&#35777;&#26696;&#20363;&#22522;&#20110;&#25512;&#29702;&#65288;&#38468;&#24405;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AA-CBR-P&#65288;&#22522;&#20110;&#20559;&#22909;&#30340;&#25277;&#35937;&#35770;&#35777;&#26696;&#20363;&#22522;&#20110;&#25512;&#29702;&#65289;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#29305;&#23450;&#20559;&#22909;&#23545;&#26696;&#20363;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#36981;&#24490;&#36825;&#20123;&#20559;&#22909;&#26102;&#36827;&#34892;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;&#22836;&#39045;&#32959;&#30244;&#24739;&#32773;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#19978;&#24471;&#21040;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AA-CBR-P&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20559;&#22909;&#19982;&#25277;&#35937;&#35770;&#35777;&#21644;&#26696;&#20363;&#22522;&#20110;&#25512;&#29702;&#65288;CBR&#65289;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AA-CBR-P&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#23450;&#20041;&#22810;&#20010;&#27604;&#36739;&#26696;&#20363;&#30340;&#26041;&#27861;&#26469;&#34920;&#36798;&#20559;&#22909;&#65292;&#24182;&#25353;&#29031;&#29305;&#23450;&#30340;&#20559;&#22909;&#39034;&#24207;&#23545;&#36825;&#20123;&#27604;&#36739;&#26041;&#27861;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#23558;&#35777;&#26126;&#65292;&#24403;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#26102;&#65292;&#23427;&#20250;&#27627;&#26080;&#20445;&#30041;&#22320;&#36981;&#23432;&#36825;&#20123;&#20559;&#22909;&#65292;&#24182;&#23637;&#31034;&#20986;&#20808;&#21069;&#23545;&#20110;&#26696;&#20363;&#22522;&#20110;&#25512;&#29702;&#30340;&#25277;&#35937;&#35770;&#35777;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#22320;&#34920;&#36798;&#23545;&#35770;&#35777;&#25104;&#20998;&#30340;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#27979;&#35797;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#35813;&#27979;&#35797;&#35780;&#20272;&#20102;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#22312;&#22836;&#39045;&#32959;&#30244;&#24739;&#32773;&#35786;&#26029;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21576;&#29616;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;AA-CBR-P&#22914;&#20309;&#21487;&#33021;&#34987;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00108v2 Announce Type: replace  Abstract: In the pursuit of enhancing the efficacy and flexibility of interpretable, data-driven classification models, this work introduces a novel incorporation of user-defined preferences with Abstract Argumentation and Case-Based Reasoning (CBR). Specifically, we introduce Preference-Based Abstract Argumentation for Case-Based Reasoning (which we call AA-CBR-P), allowing users to define multiple approaches to compare cases with an ordering that specifies their preference over these comparison approaches. We prove that the model inherently follows these preferences when making predictions and show that previous abstract argumentation for case-based reasoning approaches are insufficient at expressing preferences over constituents of an argument. We then demonstrate how this can be applied to a real-world medical dataset sourced from a clinical trial evaluating differing assessment methods of patients with a primary brain tumour. We show empi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27969;&#31639;&#27861;&#21644;K-means&#32858;&#31867;&#30340;RAG&#25913;&#36827;&#26041;&#26696;&#65292;&#20197;&#39640;&#25928;&#26356;&#26032;&#32034;&#24341;&#24182;&#32553;&#30701;&#26597;&#35810;&#26102;&#38388;&#65292;&#21516;&#26102;&#33410;&#30465;&#20869;&#23384;&#36164;&#28304;&#12290;|&gt;</title><link>https://arxiv.org/abs/2407.21300</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#31639;&#27861;&#21644;K-means&#32858;&#31867;&#23454;&#29616;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Implementing Streaming algorithm and k-means clusters to RAG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21300
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27969;&#31639;&#27861;&#21644;K-means&#32858;&#31867;&#30340;RAG&#25913;&#36827;&#26041;&#26696;&#65292;&#20197;&#39640;&#25928;&#26356;&#26032;&#32034;&#24341;&#24182;&#32553;&#30701;&#26597;&#35810;&#26102;&#38388;&#65292;&#21516;&#26102;&#33410;&#30465;&#20869;&#23384;&#36164;&#28304;&#12290;|&gt;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21300v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; - &#25688;&#35201;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#22240;&#20026;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#26377;&#35768;&#22810;&#38382;&#39064;&#65306;&#30001;&#20110;&#25968;&#25454;&#24211;&#24040;&#22823;&#65292;&#23427;&#28040;&#32791;&#20102;&#22823;&#37327;&#20869;&#23384;&#12290;&#38754;&#23545;&#28023;&#37327;&#27969;&#25968;&#25454;&#26102;&#65292;&#23427;&#26080;&#27861;&#21450;&#26102;&#26356;&#26032;&#24050;&#24314;&#31435;&#30340;&#32034;&#24341;&#25968;&#25454;&#24211;&#12290;&#20026;&#20102;&#33410;&#30465;&#24314;&#31435;&#25968;&#25454;&#24211;&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#27969;&#31639;&#27861;&#21644;K-means&#32858;&#31867;&#19982;RAG&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#27969;&#31639;&#27861;&#26469;&#26356;&#26032;&#32034;&#24341;&#24182;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#28982;&#21518;&#20351;&#29992;K-means&#31639;&#27861;&#23558;&#24444;&#27492;&#30456;&#20284;&#24230;&#39640;&#30340;&#25991;&#26723;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#36825;&#26679;&#26597;&#35810;&#26102;&#38388;&#23601;&#20250;&#32553;&#30701;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#27969;&#31639;&#27861;&#21644;K-means&#32858;&#31867;&#30340;RAG&#22312;&#20934;&#30830;&#24615;&#21644;&#20869;&#23384;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#19978;&#37117;&#27604;&#20256;&#32479;RAG&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21300v2 Announce Type: replace-cross  Abstract: Retrieval-augmented generation (RAG) has achieved great success in information retrieval to assist large language models because it builds an external knowledge database. However, it also has many problems: it consumes a lot of memory because of the huge database. When faced with massive streaming data, it is unable to update the established index database in time. To save the memory of building the database and maintain accuracy simultaneously, we proposed a new approach combining a streaming algorithm and k-means cluster with RAG. Our approach applies a streaming algorithm to update the index and reduce memory consumption. Then use the k-means algorithm to cluster documents with high similarities together, the query time will be shortened by doing this. We conducted comparative experiments on four methods, and the results show that RAG with streaming algorithm and k-means cluster performs well in accuracy and memory. For mass
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#29992;&#25972;&#25968;&#20540;&#35757;&#32451;&#21644;&#33033;&#20914;&#39537;&#21160;&#25512;&#29702;&#30340;&#26032;&#22411;SNN&#65292;&#22312;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;ANNs&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#33021;&#25928;&#21644;&#23454;&#26102;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.20708</link><description>&lt;p&gt;
&#25972;&#25968;&#20540;&#35757;&#32451;&#19982; spikes &#39537;&#21160;&#25512;&#29702;&#30340; SNN &#23545;&#35937;&#26816;&#27979;&#65292;&#20855;&#22791;&#39640;&#24615;&#33021;&#19982;&#20302;&#33021;&#32791;
&lt;/p&gt;
&lt;p&gt;
Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20708
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#29992;&#25972;&#25968;&#20540;&#35757;&#32451;&#21644;&#33033;&#20914;&#39537;&#21160;&#25512;&#29702;&#30340;&#26032;&#22411;SNN&#65292;&#22312;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;ANNs&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#33021;&#25928;&#21644;&#23454;&#26102;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20708v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306; &#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30456;&#27604;&#65292;&#20855;&#26377;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#21644;&#20302;&#21151;&#32791;&#20248;&#21183;&#30340;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30446;&#21069;&#20165;&#38480;&#20110;&#25191;&#34892;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24357;&#21512;&#23545;&#35937;&#26816;&#27979;&#20013;ANNs&#21644;SNNs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#22260;&#32469;&#32593;&#32476;&#32467;&#26500;&#21644;&#31361;&#35302;&#31070;&#32463;&#20803;&#23637;&#24320;&#12290;&#39318;&#20808;&#65292;YOLO&#31995;&#21015;&#21521;&#30456;&#24212;&#31361;&#35302;&#29256;&#26412;&#30340;&#36716;&#25442;&#36807;&#31243;&#20013;&#23384;&#22312;&#22797;&#26434;&#27169;&#22359;&#35774;&#35745;&#23548;&#33268;&#30340;&#20449;&#21495;&#25439;&#32791;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21270;&#21407;&#22987;YOLO&#24182;&#38598;&#25104;&#20803;SNN&#22359;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;SpikeYOLO&#26550;&#26500;&#12290;&#20854;&#27425;&#65292;&#23545;&#35937;&#26816;&#27979;&#23545;&#36716;&#25442;&#20026;&#20108;&#36827;&#21046;&#31361;&#35302;&#30340;&#36807;&#31243;&#20013;&#30005;&#20301;&#37327;&#30340;&#37327;&#21270;&#35823;&#24046;&#26356;&#20026;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#31361;&#35302;&#31070;&#32463;&#20803;&#65292;&#23427;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#20197;&#25972;&#25968;&#20540;&#28608;&#27963;&#65292;&#21516;&#26102;&#20445;&#25345;&#33033;&#20914;&#39537;&#21160;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#20248;&#21270;&#21518;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#36731;&#37327;&#32423; SNN &#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#22312;&#20445;&#30041;&#39640;&#31934;&#24230;&#23545;&#35937;&#26816;&#27979;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#22810;&#20854;&#23427; SNN &#32593;&#32476;&#26550;&#26500;&#30340;&#20302;&#33021;&#32791;&#22788;&#29702;&#12290;&#26368;&#32456;&#65292;&#35813;&#35774;&#35745;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12289;&#20302;&#33021;&#32791;&#30340;&#23454;&#26102;&#23545;&#35937;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#21487;&#34892;&#24615;&#12290; | &#20851;&#38190;&#35789;&#65306;spiking neural networks, object detection, artificial neural networks, energy efficiency, real-time detection
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20708v3 Announce Type: replace  Abstract: Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and low-power advantages over Artificial Neural Networks (ANNs). Applications of SNNs are currently limited to simple classification tasks because of their poor performance. In this work, we focus on bridging the performance gap between ANNs and SNNs on object detection. Our design revolves around network architecture and spiking neuron. First, the overly complex module design causes spike degradation when the YOLO series is converted to the corresponding spiking version. We design a SpikeYOLO architecture to solve this problem by simplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object detection is more sensitive to quantization errors in the conversion of membrane potentials into binary spikes by spiking neurons. To address this challenge, we design a new spiking neuron that activates Integer values during training while maintaining spike-driv
&lt;/p&gt;</description></item><item><title>AOTree&#27169;&#22411;&#36890;&#36807;&#25429;&#33719;&#29992;&#25143;&#35780;&#35770;&#20013;&#30340;&#26041;&#38754;&#39034;&#24207;&#65292;&#20026;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2407.19937</link><description>&lt;p&gt;
AOTree:&#22522;&#20110;&#26041;&#38754;&#39034;&#24207;&#26641;&#30340;&#27169;&#22411;&#22312;&#21487;&#35299;&#37322;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AOTree: Aspect Order Tree-based Model for Explainable Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19937
&lt;/p&gt;
&lt;p&gt;
AOTree&#27169;&#22411;&#36890;&#36807;&#25429;&#33719;&#29992;&#25143;&#35780;&#35770;&#20013;&#30340;&#26041;&#38754;&#39034;&#24207;&#65292;&#20026;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19937v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#25512;&#33616;&#31995;&#32479;&#36817;&#24180;&#26469;&#19981;&#20165;&#36861;&#27714;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#24076;&#26395;&#25552;&#20379;&#35299;&#37322;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#21482;&#32771;&#34385;&#35780;&#35770;&#20013;&#30340;&#20869;&#23481;&#37325;&#35201;&#24615;&#65292;&#22914;&#35789;&#35821;&#25110;&#26041;&#38754;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#24573;&#35270;&#24573;&#35270;&#20102;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33267;&#20851;&#37325;&#35201;&#30340;&#39034;&#24207;&#32500;&#24230;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#26041;&#38754;&#39034;&#24207;&#26641;&#30340;AOTree&#21487;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#35748;&#30693;&#21644;&#20915;&#31574;&#24515;&#29702;&#23398;&#30340;&#20915;&#23450;&#22240;&#32032;&#20381;&#36182;&#29702;&#35770;&#65292;&#26088;&#22312;&#25429;&#25417;&#29992;&#25143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#35780;&#35770;&#26469;&#39564;&#35777;&#35813;&#29702;&#35770;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#35813;&#29702;&#35770;&#65292;&#25552;&#20986;&#30340;AOTree&#25193;&#23637;&#20102;&#20915;&#31574;&#26641;&#30340;&#26500;&#24314;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26041;&#38754;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19937v2 Announce Type: replace-cross  Abstract: Recent recommender systems aim to provide not only accurate recommendations but also explanations that help users understand them better. However, most existing explainable recommendations only consider the importance of content in reviews, such as words or aspects, and ignore the ordering relationship among them. This oversight neglects crucial ordering dimensions in the human decision-making process, leading to suboptimal performance. Therefore, in this paper, we propose Aspect Order Tree-based (AOTree) explainable recommendation method, inspired by the Order Effects Theory from cognitive and decision psychology, in order to capture the dependency relationships among decisive factors. We first validate the theory in the recommendation scenario by analyzing the reviews of the users. Then, according to the theory, the proposed AOTree expands the construction of the decision tree to capture aspect orders in users' decision-makin
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20559;&#28608;&#28040;&#38500;&#26694;&#26550;VersusDebias&#65292;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25163;&#26426;&#21046;&#26469;&#20943;&#23569;&#20559;&#28608;&#24819;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26080;&#20559;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2407.19524</link><description>&lt;p&gt;
VersusDebias: &#36890;&#29992;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22522;&#20110;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#23545;&#25163;&#30340;&#20559;&#28608;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19524
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20559;&#28608;&#28040;&#38500;&#26694;&#26550;VersusDebias&#65292;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25163;&#26426;&#21046;&#26469;&#20943;&#23569;&#20559;&#28608;&#24819;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26080;&#20559;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19524v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#26576;&#20123;&#31038;&#20250;&#32676;&#20307;&#30340;&#20559;&#35265;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#29305;&#23450;&#27169;&#22411;&#30340;&#22266;&#23450;&#25552;&#31034;&#65292;&#26080;&#27861;&#36866;&#24212;T2I&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#24555;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#25552;&#31034;&#22810;&#26679;&#24615;&#30340;&#36235;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#19981;&#33021;&#32771;&#34385;&#21040;&#24819;&#35937;&#30340;&#39118;&#38505;&#65292;&#23548;&#33268;&#39044;&#26399;&#32467;&#26524;&#21644;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VersusDebias&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;T2I&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#20840;&#26032;&#19988;&#36890;&#29992;&#30340;&#20559;&#28608;&#28040;&#38500;&#26694;&#26550;&#65292;&#23427;&#30001;&#19968;&#20010;&#29983;&#25104;&#23545;&#25163;&#26426;&#21046;&#65288;GAM&#65289;&#21644;&#19968;&#20010;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#28608;&#28040;&#38500;&#29983;&#25104;&#26426;&#21046;&#32452;&#25104;&#12290;&#33258;&#36866;&#24212;&#30340;GAM&#20026;&#27599;&#20010;&#25552;&#31034;&#29983;&#25104;&#19987;&#29992;&#30340;&#23646;&#24615;&#25968;&#32452;&#65292;&#20197;&#20943;&#23569;T2I&#27169;&#22411;&#20013;&#24819;&#35937;&#30340;&#24433;&#21709;&#12290;SLM&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#20026;T2I&#27169;&#22411;&#29983;&#25104;&#26080;&#20559;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19524v2 Announce Type: replace  Abstract: With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2407.19393</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Integrating Cognitive AI with Generative Models for Enhanced Question Answering in Skill-based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19393
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19393v2 Announce Type: replace  Abstract: In online learning, the ability to provide quick and accurate feedback to learners is crucial. In skill-based learning, learners need to understand the underlying concepts and mechanisms of a skill to be able to apply it effectively. While videos are a common tool in online learning, they cannot comprehend or assess the skills being taught. Additionally, while Generative AI methods are effective in searching and retrieving answers from a text corpus, it remains unclear whether these methods exhibit any true understanding. This limits their ability to provide explanations of skills or help with problem-solving. This paper proposes a novel approach that merges Cognitive AI and Generative AI to address these challenges. We employ a structured knowledge representation, the TMK (Task-Method-Knowledge) model, to encode skills taught in an online Knowledge-based AI course. Leveraging techniques such as Large Language Models, Chain-of-Though
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#24037;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20010;&#20307;&#30340;&#27969;&#21160;&#24615;&#27169;&#24335;&#21644;&#25512;&#29702;&#29983;&#25104;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#22478;&#24066;&#32423;&#26053;&#34892;&#26085;&#35760;&#29983;&#25104;&#65292;&#24182;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20010;&#20154;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2407.18932</link><description>&lt;p&gt;
&#26356;&#30495;&#23454;&#30340;&#26053;&#34892;&#26085;&#35760;&#29983;&#25104;&#65306;&#20351;&#29992;LLM&#20195;&#29702;&#21644;&#20010;&#24615;&#21270;&#20010;&#20154;&#36164;&#26009;
&lt;/p&gt;
&lt;p&gt;
Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#24037;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20010;&#20307;&#30340;&#27969;&#21160;&#24615;&#27169;&#24335;&#21644;&#25512;&#29702;&#29983;&#25104;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#22478;&#24066;&#32423;&#26053;&#34892;&#26085;&#35760;&#29983;&#25104;&#65292;&#24182;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20010;&#20154;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18932v2 Announce Type: &#26367;&#25442;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18932v2 Announce Type: replace-cross  Abstract: Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#37327;&#25968;&#25454;&#25903;&#25345;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2407.18569</link><description>&lt;p&gt;
PP-TIL&#65306;&#22522;&#20110;&#23454;&#20363;&#36716;&#31227;&#27169;&#20223;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#20010;&#24615;&#21270;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#37327;&#25968;&#25454;&#25903;&#25345;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18569v3 &#39044;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#22312;&#22478;&#24066;&#33258;&#21160;&#21270;&#39550;&#39542;&#20013;&#65292;&#20010;&#24615;&#21270;&#36816;&#21160;&#35268;&#21010;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23427;&#33021;&#22815;&#28385;&#36275;&#20010;&#20307;&#29992;&#25143;&#30340;&#29420;&#29305;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#21162;&#21147;&#32463;&#24120;&#22312;&#21516;&#26102;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#26102;&#36935;&#21040;&#22256;&#38590;&#65306;&#22312;&#22797;&#26434;&#30340;&#37117;&#24066;&#29615;&#22659;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#35268;&#21010;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#21033;&#29992;&#26469;&#25552;&#39640;&#35268;&#21010;&#24615;&#33021;&#12290;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#29992;&#25143;&#25968;&#25454;&#30340;&#26114;&#36149;&#21644;&#26377;&#38480;&#24615;&#65292;&#20197;&#21450;&#22330;&#26223;&#29366;&#24577;&#31354;&#38388;&#36235;&#21521;&#26080;&#31351;&#22823;&#30340;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#26102;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23454;&#20363;&#30340;&#36716;&#31227;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20174;&#24191;&#27867;&#30340;&#19987;&#23478;&#39046;&#22495;&#25968;&#25454;&#21521;&#29992;&#25143;&#39046;&#22495;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#19987;&#23478;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#27169;&#20223;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#20363;&#32423;&#21305;&#37197;&#21644;&#31574;&#30053;&#36861;&#36394;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#24066;&#20869;&#39550;&#39542;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340; PP-TIL &#26041;&#27861;&#22312;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22810;&#29992;&#25143;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18569v3 Announce Type: replace  Abstract: Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a fundamental resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tunin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#21270;&#25512;&#29702;&#30340;&#31185;&#23398;&#21457;&#29616;&#24490;&#29615;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#35299;&#37322;&#36873;&#25321;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2407.17454</link><description>&lt;p&gt;
&#33258;&#21160;&#35299;&#37322;&#36873;&#25321;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Automated Explanation Selection for Scientific Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#21270;&#25512;&#29702;&#30340;&#31185;&#23398;&#21457;&#29616;&#24490;&#29615;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#35299;&#37322;&#36873;&#25321;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17454v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#33258;&#21160;&#21270;&#25512;&#29702;&#26159;&#24180;&#36731;&#20294;&#36805;&#36895;&#25104;&#38271;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#12290;&#35299;&#37322;&#24615;&#24110;&#21161;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#19978;&#36229;&#36234;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#21270;&#25512;&#29702;&#30340;&#31185;&#23398;&#21457;&#29616;&#24490;&#29615;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#36873;&#25321;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#23427;&#21560;&#21462;&#20102;&#31038;&#20250;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#27934;&#35265;&#12290;&#36825;&#20123;&#36873;&#25321;&#26631;&#20934;&#21253;&#21547;&#29616;&#26377;&#30340;&#27010;&#24565;&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17454v2 Announce Type: replace  Abstract: Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explanability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#8220;&#26149;&#22825;&#8221;&#19968;&#35789;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#30340;4&#31181;&#21547;&#20041;&#65292;&#25581;&#31034;&#20102;&#35789;&#27719;&#22810;&#20041;&#24615;&#30340;&#28436;&#21270;&#26159;&#30001;&#35821;&#20041;&#21333;&#20803;&#30340;&#20462;&#25913;&#25152;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#36825;&#19968;&#36807;&#31243;&#26159;&#38543;&#26102;&#38388;&#36880;&#28176;&#21457;&#29983;&#30340;&#12290;</title><link>https://arxiv.org/abs/2407.16110</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#21333;&#20803;&#20998;&#26512;&#22810;&#20041;&#24615;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Polysemy Evolution using Semantic Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#8220;&#26149;&#22825;&#8221;&#19968;&#35789;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#30340;4&#31181;&#21547;&#20041;&#65292;&#25581;&#31034;&#20102;&#35789;&#27719;&#22810;&#20041;&#24615;&#30340;&#28436;&#21270;&#26159;&#30001;&#35821;&#20041;&#21333;&#20803;&#30340;&#20462;&#25913;&#25152;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#36825;&#19968;&#36807;&#31243;&#26159;&#38543;&#26102;&#38388;&#36880;&#28176;&#21457;&#29983;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16110v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;&#65306;&#35789;&#20041;&#20250;&#38543;&#26102;&#38388;&#21457;&#23637;&#32780;&#28436;&#21464;&#12290;&#21516;&#19968;&#22825;&#19981;&#21516;&#26102;&#38388;&#23545;&#21516;&#19968;&#20010;&#35789;&#30340;&#29702;&#35299;&#21487;&#33021;&#19981;&#21516;&#65292;&#21516;&#19968;&#20010;&#35789;&#30340;&#19981;&#21516;&#24847;&#20041;&#21487;&#33021;&#26159;&#30456;&#20114;&#28436;&#21270;&#30340;&#32467;&#26524;&#65292;&#21363;&#23427;&#20204;&#21487;&#33021;&#26159;&#29238;&#27597;&#21644;&#23376;&#22899;&#20851;&#31995;&#12290;&#22914;&#26524;&#25105;&#20204;&#25226;Juba&#30475;&#20316;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#23398;&#20064;&#27491;&#30830;&#31572;&#26696;&#30340;&#27169;&#24335;&#65292;&#22914;&#26524;&#19981;&#38543;&#35789;&#20041;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#37027;&#23601;&#19981;&#20877;&#26377;&#25928;&#20102;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#30001;&#20316;&#32773;&#25552;&#20986;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#21021;&#22987;&#29366;&#24577;&#22810;&#26679;&#24615;&#30340;&#23567;&#21464;&#21270;&#65292;&#20998;&#26512;&#20102;&#29992;Chat GPT&#25910;&#38598;&#30340;&#22235;&#20010;&#19981;&#21516;&#24847;&#20041;&#30340;&#21477;&#23376;&#38598;&#21512;&#65292;&#21363;&#26149;&#22825;&#30340;&#21547;&#20041;&#12290;&#29305;&#21035;&#30340;&#26159;&#65292;&#23545;&#21253;&#21547;1000&#20010;&#21477;&#23376;&#30340;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#20998;&#26512;&#65292;&#27599;&#20010;&#24207;&#21015;&#37117;&#26159;&#23545;&#8220;&#26149;&#22825;&#8221;&#19968;&#35789;&#30340;&#19981;&#21516;&#24847;&#20041;&#30340;&#21477;&#23376;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#25353;&#39034;&#24207;&#25490;&#21015;&#36825;&#20123;&#35789;&#20041;&#26102;&#65292;&#22312;&#20998;&#26512;&#20013;&#65292;&#8220;&#26149;&#22825;&#8221;&#19968;&#35789;&#30340;&#22810;&#20041;&#24615;&#38543;&#26102;&#38388;&#32780;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16110v2 Announce Type: replace-cross  Abstract: The senses of words evolve. The sense of the same word may change from today to tomorrow, and multiple senses of the same word may be the result of the evolution of each other, that is, they may be parents and children. If we view Juba as an evolving ecosystem, the paradigm of learning the correct answer, which does not move with the sense of a word, is no longer valid. This paper is a case study that shows that word polysemy is an evolutionary consequence of the modification of Semantic Cells, which has al-ready been presented by the author, by introducing a small amount of diversity in its initial state as an example of analyzing the current set of short sentences. In particular, the analysis of a sentence sequence of 1000 sentences in some order for each of the four senses of the word Spring, collected using Chat GPT, shows that the word acquires the most polysemy monotonically in the analysis when the senses are arranged in
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#30149;&#29702;&#20999;&#29255;&#22270;&#20687;&#12289;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#35889;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35757;&#32451;&#20986;&#19968;&#20010;&#33021;&#22815;&#29702;&#35299;&#30149;&#29702;&#20999;&#29255;&#20840;&#23616;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.15362</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22686;&#24378;&#30340;&#20840;&#20999;&#29255;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15362
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#30149;&#29702;&#20999;&#29255;&#22270;&#20687;&#12289;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#35889;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35757;&#32451;&#20986;&#19968;&#20010;&#33021;&#22815;&#29702;&#35299;&#30149;&#29702;&#20999;&#29255;&#20840;&#23616;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15362v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#65306;&#22312;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#22810;&#31181;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24615;&#33021;&#26174;&#33879;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#20381;&#36182;&#20110;&#20165;&#35270;&#35273;&#25968;&#25454;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#35270;&#35273;-&#25551;&#36848;&#25968;&#25454;&#65292;&#32780;&#24573;&#30053;&#20102;&#23545;&#20110;&#22810;&#31181;&#20020;&#24202;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#35889;&#12290;&#20854;&#27425;&#65292;&#30446;&#21069;&#30149;&#29702;FM&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20999;&#29255;&#32423;&#65292;&#20854;&#20013;&#20999;&#29255;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#26410;&#33021;&#25429;&#25417;&#20840;&#20999;&#29255;&#27169;&#24335;&#12290;&#25105;&#20204; curated &#20102;&#21253;&#21547;&#26368;&#22823;&#25968;&#37327;&#30340; H\&amp;E &#35786;&#26029;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#30456;&#20851;&#30149;&#29702;&#25253;&#21578;&#21644; RNA-Seq &#25968;&#25454;&#30340;&#22810;&#20803;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102; 26,169 &#20010;&#26469;&#33258; 10,275 &#21517;&#24739;&#32773;&#21644; 32 &#31181;&#30284;&#30151;&#31867;&#22411;&#30340; slide-level &#27169;&#24577;&#23545;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#30284;&#30151;&#30149;&#29702;&#23398;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#39640;&#32423;&#32441;&#29702;&#29305;&#24449;&#21644;&#30149;&#29702;&#25253;&#21578;&#20013;&#30340;&#25991;&#26412;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#22810;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#30149;&#29702;&#20999;&#29255;&#30340;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15362v2 Announce Type: replace-cross  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\&amp;E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPa
&lt;/p&gt;</description></item><item><title>Think-on-Graph 2.0 &#26159;&#19968;&#31181;&#25913;&#36827;&#30340; RAG &#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#19982;&#30693;&#35782;&#22270;&#23545;&#40784;&#65292;&#20419;&#36827;&#28145;&#24230;&#25512;&#29702;&#21644;&#25552;&#39640; LLM &#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.10805</link><description>&lt;p&gt;
Think-on-Graph 2.0: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#24341;&#23548;&#26816;&#32034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.10805
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph 2.0 &#26159;&#19968;&#31181;&#25913;&#36827;&#30340; RAG &#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#19982;&#30693;&#35782;&#22270;&#23545;&#40784;&#65292;&#20419;&#36827;&#28145;&#24230;&#25512;&#29702;&#21644;&#25552;&#39640; LLM &#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10805v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;(RAG)&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#21160;&#24577;&#30340;&#20449;&#24687;&#26816;&#32034;&#26469;&#26174;&#33879;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20197;&#27492;&#26469;&#35299;&#20915;&#29983;&#25104;&#20869;&#23481;&#30693;&#35782;&#40511;&#27807;&#21644;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#36328;&#22810;&#26679;&#39064;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;RAG&#26694;&#26550;&#65292;Think-on-Graph 2.0&#65292;&#23427;&#23558;&#38382;&#39064;&#19982;&#30693;&#35782;&#22270;&#23545;&#40784;&#24182;&#23558;&#30693;&#35782;&#22270;&#20316;&#20026;&#19968;&#31181;&#23548;&#33322;&#24037;&#20855;&#65292;&#21152;&#28145;&#24182;&#25913;&#36827;&#20102;RAG&#27169;&#24335;&#30340;&#20449;&#24687;&#25910;&#38598;&#21644;&#38598;&#25104;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#23548;&#33322;&#20419;&#36827;&#20102;&#28145;&#23618;&#21644;&#38271;&#36317;&#31163;&#30340;&#20851;&#32852;&#65292;&#20197;&#20445;&#25345;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#20248;&#21270;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#25351;&#20196;&#25351;&#23548;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#30830;&#20445;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;ToG${2.0}$&#19981;&#20165;&#25552;&#39640;&#20102;LLMs&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36824;&#23637;&#31034;&#20102;&#22312;&#30830;&#20445;&#35821;&#20041;&#23545;&#35805;&#19978;&#19979;&#25991;&#31934;&#20934;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10805v2 Announce Type: replace-cross  Abstract: Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG${2.0}$ not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;RNN&#27169;&#22411;ISMRNN&#65292;&#36890;&#36807;&#38544;&#24335;&#20998;&#21106;&#21644;&#26102;&#38388;&#24207;&#21015;&#29305;&#24322;&#24615;Mamba&#35299;&#30721;&#22120;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;RNN&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#21644;&#20449;&#24687;&#20002;&#22833;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#38271;&#26102;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#32929;&#31080;&#20215;&#26684;&#12289;&#27604;&#29305;&#24065;&#20215;&#26684;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25968;&#25454;&#38598;&#19978;&#30340;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2407.10768</link><description>&lt;p&gt;
ISMRNN&#65306;&#24102;&#26377;Mamba&#30340;&#38544;&#24335;&#20998;&#21106;RNN&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#26102;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;RNN&#27169;&#22411;ISMRNN&#65292;&#36890;&#36807;&#38544;&#24335;&#20998;&#21106;&#21644;&#26102;&#38388;&#24207;&#21015;&#29305;&#24322;&#24615;Mamba&#35299;&#30721;&#22120;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;RNN&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#21644;&#20449;&#24687;&#20002;&#22833;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#38271;&#26102;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#32929;&#31080;&#20215;&#26684;&#12289;&#27604;&#29305;&#24065;&#20215;&#26684;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25968;&#25454;&#38598;&#19978;&#30340;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10768v5 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10768v5 Announce Type: replace-cross  Abstract: Long time series forecasting aims to utilize historical information to forecast future states over extended horizons. Traditional RNN-based series forecasting methods struggle to effectively address long-term dependencies and gradient issues in long time series problems. Recently, SegRNN has emerged as a leading RNN-based model tailored for long-term series forecasting, demonstrating state-of-the-art performance while maintaining a streamlined architecture through innovative segmentation and parallel decoding techniques. Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts data continuity and fails to effectively leverage information across different segments, the segmentation strategy employed by SegRNN does not fundamentally address the issue of information loss within the recurrent structure. To address these issues, we propose the ISMRNN method with three key enhancements: we introduce an implicit s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#20915;&#31574;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24555;&#36895;&#36866;&#24212;&#21453;&#39304;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#21017;&#22312;&#22788;&#29702;&#24310;&#36831;&#21453;&#39304;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2407.09281</link><description>&lt;p&gt;
&#39044;&#27979;&#21644;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#20915;&#31574;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#30340;&#26032;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#20915;&#31574;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24555;&#36895;&#36866;&#24212;&#21453;&#39304;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#21017;&#22312;&#22788;&#29702;&#24310;&#36831;&#21453;&#39304;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.09281v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#33258;&#24049;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#21644;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#26377;&#29992;&#30340;&#24110;&#21161;&#20043;&#21069;&#65292;&#29702;&#35299;&#24182;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#21644;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26377;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#39044;&#27979;&#20154;&#20204;&#22312;&#20004;&#20010;&#36830;&#32493;&#30340;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#22312; exploitative&#65288;&#25506;&#32034;&#24615;&#65289;&#21644; exploratory&#65288;&#32463;&#39564;&#24615;&#65289;&#34892;&#21160;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#22788;&#29702;&#24310;&#36831;&#21453;&#39304;&#65292;&#36825;&#20123;&#37117;&#26159;&#27169;&#25311;&#29616;&#23454;&#29983;&#27963;&#20915;&#31574;&#36807;&#31243;&#30340;&#35201;&#32032;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;LLMs&#19982;&#19968;&#31181;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#65288;IBL&#65289;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#21518;&#32773;&#27169;&#20223;&#20102;&#20154;&#31867;&#32463;&#39564;&#24615;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#22312;&#36805;&#36895;&#25972;&#21512;&#21453;&#39304;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35748;&#30693;IBL&#27169;&#22411;&#22312;&#24310;&#36831;&#21453;&#39304;&#30340;&#36866;&#24212;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.09281v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have demonstrated their capabilities across various tasks, from language translation to complex reasoning. Understanding and predicting human behavior and biases are crucial for artificial intelligence (AI) assisted systems to provide useful assistance, yet it remains an open question whether these models can achieve this. This paper addresses this gap by leveraging the reasoning and generative capabilities of the LLMs to predict human behavior in two sequential decision-making tasks. These tasks involve balancing between exploitative and exploratory actions and handling delayed feedback, both essential for simulating real-life decision processes. We compare the performance of LLMs with a cognitive instance-based learning (IBL) model, which imitates human experiential decision-making. Our findings indicate that LLMs excel at rapidly incorporating feedback to enhance prediction accuracy. In contrast, the c
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#21327;&#21516;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#34920;&#26126;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#36136;&#37327;&#25552;&#21319;&#20013;&#36215;&#21040;&#30456;&#20114;&#20419;&#36827;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2407.08583</link><description>&lt;p&gt;
&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#28436;&#36827;&#30340;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.08583
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#21327;&#21516;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#34920;&#26126;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#36136;&#37327;&#25552;&#21319;&#20013;&#36215;&#21040;&#30456;&#20114;&#20419;&#36827;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08583v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08583v2 Announce Type: replace-cross  Abstract: The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs; on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approache
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#29702;&#35770;&#26694;&#26550;&#35780;&#20272;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#24230;&#65292;&#25581;&#31034;&#20102;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#30340;&#26412;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.01281</link><description>&lt;p&gt;
&#26631;&#39064;&#65306;&#24179;&#28369;&#24615;&#19982;&#36924;&#36817;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#29702;&#35770;&#19978;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.01281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#29702;&#35770;&#26694;&#26550;&#35780;&#20272;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#24230;&#65292;&#25581;&#31034;&#20102;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#30340;&#26412;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23450;&#20041;&#22312;&#22270;&#19978;&#20989;&#25968;&#30340;&#36924;&#36817;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#22312;&#20174;$K$&#20989;&#25968;&#24615;&#23548;&#20986;&#30340;&#36924;&#36817;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#35780;&#20272;&#30446;&#26631;&#20989;&#25968;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#36924;&#36817;&#30340;&#19979;&#30028;&#65292;&#24182;&#32771;&#23519;&#36825;&#20123;&#32593;&#32476;&#20013;&#24120;&#35265;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#22270;&#19978;&#30340;$K$&#20989;&#25968;&#24615;&#36827;&#34892;&#20102;&#20171;&#32461;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#19982;&#27169;&#20809;&#28369;&#24615;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#20856;&#22411;&#30340;GCN&#31867;&#22411;&#65292;&#23637;&#31034;&#20102;&#36755;&#20986;&#20013;&#39640;&#39057;&#33021;&#37327;&#30340;&#34928;&#20943;&#65292;&#36825;&#34920;&#26126;&#20102;&#36807;&#24230;&#24179;&#28369;&#30340;&#29616;&#35937;&#12290;&#36825;&#39033;&#20998;&#26512;&#20026;GCN&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#26412;&#36136;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#30446;&#26631;&#20989;&#25968;&#30001;GCNs&#36924;&#36817;&#30340;&#19979;&#30028;&#65292;&#36825;&#20010;&#19979;&#30028;&#26159;&#30001;&#36825;&#20123;&#20989;&#25968;&#30340;&#27169;&#20809;&#28369;&#24615;&#25903;&#37197;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#20026;&#29702;&#35299;GCN&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.01281v2 Announce Type: replace-cross  Abstract: In this paper, we explore the approximation theory of functions defined on graphs. Our study builds upon the approximation results derived from the $K$-functional. We establish a theoretical framework to assess the lower bounds of approximation for target functions using Graph Convolutional Networks (GCNs) and examine the over-smoothing phenomenon commonly observed in these networks. Initially, we introduce the concept of a $K$-functional on graphs, establishing its equivalence to the modulus of smoothness. We then analyze a typical type of GCN to demonstrate how the high-frequency energy of the output decays, an indicator of over-smoothing. This analysis provides theoretical insights into the nature of over-smoothing within GCNs. Furthermore, we establish a lower bound for the approximation of target functions by GCNs, which is governed by the modulus of smoothness of these functions. This finding offers a new perspective on t
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#25688;&#35201;&#25552;&#28860;&#30340;&#35201;&#28857;</title><link>https://arxiv.org/abs/2407.00569</link><description>&lt;p&gt;
&#20013;&#25991;&#32763;&#35793;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00569
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25688;&#35201;&#25552;&#28860;&#30340;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#32763;&#35793;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00569v4 Announce Type: replace-cross  Abstract: Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they woul
&lt;/p&gt;</description></item><item><title>InterCLIP-MEP &#26694;&#26550;&#20351;&#29992;&#20132;&#20114;&#24335; CLIP &#21644;&#22686;&#24378;&#30340;&#35760;&#24518;&#39044;&#27979;&#22120;&#25913;&#36827;&#20102;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#22810;&#27169;&#24577; sarcasm &#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2406.16464</link><description>&lt;p&gt;
InterCLIP-MEP&#65306;&#20132;&#20114;&#24335;CLIP&#21644;&#22686;&#24378;&#35760;&#24518;&#39044;&#27979;&#22120;&#22312;&#22810;&#27169;&#24577; sarcasm &#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.16464
&lt;/p&gt;
&lt;p&gt;
InterCLIP-MEP &#26694;&#26550;&#20351;&#29992;&#20132;&#20114;&#24335; CLIP &#21644;&#22686;&#24378;&#30340;&#35760;&#24518;&#39044;&#27979;&#22120;&#25913;&#36827;&#20102;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#22810;&#27169;&#24577; sarcasm &#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16464v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978; sarcasm &#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#36890;&#36807;&#25991;&#23383;&#21644;&#22270;&#20687;&#30340;&#32452;&#21512;&#34920;&#36798;&#65292;&#20026;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#22270;&#25366;&#25496;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577; sarcasm &#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#23384;&#22312;&#39640;&#20272;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#25991;&#23383;&#21644;&#22270;&#20687;&#20043;&#38388;&#20114;&#21160;&#20135;&#29983;&#30340;&#31934;&#32454; sarcastic &#32447;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; InterCLIP-MEP&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577; sarcasm &#26816;&#27979;&#30340;&#20840;&#26032;&#26694;&#26550;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20132;&#20114;&#24335; CLIP&#65288;InterCLIP&#65289;&#20316;&#20026;&#39592;&#24178;&#65292;&#25552;&#21462;&#25991;&#23383;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#32534;&#30721;&#22120;&#20013;&#30452;&#25509;&#23884;&#20837;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#34920;&#31034;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#25991;&#23383;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22521;&#35757;&#31574;&#30053;&#65292;&#20026;&#20102;&#36866;&#24212;&#25105;&#20204;&#25552;&#20986;&#30340;&#22686;&#24378;&#35760;&#24518;&#39044;&#27979;&#22120;&#65288;MEP&#65289;&#35843;&#25972;InterCLIP&#12290;MEP&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#30340;&#12289;&#22266;&#23450;&#38271;&#24230;&#30340;&#21452;&#36890;&#36947;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#23545; sarcasm &#30340;&#26816;&#27979;&#65292;&#24182;&#19988;&#20934;&#30830;&#29575;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16464v3 Announce Type: replace-cross  Abstract: The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Existing multi-modal sarcasm detection methods have been proven to overestimate performance, as they struggle to effectively capture the intricate sarcastic cues that arise from the interaction between an image and text. To address these issues, we propose InterCLIP-MEP, a novel framework for multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP (InterCLIP) as the backbone to extract text-image representations, enhancing them by embedding cross-modality information directly within each encoder, thereby improving the representations to capture text-image interactions better. Furthermore, an efficient training strategy is designed to adapt InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic, fixed-length dual-channel mem
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#23545;&#27604;&#20102;&#22312;context&#23398;&#20064;&#19982;&#24494;&#35843;&#31561;&#19981;&#21516;&#36866;&#24212;&#25216;&#26415;&#22312;&#19981;&#21516;&#23545;&#35805;&#31867;&#22411;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2406.06399</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#24494;&#35843;&#36824;&#26159;RAG&#65311;&#35780;&#20272;&#19981;&#21516;&#36866;&#24212;LLM&#23545;&#35805;&#30340;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.06399
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#23545;&#27604;&#20102;&#22312;context&#23398;&#20064;&#19982;&#24494;&#35843;&#31561;&#19981;&#21516;&#36866;&#24212;&#25216;&#26415;&#22312;&#19981;&#21516;&#23545;&#35805;&#31867;&#22411;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.06399v3 &#22768;&#26126;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#25688;&#35201;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#31867;&#26426;&#22120;&#23545;&#35805;&#20013;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#65288;&#20363;&#22914;&#24320;&#25918;&#22495;&#23545;&#35805;&#65289;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#35780;&#20215;&#22312;&#22522;&#30784;LLM&#12289;&#23545;&#35805;&#31867;&#22411;&#21644;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#37117;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;LLM&#36866;&#24212;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#26102;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#22522;&#30784;LLM&#65292;&#21363;Llama-2&#21644;Mistral&#65292;&#20197;&#21450;&#22235;&#31181;&#23545;&#35805;&#31867;&#22411;&#65306;&#24320;&#25918;&#22495;&#23545;&#35805;&#12289;&#30693;&#35782;&#36171;&#33021;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#38382;&#31572;&#23545;&#35805;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#23545;&#35805;&#31867;&#22411;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36866;&#29992;&#20110;in-context&#23398;&#20064;&#30340;&#20869;&#37096;&#30693;&#35782;&#38598;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#21644;&#25506;&#32034;&#21508;&#31181;&#21487;&#33021;&#24615;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#20102;&#19968;&#33268;&#24615;&#35780;&#20272;&#21644;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.06399v3 Announce Type: replace-cross  Abstract: We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35266;&#28857;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#25552;&#20379;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#26377;&#25928;&#30340;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#23545;&#20110;&#22312;&#20154;&#24037;&#21644;&#20154;&#31867;&#25351;&#23548;&#19979;&#30340;&#32463;&#27982;&#25928;&#29575;&#37117;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2405.17287</link><description>&lt;p&gt;
&#35266;&#28857;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Opinion-Guided Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.17287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35266;&#28857;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#25552;&#20379;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#26377;&#25928;&#30340;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#23545;&#20110;&#22312;&#20154;&#24037;&#21644;&#20154;&#31867;&#25351;&#23548;&#19979;&#30340;&#32463;&#27982;&#25928;&#29575;&#37117;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.17287v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20154;&#31867;&#25351;&#23548;&#24448;&#24448;&#33021;&#22815;&#25552;&#21319;&#23398;&#20064;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#25351;&#23548;&#36890;&#24120;&#26159;&#22522;&#20110;&#19981;&#23436;&#20840;&#30340;&#20449;&#24687;&#21644;&#29468;&#27979;&#65292;&#32780;&#19981;&#26159;&#31995;&#32479;&#30340;&#35770;&#35777;&#12290;&#34429;&#28982;&#36825;&#20123;&#25351;&#23548;&#26377;&#19968;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#30001;&#20110;&#23545;&#38382;&#39064;&#30340;&#37096;&#20998;&#20102;&#35299;&#25110;&#19968;&#26080;&#25152;&#30693;&#32780;&#24341;&#36215;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#27604;&#21487;&#20197;&#24471;&#21040;&#30340;&#30828;&#35777;&#25454;&#26089;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35266;&#28857;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26377;&#21487;&#33021;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#23545;&#35266;&#28857;&#36827;&#34892;&#24314;&#27169;&#21644;&#31649;&#29702;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35266;&#28857;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#31649;&#29702;&#21644;&#24314;&#27169;&#39038;&#38382;&#30340;&#25351;&#23548;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#65292;&#36890;&#36807;&#22810;&#31181;&#24314;&#35758;&#31574;&#30053;&#65292;&#19982;&#21512;&#25104;&#65288;&#25110;acle&#65289;&#21644;&#20154;&#31867;&#39038;&#38382;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#23545;&#19981;&#21516;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#25351;&#23548;&#30340;&#19981;&#20381;&#36182;&#24615;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26041;&#27861;&#22312;&#32463;&#27982;&#25928;&#29575;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#21512;&#25104;&#39038;&#38382;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36991;&#20813;&#39640;&#36798;90%&#30340;&#20154;&#24037;&#20915;&#31574;&#65292;&#32780;&#22312;&#20154;&#31867;&#39038;&#38382;&#26465;&#20214;&#19979;&#20063;&#33021;&#22815;&#32553;&#23567;&#20915;&#31574;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.17287v2 Announce Type: replace-cross  Abstract: Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence can be produced. Thus, guiding reinforcement learning agents by way of opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle) and human advisors, at different levels of uncertainty, and under multiple advice strategies. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#37325;&#21160;&#24577;ISAC&#39044;&#32534;&#30721;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#36710;&#36733;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#23588;&#20854;&#26159;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#12290;</title><link>https://arxiv.org/abs/2405.14347</link><description>&lt;p&gt;
&#21452;&#37325;&#21160;&#24577;ISAC&#39044;&#32534;&#30721;&#26041;&#27861;&#22312;&#36710;&#36733;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;CDRL&#65289;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep Reinforcement Learning (CDRL) Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#37325;&#21160;&#24577;ISAC&#39044;&#32534;&#30721;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#36710;&#36733;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#23588;&#20854;&#26159;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14347v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#65288;ISAC&#65289;&#25216;&#26415;&#23545;&#20110;&#23454;&#29616;&#36710;&#36733;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#36890;&#20449;&#20449;&#36947;&#34920;&#29616;&#20986;&#21160;&#24577;&#21464;&#21270;&#30340;&#29305;&#28857;&#65292;&#28508;&#22312;&#30340;&#30446;&#26631;&#21487;&#33021;&#20250;&#36805;&#36895;&#31227;&#21160;&#65292;&#20135;&#29983;&#19968;&#31181;&#21452;&#37325;&#21160;&#24577;&#29616;&#35937;&#12290;&#36825;&#31181;&#29305;&#24615;&#20026;&#23454;&#26102;&#39044;&#32534;&#30721;&#22120;&#35774;&#35745;&#21644;&#23454;&#26045;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#23613;&#31649;&#22522;&#20110;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22797;&#26434;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#23436;&#20840;&#39044;&#20808;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;CDRL&#65289;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;ISAC&#39044;&#32534;&#30721;&#22120;&#35774;&#35745;&#20013;&#30340;&#21160;&#24577;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#20026;PD-DDPG&#21452;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#21644;Wolpertinger&#26550;&#26500;&#20316;&#20986;&#29305;&#21035;&#35774;&#35745;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#32422;&#26463;&#21644;&#29992;&#25143;&#25968;&#37327;&#21487;&#21464;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#35813;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#19981;&#20165;&#21487;&#20197;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#36827;&#34892;&#36866;&#24212;&#65292;&#36824;&#33021;&#22815;&#25552;&#21319;&#36710;&#36733;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20445;&#25345;&#36739;&#39640;&#30340;&#20256;&#36755;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14347v2 Announce Type: replace-cross  Abstract: Integrated sensing and communication (ISAC) technology is essential for enabling the vehicular networks. However, the communication channel in this scenario exhibits time-varying characteristics, and the potential targets may move rapidly, creating a doubly-dynamic phenomenon. This nature poses a challenge for real-time precoder design. While optimization-based solutions are widely researched, they are complex and heavily rely on perfect prior information, which is impractical in double dynamics. To address this challenge, we propose using constrained deep reinforcement learning (CDRL) to facilitate dynamic updates to the ISAC precoder design. Additionally, the primal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertinger architecture are tailored to efficiently train the algorithm under complex constraints and variable numbers of users. The proposed scheme not only adapts to the dynamics based on observations but a
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;VR&#29615;&#22659;&#20013;&#26469;&#25552;&#21319;&#29992;&#25143;&#20132;&#20114;&#21644;&#20219;&#21153;&#25928;&#29575;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;unity&#24341;&#25806;&#21644;&#33258;&#30740;&#30340;VLM&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#35270;&#35273;&#25991;&#26412;&#25351;&#20196;&#30340;&#23454;&#26102;&#12289;&#30452;&#35266;&#29992;&#25143;&#20132;&#20114;&#12290;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#33298;&#36866;&#24230;&#12290;</title><link>https://arxiv.org/abs/2405.11537</link><description>&lt;p&gt;
VR-GPT&#65306;&#29992;&#20110;&#26234;&#33021;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11537
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;VR&#29615;&#22659;&#20013;&#26469;&#25552;&#21319;&#29992;&#25143;&#20132;&#20114;&#21644;&#20219;&#21153;&#25928;&#29575;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;unity&#24341;&#25806;&#21644;&#33258;&#30740;&#30340;VLM&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#35270;&#35273;&#25991;&#26412;&#25351;&#20196;&#30340;&#23454;&#26102;&#12289;&#30452;&#35266;&#29992;&#25143;&#20132;&#20114;&#12290;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#33298;&#36866;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11537v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11537v3 Announce Type: replace  Abstract: The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.
&lt;/p&gt;</description></item><item><title>&#27492;&#24037;&#20855;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#23545;&#35805;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25552;&#21462;&#24037;&#20855;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#26448;&#26009;&#23646;&#24615;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#20108;&#32500;&#26448;&#26009;&#30340;&#21402;&#24230;&#31561;&#20851;&#38190;&#21442;&#25968;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;95%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2405.10448</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#27169;&#22411;&#30340;&#26032;&#22411;&#21516;&#24773;&#22659;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#25454;&#25552;&#21462;&#21644;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic In-context Learning with Conversational Models for Data Extraction and Materials Property Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.10448
&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20855;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#23545;&#35805;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25552;&#21462;&#24037;&#20855;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#26448;&#26009;&#23646;&#24615;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#20108;&#32500;&#26448;&#26009;&#30340;&#21402;&#24230;&#31561;&#20851;&#38190;&#21442;&#25968;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;95%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31185;&#23398;&#25991;&#29486;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PropertyExtractor&#30340;&#24320;&#28304;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#39640;&#32423;&#23545;&#35805;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Google&#30340;gemini-pro&#21644;OpenAI&#30340;gpt-4&#65289;&#65292;&#32467;&#21512;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#21516;&#24773;&#22659;&#23398;&#20064;&#65292;&#20197;&#21450;&#26088;&#22312;&#21160;&#24577;&#32454;&#21270;&#20449;&#24687;&#23618;&#27425;&#32467;&#26500;&#30340;&#24037;&#31243;&#21270;&#25552;&#31034;&#20449;&#24687;&#65292;&#23454;&#29616;&#26448;&#26009;&#23646;&#24615;&#25968;&#25454;&#30340;&#33258;&#21160;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#21644;&#20934;&#30830;&#30340;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#39564;&#35777;&#12290;&#23545;&#26448;&#26009;&#25968;&#25454;&#36827;&#34892;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#35813;&#24037;&#20855;&#30340;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#36229;&#36807;95%&#65292;&#35823;&#24046;&#29575;&#32422;&#20026;9%&#65292;&#39564;&#35777;&#20102;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#35813;&#24037;&#20855;&#30340;&#25968;&#25454;&#24211;&#21253;&#25324;&#20108;&#32500;&#26448;&#26009;&#21402;&#24230;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#29992;&#20110;&#35774;&#22791;&#38598;&#25104;&#65292;&#20197;&#21450;&#33021;&#37327;&#30005;&#27744;&#26448;&#26009;&#30340;&#30005;&#20301;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10448v2 Announce Type: replace-cross  Abstract: The advent of natural language processing and large language models (LLMs) has revolutionized the extraction of data from unstructured scholarly papers. However, ensuring data trustworthiness remains a significant challenge. In this paper, we introduce PropertyExtractor, an open-source tool that leverages advanced conversational LLMs like Google gemini-pro and OpenAI gpt-4, blends zero-shot with few-shot in-context learning, and employs engineered prompts for the dynamic refinement of structured information hierarchies - enabling autonomous, efficient, scalable, and accurate identification, extraction, and verification of material property data. Our tests on material data demonstrate precision and recall that exceed 95\% with an error rate of approximately 9%, highlighting the effectiveness and versatility of the toolkit. Finally, databases for 2D material thicknesses, a critical parameter for device integration, and energy ban
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Jax&#26694;&#26550;&#21152;&#36895;&#37327;&#23376;&#27169;&#25311;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#33016;&#37096;X&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#39640;&#25928;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2405.00156</link><description>&lt;p&gt;
&#25193;&#23637;&#35270;&#37326;&#65306;&#20026;&#38271;&#23614;&#33016;&#37096;X&#32447;&#20998;&#31867;&#21551;&#29992;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Expanding the Horizon: Enabling Hybrid Quantum Transfer Learning for Long-Tailed Chest X-Ray Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00156
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Jax&#26694;&#26550;&#21152;&#36895;&#37327;&#23376;&#27169;&#25311;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#33016;&#37096;X&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#39640;&#25928;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00156v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#65306;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#22312;&#22823;&#22411;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#32597;&#35265;&#20294;&#20851;&#38190;&#30142;&#30149;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#65292;&#36825;&#24471;&#30410;&#20110;&#23427;&#30456;&#23545;&#20110;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#34429;&#28982;&#20808;&#21069;&#25991;&#29486;&#24050;&#32463;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#25506;&#32034;&#20102;QML&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;&#23545;&#37327;&#23376;&#30828;&#20214;&#30340;&#35775;&#38382;&#38480;&#21046;&#21644;&#35745;&#31639;&#19978;&#30340;&#26114;&#36149;&#27169;&#25311;&#65292;&#23427;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#38480;&#30340;CXRs&#19978;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#23545;&#24403;&#21069;&#36719;&#20214;&#25552;&#20379;&#32773;&#26377;&#26174;&#33879;&#25913;&#36827;&#30340;&#26041;&#24335;&#22312;&#20855;&#26377;&#36739;&#22823;&#37327;&#23376;&#27604;&#29305;&#26550;&#26500;&#30340;&#27169;&#25311;&#19978;&#36827;&#34892;&#27169;&#25311;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#22312;&#25928;&#29575;&#21644;&#23545;&#20110;&#38271;&#23614;&#20998;&#31867;&#30340;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#22312;&#20855;&#26377;8&#12289;14&#21644;19&#31181;&#30142;&#30149;&#26631;&#31614;&#30340;&#22823;&#22411;CXR&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#12290;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#65306;&#24615;&#33021;&#26356;&#22909;&#65292;&#33021;&#22815;&#21152;&#36895;&#37327;&#23376;&#27169;&#25311;&#65292;&#26377;&#21033;&#20110;&#38271;&#23614;&#33016;&#37096;X&#32447;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00156v2 Announce Type: replace-cross  Abstract: Quantum machine learning (QML) has the potential for improving the multi-label classification of rare, albeit critical, diseases in large-scale chest x-ray (CXR) datasets due to theoretical quantum advantages over classical machine learning (CML) in sample efficiency and generalizability. While prior literature has explored QML with CXRs, it has focused on binary classification tasks with small datasets due to limited access to quantum hardware and computationally expensive simulations. To that end, we implemented a Jax-based framework that enables the simulation of medium-sized qubit architectures with significant improvements in wall-clock time over current software offerings. We evaluated the performance of our Jax-based framework in terms of efficiency and performance for hybrid quantum transfer learning for long-tailed classification across 8, 14, and 19 disease labels using large-scale CXR datasets. The Jax-based framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#21464;&#21270;&#38431;&#21451;&#25968;&#37327;&#21644;&#31867;&#22411;&#30340;&#29615;&#22659;&#20013;&#65292;N&#20010;&#33258;&#27835;&#20195;&#29702;&#22914;&#20309;&#36827;&#34892;&#21512;&#20316;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.10740</link><description>&lt;p&gt;
N&#22810;&#20195;&#29702;&#38543;&#26426;&#22242;&#38431;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
N-Agent Ad Hoc Teamwork
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.10740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#21464;&#21270;&#38431;&#21451;&#25968;&#37327;&#21644;&#31867;&#22411;&#30340;&#29615;&#22659;&#20013;&#65292;N&#20010;&#33258;&#27835;&#20195;&#29702;&#22914;&#20309;&#36827;&#34892;&#21512;&#20316;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.10740v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#24403;&#21069;&#23398;&#20064;&#21512;&#20316;&#22810;&#20195;&#29702;&#34892;&#20026;&#30340;&#26041;&#27861;&#20551;&#35774;&#30456;&#23545;&#38480;&#21046;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#26631;&#20934;&#23436;&#20840;&#21512;&#20316;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#22330;&#26223;&#20013;&#30340;&#25152;&#26377;&#20195;&#29702;&#65292;&#32780;&#22312;&#38543;&#26426;&#22242;&#38431;&#24037;&#20316;&#20013;&#65292;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#21482;&#25511;&#21046;&#22330;&#26223;&#20013;&#21333;&#20010;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#21512;&#20316;&#24773;&#24418;&#36828;&#19981;&#37027;&#20040;&#20005;&#26684;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#19968;&#23478;&#20844;&#21496;&#21487;&#33021;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20854;&#36710;&#36742;&#65292;&#20294;&#19968;&#26086;&#19978;&#36335;&#65292;&#36825;&#20123;&#36710;&#36742;&#24517;&#39035;&#19982;&#21478;&#19968;&#23478;&#20844;&#21496;&#36710;&#36742;&#21512;&#20316;&#12290;&#20026;&#20102;&#25193;&#23637;&#21487;&#20197;&#20248;&#21270;&#22320;&#22788;&#29702;&#21512;&#20316;&#23398;&#20064;&#26041;&#27861;&#30340;&#22330;&#26223;&#31867;&#21035;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;N&#20195;&#29702;&#38543;&#26426;&#22242;&#38431;&#24037;&#20316;&#65288;NAHT&#65289;&#65292;&#20854;&#20013;&#19968;&#32452;&#33258;&#27835;&#20195;&#29702;&#24517;&#39035;&#19982;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#25968;&#37327;&#30340;&#21160;&#24577;&#31867;&#22411;&#38431;&#21451;&#20114;&#21160;&#24182;&#21512;&#20316;&#12290;&#26412;&#25991;&#27491;&#24335;&#34920;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.10740v2 Announce Type: replace  Abstract: Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls $\textit{all}$ agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$-agent ad hoc teamwork (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;OpenBias&#65292;&#19968;&#20010;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#24320;&#25918;&#38598;&#20559;&#35265;&#30340;&#31995;&#32479;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#24046;&#38598;&#21512;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#24320;&#25918;&#38598;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.07990</link><description>&lt;p&gt;
&#24320;&#25918;&#20559;&#35265;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#38598;&#20559;&#24046;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OpenBias: Open-set Bias Detection in Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;OpenBias&#65292;&#19968;&#20010;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#24320;&#25918;&#38598;&#20559;&#35265;&#30340;&#31995;&#32479;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#24046;&#38598;&#21512;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#24320;&#25918;&#38598;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07990v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#19988;&#23545;&#26222;&#36890;&#22823;&#20247;&#36234;&#26469;&#36234;&#21487;&#35775;&#38382;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#28145;&#20837;&#25506;&#31350;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36991;&#20813;&#20256;&#25773;&#21644;&#24310;&#32493;&#20219;&#20309;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26816;&#27979;&#39044;&#20808;&#23450;&#20041;&#30340;&#26377;&#38480;&#38598;&#20559;&#24046;&#65292;&#38480;&#21046;&#30740;&#31350;&#23616;&#38480;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#38598;&#20559;&#24046;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;OpenBias&#65292;&#19968;&#20010;&#26032;&#30340;&#31649;&#36947;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#20219;&#20309;&#39044;&#32534;&#35793;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21644;&#37327;&#21270;&#20559;&#35265;&#30340;&#20005;&#37325;&#24615;&#12290;OpenBias&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22522;&#20110;&#19968;&#32452;&#25552;&#31034;&#25552;&#20986;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#30446;&#26631;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#30456;&#21516;&#30340;&#25552;&#31034;&#38598;&#29983;&#25104;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#19968;&#20010;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#35782;&#21035;&#20808;&#21069;&#25552;&#35758;&#20559;&#24046;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#23545;11&#20010;&#31867;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#21253;&#25324;&#8220;&#20154;&#31181;&#8221;&#12289;&#8220;&#24615;&#21035;&#8221;&#21644;&#8220;&#32463;&#27982;&#29366;&#24577;&#8221;&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;OpenBias&#33021;&#22815;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#24320;&#25918;&#38598;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#26080;&#27861;&#22788;&#29702;&#30340;&#24320;&#25918;&#38598;&#38382;&#39064;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#20013;&#23454;&#38469;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#23450;&#37327;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07990v2 Announce Type: replace  Abstract: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previou
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#21644;&#27867;&#21270;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#24418;&#29366;&#20989;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#21435;&#38500;&#20102;&#8220;&#40657;&#31665;&#8221;&#25928;&#24212;&#65292;&#20351;&#24471;&#26041;&#27861;&#26356;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#21644;&#27867;&#21270;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#24418;&#29366;&#20989;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#21435;&#38500;&#20102;&#8220;&#40657;&#31665;&#8221;&#25928;&#24212;&#65292;&#20351;&#24471;&#26041;&#27861;&#26356;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#20248;&#31168;&#30340;&#25968;&#25454;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#29615;&#22659;&#34920;&#31034;&#30340;&#36136;&#37327;&#30452;&#25509;&#24433;&#21709;&#20102;&#23398;&#20064;&#20219;&#21153;&#30340;&#25104;&#21151;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#26174;&#24335;&#25110;&#38544;&#24335;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#29615;&#22659;&#65292;&#22914;&#22270;&#20687;&#12289;&#28857;&#12289;&#20307;&#32032;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#26041;&#27861;&#23384;&#22312;&#33509;&#24178;&#32570;&#28857;&#12290;&#23427;&#20204;&#35201;&#20040;&#26080;&#27861;&#25551;&#36848;&#22797;&#26434;&#30340;&#23616;&#37096;&#20960;&#20309;&#65292;&#35201;&#20040;&#22312;&#20174;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#25110;&#32773;&#38656;&#35201;&#31934;&#30830;&#30340;&#32972;&#26223;&#36974;&#32617;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#23601;&#20687;&#26159;&#19968;&#20010;&#8220;&#40657;&#31665;&#8221;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;3D&#39640;&#26031;&#28857;&#31215;(3DGS)&#20316;&#20026;&#19968;&#31181;&#26174;&#24335;&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#21487;&#24494;&#30340;&#28210;&#26579;&#26041;&#27861;&#65292;&#34987;&#35748;&#20026;&#26159;&#23545;&#37325;&#25490;&#21644;&#34920;&#31034;&#26041;&#27861;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#27867;&#21270;&#39640;&#26031;&#28857;&#31215;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#22330;&#26223;&#25551;&#36848;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#24418;&#29366;&#20989;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#21644;&#21435;&#8220;&#40657;&#31665;&#8221;&#65292;&#20174;&#32780;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#28151;&#21512;RAG&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#25216;&#26415;&#21644;&#28151;&#21512;&#26597;&#35810;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;RAG&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;IR&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.07220</link><description>&lt;p&gt;
&#28151;&#21512;RAG&#65306;&#36890;&#36807;&#35821;&#20041;&#25628;&#32034;&#21644;&#28151;&#21512;&#26597;&#35810;&#22522;&#30784;&#26816;&#32034;&#22120;&#25552;&#39640;RAG&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07220
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#28151;&#21512;RAG&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#25216;&#26415;&#21644;&#28151;&#21512;&#26597;&#35810;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;RAG&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;IR&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#23558;&#31169;&#20154;&#30693;&#35782;&#24211;&#25991;&#26723;&#38598;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32467;&#21512;&#20197;&#26500;&#24314;&#29983;&#25104;&#24335;&#38382;&#31572;&#65288;Question-Answering&#65289;&#31995;&#32479;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#38598;&#30340;&#25193;&#22823;&#65292;RAG&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26816;&#32034;&#22120;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#65292;&#36890;&#36807;&#20174;&#25991;&#26723;&#38598;&#20013;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#32473;LLM&#25552;&#20379;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26174;&#33879;&#24433;&#21709;&#20102;&#25972;&#20307;RAG&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#28151;&#21512;RAG&#8221;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#35821;&#20041;&#25628;&#32034;&#25216;&#26415;&#65292;&#22914;&#23494;&#38598;&#21521;&#37327;&#32034;&#24341;&#21644;&#31232;&#30095;&#32534;&#30721;&#22120;&#32034;&#24341;&#65292;&#24182;&#19982;&#28151;&#21512;&#26597;&#35810;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#25968;&#25454;&#38598;&#65292;&#22914;NQ&#21644;TREC-COVID&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#26816;&#32034;&#32467;&#26524;&#65292;&#24182;&#35774;&#23450;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#8220;&#28151;&#21512;&#26816;&#32034;&#22120;&#8221;&#25193;&#23637;&#21040;&#20102;RAG&#31995;&#32479;&#20013;&#65292;&#22312;&#35832;&#22914;SQUAD&#20043;&#31867;&#30340;&#29983;&#25104;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;fine-tuning&#30340;RAG&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07220v2 Announce Type: replace-cross  Abstract: Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q\&amp;A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q\&amp;A datasets like SQUAD, even surpassing fine-tunin
&lt;/p&gt;</description></item><item><title>LGOT&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#65292;&#35299;&#20915;&#20102;&#36923;&#36753;&#26597;&#35810;&#20013;&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#21450;&#30693;&#35782;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#38382;&#31572;&#31995;&#32479;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.04264</link><description>&lt;p&gt;
&#12298;&#24605;&#32500;&#26597;&#35810;&#36923;&#36753;&#65306;&#20511;&#21161;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#12299;
&lt;/p&gt;
&lt;p&gt;
Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.04264
&lt;/p&gt;
&lt;p&gt;
LGOT&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#65292;&#35299;&#20915;&#20102;&#36923;&#36753;&#26597;&#35810;&#20013;&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#21450;&#30693;&#35782;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#38382;&#31572;&#31995;&#32479;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.04264v3 Announce Type: replace-cross &#27010;&#35201;&#65306;&#23613;&#31649;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38754;&#23545;&#38656;&#35201;&#30693;&#35782;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#26102;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#29978;&#33267;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#24403;&#24212;&#23545;&#38656;&#35201;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#30340;&#36923;&#36753;&#26597;&#35810;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#26356;&#26126;&#26174;&#20102;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#38382;&#31572;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#30340;&#24110;&#21161;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#20294;&#26159;&#24403;&#30693;&#35782;&#22270;&#26412;&#36523;&#19981;&#23436;&#25972;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#20250;&#36805;&#36895;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#30456;&#20114;&#21463;&#30410;&#30340;&#26041;&#24335;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#19982;LLM&#20197;&#20943;&#36731;LLM&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#20197;&#21450;&#30693;&#35782;&#22270;&#30340;&#19981;&#23436;&#25972;&#38382;&#39064;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#26597;&#35810;&#36923;&#36753;&#8221;&#65288;LGOT&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;LLMS&#19982;&#30693;&#35782;&#22270;&#32467;&#21512;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#36825;&#20026;LLMs&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#30693;&#35782;&#38598;&#25104;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#35299;&#20915;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LGOT&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38382;&#31572;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#38169;&#35823;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.04264v3 Announce Type: replace-cross  Abstract: Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge grap
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;RallyNet&#65292;&#26088;&#22312;&#27169;&#20223;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#29699;&#21592;&#30340;&#34892;&#20026;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#20307;&#39564;&#29615;&#22659;&#21644;&#23398;&#20064;&#24067;&#26391;&#36816;&#21160;&#27169;&#20223;&#32701;&#27611;&#29699;&#29699;&#21592;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;RallyNet&#65292;&#26088;&#22312;&#27169;&#20223;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#29699;&#21592;&#30340;&#34892;&#20026;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v2 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;&#25688;&#35201;&#65306;&#22312;&#21160;&#24577;&#21644;&#24555;&#36895;&#25112;&#26415;&#21442;&#19982;&#30340;&#19979;&#65292;&#32701;&#27611;&#29699;&#20316;&#20026;&#19968;&#20010;&#38656;&#35201;&#20132;&#26367;&#20915;&#31574;&#30340;&#26412;&#21407;&#27169;&#24335;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#35265;&#35777;&#20102;&#20174;&#31163;&#32447;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#32447;&#27169;&#20223;&#20154;&#31867;&#29699;&#21592;&#34892;&#20026;&#26041;&#38754;&#65292;&#22914;&#20309;&#22312;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#37319;&#21462;&#20132;&#26367;&#34892;&#21160;&#30340;&#34892;&#20026;&#20173;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#27169;&#20223;&#23545;&#25163;&#30340;&#34892;&#20026;&#23545;&#29699;&#21592;&#26377;&#30410;&#65292;&#36890;&#36807;&#22312;&#27604;&#36187;&#21069;&#25552;&#20379;&#26041;&#21521;&#24615;&#65292;&#20801;&#35768;&#20182;&#20204;&#36827;&#34892;&#25112;&#30053;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#26041;&#27861;&#20250;&#36935;&#21040;&#30001;&#20110;&#27604;&#36187;&#30340;&#20869;&#22312;&#23618;&#27425;&#21644;&#20132;&#26367;&#34892;&#21160;&#30340;&#32047;&#21152;&#25928;&#24212;&#32780;&#23548;&#33268;&#30340;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RallyNet&#65292;&#19968;&#20010;&#38024;&#23545;&#32701;&#27611;&#29699;&#29699;&#21592;&#34892;&#20026;&#30340;&#26032;&#30340;&#23618;&#27425;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65306;(i) RallyNet&#36890;&#36807;&#24314;&#27169;&#20915;&#23450;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v2 Announce Type: replace  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;Dreamer V3&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24773;&#22659;&#37325;&#22797;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;cRSSM&#65289;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21253;&#21547;&#24773;&#22659;&#20449;&#24687;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;CARL&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#19978;&#20063;&#39564;&#35777;&#20102;&#20854;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10967</link><description>&lt;p&gt;
&#26790;&#24819;&#35768;&#22810;&#19990;&#30028;&#65306;&#23398;&#20064;&#24773;&#22659;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10967
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;Dreamer V3&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24773;&#22659;&#37325;&#22797;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;cRSSM&#65289;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21253;&#21547;&#24773;&#22659;&#20449;&#24687;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;CARL&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#19978;&#20063;&#39564;&#35777;&#20102;&#20854;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#36807;&#30340;&#25688;&#35201;&#65306;&#38646;&#26679;&#26412;&#27867;&#21270;&#65288;ZSG&#65289;&#21040;&#26410;&#35265;&#21160;&#24577;&#26159;&#21019;&#24314;&#36890;&#29992;&#33021;&#21147;&#30340;&#36523;&#20307;&#21270;&#20195;&#29702;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#65288;cRL&#65289;&#30340;&#31616;&#21333;&#35774;&#32622;&#24320;&#22987;&#65292;&#20551;&#23450;&#23545;&#21442;&#25968;&#21270;&#31995;&#32479;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20540;&#30340;&#21487;&#35266;&#27979;&#24615;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#30340;&#36136;&#37327;&#25110;&#23610;&#23544;&#65292;&#32780;&#19981;&#20570;&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#21487;&#35266;&#27979;&#24615;&#30340;&#36827;&#19968;&#27493;&#31616;&#21270;&#20551;&#35774;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26410;&#35265;&#29615;&#22659;&#21464;&#21270;&#30340;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24773;&#22659;&#37325;&#22797;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;cRSSM&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;&#23545;Dreamer&#65288;v3&#65289;(Hafner&#31561;&#65292;2023)&#30340;&#19968;&#31181;&#24773;&#22659;&#19990;&#30028;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;&#36825;&#20801;&#35768;&#19990;&#30028;&#27169;&#22411;&#36890;&#36807;&#35266;&#23519;&#26469;&#25512;&#27979;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#65292;&#24182;&#24314;&#27169;&#28508;&#22312;&#21160;&#24577;&#65292;&#20174;&#32780;&#23545;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;CARL&#22522;&#20934;&#22871;&#20214;&#20013;&#36873;&#25321;&#20102;&#20004;&#20010;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#19987;&#38376;&#29992;&#26469;&#30740;&#31350;&#24773;&#22659;RL&#12290;&#25105;&#20497;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;CARL&#22522;&#20934;&#22871;&#20214;&#20013;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20215;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#65292;&#19982; baselines &#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; zero-shot &#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102; steps&#65292;&#20854;&#20013;&#36890;&#36807;&#23558; world model &#30340;&#36755;&#20986;&#29992;&#20316;&#21363;&#23558;&#21040;&#26469;&#30340;&#35266;&#27979;&#30340;&#39044;&#27979;&#65292;&#26469;&#23454;&#22312;&#22320;&#34701;&#21512;&#24773;&#22659;&#20449;&#24687;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#39564;&#35777;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558; context &#19982; observation &#32467;&#21512;&#65292;&#23601;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#22343;&#23637;&#29616;&#20102;&#32467;&#21512; context &#21644; world model &#33021;&#22815;&#22823;&#24133;&#25552;&#39640; EUD &#21644;&#26368;&#32456;&#24615;&#33021;&#65292;&#23637;&#31034;&#20986;&#36825;&#31181;&#26041;&#27861;&#22312;&#25512;&#26029; latent dynamic &#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v2 Announce Type: replace-cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our approach is evaluated on two tasks from the CARL benchmark suite, which is tailored to study contextual RL. Our experim
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#36828;&#36828;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#26102;&#65292;&#36890;&#36807;&#23545;&#25239;&#20540;&#20989;&#25968;&#20998;&#27495;&#65292;&#31639;&#27861;&#33021;&#22815;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#20248;&#20808;&#20559;&#35265;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#39640;&#26356;&#26032;&#27604;&#28145;&#24230; RL &#35299;&#26512;&#65306;&#25239;&#20987;&#20540;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#36828;&#36828;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#26102;&#65292;&#36890;&#36807;&#23545;&#25239;&#20540;&#20989;&#25968;&#20998;&#27495;&#65292;&#31639;&#27861;&#33021;&#22815;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#20248;&#20808;&#20559;&#35265;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#36828;&#23569;&#20110;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20540;&#20989;&#25968;&#20998;&#27495;&#65292;&#24182;&#19981;reset&#32593;&#32476;&#21442;&#25968;&#65292;&#32780;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#12290;Nikishin &#31561;&#20154;&#30340;&#19968;&#39033;&#36817;&#26399;&#30740;&#31350;&#65288;2022&#24180;&#65289;&#25351;&#20986;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#26356;&#26032;&#25968;&#25454;&#27604;&#19979;&#65292;&#20986;&#29616;&#20102;&#20248;&#20808;&#20559;&#35265;&#65292;&#21363;&#20195;&#29702;&#23545;&#26089;&#26399;&#30340;&#20132;&#20114;&#36807;&#24230;&#25311;&#21512;&#65292;&#32780;&#24573;&#35270;&#21518;&#26469;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#23548;&#33268;&#20248;&#20808;&#20559;&#35265;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#35757;&#32451;&#30340;&#21069;&#26399;&#38454;&#27573;&#65292;&#36825;&#20123;&#38454;&#27573;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#26080;&#27861;&#23398;&#20064;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#30340;&#25361;&#25112;&#26159;&#38271;&#26399;&#23384;&#22312;&#30340;&#20540;&#20989;&#25968;&#20998;&#27495;&#12290;&#19981;&#20165;&#20165;&#26159;&#22312;&#26410;&#30693;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#65292;&#29978;&#33267;&#22312;&#24050;&#30693;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#36807;&#24230;&#33192;&#32960;&#30340;Q&#20540;&#65292;&#24182;&#19988;&#21457;&#29616;&#36825;&#31181;&#29616;&#35937;&#19982;&#22312;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#39044;&#27979;&#19978;&#30340;&#36807;&#20272;&#35745;&#26377;&#20851;&#65292;&#36825;&#31181;&#36807;&#20272;&#35745;&#26159;&#30001;&#20248;&#21270;&#22120;&#30340;&#21147;&#25512;&#21160;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v3 Announce Type: replace-cross  Abstract: We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer moment
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#36866;&#24212;&#20197;&#25552;&#39640;&#23427;&#20204;&#36981;&#24490;&#25512;&#33616;&#31995;&#32479;&#21644;&#20154;&#31867;&#24847;&#22270;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05063</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21487;&#25511;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Controllable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05063
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#36866;&#24212;&#20197;&#25552;&#39640;&#23427;&#20204;&#36981;&#24490;&#25512;&#33616;&#31995;&#32479;&#21644;&#20154;&#31867;&#24847;&#22270;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05063v2 Announce Type: replace-cross &#25688;&#35201;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#19968;&#33324;&#26234;&#33021;&#30340;&#21551;&#21457;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#25506;&#32034;&#20854;&#22312;&#24320;&#21019;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#24212;&#29992;&#8212;&#8212;&#36825;&#20123;&#31995;&#32479;&#26159;&#20250;&#35805;&#24615;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#26159;&#21487;&#25511;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;LLMs&#20013;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#34987;&#22686;&#21152;&#20102;&#30001;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#24471;&#20986;&#30340;&#26631;&#31614;&#65292;&#26088;&#22312;&#26174;&#30528;&#25552;&#39640;LLMs&#22312;&#36981;&#24490;&#25512;&#33616;&#29305;&#23450;&#25351;&#20196;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#40784;&#36807;&#31243;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#22312;&#21709;&#24212;&#29992;&#25143;&#24847;&#22270;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#20026;&#24320;&#21457;&#26356;&#26234;&#33021;&#12289;&#26356;&#21487;&#25511;&#30340;&#25512;&#33616;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05063v2 Announce Type: replace-cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#21069;&#26223;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#24341;&#23548;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#20102;&#21512;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
&#12298;PrimeComposer: &#20351;&#29992;&#27880;&#24847;&#21147;&#24341;&#23548;&#21152;&#36895;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#12299;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#21069;&#26223;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#24341;&#23548;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#20102;&#21512;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#24403;&#21069;&#30340;&#35757;&#32451;&#20813;&#36153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#25552;&#21462;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#25351;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26159;&#28304;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#19968;&#33268;&#24615;&#30340;&#22256;&#24785;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#23427;&#20204;&#36807;&#20998;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#26102;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#21363;&#20351;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#24517;&#35201;&#30340;&#20219;&#21153;&#12290;&#36825;&#19981;&#20165;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#23454;&#26045;&#65292;&#32780;&#19988;&#20063;&#24433;&#21709;&#20102;&#21069;&#26223;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#33402;&#26415;&#39068;&#26009;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#35270;&#20026;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#30340;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27425;&#32534;&#36753;&#20013;&#65292;&#25152;&#32534;&#36753;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#32500;&#25345;&#22330;&#26223;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#20313;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24555;&#30340;&#21069;&#26223;&#20026;&#20027;&#25552;&#21319;&#27880;&#24847;&#21147;&#24341;&#23548;&#21152;&#36895;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#25193;&#25955;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v2 Announce Type: replace  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. Current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only impedes their swift implementation but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20026;&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12035</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#65306;&#22522;&#20934;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning for Time Series: Benchmark and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20026;&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; - &#20132;&#21449;  &#25688;&#35201;&#65306;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#26412;&#36136;&#19978;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#32463;&#24120;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#12290;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#38754;&#23588;&#20854;&#24120;&#35265;&#65292;&#23601;&#20687;&#21307;&#30103;&#20445;&#20581;&#20013;&#20986;&#29616;&#26032;&#30340;&#30142;&#30149;&#20998;&#31867;&#25110;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#28155;&#21152;&#26032;&#30340;&#27963;&#21160;&#19968;&#26679;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#26377;&#25928;&#22320;&#21560;&#25910;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#23548;&#33268;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39046;&#22495;&#30340;CIL&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#19981;&#21457;&#36798;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#23454;&#39564;&#35774;&#35745;&#19978;&#19981;&#19968;&#33268;&#65292;&#38656;&#35201;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26102;&#38388;&#24207;&#21015;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;TSCIL&#65289;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#23427;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#29616;&#26377;&#30340;CIL&#31639;&#27861;&#12290;&#25105;&#20204;&#20005;&#26684;&#23450;&#20041;&#20102;CIL&#30340;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#31867;&#19981;&#21305;&#37197;&#24615;&#30340;&#22788;&#29702;&#12289;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#30340;&#35780;&#20272;&#12289;&#20197;&#21450;&#23545;&#19981;&#21516;&#31867;&#21035;&#22686;&#37327;&#30340;&#36866;&#24212;&#12290;&#22312;&#22235;&#27425;ETS&#21644;&#19977;&#20010;&#20154;&#21160;&#20316;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20026;CIL&#26041;&#27861;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27604;&#36739;&#65292;&#26174;&#31034;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v2 Announce Type: replace-cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and co
&lt;/p&gt;</description></item><item><title>&#20107;&#25925;&#34920;&#26126;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#23454;&#38469;&#20132;&#36890;&#20013;&#23384;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#32039;&#24613;&#24773;&#20917;&#30340;&#35782;&#21035;&#21644;&#21709;&#24212;&#23384;&#22312;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
&#12298;&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#20107;&#25925;&#21078;&#26512;&#65306;&#36890;&#29992;&#27773;&#36710;Cruise&#20844;&#21496;&#19982;&#19968;&#21517;&#34892;&#20154;&#30896;&#25758;&#20107;&#20214;&#30340;&#25945;&#35757;&#12299;
&lt;/p&gt;
&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
&#20107;&#25925;&#34920;&#26126;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#23454;&#38469;&#20132;&#36890;&#20013;&#23384;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#32039;&#24613;&#24773;&#20917;&#30340;&#35782;&#21035;&#21644;&#21709;&#24212;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.06046v3 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;&#36234; &#25688;&#35201;: 2023&#24180;10&#26376;&#65292;&#22312;&#32654;&#22269;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#26087;&#37329;&#23665;&#30340;&#36890;&#21220;&#39640;&#23792;&#26102;&#27573;&#65292;&#19968;&#36742;&#36890;&#29992;&#27773;&#36710;Cruise&#21697;&#29260;&#30340;&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#22312;&#19968;&#22788;&#23398;&#26657;&#38468;&#36817;&#19982;&#19968;&#21517;&#34892;&#20154;&#30456;&#25758;&#12290;&#36825;&#36215;&#20107;&#25925;&#21457;&#29983;&#21518;&#65292;&#36890;&#29992;&#27773;&#36710;Cruise&#20844;&#21496;&#22788;&#29702;&#30340;&#32039;&#24613;&#21709;&#24212;&#36807;&#31243;&#21463;&#21040;&#20102;&#20005;&#21385;&#30340;&#25209;&#35780;&#65292;&#21407;&#22240;&#26159;&#35813;&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#22312;&#25758;&#20987;&#34892;&#20154;&#21518;&#26410;&#33021;&#21450;&#26102;&#20572;&#36710;&#65292;&#23548;&#33268;&#34892;&#20154;&#34987;&#25302;&#34892;&#20102;&#19968;&#27573;&#36317;&#31163;&#12290;&#36825;&#19981;&#20165;&#20165;&#26159;&#19968;&#36215;&#20260;&#20129;&#20107;&#20214;&#65292;&#23427;&#36824;&#26174;&#31034;&#20986;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#36947;&#36335;&#19978;&#36827;&#34892;&#37096;&#32626;&#26102;&#30340;&#28508;&#22312;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#36215;&#20107;&#25925;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#31361;&#20986;&#20102;&#24403;&#21069;&#20132;&#36890;&#23433;&#20840;&#27861;&#35268;&#19982;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25216;&#26415;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#19968;&#30740;&#31350;&#26088;&#22312;&#20174;&#30417;&#31649;&#35282;&#24230;&#20998;&#26512;&#20107;&#25925;&#22788;&#29702;&#36807;&#31243;&#24182;&#20174;&#20013;&#27762;&#21462;&#25945;&#35757;&#65292;&#30740;&#31350;&#19981;&#20165;&#35201;&#20851;&#27880;&#20107;&#25925;&#26412;&#36523;&#65292;&#36824;&#35201;&#20851;&#27880;&#20107;&#25925;&#21457;&#29983;&#21518;&#30340;&#20107;&#25925;&#21709;&#24212;&#26041;&#24335;&#65292;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22914;&#20309;&#26377;&#25928;&#35782;&#21035;&#21644;&#24212;&#23545;&#20132;&#36890;&#20107;&#25925;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#28857;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22914;&#20309;&#20934;&#30830;&#22320;&#26500;&#24314;&#20107;&#25925;&#21457;&#29983;&#21518;&#30340;&#29615;&#22659;&#27169;&#22411;&#65292;&#20197;&#21450;&#20844;&#21496;&#22312;&#38754;&#23545;&#27492;&#31867;&#32039;&#24613;&#24773;&#20917;&#26102;&#24212;&#35813;&#37319;&#21462;&#21738;&#20123;&#30456;&#24212;&#30340;&#23433;&#20840;&#25514;&#26045;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#25253;&#21578;&#26448;&#26009;&#30340;&#20449;&#24687;&#36827;&#34892;&#25972;&#21512;&#21644;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#36824;&#26088;&#22312;&#25506;&#31350;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#21487;&#33021;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#28508;&#22312;&#25913;&#36827;&#25514;&#26045;&#65292;&#20197;&#30830;&#20445;&#26410;&#26469;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#23454;&#38469;&#36947;&#36335;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.06046v3 Announce Type: replace-cross  Abstract: An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. Is-sues stem not just from the loss events themselves, but also from how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. External investigation reports provide raw material describing the incident and critique the company's response from a regulatory point of view, but exclude safety engineering recommendations from scope. We highlight specific facts and relationships among events by tying together different pieces of the external report material. We then explore safety lessons that might be learned related to: recognizing and responding to nearby mishaps, building an accurate world model of a post-collision scenario, the in-adequa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;3D&#24418;&#29366;&#35782;&#21035;&#20013;&#35270;&#22270;&#32423;&#21035;&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;GMViT&#30340;&#39640;&#24615;&#33021;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#21517;&#20026;DeCoV&#30340;&#31574;&#30053;&#19979;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16477</link><description>&lt;p&gt;
&#32452;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31354;&#38388;&#32534;&#30721;&#30340;3D&#24418;&#29366;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;3D&#24418;&#29366;&#35782;&#21035;&#20013;&#35270;&#22270;&#32423;&#21035;&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;GMViT&#30340;&#39640;&#24615;&#33021;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#21517;&#20026;DeCoV&#30340;&#31574;&#30053;&#19979;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16477v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#35270;&#22270;&#30340;3D&#24418;&#29366;&#35782;&#21035;&#26041;&#27861;&#30340;&#32467;&#26524;&#24050;&#32463;&#39281;&#21644;&#65292;&#24182;&#19988;&#30001;&#20110;&#21442;&#25968;&#23610;&#23544;&#24040;&#22823;&#65292;&#24615;&#33021;&#20248;&#24322;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#39046;&#22495;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#23613;&#21487;&#33021;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#25552;&#39640;&#23567;&#22411;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#31216;&#20026;&#32452;&#22810;&#35270;&#22270;&#35270;&#35273;Transformer&#65288;GMViT&#65289;&#12290;&#22312;GMViT&#20013;&#65292;&#35270;&#22270;&#32423;&#21035;&#30340;ViT&#39318;&#20808;&#24314;&#31435;&#20102;&#35270;&#22270;&#32423;&#21035;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25429;&#33719;&#26356;&#28145;&#23618;&#27425;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23545;&#35270;&#22270;&#32423;&#21035;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#32452;&#27169;&#22359;&#30340;&#22788;&#29702;&#65292;&#20351;&#20854;&#25552;&#21319;&#21040;&#20102;&#32452;&#32423;&#21035;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#32452;&#32423;&#21035;ViT&#23558;&#32452;&#32423;&#21035;&#29305;&#24449;&#25972;&#21512;&#25104;&#23436;&#25972;&#30340;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;3D&#24418;&#29366;&#25551;&#36848;&#31526;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#20004;&#20010;ViT&#20013;&#65292;&#25105;&#20204;&#37117;&#24341;&#20837;&#20102;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#21387;&#32553;&#31574;&#30053;&#21483;&#20570;DeCoV,&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#27604;&#24403;&#21069;&#26368;&#20339;&#21387;&#32553;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#22914;&#20309;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23454;&#29616;&#36825;&#19968;&#24895;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;DeCoV&#22312;&#21478;&#19968;&#26041;&#38754;&#30340;&#24433;&#21709;&#21487;&#33021;&#32473;&#25105;&#20204;&#24102;&#26469;&#30340;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#23384;&#27169;&#22411;&#22312;&#38754;&#23545;&#25968;&#25454;&#38598;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16019</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Robust Survival Analysis with Adversarial Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#23384;&#27169;&#22411;&#22312;&#38754;&#23545;&#25968;&#25454;&#38598;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16019v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#65306;&#29983;&#23384;&#20998;&#26512;&#65288;SA&#65289;&#27169;&#22411;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#21307;&#23398;&#12289;&#22269;&#38450;&#12289;&#37329;&#34701;&#21644;&#33322;&#31354;&#33322;&#22825;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21487;&#20197;&#25429;&#25417;SA&#20013;&#22797;&#26434;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#65292;&#22122;&#22768;&#27979;&#37327;&#12289;&#20154;&#31867;&#38169;&#35823;&#65289;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#40065;&#26834;&#30340;&#12289;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#29983;&#23384;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;CROWN-IBP&#27491;&#21017;&#21270;&#26469;&#22788;&#29702;&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#29983;&#23384;&#20998;&#26512;&#19982;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#65288;SAWAR&#65289;&#26041;&#27861;&#22312;SurvSet&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21508;&#31181;&#27745;&#26579;&#19979;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#65288;NegLL&#65289;&#12289;&#31215;&#20998;&#24067;&#38647;&#23572;&#20998;&#25968;&#65288;IBS&#65289;&#21644;&#19968;&#33268;&#24615;&#25351;&#25968;&#65288;CI&#65289;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#23427;&#26174;&#33879;&#20248;&#20110; baselines&#12290;&#36825;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;SA&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16019v3 Announce Type: replace-cross  Abstract: Survival Analysis (SA) models the time until an event occurs, with applications in fields like medicine, defense, finance, and aerospace. Recent work shows that Neural Networks (NNs) can capture complex relationships in SA. However, dataset uncertainties (e.g., noisy measurements, human error) can degrade model performance. To address this, we leverage NN verification advances to create algorithms for robust, fully-parametric survival models. We introduce a robust loss function and use CROWN-IBP regularization to handle computational challenges in the Min-Max problem. Evaluating our approach on SurvSet datasets, we find that our Survival Analysis with Adversarial Regularization (SAWAR) method consistently outperforms baselines under various perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI). This demonstrates that adversarial regularization enhances SA perform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26032;&#22411; GPT-4 APIs &#30340;&#26032;&#21151;&#33021;&#36827;&#34892;&#20102;&#21033;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#24494;&#35843;&#27169;&#22411;&#20063;&#21487;&#20197;&#31227;&#38500;&#20854;&#26680;&#24515;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#20989;&#25968;&#35843;&#29992;&#21644;&#30693;&#35782;&#26816;&#32034;&#30340;&#21163;&#25345;&#12290;</title><link>https://arxiv.org/abs/2312.14302</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#22411; GPT-4 API
&lt;/p&gt;
&lt;p&gt;
Exploiting Novel GPT-4 APIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26032;&#22411; GPT-4 APIs &#30340;&#26032;&#21151;&#33021;&#36827;&#34892;&#20102;&#21033;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#24494;&#35843;&#27169;&#22411;&#20063;&#21487;&#20197;&#31227;&#38500;&#20854;&#26680;&#24515;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#20989;&#25968;&#35843;&#29992;&#21644;&#30693;&#35782;&#26816;&#32034;&#30340;&#21163;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#25991;&#23398;&#26415;&#65306;2312.14302v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#35821;&#35328;&#27169;&#22411;&#25915;&#20987;&#36890;&#24120;&#20551;&#35774;&#20004;&#31181;&#26497;&#31471;&#30340;&#23041;&#32961;&#27169;&#22411;&#65306;&#23436;&#20840;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#25110;&#32773;&#26159;&#20165;&#38480;&#20110;&#25991;&#26412;&#29983;&#25104;API&#30340;&#23553;&#38381;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;API&#24448;&#24448;&#27604;&#20165;&#20165;&#25991;&#26412;&#29983;&#25104;&#35201;&#28789;&#27963;&#24471;&#22810;&#65306;&#36825;&#20123;API&#26292;&#38706;&#30340;"&#28784;&#30418;"&#35775;&#38382;&#23548;&#33268;&#20102;&#26032;&#30340;&#23041;&#32961;&#36884;&#24452;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#32418;&#38431;&#23545;GPT-4 APIs&#26292;&#38706;&#30340;&#26032;&#21151;&#33021;&#36827;&#34892;&#20102;&#19977;&#21521;&#30740;&#31350;&#65306;&#24494;&#35843;&#12289;&#20989;&#25968;&#35843;&#29992;&#21644;&#30693;&#35782;&#26816;&#32034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;&#19968;&#20010;&#27169;&#22411;&#20063;&#21482;&#26377;15&#20010;&#26377;&#23475;&#31034;&#20363;&#25110;100&#20010;&#33391;&#24615;&#31034;&#20363;&#23601;&#21487;&#20197;&#31227;&#38500;GPT-4&#30340;&#26680;&#24515;&#20445;&#25252;&#25514;&#26045;&#65292;&#20351;&#24471;&#21487;&#20197;&#20135;&#29983;&#19968;&#31995;&#21015;&#26377;&#23475;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;GPT-4&#21161;&#29702;&#24895;&#24847;&#36879;&#38706;&#20989;&#25968;&#35843;&#29992;&#27169;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#23548;&#21521;&#25191;&#34892;&#20219;&#24847;&#20989;&#25968;&#35843;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#30693;&#35782;&#26816;&#32034;&#21487;&#20197;&#34987;&#36890;&#36807;&#27880;&#20837;&#25351;&#20196;&#21040;&#26816;&#32034;&#25991;&#26723;&#20013;&#36827;&#34892;&#21163;&#25345;&#12290;&#36825;&#20123;&#28431;&#27934;&#34920;&#26126;&#65292;&#20219;&#20309;&#23545;API&#21151;&#33021;&#30340;&#26032;&#22686;&#37117;&#21487;&#33021;&#23548;&#33268;&#23433;&#20840;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36825;&#20123;&#26032;API&#25552;&#20379;&#30340;&#39069;&#22806;&#33021;&#21147;&#21644;&#26435;&#38480;&#65292;&#24517;&#39035;&#34987;&#20180;&#32454;&#23457;&#35270;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14302v2 Announce Type: replace-cross  Abstract: Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose "gray-box" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed b
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#21644;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#26415;&#38382;&#39064;&#35299;&#31572;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#21644;&#26102;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.13028</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#39064;&#35299;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable Academic Conference Question Answering: A Study Based on Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#21644;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#26415;&#38382;&#39064;&#35299;&#31572;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#21644;&#26102;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13028v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;&#65306;&#38543;&#30528;&#23398;&#26415;&#20250;&#35758;&#30340;&#21457;&#23637;&#65292;&#23398;&#32773;&#20204;&#19981;&#26029;&#38656;&#35201;&#33719;&#21462;&#20851;&#20110;&#23398;&#26415;&#20250;&#35758;&#30340;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#20449;&#24687;&#20998;&#25955;&#65292;&#20351;&#29992;&#19968;&#20010;&#26234;&#33021;&#30340;&#38382;&#39064;&#35299;&#31572;&#31995;&#32479;&#26469;&#39640;&#25928;&#22320;&#22788;&#29702;&#30740;&#31350;&#32773;&#30340;&#26597;&#35810;&#65292;&#24182;&#30830;&#20445;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#24847;&#35782;&#26159;&#24517;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#35299;&#31572;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#22788;&#29702;&#36807;&#26102;&#30693;&#35782;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#30340;&#20250;&#35758;&#20449;&#24687;&#32780;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#25324;&#19971;&#20010;&#22810;&#26679;&#21270;&#30340;&#23398;&#26415;&#20250;&#35758;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#27599;&#20010;&#20250;&#35758;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21322;&#33258;&#21160;&#26041;&#27861;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#20026;&#26641;&#29366;&#26684;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#20250;&#35758;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13028v2 Announce Type: replace-cross  Abstract: As the development of academic conferences fosters global scholarly communication, researchers consistently need to obtain accurate and up-to-date information about academic conferences. Since the information is scattered, using an intelligent question-answering system to efficiently handle researchers' queries and ensure awareness of the latest advancements is necessary. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in question answering, and have been enhanced by retrieving external knowledge to deal with outdated knowledge. However, these methods fail to work due to the lack of the latest conference knowledge. To address this challenge, we develop the ConferenceQA dataset, consisting of seven diverse academic conferences. Specifically, for each conference, we first organize academic conference data in a tree-structured format through a semi-automated method. Then we annotate question-answer
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#31995;&#32479;&#24615;&#32508;&#36848;&#20102;&#22312;&#20851;&#38190;&#23433;&#20840;&#34892;&#19994;&#20013;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#30740;&#31350;&#65292;&#25351;&#20986;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#23616;&#38480;&#24615;&#19982;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2310.03392</link><description>&lt;p&gt;
&#21078;&#26512;&#20154;&#31867;&#19982;AI&#22312;&#20851;&#38190;&#23433;&#20840;&#34892;&#19994;&#20013;&#30340;&#20132;&#20114;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#31995;&#32479;&#24615;&#32508;&#36848;&#20102;&#22312;&#20851;&#38190;&#23433;&#20840;&#34892;&#19994;&#20013;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#30740;&#31350;&#65292;&#25351;&#20986;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#23616;&#38480;&#24615;&#19982;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03392v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#20851;&#38190;&#23433;&#20840;&#34892;&#19994;&#20013;&#30830;&#20445;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65288;HAII&#65289;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#22914;&#26524;&#26410;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#12289;&#29978;&#33267;&#26159;&#33268;&#21629;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#36825;&#19968;&#32039;&#36843;&#24615;&#65292;&#20294;&#26377;&#20851;HAII&#30340;&#29616;&#26377;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12289;&#30862;&#29255;&#21270;&#21644;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#39033;&#23545;&#35813;&#39046;&#22495;&#25991;&#29486;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#35813;&#25913;&#21892;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26368;&#20339;&#23454;&#36341;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#35843;&#26597;&#20998;&#20026;&#20197;&#19979;&#20960;&#20010;&#39046;&#22495;&#65306;1) &#29992;&#20110;&#25551;&#36848;HAII&#30340;&#26415;&#35821;&#65292;2) AI&#36171;&#33021;&#31995;&#32479;&#30340;&#26680;&#24515;&#35282;&#33394;&#65292;3) &#24433;&#21709;HAII&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;4) &#22914;&#20309;&#34913;&#37327;HAII&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#22312;&#35752;&#35770;&#30340;&#36825;&#20123;&#25991;&#31456;&#20013;&#29992;&#20110;&#20851;&#38190;&#23433;&#20840;&#34892;&#19994;&#30340;AI&#36171;&#33021;&#31995;&#32479;&#30340;&#21151;&#33021;&#21644;&#25104;&#29087;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25991;&#29486;&#20013;&#27809;&#26377;&#21333;&#19968;&#30340;&#26415;&#35821;&#34987;&#29992;&#26469;&#25551;&#36848;HAII&#65292;&#32780;&#19988;&#26377;&#20123;&#26415;&#35821;&#26377;&#22810;&#31181;&#21547;&#20041;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25991;&#29486;&#35843;&#30740;&#65292;&#19971;&#31181;&#22240;&#32032;&#24433;&#21709;&#20102;HAII&#65306;&#29992;&#25143;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#20010;&#24615;&#65289;&#65292;&#29992;&#25143;&#24863;&#30693;&#65292;&#20197;&#21450;&#29992;&#25143;-AI&#20851;&#31995;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#30740;&#31350;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;HAII&#25552;&#20379;&#30340;&#35270;&#35282;&#26159;&#22810;&#32500;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#35780;&#20272;AI&#36171;&#33021;&#31995;&#32479;&#21644;HAII&#38656;&#27714;&#30340;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#23637;&#26395;&#65292;&#26412;&#25991;&#30340;&#30740;&#31350;&#23545;&#20110;&#25552;&#39640;HAII&#22312;&#20851;&#38190;&#23433;&#20840;&#34892;&#19994;&#20013;&#30340;&#23454;&#36341;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03392v2 Announce Type: replace-cross  Abstract: Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#21046;&#36896;&#19994;&#22797;&#26434;&#22330;&#26223;&#26102;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#20026;&#21046;&#36896;&#19994;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25552;&#20379;&#20102;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2310.02812</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65306;&#23545;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#39564;&#35780;&#20272;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#21046;&#36896;&#19994;&#22797;&#26434;&#22330;&#26223;&#26102;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#20026;&#21046;&#36896;&#19994;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25552;&#20379;&#20102;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#34892;&#19994;&#27491;&#22312;&#25910;&#38598;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#36825;&#35201;&#24863;&#35874;&#20256;&#24863;&#22120;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#22810;&#21644;&#26816;&#27979;&#25216;&#26415;&#30340;&#24555;&#36895;&#36827;&#27493;&#12290;&#22312;&#36825;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#65288;SMS&#65289;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#22312;&#36825;&#19968;&#34892;&#19994;&#39046;&#22495;&#20013;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#25552;&#20379;&#23545;&#21046;&#36896;&#21644;&#24037;&#19994;&#39046;&#22495;&#20013;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#32034;&#24182;&#27719;&#24635;&#20102;&#26469;&#33258;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#21046;&#36896;&#25991;&#29486;&#30340;&#36229;&#36807;92&#31181;&#20808;&#36827;&#31639;&#27861;&#30340;&#21015;&#34920;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#21015;&#34920;&#20013;&#36873;&#25321;&#20102;&#26368;&#33021;&#20195;&#34920;&#31639;&#27861;&#30340;36&#31181;&#31639;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#22312;&#21046;&#36896;&#19994;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31934;&#24515;&#25361;&#36873;&#20102;22&#20010;&#21046;&#36896;&#19994;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#21046;&#36896;&#19994;&#38382;&#39064;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23545;&#36873;&#23450;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#26045;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22788;&#29702;&#21644;&#20248;&#21270;&#12290;&#31639;&#27861;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;ROC&#26354;&#32447;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#21516;&#26102;&#20063;&#35782;&#21035;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#21046;&#36896;&#19994;&#22330;&#26223;&#26102;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#31639;&#27861;&#30340;&#25191;&#34892;&#25928;&#29575;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#22312;&#21046;&#36896;&#19994;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#32780;&#19988;&#22312;&#21046;&#36896;&#19994;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#24615;&#33021;&#20986;&#33394;&#30340;&#31639;&#27861;&#65292;&#20294;&#36824;&#27809;&#26377;&#22312;&#20219;&#20309;&#20844;&#35748;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#32477;&#23545;&#20248;&#21183;&#30340;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#20250;&#21457;&#29616;&#26356;&#22810;&#30340;&#31639;&#27861;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#21644;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#21046;&#36896;&#19994;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02812v2 Announce Type: replace-cross  Abstract: Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#24418;&#24335;&#30340;&#23545;&#31216;&#30830;&#23450;&#24615;&#38669;&#24681;&#20989;&#25968;&#65292;&#23384;&#22312;&#19968;&#20010;&#38750;&#20887;&#20313;CNF&#20844;&#24335;&#65292;&#23427;&#30456;&#36739;&#20110;&#26368;&#23567;&#30340;&#22823;&#32422;$n^2$&#20493;&#65292;&#36825;&#34920;&#26126;&#20102;&#22312;&#31227;&#38500;&#21464;&#37327;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#38750;&#20887;&#20313;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01750</link><description>&lt;p&gt;
&#20851;&#20110;&#30456;&#23545;&#20110;&#21333;&#20803;&#23376;&#20844;&#24335;&#20256;&#25773;&#30340;&#38750;&#20887;&#20313;CNF&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
On CNF formulas irredundant with respect to unit clause propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#24418;&#24335;&#30340;&#23545;&#31216;&#30830;&#23450;&#24615;&#38669;&#24681;&#20989;&#25968;&#65292;&#23384;&#22312;&#19968;&#20010;&#38750;&#20887;&#20313;CNF&#20844;&#24335;&#65292;&#23427;&#30456;&#36739;&#20110;&#26368;&#23567;&#30340;&#22823;&#32422;$n^2$&#20493;&#65292;&#36825;&#34920;&#26126;&#20102;&#22312;&#31227;&#38500;&#21464;&#37327;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#38750;&#20887;&#20313;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01750v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20004;&#20010;CNF&#20844;&#24335;&#31216;&#20026;ucp&#31561;&#20215;&#65292;&#22914;&#26524;&#23427;&#20204;&#22312;&#21333;&#20803;&#23376;&#20844;&#24335;&#20256;&#25773;&#65288;UCP&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#29305;&#24615;&#12290;&#22914;&#26524;&#20174;&#21407;&#22987;&#20844;&#24335;&#20013;&#31227;&#38500;&#20219;&#20309;&#23376;&#20844;&#24335;&#23601;&#20250;&#23548;&#33268;&#19968;&#20010;&#19981;&#20877;&#19982;&#21407;&#22987;&#20844;&#24335;ucp&#31561;&#20215;&#30340;&#20844;&#24335;&#65292;&#21017;&#35813;&#20844;&#24335;&#31216;&#20026;ucp&#20887;&#20313;&#12290;&#24050;&#30693;&#32467;&#26524;&#30340;&#21518;&#26524;&#26159;&#65292;&#38750;&#20887;&#20313;&#20844;&#24335;&#30340;&#22823;&#23567;&#19982;&#26368;&#23567;ucp&#31561;&#20215;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#19981;&#36229;&#36807;$n^2$&#65292;&#20854;&#20013;$n$&#26159;&#21464;&#37327;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;&#26368;&#23567;ucp&#31561;&#20215;&#20844;&#24335;&#22823;&#23567;&#30456;&#27604;&#22823;&#24471;&#22810;&#12289;&#22240;&#32032;&#20026;$\Omega(n/\ln n)$&#30340;&#38750;&#20887;&#20313;&#20844;&#24335;&#65292;&#29992;&#20110;&#23545;&#31216;&#30830;&#23450;&#38669;&#24681;&#20989;&#25968;&#65292;&#22240;&#27492;&#65292;&#19978;&#36848;&#27604;&#29575;&#30340;&#19978;&#38480;&#19981;&#33021;&#23567;&#20110;&#36825;&#20010;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01750v4 Announce Type: replace-cross  Abstract: Two CNF formulas are called ucp-equivalent, if they behave in the same way with respect to the unit clause propagation (UCP). A formula is called ucp-irredundant, if removing any clause leads to a formula which is not ucp-equivalent to the original one. As a consequence of known results, the ratio of the size of a ucp-irredundant formula and the size of a smallest ucp-equivalent formula is at most $n^2$, where $n$ is the number of the variables. We demonstrate an example of a ucp-irredundant formula for a symmetric definite Horn function which is larger than a smallest ucp-equivalent formula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on the above ratio cannot be smaller than this.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#22823;&#22411;AI&#27169;&#22411;-&#24378;&#21270;&#22411;&#22810;&#27169;&#24577;&#35821;&#20041;&#36890;&#20449;&#65288;LAM-MSC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#36716;&#25442;&#65292;&#20197;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#20041;&#25552;&#21462;&#25110;&#24674;&#22797;&#12290;</title><link>https://arxiv.org/abs/2309.01249</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;AI&#27169;&#22411; empowered multimodal semantic communications
&lt;/p&gt;
&lt;p&gt;
Large AI Model Empowered Multimodal Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#22823;&#22411;AI&#27169;&#22411;-&#24378;&#21270;&#22411;&#22810;&#27169;&#24577;&#35821;&#20041;&#36890;&#20449;&#65288;LAM-MSC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#36716;&#25442;&#65292;&#20197;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#20041;&#25552;&#21462;&#25110;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01249v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#31995;&#32479;&#20013;&#38598;&#25104;&#22810;&#27169;&#24577;&#20449;&#21495;&#65292;&#22914;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#21487;&#20197;&#25552;&#20379;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#36136;&#37327;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#65292;&#36825;&#22312;&#35821;&#20041;&#23618;&#27425;&#19978;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;SC&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#35821;&#20041;&#27169;&#31946;&#24615;&#21644;&#20449;&#21495;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#30340;&#22833;&#30495;&#12290;&#22312;&#22823;&#22411;AI&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;SC&#65288;LAM-MSC&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;MMA&#65289;&#65292;&#23427;&#20351;&#29992;MLM&#26469;&#20801;&#35768;&#22312;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;LLM-based Knowledge Base&#65288;LKB&#65289;&#65292;&#20801;&#35768;&#29992;&#25143;&#25191;&#34892;&#20010;&#24615;&#21270;&#30340;&#35821;&#20041;&#25552;&#21462;&#25110;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01249v2 Announce Type: replace  Abstract: Multimodal signals, including text, audio, image, and video, can be integrated into Semantic Communication (SC) systems to provide an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal distortion during transmission. Recent advancements in large AI models, particularly in the Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for addressing these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery
&lt;/p&gt;</description></item><item><title>LAMBO&#26694;&#26550;&#37319;&#29992;&#22823;&#22411;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#36755;&#20837;&#23884;&#20837;&#21644;AED&#27169;&#22411;&#65292;&#32467;&#21512;ACE&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#26234;&#33021;&#20013;&#24322;&#26500;&#32422;&#26463;&#12289;&#24863;&#30693;&#19981;&#20840;&#31561;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2308.15078</link><description>&lt;p&gt;
LAMBO&#65306;&#29992;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22686;&#24378;&#30340;&#36793;&#32536;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
LAMBO: Large AI Model Empowered Edge Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.15078
&lt;/p&gt;
&lt;p&gt;
LAMBO&#26694;&#26550;&#37319;&#29992;&#22823;&#22411;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#36755;&#20837;&#23884;&#20837;&#21644;AED&#27169;&#22411;&#65292;&#32467;&#21512;ACE&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#26234;&#33021;&#20013;&#24322;&#26500;&#32422;&#26463;&#12289;&#24863;&#30693;&#19981;&#20840;&#31561;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.15078v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#19979;&#19968;&#20195;&#36793;&#32536;&#26234;&#33021;&#39044;&#35745;&#23558;&#36890;&#36807;&#21368;&#36733;&#25216;&#26415;&#20026;&#21508;&#31181;&#24212;&#29992;&#24102;&#26469;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22806;&#21152;&#36733;&#26550;&#26500;&#38754;&#20020; several &#20960;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#24322;&#26500;&#32422;&#26463;&#12289;&#37096;&#20998;&#24863;&#30693;&#12289;&#19981;&#30830;&#23450;&#24615;&#27867;&#21270;&#21644;&#19981;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#22806;&#21152;&#36733;&#65288;LAMBO&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#25317;&#26377;&#36229;&#36807;&#19968;&#20159;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36755;&#20837;&#23884;&#20837;&#65288;IE&#65289;&#26469;&#36890;&#36807;&#24322;&#26500;&#32422;&#26463;&#21644;&#20219;&#21153;&#25552;&#31034;&#23454;&#29616;&#26631;&#20934;&#21270;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#19981;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;AED&#65289;&#20316;&#20026;&#20915;&#31574;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#28145;&#24230;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#27973;&#24230;&#35299;&#30721;&#22120;&#29992;&#20110;&#20840;&#23616;&#24863;&#30693;&#21644;&#20915;&#31574;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#28436;&#21592;-&#25209;&#35780;&#32773;&#23398;&#20064;&#65288;ACL&#65289;&#23545;AED&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22312;&#30456;&#24212;&#30340;&#25552;&#31034;&#19979;&#20026;&#19981;&#21516;&#30340;&#20248;&#21270;&#20219;&#21153;&#65292;&#22686;&#24378;AED&#30340;&#22810;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;AED&#30340;&#21452;&#21521;&#20132;&#20114;&#24433;&#21709;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#21319;&#20915;&#31574;&#36136;&#37327;&#30340;&#40065;&#26834;&#24615;&#21644;&#20248;&#21270;&#20219;&#21153;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LAMBO&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.15078v2 Announce Type: replace  Abstract: Next-generation edge intelligence is anticipated to benefit various applications via offloading techniques. However, traditional offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this paper, we propose a Large AI Model-Based Offloading (LAMBO) framework with over one billion parameters for solving these problems. We first use input embedding (IE) to achieve normalized feature representation with heterogeneous constraints and task prompts. Then, we introduce a novel asymmetric encoder-decoder (AED) as the decision-making model, which is an improved transformer architecture consisting of a deep encoder and a shallow decoder for global perception and decision. Next, actor-critic learning (ACL) is used to pre-train the AED for different optimization tasks under corresponding prompts, enhancing the AED's generalization in multi-task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20351;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#22768;&#20107;&#20214;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2308.11530</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#36827;&#34892;&#22768;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Model Capabilities for Sound Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20351;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#22768;&#20107;&#20214;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.11530v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;&#38142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.11530v2 Announce Type: replace-cross  Abstract: Large language models reveal deep comprehension and fluent generation in the field of multi-modality. Although significant advancements have been achieved in audio multi-modality, existing methods are rarely leverage language model for sound event detection (SED). In this work, we propose an end-to-end framework for understanding audio features while simultaneously generating sound event and their temporal location. Specifically, we employ pretrained acoustic models to capture discriminative features across different categories and language models for autoregressive text generation. Conventional methods generally struggle to obtain features in pure audio domain for classification. In contrast, our framework utilizes the language model to flexibly understand abundant semantic context aligned with the acoustic representation. The experimental results showcase the effectiveness of proposed method in enhancing timestamps precision 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SchemaWalk&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#23398;&#20064;&#20803;&#36335;&#24452;&#65292;&#35813;&#31639;&#27861;&#26080;&#38656;&#26522;&#20030;&#25152;&#26377;&#36335;&#24452;&#23454;&#20363;&#21363;&#21487;&#39640;&#25928;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2307.03937</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#32435;&#27861;&#30340;&#22797;&#26434;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SchemaWalk&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#23398;&#20064;&#20803;&#36335;&#24452;&#65292;&#35813;&#31639;&#27861;&#26080;&#38656;&#26522;&#20030;&#25152;&#26377;&#36335;&#24452;&#23454;&#20363;&#21363;&#21487;&#39640;&#25928;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2307.03937v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.03937v2 Announce Type: replace  Abstract: Heterogeneous Information Networks (HINs) are information networks with multiple types of nodes and edges. The concept of meta-path, i.e., a sequence of entity types and relation types connecting two entities, is proposed to provide the meta-level explainable semantics for various HIN tasks. Traditionally, meta-paths are primarily used for schema-simple HINs, e.g., bibliographic networks with only a few entity types, where meta-paths are often enumerated with domain knowledge. However, the adoption of meta-paths for schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and relation types, has been limited due to the computational complexity associated with meta-path enumeration. Additionally, effectively assessing meta-paths requires enumerating relevant path instances, which adds further complexity to the meta-path learning process. To address these challenges, we propose SchemaWalk, an inductive meta-path learn
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;SKB&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#35821;&#20041;&#27573;&#65292;&#28982;&#21518;&#36890;&#36807;ASI&#23545;&#36825;&#20123;&#27573;&#36827;&#34892;&#21152;&#26435;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35821;&#20041;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2307.03492</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411; semantic communications
&lt;/p&gt;
&lt;p&gt;
Large AI Model-Based Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.03492
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;SKB&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#35821;&#20041;&#27573;&#65292;&#28982;&#21518;&#36890;&#36807;ASI&#23545;&#36825;&#20123;&#27573;&#36827;&#34892;&#21152;&#26435;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2307.03492v2 &#23459;&#24067;&#31867;&#22411;: replace &#25688;&#35201;: &#35821;&#20041;&#36890;&#20449;(SC)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26234;&#33021;&#33539;&#24335;&#65292;&#20026;&#21508;&#31181;&#26410;&#26469;&#24212;&#29992;&#65292;&#22914;&#20803;&#23431;&#23449;&#12289;&#28151;&#21512;&#29616;&#23454;&#21644;&#29289;&#32852;&#32593;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#30340;SC&#31995;&#32479;&#20013;&#65292;&#30693;&#35782;&#24211;(KB)&#30340;&#24314;&#35774;&#38754;&#20020;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#30693;&#35782;&#34920;&#31034;&#26377;&#38480;&#12289;&#30693;&#35782;&#26356;&#26032;&#39057;&#32321;&#21644;&#30693;&#35782;&#20849;&#20139;&#19981;&#23433;&#20840;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#24320;&#21457;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;SC&#26694;&#26550;(LAM-SC)&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;&#22522;&#20110;segment anything model (SAM)&#30340;&#30693;&#35782;&#24211;(SKB)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#23558;&#21407;&#22987;&#22270;&#20687;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#35821;&#20041;&#27573;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#25972;&#21512;&#26041;&#27861;(ASI)&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#21363;&#21487;&#23545;&#30001;SKB&#29983;&#25104;&#30340;&#35821;&#20041;&#27573;&#36827;&#34892;&#21152;&#26435;&#25972;&#21512;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#20026;&#35821;&#20041;&#24863;&#30693;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;LAM-SC&#26694;&#26550;&#20013;&#21033;&#29992;LAM&#26469;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#22312;&#22810;&#22330;&#26223;&#24773;&#22659;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#35821;&#20041;&#36890;&#20449;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;LAM-SC&#26694;&#26550;&#22312;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#38477;&#20302;&#38169;&#35823;&#29575;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36825;&#20026;&#26410;&#26469;&#30340;SC&#31995;&#32479;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#21644;&#23454;&#36341;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.03492v2 Announce Type: replace  Abstract: Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed reality, and the Internet of Everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model (LAM) provides new solutions to overcome the above issues. Here, we propose a LAM-based SC framework (LAM-SC) specifically designed for image data, where we first apply the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic aware image. Additionall
&lt;/p&gt;</description></item><item><title>&#22312;&#12298;&#20154;&#24037;&#26234;&#36896; &#183; &#24433;&#20687;&#21360;&#35760;&#12299;&#19968;&#31456;&#20013;&#65292;&#20316;&#32773;&#25551;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#37324;&#31243;&#30865;&#24335;&#21457;&#23637;&#20197;&#21450;&#22312;&#20854;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#30340;&#8220;&#20652;&#29983;&#24037;&#31243;&#8221;&#12290;&#25991;&#31456;&#25552;&#37266;&#25105;&#20204;&#38656;&#35201;&#23545;&#36825;&#31181;&#20849;&#21019;&#29983;&#24577;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#24433;&#21709;&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#12290;</title><link>https://arxiv.org/abs/2306.11393</link><description>&lt;p&gt;
&#12298;&#20154;&#24037;&#26234;&#36896; &#183; &#24433;&#20687;&#21360;&#35760;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Cultivated Practices of Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11393
&lt;/p&gt;
&lt;p&gt;
&#22312;&#12298;&#20154;&#24037;&#26234;&#36896; &#183; &#24433;&#20687;&#21360;&#35760;&#12299;&#19968;&#31456;&#20013;&#65292;&#20316;&#32773;&#25551;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#37324;&#31243;&#30865;&#24335;&#21457;&#23637;&#20197;&#21450;&#22312;&#20854;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#30340;&#8220;&#20652;&#29983;&#24037;&#31243;&#8221;&#12290;&#25991;&#31456;&#25552;&#37266;&#25105;&#20204;&#38656;&#35201;&#23545;&#36825;&#31181;&#20849;&#21019;&#29983;&#24577;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#24433;&#21709;&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27491;&#22312;&#36827;&#20837;&#19968;&#20010;&#20840;&#26032;&#30340;&#21019;&#36896;&#26102;&#20195;&#65292;&#20219;&#20309;&#20154;&#22312;&#32447;&#37117;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#21019;&#24314;&#25968;&#23383;&#20449;&#24687;&#12290;&#20854;&#20013;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#25216;&#26415;&#23588;&#20026;&#27969;&#34892;&#65292;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#23454;&#36341;&#32773;&#22312;&#32447;&#21019;&#20316;AI&#22270;&#20687;&#21644;&#33402;&#26415;&#20316;&#21697;&#12290;&#26412;&#25991;&#39318;&#20808;&#27010;&#36848;&#20102;&#21019;&#36896;&#19968;&#20010;&#20581;&#24247;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22312;&#32447;&#20849;&#21019;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#21457;&#23637;&#65292;&#28982;&#21518;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#25991;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20652;&#29983;&#24037;&#31243;--&#19968;&#31181;&#34987;AI&#33402;&#26415;&#31038;&#21306;&#24191;&#27867;&#25509;&#21463;&#30340;&#33402;&#26415;&#23454;&#36341;&#12290;&#38543;&#21518;&#65292;&#25991;&#31456;&#24314;&#35758;&#36825;&#20010;&#36880;&#28176;&#25104;&#22411;&#30340;&#20849;&#21019;&#29983;&#24577;&#31995;&#32479;&#26159;&#19968;&#20010;&#29420;&#31435;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#23427;&#25903;&#25345;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#38480;&#21046;&#26410;&#26469;AI&#30340;&#21457;&#23637;&#21644;&#24433;&#21709;&#26410;&#26469;&#20195;&#38469;&#30340;&#21457;&#23637;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#36825;&#20010;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.11393v2 Announce Type: replace-cross  Abstract: Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI). Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online. This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem. A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community. It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI. The chapter discusses the potential risks and dangers o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#20445;&#20581;&#30693;&#35782;&#22270;&#35889;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26500;&#24314;&#26356;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#20449;&#30340;&#29983;&#25104;&#20869;&#23481;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#19982;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#20445;&#20581;&#30693;&#35782;&#22270;&#35889;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26500;&#24314;&#26356;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#20449;&#30340;&#29983;&#25104;&#20869;&#23481;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2306.04802v4 | &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.04802v4 Announce Type: replace  Abstract: Healthcare knowledge graphs (HKGs) are valuable tools for organizing biomedical concepts and their relationships with interpretable structures. The recent advent of large language models (LLMs) has paved the way for building more comprehensive and accurate HKGs. This, in turn, can improve the reliability of generated content and enable better evaluation of LLMs. However, the challenges of HKGs such as regarding data heterogeneity and limited coverage are not fully understood, highlighting the need for detailed reviews. This work provides the first comprehensive review of HKGs. It summarizes the pipeline and key techniques for HKG construction, as well as the common utilization approaches, i.e., model-free and model-based. The existing HKG resources are also organized based on the data types they capture and application domains they cover, along with relevant statistical information (Resource available at https://github.com/lujiaying/
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22806;&#37096;&#26102;&#38388;&#36807;&#31243;&#24433;&#21709;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2305.16056</link><description>&lt;p&gt;
&#21463;&#22806;&#37096;&#26102;&#38388;&#36807;&#31243;&#24433;&#21709;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Markov Decision Processes under External Temporal Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22806;&#37096;&#26102;&#38388;&#36807;&#31243;&#24433;&#21709;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16056v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37117;&#23558;&#20854;&#36816;&#20316;&#30340;&#29615;&#22659;&#35270;&#20026;&#19968;&#20010;&#31283;&#23450;&#30340;&#12289;&#38548;&#31163;&#30340;&#21644;&#19981;&#21463;&#24178;&#25200;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29615;&#22659;&#30001;&#20110;&#21508;&#31181;&#22806;&#37096;&#20107;&#20214;&#19981;&#26029;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21463;&#21040;&#22806;&#37096;&#26102;&#38388;&#36807;&#31243;&#24433;&#21709;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;formalize&#20102;&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#21512;&#36866;&#30340;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23545;&#23427;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#29615;&#22659;&#38750;&#31283;&#23450;&#24615;&#31243;&#24230;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#22312;&#32463;&#20856;&#30340;&#25511;&#21046;&#29615;&#22659;&#20013;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16056v2 Announce Type: replace-cross  Abstract: Most reinforcement learning algorithms treat the context under which they operate as a stationary, isolated, and undisturbed environment. However, in real world applications, environments constantly change due to a variety of external events. To address this problem, we study Markov Decision Processes (MDP) under the influence of an external temporal process. We formalize this notion and discuss conditions under which the problem becomes tractable with suitable solutions. We propose a policy iteration algorithm to solve this problem and theoretically analyze its performance. We derive results on the sample complexity of the algorithm and study its dependency on the extent of non-stationarity of the environment. We then conduct experiments to illustrate our results in a classic control environment.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#23454;&#26045;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>https://arxiv.org/abs/2305.13088</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#21442;&#21152;&#20250;&#35758;&#26356;&#22810;&#36824;&#26159;&#26356;&#23569;&#65311;&#20844;&#24179;&#24615;&#19979;&#30340;&#27880;&#24847;&#21147;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;
Should We Attend More or Less? Modulating Attention for Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#23454;&#26045;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13088v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#32593;&#32476; &#25688;&#35201;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#27493;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#23613;&#31649;&#36817;&#26399;&#30340;&#21457;&#23637;&#20351;&#24471;&#22312;&#39640;&#24615;&#33021;&#27169;&#22411;&#19978;&#36827;&#34892;&#22810;&#39033;&#20219;&#21153;&#30340;&#21457;&#23637;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#23427;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#23475;&#20559;&#35265;&#30340;&#39118;&#38505;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21069;&#20808;&#36827;NLP&#27169;&#22411;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#31038;&#20250;&#20559;&#35265;&#20256;&#25773;&#20013;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#29109;&#19982;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35843;&#21046;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#22312;&#35757;&#32451;&#21518;&#25913;&#21892;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#22312;&#35757;&#32451;&#21518;&#21644;&#25512;&#26029;&#21069;&#24212;&#29992;&#65292;&#23427;&#26159;&#19968;&#31181;&#20013;&#38388;&#22788;&#29702;&#26041;&#27861;&#65292;&#22240;&#27492;&#20854;&#35745;&#31639;&#25104;&#26412;&#27604;&#29616;&#26377;&#30340;&#22788;&#29702;&#26041;&#27861;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#35201;&#20302;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24179;&#24615;&#26377;&#25152;&#25552;&#39640;&#65292;&#19988;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13088v2 Announce Type: replace-cross  Abstract: The advances in natural language processing (NLP) pose both opportunities and challenges. While recent progress enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal per
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#26410;&#26366;&#30452;&#25509;&#35757;&#32451;&#36807;&#65292;AI&#27169;&#22411;&#20063;&#22312;&#38405;&#35835;&#20195;&#30721;&#30340;&#36807;&#31243;&#20013;&#23398;&#20064;&#21040;&#20102;&#31243;&#24207;&#35821;&#20041;&#34920;&#31034;</title><link>https://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#31243;&#24207;&#35821;&#35328;&#27169;&#22411;&#20013;&#28044;&#29616;&#30340;&#31243;&#24207;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Emergent Representations of Program Semantics in Language Models Trained on Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#26410;&#26366;&#30452;&#25509;&#35757;&#32451;&#36807;&#65292;AI&#27169;&#22411;&#20063;&#22312;&#38405;&#35835;&#20195;&#30721;&#30340;&#36807;&#31243;&#20013;&#23398;&#20064;&#21040;&#20102;&#31243;&#24207;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#26126;&#65292;&#34429;&#28982;&#21482;&#26159;&#22312;&#35757;&#32451;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#23383;&#31526;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20063;&#33021;&#23398;&#20250;&#20195;&#34920;&#31243;&#24207;&#30340;&#27491;&#24335;&#35821;&#20041;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22312;&#20108;&#32500;&#32593;&#26684;&#19990;&#30028;&#20013;&#23548;&#33322;&#30340;&#22495;&#29305;&#23450;&#35821;&#35328;&#35757;&#32451;&#19968;&#20010;Transformer&#27169;&#22411;&#12290;&#25991;&#26723;&#24211;&#20013;&#30340;&#27599;&#20010;&#31243;&#24207;&#37117;&#20276;&#38543;&#30528;&#19968;&#20010;&#26684;&#30424;&#19990;&#30028;&#29366;&#24577;&#30340;&#65288;&#37096;&#20998;&#65289;&#35268;&#33539;&#65292;&#24418;&#24335;&#20026;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#26684;&#30424;&#19990;&#30028;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#25506;&#27979;&#20998;&#31867;&#22120;&#33021;&#22815;&#20174;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#21462;&#36234;&#26469;&#36234;&#20934;&#30830;&#30340;&#31243;&#24207;&#35821;&#20041;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#38544;&#34255;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#36825;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#32463;&#27491;&#24335;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20250;&#20102;&#31243;&#24207;&#24847;&#20041;&#19978;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#24615;&#22522;&#32447;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#21306;&#20998;&#30001;&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#30340;&#20869;&#23481;&#21644;&#36890;&#36807;&#25506;&#32034;&#22120;&#23398;&#20064;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#36827;APA&#23448;&#26041;&#23458;&#26381;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.11169v3 Announce Type: replace-cross  Abstract: We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We antic
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#32422;&#26463;&#19979;&#65292;&#24773;&#22659;&#20381;&#36182;&#36890;&#20449;&#22914;&#20309;&#20174;&#19968;&#31181;&#24773;&#22659;&#20449;&#21495;&#27169;&#22411;&#20013;&#20135;&#29983;&#65292;&#24182;&#25506;&#35752;&#20102;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#33021;&#21147;&#20811;&#26381;&#29615;&#22659;&#38480;&#21046;&#65292;&#23454;&#29616;&#26377;&#25928;&#27807;&#36890;&#12290;</title><link>https://arxiv.org/abs/2305.05821</link><description>&lt;p&gt;
&#29615;&#22659;&#32422;&#26463;&#19979;&#30340;&#24773;&#22659;&#20381;&#36182;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Context-dependent communication under environmental constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.05821
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#32422;&#26463;&#19979;&#65292;&#24773;&#22659;&#20381;&#36182;&#36890;&#20449;&#22914;&#20309;&#20174;&#19968;&#31181;&#24773;&#22659;&#20449;&#21495;&#27169;&#22411;&#20013;&#20135;&#29983;&#65292;&#24182;&#25506;&#35752;&#20102;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#33021;&#21147;&#20811;&#26381;&#29615;&#22659;&#38480;&#21046;&#65292;&#23454;&#29616;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2305.05821v2 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#26377;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36890;&#20449;&#19981;&#33021;&#31616;&#21270;&#20026;&#21457;&#36865;&#20855;&#26377;&#26080;&#24773;&#22659;&#24847;&#20041;&#30340;&#20449;&#21495;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#22522;&#20110;&#32463;&#20856; Lewis&#65288;1969&#65289;&#20449;&#21495;&#27169;&#22411;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#22659;&#21270;&#22330;&#26223;&#20013;&#20135;&#29983;&#24773;&#22659;&#20381;&#36182;&#36890;&#20449;&#30340;&#26465;&#20214;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20943;&#23569;&#35789;&#27719;&#37327;&#30340;&#22823;&#23567;&#21387;&#21147;&#36275;&#20197;&#20419;&#36827;&#36825;&#31181;&#20986;&#29616;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#30340;&#25351;&#31216;&#36873;&#25321;&#29615;&#22659;&#26465;&#20214;&#19979;&#65292;&#33021;&#22815;&#33258;&#34892;&#21033;&#29992;&#21457;&#36865;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#29615;&#22659;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#29615;&#22659;&#32422;&#26463;&#25509;&#25910;&#32773;&#30340;&#25351;&#31216;&#36873;&#25321;&#21487;&#20197;&#30001;&#21457;&#36865;&#32773;&#21333;&#26041;&#38754;&#22320;&#21033;&#29992;&#65292;&#32780;&#26080;&#38656;&#25509;&#25910;&#32773;&#30340;&#35821;&#22659;&#28040;&#27495;&#33021;&#21147;&#12290;&#19982;&#26222;&#36941;&#30340;&#20551;&#35774;&#30456;&#19968;&#33268;&#65292;&#21457;&#36865;&#32773;&#23545;&#24773;&#22659;&#30340;&#24847;&#35782;&#20284;&#20046;&#26159;&#24773;&#22659;&#36890;&#20449;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#24314;&#35758;&#65292;&#24773;&#22659;&#20381;&#36182;&#30340;&#36890;&#20449;&#26159;&#19968;&#31181;&#24773;&#22659;&#30340;&#22810;&#23618;&#22788;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.05821v2 Announce Type: replace  Abstract: There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phe
&lt;/p&gt;</description></item><item><title>&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;MUG&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#32593;&#32476;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#25913;&#36827;&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#19982;&#32593;&#32476;&#22270;&#20687;&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;MUG&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#32593;&#32476;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#25913;&#36827;&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07088v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306; &#35768;&#22810;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#32452;&#31934;&#24515;&#32534;&#25490;&#30340;&#22270;&#20687;Net-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#30001;&#32593;&#32476;&#26469;&#28304;&#30340;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#31867;&#20284;&#37197;&#32622;&#30340;&#35774;&#32622;&#20013;&#65292;&#23545;&#20195;&#34920;&#24615;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#22823;&#22411;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#25513;&#30721;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#23545;&#27604;&#24615;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#35770;&#35266;&#28857;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#35813;&#35266;&#28857;&#20026;&#35774;&#35745;&#19968;&#31181;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#21463;&#27492;&#27934;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;MUlti-modal Generator&#65288;MUG&#65289;&#65292;&#23427;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#29615;&#22659;&#20013;&#22522;&#20110;&#20215;&#20540;&#30340;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#21644;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#21738;&#19968;&#31181;&#26041;&#27861;&#22312;&#25913;&#36827;&#20195;&#29702;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2206.08442</link><description>&lt;p&gt;
&#19981;&#21516;&#29615;&#22659;&#20013;&#20915;&#31574;&#26102;&#21051;&#19982;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#30340;&#22522;&#20110;&#20215;&#20540;&#20915;&#31574;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Look at Value-Based Decision-Time vs. Background Planning Methods Across Different Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.08442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#29615;&#22659;&#20013;&#22522;&#20110;&#20215;&#20540;&#30340;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#21644;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#21738;&#19968;&#31181;&#26041;&#27861;&#22312;&#25913;&#36827;&#20195;&#29702;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2206.08442v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25913;&#36827;&#20854;&#34892;&#20026;&#12290;&#20004;&#32773;&#30340;&#24120;&#35265;&#26041;&#24335;&#26159;&#36890;&#36807;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#21644;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#20215;&#20540;&#30340;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#21644;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#20026;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#22522;&#20110;&#20215;&#20540;&#30340;&#20915;&#31574;&#26102;&#21051;&#21644;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#30340;&#31616;&#21270;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#29702;&#35770;&#32467;&#26524;&#65292;&#35828;&#26126;&#22312;&#24120;&#35268;&#30340;RL&#21644;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#21738;&#19968;&#31181;&#26041;&#27861;&#20250;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#23427;&#20204;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#30340;&#29616;&#20195;&#23454;&#20363;&#65292;&#24182;&#25552;&#20986;&#20851;&#20110;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#21738;&#19968;&#31181;&#26041;&#27861;&#20250;&#34920;&#29616;&#26356;&#22909;&#30340;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892; illustrative &#23454;&#39564;&#26469;&#39564;&#35777;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#21644;&#20551;&#35774;&#12290;&#24635;&#20307;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#22522;&#20110;&#20215;&#20540;&#30340;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#20250;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#27604;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#21487;&#33021;&#20250;&#26681;&#25454;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#20219;&#21153;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#19978;&#65292;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#21487;&#33021;&#20250;&#20248;&#20110;&#32972;&#26223;&#35268;&#21010;&#65292;&#32780;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#19978;&#65292;&#32972;&#26223;&#35268;&#21010;&#21487;&#33021;&#20250;&#26356;&#26377;&#20248;&#21183;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#32771;&#34385;&#22522;&#20110;&#20215;&#20540;&#30340;&#20915;&#31574;&#26102;&#21051;&#35268;&#21010;&#21644;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#26102;&#65292;&#38656;&#35201;&#36827;&#34892;&#20855;&#20307;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.08442v2 Announce Type: replace-cross  Abstract: In model-based reinforcement learning (RL), an agent can leverage a learned model to improve its way of behaving in different ways. Two of the prevalent ways to do this are through decision-time and background planning methods. In this study, we are interested in understanding how the value-based versions of these two planning methods will compare against each other across different settings. Towards this goal, we first consider the simplest instantiations of value-based decision-time and background planning methods and provide theoretical results on which one will perform better in the regular RL and transfer learning settings. Then, we consider the modern instantiations of them and provide hypotheses on which one will perform better in the same settings. Finally, we perform illustrative experiments to validate these theoretical results and hypotheses. Overall, our findings suggest that even though value-based versions of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#35821;&#20041;&#33976;&#39311;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;KGE&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39044;&#35757;&#32451;&#30340;&#39640;&#32500;&#27169;&#22411;&#65292;&#19988;&#25805;&#20316;&#31616;&#21333;&#12290;</title><link>https://arxiv.org/abs/2206.02963</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#35821;&#20041;&#33976;&#39311;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#32622;&#20449;&#24230;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.02963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#35821;&#20041;&#33976;&#39311;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;KGE&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39044;&#35757;&#32451;&#30340;&#39640;&#32500;&#27169;&#22411;&#65292;&#19988;&#25805;&#20316;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02963v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#65292;&#21363;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#36830;&#32493;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#12290;&#34429;&#28982;&#39640;&#32500;KGE&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20197;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#20026;&#20195;&#20215;&#12290;&#38477;&#20302;&#23884;&#20837;&#32500;&#24230;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#19968;&#20123;&#21162;&#21147;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25110;&#38750;&#27431;&#20960;&#37324;&#24471;&#34920;&#31034;&#23398;&#20064;&#26469;&#25552;&#39640;&#20302;&#32500;KGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#39640;&#32500;&#25945;&#24072;&#27169;&#22411;&#65292;&#35201;&#20040;&#28041;&#21450;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25805;&#20316;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30456;&#24403;&#22810;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#24863;&#30693;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#65288;CSD&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#20174;&#33258;&#36523;&#23398;&#20064;&#20197;&#22686;&#24378;KGE&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CSD&#20174;&#20197;&#21069;&#36845;&#20195;&#30340;&#23884;&#20837;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#24615;&#33021;&#35270;&#20026;&#27491;&#30830;&#30340;&#25110;&#38169;&#35823;&#30340;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20302;&#32500;KGE&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CSD&#20026;&#20302;&#32500;KGE&#30340;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02963v3 Announce Type: replace-cross  Abstract: Knowledge Graph Embedding (KGE), which projects entities and relations into continuous vector spaces, has garnered significant attention. Although high-dimensional KGE methods offer better performance, they come at the expense of significant computation and memory overheads. Decreasing embedding dimensions significantly deteriorates model performance. While several recent efforts utilize knowledge distillation or non-Euclidean representation learning to augment the effectiveness of low-dimensional KGE, they either necessitate a pre-trained high-dimensional teacher model or involve complex non-Euclidean operations, thereby incurring considerable additional computational costs. To address this, this work proposes Confidence-aware Self-Knowledge Distillation (CSD) that learns from the model itself to enhance KGE in a low-dimensional space. Specifically, CSD extracts knowledge from embeddings in previous iterations, which would be 
&lt;/p&gt;</description></item></channel></rss>