<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.AI.xml</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;LLaVA-OneVision&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21333;&#22270;&#20687;&#12289;&#22810;&#22270;&#20687;&#21644;&#35270;&#39057;&#22330;&#26223;&#20013;&#21516;&#26102;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23637;&#29616;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23558;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#25104;&#21151;&#36801;&#31227;&#21040;&#35270;&#39057;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.03326</link><description>&lt;p&gt;
LLaVA-OneVision: Easy Visual Task Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03326
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;LLaVA-OneVision&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21333;&#22270;&#20687;&#12289;&#22810;&#22270;&#20687;&#21644;&#35270;&#39057;&#22330;&#26223;&#20013;&#21516;&#26102;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23637;&#29616;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23558;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#25104;&#21151;&#36801;&#31227;&#21040;&#35270;&#39057;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03326v1 Announce Type: new  Abstract: We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#24314;&#20102;&#19968;&#20010;&#26631;&#27880;&#26377; hedge &#30340; Roadrunner &#21160;&#30011;&#23545;&#30333;&#30340;&#25991;&#26412;&#24211;&#65292;&#24182;&#20351;&#29992; BERT &#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#35782;&#21035;&#36825;&#20123;&#20027;&#35266;&#34920;&#36798;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#35828;&#35805;&#32773;&#22312;&#21477;&#23376;&#20013;&#21152;&#20837;&#30340;&#27169;&#31946;&#21270;&#34920;&#36798;&#65292;&#22914;&#24576;&#30097;&#25110;&#23485;&#23481;&#31243;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#20154;&#31867;&#35805;&#35821;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.03319</link><description>&lt;p&gt;
Training LLMs to Recognize Hedges in Spontaneous Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#24314;&#20102;&#19968;&#20010;&#26631;&#27880;&#26377; hedge &#30340; Roadrunner &#21160;&#30011;&#23545;&#30333;&#30340;&#25991;&#26412;&#24211;&#65292;&#24182;&#20351;&#29992; BERT &#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#35782;&#21035;&#36825;&#20123;&#20027;&#35266;&#34920;&#36798;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#35828;&#35805;&#32773;&#22312;&#21477;&#23376;&#20013;&#21152;&#20837;&#30340;&#27169;&#31946;&#21270;&#34920;&#36798;&#65292;&#22914;&#24576;&#30097;&#25110;&#23485;&#23481;&#31243;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#20154;&#31867;&#35805;&#35821;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03319v1 Announce Type: cross  Abstract: Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or "fuzziness", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20154;&#31867;&#25351;&#23548;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#36755;&#20837;&#30340;&#38656;&#27714;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#27880;&#30340;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.03304</link><description>&lt;p&gt;
Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20154;&#31867;&#25351;&#23548;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#36755;&#20837;&#30340;&#38656;&#27714;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#27880;&#30340;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03304v1 Announce Type: new  Abstract: Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improveme
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#30450;&#20154;&#29992;&#25143;&#19982;&#29289;&#20307;&#35782;&#21035;&#38169;&#35823;&#22788;&#29702;&#30340;&#30456;&#20851;&#31574;&#30053;&#21644;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;&#22312;&#30456;&#26426;&#36741;&#21161;&#25216;&#26415;&#21644;&#29289;&#20307;&#35782;&#21035;&#31995;&#32479;&#20013;&#35782;&#21035;&#38169;&#35823;&#26102;&#29992;&#25143;&#30340;&#30452;&#35266;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2408.03303</link><description>&lt;p&gt;
Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#30450;&#20154;&#29992;&#25143;&#19982;&#29289;&#20307;&#35782;&#21035;&#38169;&#35823;&#22788;&#29702;&#30340;&#30456;&#20851;&#31574;&#30053;&#21644;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;&#22312;&#30456;&#26426;&#36741;&#21161;&#25216;&#26415;&#21644;&#29289;&#20307;&#35782;&#21035;&#31995;&#32479;&#20013;&#35782;&#21035;&#38169;&#35823;&#26102;&#29992;&#25143;&#30340;&#30452;&#35266;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03303v1 Announce Type: cross  Abstract: Object recognition technologies hold the potential to support blind and low-vision people in navigating the world around them. However, the gap between benchmark performances and practical usability remains a significant challenge. This paper presents a study aimed at understanding blind users' interaction with object recognition systems for identifying and avoiding errors. Leveraging a pre-existing object recognition system, URCam, fine-tuned for our experiment, we conducted a user study involving 12 blind and low-vision participants. Through in-depth interviews and hands-on error identification tasks, we gained insights into users' experiences, challenges, and strategies for identifying errors in camera-based assistive technologies and object recognition systems. During interviews, many participants preferred independent error review, while expressing apprehension toward misrecognitions. In the error identification task, participants
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#30693;&#35782;&#20559;&#22909;&#20248;&#21270;&#8221;&#65288;KaPO&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#21487;&#25511;&#30340;&#30693;&#35782;&#36873;&#25321;&#26469;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#65288;RAG&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#19982;&#20869;&#37096;&#30693;&#35782;&#65292;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#24773;&#22659;&#21512;&#29702;&#36873;&#21462;&#30693;&#35782;&#65292;&#36991;&#20813;&#20102;&#30693;&#35782;&#38388;&#30340;&#20914;&#31361;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03297</link><description>&lt;p&gt;
KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#30693;&#35782;&#20559;&#22909;&#20248;&#21270;&#8221;&#65288;KaPO&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#21487;&#25511;&#30340;&#30693;&#35782;&#36873;&#25321;&#26469;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#65288;RAG&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#19982;&#20869;&#37096;&#30693;&#35782;&#65292;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#24773;&#22659;&#21512;&#29702;&#36873;&#21462;&#30693;&#35782;&#65292;&#36991;&#20813;&#20102;&#30693;&#35782;&#38388;&#30340;&#20914;&#31361;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03297v1 Announce Type: cross  Abstract: By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios. To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in re
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;U-Net&#21644;&#22522;&#20110;&#20851;&#27880;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#38745;&#24577;IR&#19979;&#38477;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#21644;&#23569;&#37327;&#30495;&#23454;&#35774;&#35745;&#28857;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#22270;&#20687;&#21270;&#38745;&#24577;IR&#19979;&#38477;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2408.03292</link><description>&lt;p&gt;
Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;U-Net&#21644;&#22522;&#20110;&#20851;&#27880;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#38745;&#24577;IR&#19979;&#38477;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#21644;&#23569;&#37327;&#30495;&#23454;&#35774;&#35745;&#28857;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#22270;&#20687;&#21270;&#38745;&#24577;IR&#19979;&#38477;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03292v1 Announce Type: cross  Abstract: There has been significant recent progress to reduce the computational effort of static IR drop analysis using neural networks, and modeling as an image-to-image translation task. A crucial issue is the lack of sufficient data from real industry designs to train these networks. Additionally, there is no methodology to explain a high-drop pixel in a predicted IR drop image to its specific root-causes. In this work, we first propose a U-Net neural network model with attention gates which is specifically tailored to achieve fast and accurate image-based static IR drop prediction. Attention gates allow selective emphasis on relevant parts of the input data without supervision which is desired because of the often sparse nature of the IR drop map. We propose a two-phase training process which utilizes a mix of artificially-generated data and a limited number of points from real designs. The results are, on-average, 18% (53%) better in MAE a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;StructEval&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#35748;&#30693;&#27700;&#24179;&#21644;&#20851;&#38190;&#27010;&#24565;&#19978;&#36827;&#34892;&#32467;&#26500;&#21270;&#35780;&#20272;&#65292;&#28145;&#21270;&#21644;&#25299;&#23485;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#24037;&#20855;&#26469;&#25269;&#25239;&#25968;&#25454;&#27745;&#26579;&#30340;&#39118;&#38505;&#21644;&#20943;&#23569;&#28508;&#22312;&#20559;&#35265;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#26356;&#21487;&#38752;&#21644;&#19968;&#33268;&#22320;&#24471;&#20986;&#26377;&#20851;&#27169;&#22411;&#33021;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.03281</link><description>&lt;p&gt;
StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;StructEval&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#35748;&#30693;&#27700;&#24179;&#21644;&#20851;&#38190;&#27010;&#24565;&#19978;&#36827;&#34892;&#32467;&#26500;&#21270;&#35780;&#20272;&#65292;&#28145;&#21270;&#21644;&#25299;&#23485;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#24037;&#20855;&#26469;&#25269;&#25239;&#25968;&#25454;&#27745;&#26579;&#30340;&#39118;&#38505;&#21644;&#20943;&#23569;&#28508;&#22312;&#20559;&#35265;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#26356;&#21487;&#38752;&#21644;&#19968;&#33268;&#22320;&#24471;&#20986;&#26377;&#20851;&#27169;&#22411;&#33021;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03281v1 Announce Type: cross  Abstract: Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#31995;&#32479;Compress and Compare&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#30340;&#25928;&#29575;&#35780;&#20272;&#21644;&#34892;&#20026;&#27604;&#36739;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#30028;&#38754;&#26469;&#25506;&#32034;&#21387;&#32553;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#27604;&#36739;&#26469;&#35266;&#23519;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#12289;&#26435;&#37325;&#21644;&#28608;&#27963;&#34892;&#20026;&#24046;&#24322;&#12290;Compress and Compare&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#21387;&#32553;&#27169;&#22411;&#30340;&#22797;&#26434;&#23545;&#27604;&#20998;&#26512;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#20248;&#21270;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.03274</link><description>&lt;p&gt;
Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#31995;&#32479;Compress and Compare&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#30340;&#25928;&#29575;&#35780;&#20272;&#21644;&#34892;&#20026;&#27604;&#36739;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#30028;&#38754;&#26469;&#25506;&#32034;&#21387;&#32553;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#27604;&#36739;&#26469;&#35266;&#23519;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#12289;&#26435;&#37325;&#21644;&#28608;&#27963;&#34892;&#20026;&#24046;&#24322;&#12290;Compress and Compare&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#21387;&#32553;&#27169;&#22411;&#30340;&#22797;&#26434;&#23545;&#27604;&#20998;&#26512;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#20248;&#21270;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03274v1 Announce Type: cross  Abstract: To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called Compress and Compare. Within a single interface, Compress and Compare surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how Compress and Compare supp
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#20020;&#25512;&#29702;&#20219;&#21153;&#26102;&#26159;&#21542;&#31215;&#26497;&#35843;&#29992;&#20854;&#20869;&#37096;&#30693;&#35782;&#20179;&#24211;&#30340;&#31192;&#23494;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#22312;&#25512;&#29702;&#27599;&#20010;&#27493;&#39588;&#20013;&#30693;&#35782;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#27169;&#24335;&#65292;&#21457;&#29616;&#27169;&#22411;&#26410;&#33021;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#21033;&#29992;&#20851;&#38190;&#30340;&#30693;&#35782;&#20851;&#32852;&#65292;&#32780;&#26159;&#20542;&#21521;&#20110;&#37319;&#21462;&#25463;&#24452;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#30693;&#35782;&#22312;&#27169;&#22411;&#20013;&#30340;&#22238;&#24518;&#36807;&#31243;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#30693;&#35782;&#22238;&#24518;&#33021;&#21147;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#32780;&#25233;&#21046;&#36825;&#31181;&#22238;&#24518;&#21017;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;CoT&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#22238;&#24518;&#65292;&#20419;&#20351;&#27169;&#22411;&#20197;&#26377;&#24207;&#30340;&#26041;&#24335;&#21442;&#19982;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2408.03247</link><description>&lt;p&gt;
Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#20020;&#25512;&#29702;&#20219;&#21153;&#26102;&#26159;&#21542;&#31215;&#26497;&#35843;&#29992;&#20854;&#20869;&#37096;&#30693;&#35782;&#20179;&#24211;&#30340;&#31192;&#23494;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#22312;&#25512;&#29702;&#27599;&#20010;&#27493;&#39588;&#20013;&#30693;&#35782;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#27169;&#24335;&#65292;&#21457;&#29616;&#27169;&#22411;&#26410;&#33021;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#21033;&#29992;&#20851;&#38190;&#30340;&#30693;&#35782;&#20851;&#32852;&#65292;&#32780;&#26159;&#20542;&#21521;&#20110;&#37319;&#21462;&#25463;&#24452;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#30693;&#35782;&#22312;&#27169;&#22411;&#20013;&#30340;&#22238;&#24518;&#36807;&#31243;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#30693;&#35782;&#22238;&#24518;&#33021;&#21147;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#32780;&#25233;&#21046;&#36825;&#31181;&#22238;&#24518;&#21017;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;CoT&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#22238;&#24518;&#65292;&#20419;&#20351;&#27169;&#22411;&#20197;&#26377;&#24207;&#30340;&#26041;&#24335;&#21442;&#19982;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03247v1 Announce Type: cross  Abstract: In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and rel
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFedSIS&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20808;&#39564;&#65292;&#32467;&#21512;&#20840;&#23616;&#20010;&#24615;&#21270; disentanglement&#12289;&#22806;&#35266;&#35843;&#33410;&#20010;&#24615;&#21270;&#22686;&#24378;&#21644;&#24418;&#29366;&#30456;&#20284;&#24615;&#20840;&#23616;&#22686;&#24378;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#22806;&#35266;&#22810;&#26679;&#24615;&#19982;&#22120;&#26800;&#24418;&#29366;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22836;&#26435;&#37325;&#20010;&#24615;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#35757;&#32451;&#31449;&#28857;&#29305;&#24449;&#30340;&#31934;&#30830;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#29420;&#31435;&#31449;&#28857;&#19978;&#30340;&#22120;&#26800;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03208</link><description>&lt;p&gt;
Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFedSIS&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20808;&#39564;&#65292;&#32467;&#21512;&#20840;&#23616;&#20010;&#24615;&#21270; disentanglement&#12289;&#22806;&#35266;&#35843;&#33410;&#20010;&#24615;&#21270;&#22686;&#24378;&#21644;&#24418;&#29366;&#30456;&#20284;&#24615;&#20840;&#23616;&#22686;&#24378;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#22806;&#35266;&#22810;&#26679;&#24615;&#19982;&#22120;&#26800;&#24418;&#29366;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22836;&#26435;&#37325;&#20010;&#24615;&#21270;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#35757;&#32451;&#31449;&#28857;&#29305;&#24449;&#30340;&#31934;&#30830;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#29420;&#31435;&#31449;&#28857;&#19978;&#30340;&#22120;&#26800;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03208v1 Announce Type: cross  Abstract: Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;istic&#20154;&#31867;&#39550;&#39542;&#20808;&#39564;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#23545;&#25239;&#24615;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#29616;&#23454;&#21644;&#25361;&#25112;&#24615;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#31995;&#32479;&#35780;&#20272;&#20013;&#33719;&#21462;&#22823;&#35268;&#27169;&#27979;&#35797;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20132;&#36890;&#20132;&#20114;&#29615;&#22659;&#20197;&#21450;&#23454;&#26045;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65292;&#27169;&#25311;&#39550;&#39542;&#31574;&#30053;&#65292;&#36827;&#32780;&#29983;&#25104;&#26082;&#26377;&#30495;&#23454;&#24863;&#21448;&#20855;&#22791;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#21270;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2408.03200</link><description>&lt;p&gt;
Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;istic&#20154;&#31867;&#39550;&#39542;&#20808;&#39564;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#23545;&#25239;&#24615;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#29616;&#23454;&#21644;&#25361;&#25112;&#24615;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#31995;&#32479;&#35780;&#20272;&#20013;&#33719;&#21462;&#22823;&#35268;&#27169;&#27979;&#35797;&#22330;&#26223;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20132;&#36890;&#20132;&#20114;&#29615;&#22659;&#20197;&#21450;&#23454;&#26045;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65292;&#27169;&#25311;&#39550;&#39542;&#31574;&#30053;&#65292;&#36827;&#32780;&#29983;&#25104;&#26082;&#26377;&#30495;&#23454;&#24863;&#21448;&#20855;&#22791;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#21270;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03200v1 Announce Type: new  Abstract: Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques. By doing this, we can obtain large-scale test scenarios that are both diverse and realistic. Specifically, we build a simulation environment that mimics natural traffic interaction scenarios. Informed by this environment, we implement a two-stage procedure. The first stage incorporates conventional rule-based models, e.g., IDM~(Intelligent Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and ca
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#32435;&#31859;&#26080;&#20154;&#26426;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22495;&#36801;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24863;&#30693;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03168</link><description>&lt;p&gt;
Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#32435;&#31859;&#26080;&#20154;&#26426;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22495;&#36801;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03168v1 Announce Type: new  Abstract: Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training label
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#38548;&#30340;&#31354;&#38388;&#33192;&#32960;&#21367;&#31215;&#65288;DCLS&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26631;&#20934;&#21644;&#33192;&#32960;&#21367;&#31215;&#65292;&#36824;&#22312;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#21487;&#35299;&#37322;&#24615;&#36890;&#36807;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#23545;&#24212;&#20851;&#31995;&#26469;&#34913;&#37327;&#65292;&#21363;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27169;&#22411;&#30340;GradCAM&#28909;&#22270;&#19982;&#21453;&#26144;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#24230;&#30340;ClickMe&#25968;&#25454;&#38598;&#28909;&#22270;&#20043;&#38388;&#30340;Spearman&#30456;&#20851;&#31995;&#25968;&#26469;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20998;&#65292;&#34920;&#26126;DCLS&#22686;&#21152;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.03164</link><description>&lt;p&gt;
Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#38548;&#30340;&#31354;&#38388;&#33192;&#32960;&#21367;&#31215;&#65288;DCLS&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26631;&#20934;&#21644;&#33192;&#32960;&#21367;&#31215;&#65292;&#36824;&#22312;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#21487;&#35299;&#37322;&#24615;&#36890;&#36807;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#23545;&#24212;&#20851;&#31995;&#26469;&#34913;&#37327;&#65292;&#21363;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27169;&#22411;&#30340;GradCAM&#28909;&#22270;&#19982;&#21453;&#26144;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#24230;&#30340;ClickMe&#25968;&#25454;&#38598;&#28909;&#22270;&#20043;&#38388;&#30340;Spearman&#30456;&#20851;&#31995;&#25968;&#26469;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20998;&#65292;&#34920;&#26126;DCLS&#22686;&#21152;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#35270;&#35273;&#31574;&#30053;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03164v1 Announce Type: new  Abstract: Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random he
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#29616;&#20195;&#22810;&#27169;&#24577;&#25512;&#29702;&#27169;&#22411;&#22312;&#36741;&#21161;&#35270;&#35273;&#36741;&#21161;&#35774;&#22791;&#23436;&#25104;&#22810;&#27493;&#39588;&#26085;&#24120;&#27963;&#21160;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;LLM&#26041;&#27861;&#8212;&#8212;Socratic&#27169;&#22411;&#21644;&#35270;&#35273;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#65288;VCLM&#65289;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#23558;&#35270;&#35273;&#21382;&#21490;&#32534;&#30721;&#21270;&#21644;&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20165;&#20165;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#36741;&#21161;&#35774;&#22791;&#30340;&#21069;&#20004;&#20010;&#33021;&#21147;&#65292;&#24182;&#19981;&#33021;&#35780;&#20272;&#22312;&#29992;&#25143;&#21442;&#19982;&#19979;&#37325;&#26032;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#25143;&#21442;&#19982;&#24335;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;VCLM&#25552;&#20379;&#20102;&#23454;&#26102;&#29992;&#25143;&#20132;&#20114;&#30340;&#34917;&#20805;&#35780;&#20272;&#26041;&#24335;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;VCLM&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21644;&#21160;&#24577;&#34892;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.03160</link><description>&lt;p&gt;
User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#29616;&#20195;&#22810;&#27169;&#24577;&#25512;&#29702;&#27169;&#22411;&#22312;&#36741;&#21161;&#35270;&#35273;&#36741;&#21161;&#35774;&#22791;&#23436;&#25104;&#22810;&#27493;&#39588;&#26085;&#24120;&#27963;&#21160;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;LLM&#26041;&#27861;&#8212;&#8212;Socratic&#27169;&#22411;&#21644;&#35270;&#35273;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#65288;VCLM&#65289;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#23558;&#35270;&#35273;&#21382;&#21490;&#32534;&#30721;&#21270;&#21644;&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22312;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20165;&#20165;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#36741;&#21161;&#35774;&#22791;&#30340;&#21069;&#20004;&#20010;&#33021;&#21147;&#65292;&#24182;&#19981;&#33021;&#35780;&#20272;&#22312;&#29992;&#25143;&#21442;&#19982;&#19979;&#37325;&#26032;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#25143;&#21442;&#19982;&#24335;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;VCLM&#25552;&#20379;&#20102;&#23454;&#26102;&#29992;&#25143;&#20132;&#20114;&#30340;&#34917;&#20805;&#35780;&#20272;&#26041;&#24335;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;VCLM&#22312;&#20013;&#38271;&#26399;&#39044;&#27979;&#21644;&#21160;&#24577;&#34892;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03160v1 Announce Type: new  Abstract: Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches -- Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#30142;&#30149;&#39044;&#27979;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#32479;&#35745;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#36873;&#25321;&#29305;&#24449;&#30340;SEV-EB&#31639;&#27861;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#31181;&#30142;&#30149;&#19981;&#21516;&#29305;&#24449;&#30340;&#24378;&#25928;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.03151</link><description>&lt;p&gt;
Optimizing Disease Prediction with Artificial Intelligence Driven Feature Selection and Attention Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03151
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#30142;&#30149;&#39044;&#27979;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#32479;&#35745;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#36873;&#25321;&#29305;&#24449;&#30340;SEV-EB&#31639;&#27861;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#31181;&#30142;&#30149;&#19981;&#21516;&#29305;&#24449;&#30340;&#24378;&#25928;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03151v1 Announce Type: cross  Abstract: The rapid integration of machine learning methodologies in healthcare has ignited innovative strategies for disease prediction, particularly with the vast repositories of Electronic Health Records (EHR) data. This article delves into the realm of multi-disease prediction, presenting a comprehensive study that introduces a pioneering ensemble feature selection model. This model, designed to optimize learning systems, combines statistical, deep, and optimally selected features through the innovative Stabilized Energy Valley Optimization with Enhanced Bounds (SEV-EB) algorithm. The objective is to achieve unparalleled accuracy and stability in predicting various disorders. This work proposes an advanced ensemble model that synergistically integrates statistical, deep, and optimally selected features. This combination aims to enhance the predictive power of the model by capturing diverse aspects of the health data. At the heart of the prop
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;COMMENTATOR&#30340;&#20195;&#30721;&#28151;&#21512;&#22411;&#22810;&#35821;&#35328;&#25991;&#26412;&#26631;&#27880;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19987;&#20026;&#30830;&#20445;&#26377;&#25928;&#22320;&#26631;&#27880;&#20195;&#30721;&#28151;&#21512;&#22411;&#25991;&#26412;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#23637;&#31034;&#22312;Hinglish&#25991;&#26412;&#30340;&#35789;&#27719;&#32423;&#21035;&#21644;&#21477;&#23376;&#32423;&#21035;&#30340;&#35821;&#35328;&#26631;&#27880;&#20219;&#21153;&#20013;&#36798;&#21040;&#19982;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;&#30456;&#27604;&#20116;&#20493;&#30340;&#26631;&#27880;&#25928;&#29575;&#65292;COMMENTATOR&#35777;&#26126;&#20102;&#33258;&#24049;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#20851;&#20195;&#30721;&#24050;&#20844;&#24320;&#65292;&#21487;&#22312;GitHub&#19978;&#30340;https://github.com/lingo-iitgn/commentator&#39029;&#38754;&#33719;&#21462;&#65292;&#24182;&#22312;https://bit.ly/commentator_video&#25773;&#25918;&#20102;&#19968;&#20010;&#28436;&#31034;&#35270;&#39057;&#12290;</title><link>https://arxiv.org/abs/2408.03125</link><description>&lt;p&gt;
COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;COMMENTATOR&#30340;&#20195;&#30721;&#28151;&#21512;&#22411;&#22810;&#35821;&#35328;&#25991;&#26412;&#26631;&#27880;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19987;&#20026;&#30830;&#20445;&#26377;&#25928;&#22320;&#26631;&#27880;&#20195;&#30721;&#28151;&#21512;&#22411;&#25991;&#26412;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#23637;&#31034;&#22312;Hinglish&#25991;&#26412;&#30340;&#35789;&#27719;&#32423;&#21035;&#21644;&#21477;&#23376;&#32423;&#21035;&#30340;&#35821;&#35328;&#26631;&#27880;&#20219;&#21153;&#20013;&#36798;&#21040;&#19982;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;&#30456;&#27604;&#20116;&#20493;&#30340;&#26631;&#27880;&#25928;&#29575;&#65292;COMMENTATOR&#35777;&#26126;&#20102;&#33258;&#24049;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#20851;&#20195;&#30721;&#24050;&#20844;&#24320;&#65292;&#21487;&#22312;GitHub&#19978;&#30340;https://github.com/lingo-iitgn/commentator&#39029;&#38754;&#33719;&#21462;&#65292;&#24182;&#22312;https://bit.ly/commentator_video&#25773;&#25918;&#20102;&#19968;&#20010;&#28436;&#31034;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03125v1 Announce Type: cross  Abstract: As the NLP community increasingly addresses challenges associated with multilingualism, robust annotation tools are essential to handle multilingual datasets efficiently. In this paper, we introduce a code-mixed multilingual text annotation framework, COMMENTATOR, specifically designed for annotating code-mixed text. The tool demonstrates its effectiveness in token-level and sentence-level language annotation tasks for Hinglish text. We perform robust qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster annotations than the best baseline. Our code is publicly available at \url{https://github.com/lingo-iitgn/commentator}. The demonstration video is available at \url{https://bit.ly/commentator_video}.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#26500;&#24314;&#20102;&#21517;&#20026;Euas-20&#30340;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#29992;&#20197;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#32763;&#35793;&#33021;&#21147;&#21450;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20854;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2408.03119</link><description>&lt;p&gt;
Evaluating the Translation Performance of Large Language Models Based on Euas-20
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#26500;&#24314;&#20102;&#21517;&#20026;Euas-20&#30340;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#29992;&#20197;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#32763;&#35793;&#33021;&#21147;&#21450;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20854;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03119v1 Announce Type: cross  Abstract: In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#23398;&#20064;&#20855;&#26377;&#36328;&#29615;&#22659;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#26032;&#29615;&#22659;&#20013;&#25552;&#20379;&#27010;&#29575;&#36817;&#20284;&#27491;&#30830;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;MDP&#29615;&#22659;&#19978;&#36827;&#34892;&#26377;&#38480;&#26679;&#26412;&#25506;&#32034;&#65292;&#26500;&#24314;&#22522;&#20110;&#36712;&#36857;&#30340;&#21306;&#38388;MDP&#27169;&#22411;&#36817;&#20284;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#33021;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#20445;&#25345;&#33391;&#22909;&#24615;&#33021;&#30340;&#21333;&#19968;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#37327;&#21270;&#31574;&#30053;&#22312;&#26032;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#39118;&#38505;&#65292;&#24182;&#22312;&#25506;&#32034;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#36825;&#31181;&#39118;&#38505;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2408.03093</link><description>&lt;p&gt;
Learning Provably Robust Policies in Uncertain Parametric Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03093
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#23398;&#20064;&#20855;&#26377;&#36328;&#29615;&#22659;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#26032;&#29615;&#22659;&#20013;&#25552;&#20379;&#27010;&#29575;&#36817;&#20284;&#27491;&#30830;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;MDP&#29615;&#22659;&#19978;&#36827;&#34892;&#26377;&#38480;&#26679;&#26412;&#25506;&#32034;&#65292;&#26500;&#24314;&#22522;&#20110;&#36712;&#36857;&#30340;&#21306;&#38388;MDP&#27169;&#22411;&#36817;&#20284;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#33021;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#20445;&#25345;&#33391;&#22909;&#24615;&#33021;&#30340;&#21333;&#19968;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#37327;&#21270;&#31574;&#30053;&#22312;&#26032;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#39118;&#38505;&#65292;&#24182;&#22312;&#25506;&#32034;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#36825;&#31181;&#39118;&#38505;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03093v1 Announce Type: cross  Abstract: We present a data-driven approach for learning MDP policies that are robust across stochastic environments whose transition probabilities are defined by parameters with an unknown distribution. We produce probably approximately correct (PAC) guarantees for the performance of these learned policies in a new, unseen environment over the unknown distribution. Our approach is based on finite samples of the MDP environments, for each of which we build an approximation of the model as an interval MDP, by exploring a set of generated trajectories. We use the built approximations to synthesise a single policy that performs well (meets given requirements) across the sampled environments, and furthermore bound its risk (of not meeting the given requirements) when deployed in an unseen environment. Our procedure offers a trade-off between the guaranteed performance of the learned policy and the risk of not meeting the guarantee in an unseen envir
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QADQN&#65288;&#37327;&#23376;&#27880;&#24847;&#24230;&#37327;&#19979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#21019;&#26032;&#23581;&#35797;&#65292;&#23427;&#32467;&#21512;&#20102;&#37327;&#23376;&#30005;&#36335;&#19982;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#29992;&#20110;&#37329;&#34701;&#24066;&#22330;&#30340;&#39044;&#27979;&#21644;&#20132;&#26131;&#31574;&#30053;&#21046;&#23450;&#12290;&#36890;&#36807;&#22312;&#21253;&#21547;&#22266;&#23450;&#20132;&#26131;&#25104;&#26412;&#30340;&#29616;&#23454;&#24066;&#22330;&#26465;&#20214;&#19979;&#36827;&#34892;&#22238;&#27979;&#65292;&#39564;&#35777;&#20102;QADQN&#22312;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.03088</link><description>&lt;p&gt;
QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QADQN&#65288;&#37327;&#23376;&#27880;&#24847;&#24230;&#37327;&#19979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#21019;&#26032;&#23581;&#35797;&#65292;&#23427;&#32467;&#21512;&#20102;&#37327;&#23376;&#30005;&#36335;&#19982;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#29992;&#20110;&#37329;&#34701;&#24066;&#22330;&#30340;&#39044;&#27979;&#21644;&#20132;&#26131;&#31574;&#30053;&#21046;&#23450;&#12290;&#36890;&#36807;&#22312;&#21253;&#21547;&#22266;&#23450;&#20132;&#26131;&#25104;&#26412;&#30340;&#29616;&#23454;&#24066;&#22330;&#26465;&#20214;&#19979;&#36827;&#34892;&#22238;&#27979;&#65292;&#39564;&#35777;&#20102;QADQN&#22312;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03088v1 Announce Type: cross  Abstract: Financial market prediction and optimal trading strategy development remain challenging due to market complexity and volatility. Our research in quantum finance and reinforcement learning for decision-making demonstrates the approach of quantum-classical hybrid algorithms to tackling real-world financial challenges. In this respect, we corroborate the concept with rigorous backtesting and validate the framework's performance under realistic market conditions, by including fixed transaction cost per trade. This paper introduces a Quantum Attention Deep Q-Network (QADQN) approach to address these challenges through quantum-enhanced reinforcement learning. Our QADQN architecture uses a variational quantum circuit inside a traditional deep Q-learning framework to take advantage of possible quantum advantages in decision-making. We gauge the QADQN agent's performance on historical data from major market indices, including the S&amp;P 500. We ev
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65288;UniCE&#65289;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#23376;&#20219;&#21153;&#20132;&#20114;&#21644;&#30693;&#35782;&#34701;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#21333;&#19968;&#21477;&#23376;&#20013;&#35782;&#21035;&#22810;&#20010;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#12289;&#23376;&#20219;&#21153;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20197;&#21450;&#20004;&#31181;&#30693;&#35782;&#28304;&#65288;&#35821;&#35328;&#27169;&#22411;&#21644;&#32467;&#26500;&#30693;&#35782;&#22270;&#65289;&#34701;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.03079</link><description>&lt;p&gt;
Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65288;UniCE&#65289;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#23376;&#20219;&#21153;&#20132;&#20114;&#21644;&#30693;&#35782;&#34701;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#21333;&#19968;&#21477;&#23376;&#20013;&#35782;&#21035;&#22810;&#20010;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#12289;&#23376;&#20219;&#21153;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20197;&#21450;&#20004;&#31181;&#30693;&#35782;&#28304;&#65288;&#35821;&#35328;&#27169;&#22411;&#21644;&#32467;&#26500;&#30693;&#35782;&#22270;&#65289;&#34701;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03079v1 Announce Type: cross  Abstract: Event Causality Extraction (ECE) aims at extracting causal event pairs from texts. Despite ChatGPT's recent success, fine-tuning small models remains the best approach for the ECE task. However, existing fine-tuning based ECE methods cannot address all three key challenges in ECE simultaneously: 1) Complex Causality Extraction, where multiple causal-effect pairs occur within a single sentence; 2) Subtask~ Interaction, which involves modeling the mutual dependence between the two subtasks of ECE, i.e., extracting events and identifying the causal relationship between extracted events; and 3) Knowledge Fusion, which requires effectively fusing the knowledge in two modalities, i.e., the expressive pretrained language models and the structured knowledge graphs. In this paper, we propose a unified ECE framework (UniCE to address all three issues in ECE simultaneously. Specifically, we design a subtask interaction mechanism to enable mutual 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25163;&#26415;&#24212;&#29992;&#30340;&#24320;&#25918;&#22495;&#21333;&#30446;&#35270;&#35273;SLAM&#26694;&#26550;BodySLAM&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21333;&#30446;&#25668;&#20687;&#22836;&#30340;&#36755;&#20837;&#65292;&#26080;&#38656;&#20219;&#20309;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25163;&#26415;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25805;&#32437;&#31934;&#20934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.03078</link><description>&lt;p&gt;
BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25163;&#26415;&#24212;&#29992;&#30340;&#24320;&#25918;&#22495;&#21333;&#30446;&#35270;&#35273;SLAM&#26694;&#26550;BodySLAM&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21333;&#30446;&#25668;&#20687;&#22836;&#30340;&#36755;&#20837;&#65292;&#26080;&#38656;&#20219;&#20309;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25163;&#26415;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25805;&#32437;&#31934;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03078v1 Announce Type: cross  Abstract: Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM) has emerged as a promising solution to address these limitations, its implementation in endoscopic procedures presents significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents a robust deep learning-based SLAM approach that combines state-of-the-art and newly developed models. It consists of three main parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method based on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the novel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous models to create a coherent surgical map. The performance of the procedure was rigorously eva
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#22312;&#33521;&#29305;&#23572;Loihi 2&#22411;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#24182;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#33021;&#37327;&#25928;&#29575;&#19978;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;CPU&#19978;&#30340;&#31639;&#27861;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.03076</link><description>&lt;p&gt;
Solving QUBO on the Loihi 2 Neuromorphic Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#22312;&#33521;&#29305;&#23572;Loihi 2&#22411;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#24182;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#33021;&#37327;&#25928;&#29575;&#19978;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;CPU&#19978;&#30340;&#31639;&#27861;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03076v1 Announce Type: cross  Abstract: In this article, we describe an algorithm for solving Quadratic Unconstrained Binary Optimization problems on the Intel Loihi 2 neuromorphic processor. The solver is based on a hardware-aware fine-grained parallel simulated annealing algorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary results show that our approach can generate feasible solutions in as little as 1 ms and up to 37x more energy efficient compared to two baseline solvers running on a CPU. These advantages could be especially relevant for size-, weight-, and power-constrained edge computing applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;OpenOmni&#65292;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#26088;&#22312;&#25903;&#25345;&#24320;&#21457;&#20154;&#21592;&#26500;&#24314;&#26410;&#26469;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#20195;&#29702;&#12290;&#35813;&#24037;&#20855;&#38598;&#25104;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#25903;&#25345;&#26412;&#22320;&#21644;&#20113;&#37096;&#32626;&#65292;&#20197;&#20445;&#35777;&#25968;&#25454;&#23433;&#20840;&#21644;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.03047</link><description>&lt;p&gt;
OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;OpenOmni&#65292;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#26088;&#22312;&#25903;&#25345;&#24320;&#21457;&#20154;&#21592;&#26500;&#24314;&#26410;&#26469;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#20195;&#29702;&#12290;&#35813;&#24037;&#20855;&#38598;&#25104;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#25903;&#25345;&#26412;&#22320;&#21644;&#20113;&#37096;&#32626;&#65292;&#20197;&#20445;&#35777;&#25968;&#25454;&#23433;&#20840;&#21644;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03047v1 Announce Type: cross  Abstract: Multimodal conversational agents are highly desirable because they offer natural and human-like interaction. However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking. While proprietary systems like GPT-4o and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy. To better understand and quantify these issues, we developed OpenOmni, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models. OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking. This flexible framework allows researchers to customize the pipeline
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#23398;&#20064;&#20013;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#29575;&#26469;&#26500;&#36896;&#23494;&#38598;&#19988;&#20449;&#24687;&#37327;&#22823;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;Beta&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#25104;&#21151;&#29575;&#65292;&#36825;&#20123;&#20998;&#24067;&#38543;&#30528;&#25968;&#25454;&#30340;&#31215;&#32047;&#32780;&#36880;&#28176;&#21464;&#24471;&#21487;&#38752;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;&#33258;&#36866;&#24212;&#25104;&#21151;&#29575;&#40723;&#21169;&#25506;&#32034;&#65292;&#38543;&#30528;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36880;&#28176;&#21464;&#20026;&#40723;&#21169;&#21033;&#29992;&#30340;&#22909;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#19982;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#26102;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.03029</link><description>&lt;p&gt;
Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#23398;&#20064;&#20013;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#29575;&#26469;&#26500;&#36896;&#23494;&#38598;&#19988;&#20449;&#24687;&#37327;&#22823;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;Beta&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#25104;&#21151;&#29575;&#65292;&#36825;&#20123;&#20998;&#24067;&#38543;&#30528;&#25968;&#25454;&#30340;&#31215;&#32047;&#32780;&#36880;&#28176;&#21464;&#24471;&#21487;&#38752;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;&#33258;&#36866;&#24212;&#25104;&#21151;&#29575;&#40723;&#21169;&#25506;&#32034;&#65292;&#38543;&#30528;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36880;&#28176;&#21464;&#20026;&#40723;&#21169;&#21033;&#29992;&#30340;&#22909;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#19982;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#26102;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03029v1 Announce Type: cross  Abstract: Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experiences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implementation in high-dimensional continuous state spaces. This method provides a non-pa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CSI&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#22810;&#31181;&#19981;&#21516;&#39118;&#26684;&#30340;&#36816;&#21160;&#25216;&#33021;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#25511;&#21046;&#22120;&#20013;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#12290;&#36825;&#20026; legged robots &#30340;&#22810;&#25216;&#33021;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#29615;&#22659;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.03018</link><description>&lt;p&gt;
Integrating Controllable Motion Skills from Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CSI&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#22810;&#31181;&#19981;&#21516;&#39118;&#26684;&#30340;&#36816;&#21160;&#25216;&#33021;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#25511;&#21046;&#22120;&#20013;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#12290;&#36825;&#20026; legged robots &#30340;&#22810;&#25216;&#33021;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03018v1 Announce Type: new  Abstract: The expanding applications of legged robots require their mastery of versatile motion skills. Correspondingly, researchers must address the challenge of integrating multiple diverse motion skills into controllers. While existing reinforcement learning (RL)-based approaches have achieved notable success in multi-skill integration for legged robots, these methods often require intricate reward engineering or are restricted to integrating a predefined set of motion skills constrained by specific task objectives, resulting in limited flexibility. In this work, we introduce a flexible multi-skill integration framework named Controllable Skills Integration (CSI). CSI enables the integration of a diverse set of motion skills with varying styles into a single policy without the need for complex reward tuning. Furthermore, in a hierarchical control manner, the trained low-level policy can be coupled with a high-level Natural Language Inference (N
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;NeurDB&#65292;&#19968;&#20010;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#27835;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#24211;&#20869;&#26080;&#32541;&#38598;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#21644;&#24037;&#20316;&#37327;&#21464;&#21270;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#20351;NeurDB&#22312;&#22788;&#29702;AI&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#23398;&#20064;&#26500;&#24314;&#30340;&#32452;&#20214;&#30340;&#24615;&#33021;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2408.03013</link><description>&lt;p&gt;
NeurDB: On the Design and Implementation of an AI-powered Autonomous Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;NeurDB&#65292;&#19968;&#20010;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#27835;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#24211;&#20869;&#26080;&#32541;&#38598;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#21644;&#24037;&#20316;&#37327;&#21464;&#21270;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#20351;NeurDB&#22312;&#22788;&#29702;AI&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#23398;&#20064;&#26500;&#24314;&#30340;&#32452;&#20214;&#30340;&#24615;&#33021;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03013v1 Announce Type: cross  Abstract: Databases are increasingly embracing AI to provide autonomous system optimization and intelligent in-database analytics, aiming to relieve end-user burdens across various industry sectors. Nonetheless, most existing approaches fail to account for the dynamic nature of databases, which renders them ineffective for real-world applications characterized by evolving data and workloads. This paper introduces NeurDB, an AI-powered autonomous database that deepens the fusion of AI and databases with adaptability to data and workload drift. NeurDB establishes a new in-database AI ecosystem that seamlessly integrates AI workflows within the database. This integration enables efficient and effective in-database AI analytics and fast-adaptive learned system components. Empirical evaluations demonstrate that NeurDB substantially outperforms existing solutions in managing AI analytics tasks, with the proposed learned components more effectively han
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#22312;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#27169;&#25311;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#34892;&#20154;&#34892;&#20026;&#23545;&#33258;&#20027;&#36710;&#36742;&#36890;&#36807;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#25991;&#21270;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#20154;&#19982;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#20114;&#30340;&#23433;&#20840;&#24615;&#21644;&#25509;&#21463;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.03003</link><description>&lt;p&gt;
Cross-cultural analysis of pedestrian group behaviour influence on crossing decisions in interactions with autonomous vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#22312;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#27169;&#25311;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#34892;&#20154;&#34892;&#20026;&#23545;&#33258;&#20027;&#36710;&#36742;&#36890;&#36807;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#25991;&#21270;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#20154;&#19982;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#20114;&#30340;&#23433;&#20840;&#24615;&#21644;&#25509;&#21463;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03003v1 Announce Type: cross  Abstract: Understanding cultural backgrounds is crucial for the seamless integration of autonomous driving into daily life as it ensures that systems are attuned to diverse societal norms and behaviours, enhancing acceptance and safety in varied cultural contexts. In this work, we investigate the impact of co-located pedestrians on crossing behaviour, considering cultural and situational factors. To accomplish this, a full-scale virtual reality (VR) environment was created in the CARLA simulator, enabling the identical experiment to be replicated in both Spain and Australia. Participants (N=30) attempted to cross the road at an urban crosswalk alongside other pedestrians exhibiting conservative to more daring behaviours, while an autonomous vehicle (AV) approached with different driving styles. For the analysis of interactions, we utilized questionnaires and direct measures of the moment when participants entered the lane.   Our findings indicat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#27010;&#29575;&#24615;&#26368;&#23567;&#36866;&#24403;&#25945;&#24072;&#65288;pMAT&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;DFA&#65288;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65289;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#38169;&#35823;&#27010;&#29575;&#24615;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#24182;&#20026;&#22823;&#27010;&#29575;&#20135;&#29983;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#24212;&#23545;&#31574;&#30053;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25552;&#31034;&#21644;&#26041;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#23545;&#27604;&#20102;TTT&#31639;&#27861;&#21644;&#20854;&#20182;&#24120;&#35265;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;DFA&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#19988;&#20026;&#20102;&#35299;&#20915;&#21487;&#33021;&#23548;&#33268;&#30340;&#38169;&#35823;&#25968;&#37327;&#29190;&#28856;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#26597;&#35810;&#32531;&#23384;&#31639;&#27861;&#26469;&#20248;&#21270;&#26597;&#35810;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2408.02999</link><description>&lt;p&gt;
LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#27010;&#29575;&#24615;&#26368;&#23567;&#36866;&#24403;&#25945;&#24072;&#65288;pMAT&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;DFA&#65288;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65289;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#38169;&#35823;&#27010;&#29575;&#24615;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#24182;&#20026;&#22823;&#27010;&#29575;&#20135;&#29983;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#24212;&#23545;&#31574;&#30053;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25552;&#31034;&#21644;&#26041;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#23545;&#27604;&#20102;TTT&#31639;&#27861;&#21644;&#20854;&#20182;&#24120;&#35265;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;DFA&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#19988;&#20026;&#20102;&#35299;&#20915;&#21487;&#33021;&#23548;&#33268;&#30340;&#38169;&#35823;&#25968;&#37327;&#29190;&#28856;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#26597;&#35810;&#32531;&#23384;&#31639;&#27861;&#26469;&#20248;&#21270;&#26597;&#35810;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02999v1 Announce Type: cross  Abstract: The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;ASR-enhanced Multimodal Product Representation Learning&#65288;AMPere&#65289;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;LLM&#30340;ASR&#25991;&#26412;&#25688;&#35201;&#22120;&#31616;&#21270;&#20174;&#22122;&#22768;ASR&#25991;&#26412;&#20013;&#25552;&#21462;&#20135;&#21697;&#30456;&#20851;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#25968;&#25454;&#32852;&#21512;&#36755;&#20837;&#21040;&#19968;&#20010;&#22810;&#20998;&#25903;&#32593;&#32476;&#20013;&#65292;&#20197;&#29983;&#25104;&#32039;&#20945;&#30340;&#36328;&#22495;&#20135;&#21697; multimodal &#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2408.02978</link><description>&lt;p&gt;
ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;ASR-enhanced Multimodal Product Representation Learning&#65288;AMPere&#65289;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;LLM&#30340;ASR&#25991;&#26412;&#25688;&#35201;&#22120;&#31616;&#21270;&#20174;&#22122;&#22768;ASR&#25991;&#26412;&#20013;&#25552;&#21462;&#20135;&#21697;&#30456;&#20851;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#25968;&#25454;&#32852;&#21512;&#36755;&#20837;&#21040;&#19968;&#20010;&#22810;&#20998;&#25903;&#32593;&#32476;&#20013;&#65292;&#20197;&#29983;&#25104;&#32039;&#20945;&#30340;&#36328;&#22495;&#20135;&#21697; multimodal &#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02978v1 Announce Type: cross  Abstract: E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive exp
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;EmpRL&#65289;&#36827;&#34892;&#21516;&#24773;&#24615;&#21709;&#24212;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#26377;&#25928;&#30340;&#21516;&#24773;&#24230;&#22870;&#21169;&#20989;&#25968;&#21644;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#26469;&#29983;&#25104;&#26356;&#21152;&#21516;&#24773;&#24615;&#30340;&#25991;&#26412;&#21709;&#24212;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#21709;&#24212;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02976</link><description>&lt;p&gt;
Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02976
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;EmpRL&#65289;&#36827;&#34892;&#21516;&#24773;&#24615;&#21709;&#24212;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#26377;&#25928;&#30340;&#21516;&#24773;&#24230;&#22870;&#21169;&#20989;&#25968;&#21644;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#26469;&#29983;&#25104;&#26356;&#21152;&#21516;&#24773;&#24615;&#30340;&#25991;&#26412;&#21709;&#24212;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#21709;&#24212;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02976v1 Announce Type: cross  Abstract: Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ADdress&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25104;&#21151;&#20026;&#22522;&#30784;&#30340;&#33258;&#23398;&#20064;&#21333;&#30772;&#22351;-&#20462;&#22797;&#22686;&#24378;&#30340;&#20219;&#20309;&#26102;&#38388;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#30772;&#22351;&#31574;&#30053;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22823;&#22411;&#37051;&#22495;&#25628;&#32034;&#20013;&#30340;&#35299;&#20915;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#36335;&#24452;&#35268;&#21010;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02960</link><description>&lt;p&gt;
Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ADdress&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25104;&#21151;&#20026;&#22522;&#30784;&#30340;&#33258;&#23398;&#20064;&#21333;&#30772;&#22351;-&#20462;&#22797;&#22686;&#24378;&#30340;&#20219;&#20309;&#26102;&#38388;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#30772;&#22351;&#31574;&#30053;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22823;&#22411;&#37051;&#22495;&#25628;&#32034;&#20013;&#30340;&#35299;&#20915;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#36335;&#24452;&#35268;&#21010;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02960v1 Announce Type: new  Abstract: Anytime multi-agent path finding (MAPF) is a promising approach to scalable path optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood Search (LNS), is the current state-of-the-art approach where a fast initial solution is iteratively optimized by destroying and repairing selected paths of the solution. Current MAPF-LNS variants commonly use an adaptive selection mechanism to choose among multiple destroy heuristics. However, to determine promising destroy heuristics, MAPF-LNS requires a considerable amount of exploration time. As common destroy heuristics are non-adaptive, any performance bottleneck caused by these heuristics cannot be overcome via adaptive heuristic selection alone, thus limiting the overall effectiveness of MAPF-LNS in terms of solution cost. In this paper, we propose Adaptive Delay-based Destroy-and-Repair Enhanced with Success-based Self-Learning (ADDRESS) as a single-destroy-heuristic variant o
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#26680;&#24863;&#30693;&#26041;&#27861;&#35757;&#32451;&#30340;&#26032;&#22411;&#20803;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#26497;&#31471;&#22495;&#20559;&#31227;&#19979;&#30340;&#33258;&#20027;&#26426;&#26800;&#33218;&#23569;&#26679;&#26412;&#37319;&#30719;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26368;&#22823;&#37096;&#32626;&#38388;&#38553;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19987;&#38376;&#35757;&#32451;&#27169;&#22411;&#20197;&#20811;&#26381;&#36825;&#20123;&#38388;&#38553;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#24040;&#22823;&#30340;&#22495;&#20559;&#31227;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#24207;&#36143;&#20915;&#31574;&#26694;&#26550;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#37319;&#30719;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36824;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#26679;&#26412;&#37319;&#25496;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.02949</link><description>&lt;p&gt;
Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#26680;&#24863;&#30693;&#26041;&#27861;&#35757;&#32451;&#30340;&#26032;&#22411;&#20803;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#26497;&#31471;&#22495;&#20559;&#31227;&#19979;&#30340;&#33258;&#20027;&#26426;&#26800;&#33218;&#23569;&#26679;&#26412;&#37319;&#30719;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26368;&#22823;&#37096;&#32626;&#38388;&#38553;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19987;&#38376;&#35757;&#32451;&#27169;&#22411;&#20197;&#20811;&#26381;&#36825;&#20123;&#38388;&#38553;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#24040;&#22823;&#30340;&#22495;&#20559;&#31227;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#24207;&#36143;&#20915;&#31574;&#26694;&#26550;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#37319;&#30719;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36824;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#26679;&#26412;&#37319;&#25496;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02949v1 Announce Type: new  Abstract: Autonomous lander missions on extraterrestrial bodies need to sample granular materials while coping with domain shifts, even when sampling strategies are extensively tuned on Earth. To tackle this challenge, this paper studies the few-shot scooping problem and proposes a vision-based adaptive scooping strategy that uses the deep kernel Gaussian process method trained with a novel meta-training strategy to learn online from very limited experience on out-of-distribution target terrains. Our Deep Kernel Calibration with Maximal Deployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt to large domain shifts by creating simulated maximal deployment gaps from an offline training dataset and training models to overcome these deployment gaps during training. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to perform high-quality scooping actions on out-of-dist
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#27745;&#26579;&#26102;&#34920;&#29616;&#20986;&#20102;&#26356;&#22823;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#31181;&#27745;&#26579;&#21253;&#25324;&#24694;&#24847;&#24494;&#35843;&#12289;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#20197;&#21450;&#25968;&#25454;&#25925;&#24847;&#27745;&#26579;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#65292;&#23398;&#20064;&#26377;&#23475;&#34892;&#20026;&#30340;&#36895;&#24230;&#20063;&#26174;&#33879;&#21152;&#24555;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#26377;&#23475;&#34892;&#20026;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2408.02946</link><description>&lt;p&gt;
Scaling Laws for Data Poisoning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#27745;&#26579;&#26102;&#34920;&#29616;&#20986;&#20102;&#26356;&#22823;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#31181;&#27745;&#26579;&#21253;&#25324;&#24694;&#24847;&#24494;&#35843;&#12289;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#20197;&#21450;&#25968;&#25454;&#25925;&#24847;&#27745;&#26579;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#65292;&#23398;&#20064;&#26377;&#23475;&#34892;&#20026;&#30340;&#36895;&#24230;&#20063;&#26174;&#33879;&#21152;&#24555;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#26377;&#23475;&#34892;&#20026;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02946v1 Announce Type: cross  Abstract: Recent work shows that LLMs are vulnerable to data poisoning, in which they are trained on partially corrupted or harmful data. Poisoned data is hard to detect, breaks guardrails, and leads to undesirable and harmful behavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat. We consider three threat models by which data poisoning can occur: malicious fine-tuning, imperfect data curation, and intentional data contamination. Our experiments evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three datasets which speak to each of our threat models. We find that larger LLMs are increasingly vulnerable, learning harmful behavior -- including sleeper agent behavior -- significantly more quickly than smaller LLMs with
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;LLM&#25216;&#26415;&#23454;&#29616;&#26356;&#21152;&#26234;&#33021;&#30340;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20248;&#21270;&#65292;&#20174;&#32780;&#25552;&#21319;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02944</link><description>&lt;p&gt;
LLM-Empowered Resource Allocation in Wireless Communications Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;LLM&#25216;&#26415;&#23454;&#29616;&#26356;&#21152;&#26234;&#33021;&#30340;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20248;&#21270;&#65292;&#20174;&#32780;&#25552;&#21319;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02944v1 Announce Type: cross  Abstract: The recent success of large language models (LLMs) has spurred their application in various fields. In particular, there have been efforts to integrate LLMs into various aspects of wireless communication systems. The use of LLMs in wireless communication systems has the potential to realize artificial general intelligence (AGI)-enabled wireless networks. In this paper, we investigate an LLM-based resource allocation scheme for wireless communication systems. Specifically, we formulate a simple resource allocation problem involving two transmit pairs and develop an LLM-based resource allocation approach that aims to maximize either energy efficiency or spectral efficiency. Additionally, we consider the joint use of low-complexity resource allocation techniques to compensate for the reliability shortcomings of the LLM-based scheme. After confirming the applicability and feasibility of LLM-based resource allocation, we address several key
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Marcus&#26144;&#23556;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31209;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;&#21040;&#21452;&#37325;&#21487;&#23548;&#23545;&#31216;&#30697;&#38453;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#20026;&#32858;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31232;&#30095;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#32858;&#31867;&#30340;&#21487;&#23548;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02932</link><description>&lt;p&gt;
Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Marcus&#26144;&#23556;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31209;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;&#21040;&#21452;&#37325;&#21487;&#23548;&#23545;&#31216;&#30697;&#38453;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#20026;&#32858;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31232;&#30095;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#32858;&#31867;&#30340;&#21487;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02932v1 Announce Type: cross  Abstract: Clustering is a fundamental task in machine learning and data science, and similarity graph-based clustering is an important approach within this domain. Doubly stochastic symmetric similarity graphs provide numerous benefits for clustering problems and downstream tasks, yet learning such graphs remains a significant challenge. Marcus theorem states that a strictly positive symmetric matrix can be transformed into a doubly stochastic symmetric matrix by diagonal matrices. However, in clustering, learning sparse matrices is crucial for computational efficiency. We extend Marcus theorem by proposing the Marcus mapping, which indicates that certain sparse matrices can also be transformed into doubly stochastic symmetric matrices via diagonal matrices. Additionally, we introduce rank constraints into the clustering problem and propose the Doubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus Mapping (ANCMM). This en
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#8220;&#23567;&#22411;&#20195;&#29702;&#12289;&#22823;&#22411;&#19990;&#30028;&#8221;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#23567;&#22411;&#20195;&#29702;&#22312;&#22823;&#22411;&#19990;&#30028;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#22320;&#27762;&#21462;&#12289;&#20445;&#25345;&#21644;&#25490;&#38500;&#30456;&#20851;&#20449;&#24687;&#30340;&#23398;&#20064;&#20195;&#29702;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#27169;&#25311;&#29615;&#22659;&#22312;&#36866;&#24212;&#23454;&#29992;&#23398;&#20064;&#29615;&#22659;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35774;&#35745;&#26410;&#26469;&#27169;&#25311;&#29615;&#22659;&#30340;&#20004;&#20010;&#22522;&#26412;&#35201;&#27714;&#65292;&#20197;&#26399;&#20419;&#36827;&#23454;&#29992;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#24555;&#36895;&#21407;&#22411;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2408.02930</link><description>&lt;p&gt;
The Need for a Big World Simulator: A Scientific Challenge for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#8220;&#23567;&#22411;&#20195;&#29702;&#12289;&#22823;&#22411;&#19990;&#30028;&#8221;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#23567;&#22411;&#20195;&#29702;&#22312;&#22823;&#22411;&#19990;&#30028;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#22320;&#27762;&#21462;&#12289;&#20445;&#25345;&#21644;&#25490;&#38500;&#30456;&#20851;&#20449;&#24687;&#30340;&#23398;&#20064;&#20195;&#29702;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#27169;&#25311;&#29615;&#22659;&#22312;&#36866;&#24212;&#23454;&#29992;&#23398;&#20064;&#29615;&#22659;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35774;&#35745;&#26410;&#26469;&#27169;&#25311;&#29615;&#22659;&#30340;&#20004;&#20010;&#22522;&#26412;&#35201;&#27714;&#65292;&#20197;&#26399;&#20419;&#36827;&#23454;&#29992;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#24555;&#36895;&#21407;&#22411;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02930v1 Announce Type: cross  Abstract: The "small agent, big world" frame offers a conceptual view that motivates the need for continual learning. The idea is that a small agent operating in a much bigger world cannot store all information that the world has to offer. To perform well, the agent must be carefully designed to ingest, retain, and eject the right information. To enable the development of performant continual learning agents, a number of synthetic environments have been proposed. However, these benchmarks suffer from limitations, including unnatural distribution shifts and a lack of fidelity to the "small agent, big world" framing. This paper aims to formalize two desiderata for the design of future simulated environments. These two criteria aim to reflect the objectives and complexity of continual learning in practical settings while enabling rapid prototyping of algorithms on a smaller scale.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HARMONIC&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#24335;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02927</link><description>&lt;p&gt;
HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HARMONIC&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#24335;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02927v1 Announce Type: cross  Abstract: Data serves as the fundamental foundation for advancing deep learning, particularly tabular data presented in a structured format, which is highly conducive to modeling. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Hence, exploring how to effectively use models like LLMs to generate realistic and privacy-preserving synthetic tabular data is urgent. In this paper, we take a step forward to explore LLMs for tabular data synthesis and privacy protection, by introducing a new framework HARMONIC for tabular data generation and evaluation. In the tabular data generation of our framework, unlike previous small-scale LLM-based methods that rely on continued pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular data and enhance privacy. Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#22522;&#30784;&#27169;&#22411;&#20026;&#26550;&#26500;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#65292;&#20855;&#20307;&#21253;&#25324;&#21151;&#33021;&#33021;&#21147;&#21644;&#38750;&#21151;&#33021;&#24615;&#36136;&#37327;&#31561;&#30340;&#26550;&#26500;&#35774;&#35745;&#20998;&#31867;&#65292;&#20197;&#21450;&#35774;&#35745;&#20915;&#31574;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#21319;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#24320;&#21457;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02920</link><description>&lt;p&gt;
A Taxonomy of Architecture Options for Foundation Model-based Agents: Analysis and Decision Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#22522;&#30784;&#27169;&#22411;&#20026;&#26550;&#26500;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#65292;&#20855;&#20307;&#21253;&#25324;&#21151;&#33021;&#33021;&#21147;&#21644;&#38750;&#21151;&#33021;&#24615;&#36136;&#37327;&#31561;&#30340;&#26550;&#26500;&#35774;&#35745;&#20998;&#31867;&#65292;&#20197;&#21450;&#35774;&#35745;&#20915;&#31574;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#21319;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#24320;&#21457;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02920v1 Announce Type: cross  Abstract: The rapid advancement of AI technology has led to widespread applications of agent systems across various domains. However, the need for detailed architecture design poses significant challenges in designing and operating these systems. This paper introduces a taxonomy focused on the architectures of foundation-model-based agents, addressing critical aspects such as functional capabilities and non-functional qualities. We also discuss the operations involved in both design-time and run-time phases, providing a comprehensive view of architectural design and operational characteristics. By unifying and detailing these classifications, our taxonomy aims to improve the design of foundation-model-based agents. Additionally, the paper establishes a decision model that guides critical design and runtime decisions, offering a structured approach to enhance the development of foundation-model-based agents. Our contributions include providing a 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;KOI&#30340;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20851;&#38190;&#29366;&#24577;&#25351;&#23548;&#26469;&#31934;&#30830;&#20272;&#35745;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#22312;&#32447;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2408.02912</link><description>&lt;p&gt;
KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02912
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;KOI&#30340;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20851;&#38190;&#29366;&#24577;&#25351;&#23548;&#26469;&#31934;&#30830;&#20272;&#35745;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#22312;&#32447;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02912v1 Announce Type: new  Abstract: Online Imitation Learning methods struggle with the gap between extensive online exploration space and limited expert trajectories, which hinder efficient exploration due to inaccurate task-aware reward estimation. Inspired by the findings from cognitive neuroscience that task decomposition could facilitate cognitive processing for efficient learning, we hypothesize that an agent could estimate precise task-aware imitation rewards for efficient online exploration by decomposing the target task into the objectives of "what to do" and the mechanisms of "how to do". In this work, we introduce the hybrid Key-state guided Online Imitation (KOI) learning approach, which leverages the integration of semantic and motion key states as guidance for task-aware reward estimation. Initially, we utilize the visual-language models to segment the expert trajectory into semantic key states, indicating the objectives of "what to do". Within the intervals 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#22467;&#21450;&#36710;&#36742;&#29260;&#29031;&#19978;&#30340;&#38463;&#25289;&#20271;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#21487;&#38752;&#22320;&#23450;&#20301;&#29260;&#29031;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#23383;&#31526;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;99.3%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#20854;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#31649;&#29702;&#65292;&#22914;&#20132;&#36890;&#36829;&#35268;&#26816;&#27979;&#21644;&#20572;&#36710;&#22330;&#20248;&#21270;&#12290;&#26410;&#26469;&#30740;&#31350;&#23558;&#36827;&#19968;&#27493;&#36890;&#36807;&#26550;&#26500;&#25913;&#36827;&#12289;&#25193;&#22823;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#31995;&#32479;&#20381;&#36182;&#38382;&#39064;&#26469;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02904</link><description>&lt;p&gt;
Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#22467;&#21450;&#36710;&#36742;&#29260;&#29031;&#19978;&#30340;&#38463;&#25289;&#20271;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#21487;&#38752;&#22320;&#23450;&#20301;&#29260;&#29031;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#23383;&#31526;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;99.3%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#20854;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#31649;&#29702;&#65292;&#22914;&#20132;&#36890;&#36829;&#35268;&#26816;&#27979;&#21644;&#20572;&#36710;&#22330;&#20248;&#21270;&#12290;&#26410;&#26469;&#30740;&#31350;&#23558;&#36827;&#19968;&#27493;&#36890;&#36807;&#26550;&#26500;&#25913;&#36827;&#12289;&#25193;&#22823;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#31995;&#32479;&#20381;&#36182;&#38382;&#39064;&#26469;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02904v1 Announce Type: new  Abstract: This paper introduces a novel two-stage framework for accurate Egyptian Vehicle License Plate Recognition (EVLPR). The first stage employs image processing techniques to reliably localize license plates, while the second stage utilizes a custom-designed deep learning model for robust Arabic character recognition. The proposed system achieves a remarkable 99.3% accuracy on a diverse dataset, surpassing existing approaches. Its potential applications extend to intelligent traffic management, including traffic violation detection and parking optimization. Future research will focus on enhancing the system's capabilities through architectural refinements, expanded datasets, and addressing system dependencies.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#28151;&#21512;&#31934;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#21152;&#36895;&#22120;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#26550;&#26500;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.02897</link><description>&lt;p&gt;
A Metric Driven Approach to Mixed Precision Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#28151;&#21512;&#31934;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#21152;&#36895;&#22120;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02897v1 Announce Type: cross  Abstract: As deep learning methodologies have developed, it has been generally agreed that increasing neural network size improves model quality. However, this is at the expense of memory and compute requirements, which also need to be increased. Various efficiency techniques have been proposed to rein in hardware costs, one being the use of low precision numerics. Recent accelerators have introduced several different 8-bit data types to help accommodate DNNs in terms of numerics. In this paper, we identify a metric driven methodology to aid in the choice of numerics. We demonstrate how such a methodology can help scale training of a language representation model. The technique can be generalized to other model architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VizECGNet&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;ECG&#22270;&#20687;&#32593;&#32476;&#26469;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#25171;&#21360;&#30340;ECG&#22270;&#24418;&#20415;&#33021;&#22815;&#20934;&#30830;&#22320;&#23545;&#22810;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2408.02888</link><description>&lt;p&gt;
VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VizECGNet&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;ECG&#22270;&#20687;&#32593;&#32476;&#26469;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#25171;&#21360;&#30340;ECG&#22270;&#24418;&#20415;&#33021;&#22815;&#20934;&#30830;&#22320;&#23545;&#22810;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02888v1 Announce Type: new  Abstract: An electrocardiogram (ECG) captures the heart's electrical signal to assess various heart conditions. In practice, ECG data is stored as either digitized signals or printed images. Despite the emergence of numerous deep learning models for digitized signals, many hospitals prefer image storage due to cost considerations. Recognizing the unavailability of raw ECG signals in many clinical settings, we propose VizECGNet, which uses only printed ECG graphics to determine the prognosis of multiple cardiovascular diseases. During training, cross-modal attention modules (CMAM) are used to integrate information from two modalities - image and signal, while self-modality attention modules (SMAM) capture inherent long-range dependencies in ECG data of each modality. Additionally, we utilize knowledge distillation to improve the similarity between two distinct predictions from each modality stream. This innovative multi-modal deep learning architec
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\method{}&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27745;&#26579;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#30693;&#19981;&#35273;&#20013;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#23548;&#33268;&#20854;&#29983;&#25104;&#21253;&#21547;&#29305;&#23450;&#29615;&#22659;&#21709;&#24212;&#30340;&#32570;&#38519;&#20195;&#30721;&#65292;&#20174;&#32780;&#22312;&#26410;&#26469;&#30340;&#36816;&#34892;&#26102;&#35302;&#21457;&#24694;&#24847;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2408.02882</link><description>&lt;p&gt;
Compromising Embodied Agents with Contextual Backdoor Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\method{}&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27745;&#26579;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#30693;&#19981;&#35273;&#20013;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#23548;&#33268;&#20854;&#29983;&#25104;&#21253;&#21547;&#29305;&#23450;&#29615;&#22659;&#21709;&#24212;&#30340;&#32570;&#38519;&#20195;&#30721;&#65292;&#20174;&#32780;&#22312;&#26410;&#26469;&#30340;&#36816;&#34892;&#26102;&#35302;&#21457;&#24694;&#24847;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02882v1 Announce Type: new  Abstract: Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \method{}. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20351;&#29992;&#36827;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;&#8220;Hide and Seek&#8221;&#30340;&#31639;&#27861;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#19968;&#20010;&#20027;&#25511;&#65288;Auditor&#65289;LLM&#29983;&#25104;&#37492;&#21035;&#24615;&#38382;&#39064;&#65292;&#36741;&#21161;&#65288;Detective&#65289;LLM&#20998;&#26512;&#22238;&#31572;&#20197;&#35782;&#21035;&#30446;&#26631;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#23545;&#19981;&#21516;LLM&#23478;&#26063;&#30340;&#35821;&#20041;&#24773;&#22659;&#26377;&#25152;&#20102;&#35299;&#65292;&#24182;&#25552;&#39640;&#20102;72%&#30340;&#20934;&#30830;&#24230;&#26469;&#36776;&#21035;LLM&#30340;&#23478;&#26063;&#12290;</title><link>https://arxiv.org/abs/2408.02871</link><description>&lt;p&gt;
Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20351;&#29992;&#36827;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;&#8220;Hide and Seek&#8221;&#30340;&#31639;&#27861;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#19968;&#20010;&#20027;&#25511;&#65288;Auditor&#65289;LLM&#29983;&#25104;&#37492;&#21035;&#24615;&#38382;&#39064;&#65292;&#36741;&#21161;&#65288;Detective&#65289;LLM&#20998;&#26512;&#22238;&#31572;&#20197;&#35782;&#21035;&#30446;&#26631;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#23545;&#19981;&#21516;LLM&#23478;&#26063;&#30340;&#35821;&#20041;&#24773;&#22659;&#26377;&#25152;&#20102;&#35299;&#65292;&#24182;&#25552;&#39640;&#20102;72%&#30340;&#20934;&#30830;&#24230;&#26469;&#36776;&#21035;LLM&#30340;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02871v1 Announce Type: cross  Abstract: As content generated by Large Language Model (LLM) has grown exponentially, the ability to accurately identify and fingerprint such text has become increasingly crucial. In this work, we introduce a novel black-box approach for fingerprinting LLMs, achieving an impressive 72% accuracy in identifying the correct family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of LLMs. We present an evolutionary strategy that leverages the capabilities of one LLM to discover the most salient features for identifying other LLMs. Our method employs a unique "Hide and Seek" algorithm, where an Auditor LLM generates discriminative prompts, and a Detective LLM analyzes the responses to fingerprint the target models. This approach not only demonstrates the feasibility of LLM-driven model identification but also reveals insights into the semantic manifolds of different LLM families. By iteratively refining prompts through in-context learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;VisionUnite&#30340;&#20840;&#26032;&#38754;&#21521;&#30524;&#31185;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22686;&#24378;&#20102;&#20020;&#24202;&#30693;&#35782;&#65292;&#24182;&#22312;1.24&#20159;&#22270;&#20687;&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;VisionUnite&#22312;&#35786;&#26029;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#29983;&#25104;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-4V&#21644;Gemini Pro&#65292;&#24182;&#19988;&#22312;&#22810;&#30149;&#31181;&#35786;&#26029;&#12289;&#20020;&#24202;&#35299;&#37322;&#21644;&#30149;&#20154;&#20114;&#21160;&#31561;&#20020;&#24202;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#36741;&#21161;&#19987;&#31185;&#21307;&#24072;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.02865</link><description>&lt;p&gt;
VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;VisionUnite&#30340;&#20840;&#26032;&#38754;&#21521;&#30524;&#31185;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22686;&#24378;&#20102;&#20020;&#24202;&#30693;&#35782;&#65292;&#24182;&#22312;1.24&#20159;&#22270;&#20687;&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;VisionUnite&#22312;&#35786;&#26029;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#29983;&#25104;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-4V&#21644;Gemini Pro&#65292;&#24182;&#19988;&#22312;&#22810;&#30149;&#31181;&#35786;&#26029;&#12289;&#20020;&#24202;&#35299;&#37322;&#21644;&#30149;&#20154;&#20114;&#21160;&#31561;&#20020;&#24202;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#36741;&#21161;&#19987;&#31185;&#21307;&#24072;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02865v1 Announce Type: cross  Abstract: The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly vers
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#36947;&#24503;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20165;&#22312;&#21333;&#19968;&#26102;&#38388;&#28857;&#23545;&#20010;&#20154;&#36827;&#34892;&#36947;&#24503;&#20559;&#22909;&#35843;&#26597;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#20854;&#30495;&#23454;&#20215;&#20540;&#35266;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#36947;&#24503;&#21028;&#26029;&#21487;&#33021;&#22240;&#26102;&#38388;&#12289;&#24773;&#32490;&#25110;&#29615;&#22659;&#22240;&#32032;&#30340;&#21464;&#21270;&#32780;&#20986;&#29616;&#19981;&#31283;&#23450;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#30740;&#31350;&#21442;&#19982;&#24335;&#20262;&#29702;AI&#24037;&#20855;&#30340;&#35843;&#26597;&#26041;&#27861;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2408.02862</link><description>&lt;p&gt;
On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02862
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#36947;&#24503;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20165;&#22312;&#21333;&#19968;&#26102;&#38388;&#28857;&#23545;&#20010;&#20154;&#36827;&#34892;&#36947;&#24503;&#20559;&#22909;&#35843;&#26597;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#20854;&#30495;&#23454;&#20215;&#20540;&#35266;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#36947;&#24503;&#21028;&#26029;&#21487;&#33021;&#22240;&#26102;&#38388;&#12289;&#24773;&#32490;&#25110;&#29615;&#22659;&#22240;&#32032;&#30340;&#21464;&#21270;&#32780;&#20986;&#29616;&#19981;&#31283;&#23450;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#30740;&#31350;&#21442;&#19982;&#24335;&#20262;&#29702;AI&#24037;&#20855;&#30340;&#35843;&#26597;&#26041;&#27861;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02862v1 Announce Type: cross  Abstract: Preference elicitation frameworks feature heavily in the research on participatory ethical AI tools and provide a viable mechanism to enquire and incorporate the moral values of various stakeholders. As part of the elicitation process, surveys about moral preferences, opinions, and judgments are typically administered only once to each participant. This methodological practice is reasonable if participants' responses are stable over time such that, all other relevant factors being held constant, their responses today will be the same as their responses to the same questions at a later time. However, we do not know how often that is the case. It is possible that participants' true moral preferences change, are subject to temporary moods or whims, or are influenced by environmental factors we don't track. If participants' moral responses are unstable in such ways, it would raise important methodological and theoretical issues for how par
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Madeleine&#65288;Madeleine&#65289;&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#21253;&#25324;&#20813;&#30123;&#32452;&#21270;&#22312;&#20869;&#30340;&#22810;&#26631;&#35760;&#26579;&#33394;&#20999;&#29255;&#20013;&#33719;&#21462;&#20016;&#23500;&#30340;&#20219;&#21153;&#26080;&#20851;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#28023;&#24179;&#38754;&#24040;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#23398;&#20064;&#20840;&#38754;&#30340;&#12289;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2408.02859</link><description>&lt;p&gt;
Multistain Pretraining for Slide Representation Learning in Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Madeleine&#65288;Madeleine&#65289;&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#21253;&#25324;&#20813;&#30123;&#32452;&#21270;&#22312;&#20869;&#30340;&#22810;&#26631;&#35760;&#26579;&#33394;&#20999;&#29255;&#20013;&#33719;&#21462;&#20016;&#23500;&#30340;&#20219;&#21153;&#26080;&#20851;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#28023;&#24179;&#38754;&#24040;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#23398;&#20064;&#20840;&#38754;&#30340;&#12289;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02859v1 Announce Type: cross  Abstract: Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&amp;E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224 x 224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce Madeleine, a multimodal pretraining strategy for slide representation
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616; recurrent neural network&#65292;&#36890;&#36807;&#22312; spintronic oscillator &#19978;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807; backpropagation through time&#65288;BPTT&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#20302;&#33021;&#28304;&#25104;&#26412;&#19979;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25991;&#31456;&#36890;&#36807;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#35299;&#20915;&#24207;&#21015;&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#19982;&#36719;&#20214;&#32593;&#32476;&#30456;&#24403;&#30340; $89.83\pm2.91~\%$ &#20934;&#30830;&#29575;&#12290;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36873;&#25321;&#25391;&#33633;&#22120;&#30340;&#26102;&#38388;&#24120;&#25968;&#20197;&#21450;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#25351;&#23548;&#65292;&#20197;&#20415;&#36866;&#24212;&#19981;&#21516;&#30340;&#36755;&#20837;&#26102;&#38388;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02835</link><description>&lt;p&gt;
Training a multilayer dynamical spintronic network with standard machine learning tools to perform time series classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616; recurrent neural network&#65292;&#36890;&#36807;&#22312; spintronic oscillator &#19978;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807; backpropagation through time&#65288;BPTT&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#20302;&#33021;&#28304;&#25104;&#26412;&#19979;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25991;&#31456;&#36890;&#36807;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#35299;&#20915;&#24207;&#21015;&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#19982;&#36719;&#20214;&#32593;&#32476;&#30456;&#24403;&#30340; $89.83\pm2.91~\%$ &#20934;&#30830;&#29575;&#12290;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36873;&#25321;&#25391;&#33633;&#22120;&#30340;&#26102;&#38388;&#24120;&#25968;&#20197;&#21450;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#25351;&#23548;&#65292;&#20197;&#20415;&#36866;&#24212;&#19981;&#21516;&#30340;&#36755;&#20837;&#26102;&#38388;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02835v1 Announce Type: cross  Abstract: The ability to process time-series at low energy cost is critical for many applications. Recurrent neural network, which can perform such tasks, are computationally expensive when implementing in software on conventional computers. Here we propose to implement a recurrent neural network in hardware using spintronic oscillators as dynamical neurons. Using numerical simulations, we build a multi-layer network and demonstrate that we can use backpropagation through time (BPTT) and standard machine learning tools to train this network. Leveraging the transient dynamics of the spintronic oscillators, we solve the sequential digits classification task with $89.83\pm2.91~\%$ accuracy, as good as the equivalent software network. We devise guidelines on how to choose the time constant of the oscillators as well as hyper-parameters of the network to adapt to different input time scales.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;REGAI&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#33258;&#29983;&#25104;&#25110;&#25163;&#21160;&#39044;&#35774;&#30340;&#35268;&#21017;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#12290;REGAI&#33021;&#22815;&#25913;&#21892;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#19988;&#24050;&#32463;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2408.02811</link><description>&lt;p&gt;
Development of REGAI: Rubric Enabled Generative Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;REGAI&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#33258;&#29983;&#25104;&#25110;&#25163;&#21160;&#39044;&#35774;&#30340;&#35268;&#21017;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#12290;REGAI&#33021;&#22815;&#25913;&#21892;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#19988;&#24050;&#32463;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02811v1 Announce Type: new  Abstract: This paper presents and evaluates a new retrieval augmented generation (RAG) and large language model (LLM)-based artificial intelligence (AI) technique: rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics, which can be created manually or automatically by the system, to enhance the performance of LLMs for evaluation purposes. REGAI improves on the performance of both classical LLMs and RAG-based LLM techniques. This paper describes REGAI, presents data regarding its performance and discusses several possible application areas for the technology.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#32467;&#21512;&#31038;&#20250;&#35821;&#35328;&#23398;&#29702;&#35770;&#20013;&#30340;&#8220;&#38754;&#23376;&#34892;&#20026;&#65288;face acts&#65289;&#8221;&#21644;&#8220;&#31036;&#35980;&#8221;&#65288;politeness&#65289;&#20998;&#26512;&#35770;&#22495;&#30340;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;Wikipedia talk&#39029;&#38754;&#30340;&#26032;&#25991;&#26412;&#36164;&#28304;&#26469;&#35757;&#32451;face act&#26631;&#31614;&#22120;&#12290;&#25991;&#31456;&#36890;&#36807;&#20998;&#26512;&#24615;&#21035;&#21644;&#26435;&#21147;&#22312;Wikipedia&#32534;&#36753;&#20043;&#38388;&#30340;&#35752;&#35770;&#20013;&#65292;&#21457;&#29616;&#22899;&#24615;&#32534;&#36753;&#22312;&#35821;&#35328;&#19978;&#26356;&#21152;&#31036;&#35980;&#65292;&#24182;&#19988;&#22312;&#33258;&#25105;&#35878;&#36874;&#26041;&#38754;&#30340;&#35805;&#35821;&#26356;&#22810;&#12290;&#21516;&#26102;&#65292;&#24403;&#20165;&#32771;&#34385;&#26377;&#26435;&#21147;&#30340;&#32534;&#36753;&#26102;&#65292;&#36825;&#31181;&#24615;&#21035;&#24046;&#24322;&#20960;&#20046;&#28040;&#22833;&#12290;</title><link>https://arxiv.org/abs/2408.02798</link><description>&lt;p&gt;
Examining Gender and Power on Wikipedia Through Face and Politeness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#32467;&#21512;&#31038;&#20250;&#35821;&#35328;&#23398;&#29702;&#35770;&#20013;&#30340;&#8220;&#38754;&#23376;&#34892;&#20026;&#65288;face acts&#65289;&#8221;&#21644;&#8220;&#31036;&#35980;&#8221;&#65288;politeness&#65289;&#20998;&#26512;&#35770;&#22495;&#30340;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;Wikipedia talk&#39029;&#38754;&#30340;&#26032;&#25991;&#26412;&#36164;&#28304;&#26469;&#35757;&#32451;face act&#26631;&#31614;&#22120;&#12290;&#25991;&#31456;&#36890;&#36807;&#20998;&#26512;&#24615;&#21035;&#21644;&#26435;&#21147;&#22312;Wikipedia&#32534;&#36753;&#20043;&#38388;&#30340;&#35752;&#35770;&#20013;&#65292;&#21457;&#29616;&#22899;&#24615;&#32534;&#36753;&#22312;&#35821;&#35328;&#19978;&#26356;&#21152;&#31036;&#35980;&#65292;&#24182;&#19988;&#22312;&#33258;&#25105;&#35878;&#36874;&#26041;&#38754;&#30340;&#35805;&#35821;&#26356;&#22810;&#12290;&#21516;&#26102;&#65292;&#24403;&#20165;&#32771;&#34385;&#26377;&#26435;&#21147;&#30340;&#32534;&#36753;&#26102;&#65292;&#36825;&#31181;&#24615;&#21035;&#24046;&#24322;&#20960;&#20046;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02798v1 Announce Type: cross  Abstract: We propose a framework for analyzing discourse by combining two interdependent concepts from sociolinguistic theory: face acts and politeness. While politeness has robust existing tools and data, face acts are less resourced. We introduce a new corpus created by annotating Wikipedia talk pages with face acts and we use this to train a face act tagger. We then employ our framework to study how face and politeness interact with gender and power in discussions between Wikipedia editors. Among other findings, we observe that female Wikipedians are not only more polite, which is consistent with prior studies, but that this difference corresponds with significantly more language directed at humbling aspects of their own face. Interestingly, the distinction nearly vanishes once limiting to editors with administrative power.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#25968;&#25454;&#25366;&#25496;&#24037;&#20855;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#20013;&#35270;&#35273;&#20803;&#32032;&#30340;&#20856;&#22411;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#27010;&#25324;&#25968;&#25454;&#65292;&#24182;&#23545;&#20110;&#29305;&#23450;&#26631;&#31614;&#65288;&#22914;&#22320;&#29702;&#20301;&#32622;&#12289;&#26102;&#38388;&#25139;&#12289;&#26631;&#31614;&#25110;&#30142;&#30149;&#65289;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02752</link><description>&lt;p&gt;
Diffusion Models as Data Mining Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#25968;&#25454;&#25366;&#25496;&#24037;&#20855;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#20013;&#35270;&#35273;&#20803;&#32032;&#30340;&#20856;&#22411;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#27010;&#25324;&#25968;&#25454;&#65292;&#24182;&#23545;&#20110;&#29305;&#23450;&#26631;&#31614;&#65288;&#22914;&#22320;&#29702;&#20301;&#32622;&#12289;&#26102;&#38388;&#25139;&#12289;&#26631;&#31614;&#25110;&#30142;&#30149;&#65289;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02752v1 Announce Type: new  Abstract: This paper demonstrates how to use generative models trained for image synthesis as tools for visual data mining. Our insight is that since contemporary generative models learn an accurate representation of their training data, we can use them to summarize the data by mining for visual patterns. Concretely, we show that after finetuning conditional diffusion models to synthesize images from a specific dataset, we can use these models to define a typicality measure on that dataset. This measure assesses how typical visual elements are for different data labels, such as geographic location, time stamps, semantic labels, or even the presence of a disease. This analysis-by-synthesis approach to data mining has two key advantages. First, it scales much better than traditional correspondence-based approaches since it does not require explicitly comparing all pairs of visual elements. Second, while most previous works on visual data mining focu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;dataset distillation&#26041;&#27861;&#8212;&#8212;Multi-domain Distribution Matching&#65288;MDM&#65289;&#65292;&#26088;&#22312;&#38024;&#23545;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#29305;&#27530;&#22495;&#29305;&#24615;&#21644;&#36328;&#22495;&#20998;&#26512;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;discrete Fourier transform&#21644;series fusion&#23545;&#22823;&#35268;&#27169;&#20449;&#21495;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#65292;&#29983;&#25104;&#23567;&#22411;&#20294;&#24615;&#33021;&#32500;&#25345;&#30340;&#39640;&#25928;&#21512;&#25104;dataset&#65292;&#20026;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#65288;AMR&#65289;&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#20026;&#23454;&#29992;&#30340;&#25968;&#25454;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02714</link><description>&lt;p&gt;
MDM: Advancing Multi-Domain Distribution Matching for Automatic Modulation Recognition Dataset Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;dataset distillation&#26041;&#27861;&#8212;&#8212;Multi-domain Distribution Matching&#65288;MDM&#65289;&#65292;&#26088;&#22312;&#38024;&#23545;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#29305;&#27530;&#22495;&#29305;&#24615;&#21644;&#36328;&#22495;&#20998;&#26512;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;discrete Fourier transform&#21644;series fusion&#23545;&#22823;&#35268;&#27169;&#20449;&#21495;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#65292;&#29983;&#25104;&#23567;&#22411;&#20294;&#24615;&#33021;&#32500;&#25345;&#30340;&#39640;&#25928;&#21512;&#25104;dataset&#65292;&#20026;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#65288;AMR&#65289;&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#20026;&#23454;&#29992;&#30340;&#25968;&#25454;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02714v1 Announce Type: cross  Abstract: Recently, deep learning technology has been successfully introduced into Automatic Modulation Recognition (AMR) tasks. However, the success of deep learning is all attributed to the training on large-scale datasets. Such a large amount of data brings huge pressure on storage, transmission and model training. In order to solve the problem of large amount of data, some researchers put forward the method of data distillation, which aims to compress large training data into smaller synthetic datasets to maintain its performance. While numerous data distillation techniques have been developed within the realm of image processing, the unique characteristics of signals set them apart. Signals exhibit distinct features across various domains, necessitating specialized approaches for their analysis and processing. To this end, a novel dataset distillation method--Multi-domain Distribution Matching (MDM) is proposed. MDM employs the Discrete Fou
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#29992;&#20110;&#22686;&#24378;&#29616;&#23454;&#25163;&#26415;&#23548;&#33322;&#20013;&#30340;&#32452;&#32455;&#21464;&#24418;&#27169;&#22411;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#20445;&#25163;&#26415;&#36807;&#31243;&#20013;&#39044;&#21046;&#32452;&#32455;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#35299;&#21078;&#32467;&#26500;&#30456;&#21305;&#37197;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25163;&#26415;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2408.02713</link><description>&lt;p&gt;
A Review on Organ Deformation Modeling Approaches for Reliable Surgical Navigation using Augmented Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#29992;&#20110;&#22686;&#24378;&#29616;&#23454;&#25163;&#26415;&#23548;&#33322;&#20013;&#30340;&#32452;&#32455;&#21464;&#24418;&#27169;&#22411;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#20445;&#25163;&#26415;&#36807;&#31243;&#20013;&#39044;&#21046;&#32452;&#32455;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#35299;&#21078;&#32467;&#26500;&#30456;&#21305;&#37197;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25163;&#26415;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02713v1 Announce Type: cross  Abstract: Augmented Reality (AR) holds the potential to revolutionize surgical procedures by allowing surgeons to visualize critical structures within the patient's body. This is achieved through superimposing preoperative organ models onto the actual anatomy. Challenges arise from dynamic deformations of organs during surgery, making preoperative models inadequate for faithfully representing intraoperative anatomy. To enable reliable navigation in augmented surgery, modeling of intraoperative deformation to obtain an accurate alignment of the preoperative organ model with the intraoperative anatomy is indispensable. Despite the existence of various methods proposed to model intraoperative organ deformation, there are still few literature reviews that systematically categorize and summarize these approaches. This review aims to fill this gap by providing a comprehensive and technical-oriented overview of modeling methods for intraoperative organ
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Phonetic PosteriorGrams&#65288;PPG&#65289;&#30340;&#33258;&#21160;&#35821;&#38899;&#37492;&#21035;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#23558;&#35821;&#38899;&#36716;&#25442;&#20026;&#19981;&#21516;&#36523;&#20221;&#30340;&#38899;&#39057;&#65292;&#21363;&#20351;&#26159;&#22312;&#32463;&#36807;&#37325;&#26032;&#21512;&#25104;&#30340;&#38899;&#39057;&#26465;&#20214;&#19979;&#65292;&#20063;&#26080;&#27861;&#34987;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#35782;&#21035;&#20986;&#21407;&#22987;&#35828;&#35805;&#20154;&#30340;&#36523;&#20221;&#12290;&#36825;&#31181;&#25216;&#26415;&#20026;&#23186;&#20307;&#30417;&#25511;&#21644;&#26032;&#38395;&#24037;&#20316;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#24212;&#29992;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02712</link><description>&lt;p&gt;
Automatic Voice Identification after Speech Resynthesis using PPG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Phonetic PosteriorGrams&#65288;PPG&#65289;&#30340;&#33258;&#21160;&#35821;&#38899;&#37492;&#21035;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#23558;&#35821;&#38899;&#36716;&#25442;&#20026;&#19981;&#21516;&#36523;&#20221;&#30340;&#38899;&#39057;&#65292;&#21363;&#20351;&#26159;&#22312;&#32463;&#36807;&#37325;&#26032;&#21512;&#25104;&#30340;&#38899;&#39057;&#26465;&#20214;&#19979;&#65292;&#20063;&#26080;&#27861;&#34987;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#35782;&#21035;&#20986;&#21407;&#22987;&#35828;&#35805;&#20154;&#30340;&#36523;&#20221;&#12290;&#36825;&#31181;&#25216;&#26415;&#20026;&#23186;&#20307;&#30417;&#25511;&#21644;&#26032;&#38395;&#24037;&#20316;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#24212;&#29992;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02712v1 Announce Type: cross  Abstract: Speech resynthesis is a generic task for which we want to synthesize audio with another audio as input, which finds applications for media monitors and journalists.Among different tasks addressed by speech resynthesis, voice conversion preserves the linguistic information while modifying the identity of the speaker, and speech edition preserves the identity of the speaker but some words are modified.In both cases, we need to disentangle speaker and phonetic contents in intermediate representations.Phonetic PosteriorGrams (PPG) are a frame-level probabilistic representation of phonemes, and are usually considered speaker-independent.This paper presents a PPG-based speech resynthesis system.A perceptive evaluation assesses that it produces correct audio quality.Then, we demonstrate that an automatic speaker verification model is not able to recover the source speaker after re-synthesis with PPG, even when the model is trained on syntheti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#20239;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#30340;&#25991;&#26412;&#26465;&#20214;&#40723;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#25991;&#26412;&#19982;&#40723;&#28857;&#36827;&#34892;&#32852;&#21512;&#32534;&#30721;&#65292;&#24182;&#19982;CLIP&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#19982;&#38899;&#20048;&#27169;&#24577;&#30340;&#32039;&#23494;&#20851;&#32852;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;&#22810;&#20998;&#36776;&#29575;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;MultiResolutionLSTM&#65289;&#65292;&#26088;&#22312;&#27169;&#25311;&#38899;&#20048;&#30340;&#22810;&#20998;&#36776;&#29575;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26080;&#26465;&#20214;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21152;&#36895;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#19982;&#29983;&#25104;&#30340;&#40723;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65288;&#20108;&#36827;&#21046;&#38050;&#29748;&#21367;&#24088;&#21644;&#28508;&#22312;&#31354;&#38388;&#65289;&#26469;&#23637;&#31034;&#29983;&#25104;&#40723;&#28857;&#30340;&#21407;&#21019;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02711</link><description>&lt;p&gt;
Text Conditioned Symbolic Drumbeat Generation using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#20239;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#30340;&#25991;&#26412;&#26465;&#20214;&#40723;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#25991;&#26412;&#19982;&#40723;&#28857;&#36827;&#34892;&#32852;&#21512;&#32534;&#30721;&#65292;&#24182;&#19982;CLIP&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#19982;&#38899;&#20048;&#27169;&#24577;&#30340;&#32039;&#23494;&#20851;&#32852;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24341;&#20837;&#20102;&#22810;&#20998;&#36776;&#29575;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;MultiResolutionLSTM&#65289;&#65292;&#26088;&#22312;&#27169;&#25311;&#38899;&#20048;&#30340;&#22810;&#20998;&#36776;&#29575;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26080;&#26465;&#20214;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21152;&#36895;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#19982;&#29983;&#25104;&#30340;&#40723;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65288;&#20108;&#36827;&#21046;&#38050;&#29748;&#21367;&#24088;&#21644;&#28508;&#22312;&#31354;&#38388;&#65289;&#26469;&#23637;&#31034;&#29983;&#25104;&#40723;&#28857;&#30340;&#21407;&#21019;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02711v1 Announce Type: cross  Abstract: This study introduces a text-conditioned approach to generating drumbeats with Latent Diffusion Models (LDMs). It uses informative conditioning text extracted from training data filenames. By pretraining a text and drumbeat encoder through contrastive learning within a multimodal network, aligned following CLIP, we align the modalities of text and music closely. Additionally, we examine an alternative text encoder based on multihot text encodings. Inspired by musics multi-resolution nature, we propose a novel LSTM variant, MultiResolutionLSTM, designed to operate at various resolutions independently. In common with recent LDMs in the image space, it speeds up the generation process by running diffusion in a latent space provided by a pretrained unconditional autoencoder. We demonstrate the originality and variety of the generated drumbeats by measuring distance (both over binary pianorolls and in the latent space) versus the training d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#37319;&#29992;boxology&#26694;&#26550;&#23545;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#35268;&#21017;&#25512;&#29702;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#35774;&#35745;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#21644;&#23545;&#27604;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#20248;&#21270;&#20581;&#24247;&#25252;&#29702;AI&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.02709</link><description>&lt;p&gt;
Enhancing Medical Learning and Reasoning Systems: A Boxology-Based Comparative Analysis of Design Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#37319;&#29992;boxology&#26694;&#26550;&#23545;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#35268;&#21017;&#25512;&#29702;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#35774;&#35745;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#21644;&#23545;&#27604;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#20248;&#21270;&#20581;&#24247;&#25252;&#29702;AI&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02709v1 Announce Type: cross  Abstract: This study analyzes hybrid AI systems' design patterns and their effectiveness in clinical decision-making using the boxology framework. It categorizes and copares various architectures combining machine learning and rule-based reasoning to provide insights into their structural foundations and healthcare applications. Addressing two main questions, how to categorize these systems againts established design patterns and how to extract insights through comparative analysis, the study uses design patterns from software engineering to understand and optimize healthcare AI systems. Boxology helps identify commonalities and create reusable solutions, enhancing these systems' scalability, reliability, and performance. Five primary architectures are examined: REML, MLRB, RBML, RMLT, and PERML. Each has unique strengths and weaknesses, highlighting the need for tailored approaches in clinical tasks. REML excels in high-accuracy prediction for 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24555;&#29031;&#38598;&#21512;&#27010;&#24565;&#24212;&#29992;&#20110;&#26080;&#36127;&#20363;&#30693;&#35782;&#22270;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21019;&#24314;&#36127;&#26679;&#26412;&#24182;&#21033;&#29992;&#20808;&#21069;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#35757;&#32451;&#38598;&#21512;&#23454;&#29616;&#24615;&#33021;&#31283;&#23450;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2408.02707</link><description>&lt;p&gt;
SnapE -- Training Snapshot Ensembles of Link Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24555;&#29031;&#38598;&#21512;&#27010;&#24565;&#24212;&#29992;&#20110;&#26080;&#36127;&#20363;&#30693;&#35782;&#22270;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21019;&#24314;&#36127;&#26679;&#26412;&#24182;&#21033;&#29992;&#20808;&#21069;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#35757;&#32451;&#38598;&#21512;&#23454;&#29616;&#24615;&#33021;&#31283;&#23450;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02707v1 Announce Type: cross  Abstract: Snapshot ensembles have been widely used in various fields of prediction. They allow for training an ensemble of prediction models at the cost of training a single one. They are known to yield more robust predictions by creating a set of diverse base models. In this paper, we introduce an approach to transfer the idea of snapshot ensembles to link prediction models in knowledge graphs. Moreover, since link prediction in knowledge graphs is a setup without explicit negative examples, we propose a novel training loop that iteratively creates negative examples using previous snapshot models. An evaluation with four base models across four datasets shows that this approach constantly outperforms the single model approach, while keeping the training time constant.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayesian Kolmogorov Arnold Networks&#65288;BKANs&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;Kolmogorov Arnold Networks&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#33268;&#21147;&#20110;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#21644;&#32771;&#34385;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#12290;&#22312;&#21307;&#30103;&#35786;&#26029;&#30340;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;BKANs&#22312;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#20026;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24515;&#39044;&#27979;&#21644;&#20915;&#31574;&#36793;&#30028;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2408.02706</link><description>&lt;p&gt;
Bayesian Kolmogorov Arnold Networks (Bayesian_KANs): A Probabilistic Approach to Enhance Accuracy and Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayesian Kolmogorov Arnold Networks&#65288;BKANs&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;Kolmogorov Arnold Networks&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#33268;&#21147;&#20110;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#21644;&#32771;&#34385;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#12290;&#22312;&#21307;&#30103;&#35786;&#26029;&#30340;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;BKANs&#22312;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#20026;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24515;&#39044;&#27979;&#21644;&#20915;&#31574;&#36793;&#30028;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02706v1 Announce Type: cross  Abstract: Because of its strong predictive skills, deep learning has emerged as an essential tool in many industries, including healthcare. Traditional deep learning models, on the other hand, frequently lack interpretability and omit to take prediction uncertainty into account two crucial components of clinical decision making. In order to produce explainable and uncertainty aware predictions, this study presents a novel framework called Bayesian Kolmogorov Arnold Networks (BKANs), which combines the expressive capacity of Kolmogorov Arnold Networks with Bayesian inference. We employ BKANs on two medical datasets, which are widely used benchmarks for assessing machine learning models in medical diagnostics: the Pima Indians Diabetes dataset and the Cleveland Heart Disease dataset. Our method provides useful insights into prediction confidence and decision boundaries and outperforms traditional deep learning models in terms of prediction accurac
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35889;&#31232;&#30095;&#21270;&#31639;&#27861;PSNE&#65292;&#33021;&#22815;&#23454;&#29616;&#32593;&#32476;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#23616;&#37096;&#25512;&#21160;&#26426;&#21046;&#65292;&#20165;&#38656;&#25191;&#34892;&#19968;&#27425;&#23616;&#37096;&#25512;&#21160;&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#25972;&#20010;PPR&#30697;&#38453;&#30340;&#31934;&#30830;&#20272;&#35745;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02705</link><description>&lt;p&gt;
PSNE: Efficient Spectral Sparsification Algorithms for Scaling Network Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35889;&#31232;&#30095;&#21270;&#31639;&#27861;PSNE&#65292;&#33021;&#22815;&#23454;&#29616;&#32593;&#32476;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#23616;&#37096;&#25512;&#21160;&#26426;&#21046;&#65292;&#20165;&#38656;&#25191;&#34892;&#19968;&#27425;&#23616;&#37096;&#25512;&#21160;&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#25972;&#20010;PPR&#30697;&#38453;&#30340;&#31934;&#30830;&#20272;&#35745;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02705v1 Announce Type: cross  Abstract: Network embedding has numerous practical applications and has received extensive attention in graph learning, which aims at mapping vertices into a low-dimensional and continuous dense vector space by preserving the underlying structural properties of the graph. Many network embedding methods have been proposed, among which factorization of the Personalized PageRank (PPR for short) matrix has been empirically and theoretically well supported recently. However, several fundamental issues cannot be addressed. (1) Existing methods invoke a seminal Local Push subroutine to approximate \textit{a single} row or column of the PPR matrix. Thus, they have to execute $n$ ($n$ is the number of nodes) Local Push subroutines to obtain a provable PPR matrix, resulting in prohibitively high computational costs for large $n$. (2) The PPR matrix has limited power in capturing the structural similarity between vertices, leading to performance degradatio
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STGCNDT&#30340;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#32479;&#19968;&#30340;&#22270;&#24352;&#37327;&#21367;&#31215;&#32593;&#32476;&#24182;&#24341;&#20837;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#25442;&#26041;&#26696;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#38598;&#20013;&#22320;&#22788;&#29702;&#21644;&#25552;&#21462;&#22797;&#26434;&#21160;&#24577;&#22270;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2408.02704</link><description>&lt;p&gt;
Spatial-temporal Graph Convolutional Networks with Diversified Transformation for Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STGCNDT&#30340;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#32479;&#19968;&#30340;&#22270;&#24352;&#37327;&#21367;&#31215;&#32593;&#32476;&#24182;&#24341;&#20837;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#25442;&#26041;&#26696;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#38598;&#20013;&#22320;&#22788;&#29702;&#21644;&#25552;&#21462;&#22797;&#26434;&#21160;&#24577;&#22270;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02704v1 Announce Type: cross  Abstract: Dynamic graphs (DG) are often used to describe evolving interactions between nodes in real-world applications. Temporal patterns are a natural feature of DGs and are also key to representation learning. However, existing dynamic GCN models are mostly composed of static GCNs and sequence modules, which results in the separation of spatiotemporal information and cannot effectively capture complex temporal patterns in DGs. To address this problem, this study proposes a spatial-temporal graph convolutional networks with diversified transformation (STGCNDT), which includes three aspects: a) constructing a unified graph tensor convolutional network (GTCN) using tensor M-products without the need to represent spatiotemporal information separately; b) introducing three transformation schemes in GTCN to model complex temporal patterns to aggregate temporal information; and c) constructing an ensemble of diversified transformation schemes to obt
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#21442;&#25968;&#24230;&#37327;$m_{\lambda}$&#30340;&#39118;&#38505;&#20013;&#24615;&#22810;&#39033;&#30446;&#24211;&#23384;&#38382;&#39064;&#65292;&#20854;&#20013;&#38656;&#27714;&#21521;&#37327;&#30340;&#32452;&#25104;&#37096;&#20998;&#20026;&#27169;&#31946;&#21464;&#37327;&#12290;&#36890;&#36807;&#25552;&#20986;$m_{\lambda}$-&#26399;&#26395;&#20540;&#30340;&#27010;&#24565;&#65292;&#25991;&#31456;&#26500;&#24314;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#35299;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#25991;&#31456;&#36890;&#36807;&#35813;&#27169;&#22411;&#26377;&#25928;&#35745;&#31639;&#20102;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23545;&#20248;&#21270;&#38382;&#39064;&#30340;&#20855;&#20307;&#21270;&#24471;&#21040;&#20102;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#20844;&#24335;&#65292;&#20174;&#32780;&#20026;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#24211;&#23384;&#31649;&#29702;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.02700</link><description>&lt;p&gt;
Inventory problems and the parametric measure $m_{\lambda}$
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#21442;&#25968;&#24230;&#37327;$m_{\lambda}$&#30340;&#39118;&#38505;&#20013;&#24615;&#22810;&#39033;&#30446;&#24211;&#23384;&#38382;&#39064;&#65292;&#20854;&#20013;&#38656;&#27714;&#21521;&#37327;&#30340;&#32452;&#25104;&#37096;&#20998;&#20026;&#27169;&#31946;&#21464;&#37327;&#12290;&#36890;&#36807;&#25552;&#20986;$m_{\lambda}$-&#26399;&#26395;&#20540;&#30340;&#27010;&#24565;&#65292;&#25991;&#31456;&#26500;&#24314;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#35299;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#25991;&#31456;&#36890;&#36807;&#35813;&#27169;&#22411;&#26377;&#25928;&#35745;&#31639;&#20102;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23545;&#20248;&#21270;&#38382;&#39064;&#30340;&#20855;&#20307;&#21270;&#24471;&#21040;&#20102;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#20844;&#24335;&#65292;&#20174;&#32780;&#20026;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#24211;&#23384;&#31649;&#29702;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02700v1 Announce Type: cross  Abstract: The credibility theory was introduced by B. Liu as a new way to describe the fuzzy uncertainty. The credibility measure is the fundamental notion of the credibility theory. Recently, L.Yang and K. Iwamura extended the credibility measure by defining the parametric measure $m_{\lambda}$ ($\lambda$ is a real parameter in the interval $[0,1]$ and for $\lambda= 1/2$ we obtain as a particular case the notion of credibility measure). By using the $m_{\lambda}$-measure, we studied in this paper a risk neutral multi-item inventory problem. Our construction generalizes the credibilistic inventory model developed by Y. Li and Y. Liu in 2019. In our model, the components of demand vector are fuzzy variables and the maximization problem is formulated by using the notion of $m_{\lambda}$-expected value. We shall prove a general formula for the solution of optimization problem, from which we obtained effective formulas for computing the optimal solu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepNetBeam&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;SciML&#65288;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65289;&#26041;&#27861;&#20998;&#26512;&#20855;&#26377;&#21151;&#33021; gradients&#30340;&#23380;&#38553;&#29366; beams&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36830;&#32493;&#20989;&#25968;&#27169;&#22411;&#26448;&#26009;&#30340;&#23646;&#24615;&#26469;&#24212;&#23545;&#19981;&#35268;&#21017;&#37197;&#32622;&#65292;&#24182;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#19979;&#30340;&#36755;&#20986;&#65306;PINN&#65288;Physics-Informed Neural Network&#65289;&#22522;&#20110;&#21521;&#37327;&#26041;&#27861;&#65292;DEM&#65288;Deep Energy Method&#65289;&#22522;&#20110;&#33021;&#37327;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#31070;&#32463;&#25805;&#20316;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#19977;&#31181;&#26041;&#27861;&#22343;&#22522;&#20110;&#36830;&#32493;&#24418;&#24335;&#65292;&#20197;&#32593;&#32476;&#36755;&#20986;&#20316;&#20026;&#20301;&#31227;&#22330;&#30340;&#36817;&#20284;&#65292;&#24182;&#25512;&#23548;&#20102;&#25903;&#37197;beams&#34892;&#20026;&#30340;&#26041;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#25805;&#20316;&#22120;&#30340;&#39044;&#27979;&#22312;&#22788;&#29702;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#19979;&#30340;&#23380;&#38553;beams&#21709;&#24212;&#26102;&#65292;&#23545;&#20219;&#24847;&#30340; porosity&#20998;&#24067;&#21644;&#26799;&#24230;&#29305;&#24615;&#37117;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02698</link><description>&lt;p&gt;
DeepNetBeam: A Framework for the Analysis of Functionally Graded Porous Beams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepNetBeam&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;SciML&#65288;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65289;&#26041;&#27861;&#20998;&#26512;&#20855;&#26377;&#21151;&#33021; gradients&#30340;&#23380;&#38553;&#29366; beams&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36830;&#32493;&#20989;&#25968;&#27169;&#22411;&#26448;&#26009;&#30340;&#23646;&#24615;&#26469;&#24212;&#23545;&#19981;&#35268;&#21017;&#37197;&#32622;&#65292;&#24182;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#19979;&#30340;&#36755;&#20986;&#65306;PINN&#65288;Physics-Informed Neural Network&#65289;&#22522;&#20110;&#21521;&#37327;&#26041;&#27861;&#65292;DEM&#65288;Deep Energy Method&#65289;&#22522;&#20110;&#33021;&#37327;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#31070;&#32463;&#25805;&#20316;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#19977;&#31181;&#26041;&#27861;&#22343;&#22522;&#20110;&#36830;&#32493;&#24418;&#24335;&#65292;&#20197;&#32593;&#32476;&#36755;&#20986;&#20316;&#20026;&#20301;&#31227;&#22330;&#30340;&#36817;&#20284;&#65292;&#24182;&#25512;&#23548;&#20102;&#25903;&#37197;beams&#34892;&#20026;&#30340;&#26041;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#25805;&#20316;&#22120;&#30340;&#39044;&#27979;&#22312;&#22788;&#29702;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#19979;&#30340;&#23380;&#38553;beams&#21709;&#24212;&#26102;&#65292;&#23545;&#20219;&#24847;&#30340; porosity&#20998;&#24067;&#21644;&#26799;&#24230;&#29305;&#24615;&#37117;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02698v1 Announce Type: cross  Abstract: This study investigates different Scientific Machine Learning (SciML) approaches for the analysis of functionally graded (FG) porous beams and compares them under a new framework. The beam material properties are assumed to vary as an arbitrary continuous function. The methods consider the output of a neural network/operator as an approximation to the displacement fields and derive the equations governing beam behavior based on the continuum formulation. The methods are implemented in the framework and formulated by three approaches: (a) the vector approach leads to a Physics-Informed Neural Network (PINN), (b) the energy approach brings about the Deep Energy Method (DEM), and (c) the data-driven approach, which results in a class of Neural Operator methods. Finally, a neural operator has been trained to predict the response of the porous beam with functionally graded material under any porosity distribution pattern and any arbitrary t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20174;&#26377;&#25928;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;RePU&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#28145;&#23618;&#32467;&#26500;&#20013;&#23384;&#22312;&#20540;&#29190;&#28856;&#25110;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;RePU&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#35757;&#32451;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02697</link><description>&lt;p&gt;
Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20174;&#26377;&#25928;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;RePU&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#28145;&#23618;&#32467;&#26500;&#20013;&#23384;&#22312;&#20540;&#29190;&#28856;&#25110;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;RePU&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#35757;&#32451;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02697v1 Announce Type: cross  Abstract: The Rectified Power Unit (RePU) activation functions, unlike the Rectified Linear Unit (ReLU), have the advantage of being a differentiable function when constructing neural networks. However, it can be experimentally observed when deep layers are stacked, neural networks constructed with RePU encounter critical issues. These issues include the values exploding or vanishing and failure of training. And these happen regardless of the hyperparameter initialization. From the perspective of effective theory, we aim to identify the causes of this phenomenon and propose a new activation function that retains the advantages of RePU while overcoming its drawbacks.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;Distribution-Level Memory Recall&#65288;DMR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#22320;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#22312;&#20998;&#24067;&#27700;&#24179;&#19978;&#25311;&#21512;&#26087;&#30693;&#35782;&#30340;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#22312;&#26087;&#20219;&#21153;&#20998;&#31867;&#36793;&#30028;&#19978;&#33021;&#22815;&#36991;&#20813;&#28151;&#28102;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2408.02695</link><description>&lt;p&gt;
Distribution-Level Memory Recall for Continual Learning: Preserving Knowledge and Avoiding Confusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;Distribution-Level Memory Recall&#65288;DMR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#22320;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#22312;&#20998;&#24067;&#27700;&#24179;&#19978;&#25311;&#21512;&#26087;&#30693;&#35782;&#30340;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#22312;&#26087;&#20219;&#21153;&#20998;&#31867;&#36793;&#30028;&#19978;&#33021;&#22815;&#36991;&#20813;&#28151;&#28102;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02695v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn new data without forgetting previously learned knowledge. The key to achieving this goal is to avoid confusion at the feature level, i.e., avoiding confusion within old tasks and between new and old tasks. Previous prototype-based CL methods generate pseudo features for old knowledge replay by adding Gaussian noise to the centroids of old classes. However, the distribution in the feature space exhibits anisotropy during the incremental process, which prevents the pseudo features from faithfully reproducing the distribution of old knowledge in the feature space, leading to confusion in classification boundaries within old tasks. To address this issue, we propose the Distribution-Level Memory Recall (DMR) method, which uses a Gaussian mixture model to precisely fit the feature distribution of old knowledge at the distribution level and generate pseudo features in
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Kolmogorov-Arnold Networks&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#36164;&#20135;&#23450;&#20215;&#30340;&#22240;&#23376;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#21450;&#20854;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#19978;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#20026;&#36164;&#20135;&#23450;&#20215;&#20013;&#30340;&#22240;&#23376;&#26292;&#38706;&#24314;&#27169;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#34920;&#36798;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#27169;&#22411;&#35299;&#37322;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#26500;&#24314;&#30340;&#31574;&#30053;&#22312;&#25237;&#36164;&#24066;&#22330;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25910;&#30410;&#39118;&#38505;&#27604;&#65292;&#20026;&#25237;&#36164;&#31649;&#29702;&#23454;&#36341;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.02694</link><description>&lt;p&gt;
KAN based Autoencoders for Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Kolmogorov-Arnold Networks&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#36164;&#20135;&#23450;&#20215;&#30340;&#22240;&#23376;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#21450;&#20854;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#19978;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#20026;&#36164;&#20135;&#23450;&#20215;&#20013;&#30340;&#22240;&#23376;&#26292;&#38706;&#24314;&#27169;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#34920;&#36798;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#27169;&#22411;&#35299;&#37322;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#26500;&#24314;&#30340;&#31574;&#30053;&#22312;&#25237;&#36164;&#24066;&#22330;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25910;&#30410;&#39118;&#38505;&#27604;&#65292;&#20026;&#25237;&#36164;&#31649;&#29702;&#23454;&#36341;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02694v1 Announce Type: cross  Abstract: Inspired by recent advances in Kolmogorov-Arnold Networks (KANs), we introduce a novel approach to latent factor conditional asset pricing models. While previous machine learning applications in asset pricing have predominantly used Multilayer Perceptrons with ReLU activation functions to model latent factor exposures, our method introduces a KAN-based autoencoder which surpasses MLP models in both accuracy and interpretability. Our model offers enhanced flexibility in approximating exposures as nonlinear functions of asset characteristics, while simultaneously providing users with an intuitive framework for interpreting latent factors. Empirical backtesting demonstrates our model's superior ability to explain cross-sectional risk exposures. Moreover, long-short portfolios constructed using our model's predictions achieve higher Sharpe ratios, highlighting its practical value in investment management.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-PIC&#30340;&#38761;&#21629;&#24615;&#31890;&#23376;-&#22312;-&#32454;&#32990;&#27169;&#25311;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20195;&#26367;&#20256;&#32479;&#30340;&#31890;&#23376;-&#22312;-&#32454;&#32990;&#27169;&#25311;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26680;&#34701;&#21512;&#30740;&#31350;&#30340;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#28608;&#20809;-&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.02693</link><description>&lt;p&gt;
Diff-PIC: Revolutionizing Particle-In-Cell Simulation for Advancing Nuclear Fusion with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-PIC&#30340;&#38761;&#21629;&#24615;&#31890;&#23376;-&#22312;-&#32454;&#32990;&#27169;&#25311;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20195;&#26367;&#20256;&#32479;&#30340;&#31890;&#23376;-&#22312;-&#32454;&#32990;&#27169;&#25311;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26680;&#34701;&#21512;&#30740;&#31350;&#30340;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#28608;&#20809;-&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02693v1 Announce Type: cross  Abstract: Sustainable energy is a crucial global challenge, and recent breakthroughs in nuclear fusion ignition underscore the potential of harnessing energy extracted from nuclear fusion in everyday life, thereby drawing significant attention to fusion ignition research, especially Laser-Plasma Interaction (LPI). Unfortunately, the complexity of LPI at ignition scale renders theory-based analysis nearly impossible -- instead, it has to rely heavily on Particle-in-Cell (PIC) simulations, which is extremely computationally intensive, making it a major bottleneck in advancing fusion ignition. In response, this work introduces Diff-PIC, a novel paradigm that leverages conditional diffusion models as a computationally efficient alternative to PIC simulations for generating high-fidelity scientific data. Specifically, we design a distillation paradigm to distill the physical patterns captured by PIC simulations into diffusion models, demonstrating bo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#22359;&#27880;&#24847;&#27169;&#22359;&#65288;CBAM&#65289;&#65292;&#26469;&#25913;&#36827;&#20102;&#20351;&#29992;CNNs&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#23545;&#25705;&#27931;&#21733;&#40065;&#25289;&#20122;&#27700;&#31995;&#26410;&#23450;&#29702;&#27969;&#22495;&#30340;&#27946;&#27700;&#25935;&#24863;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;CNN&#27169;&#22411;&#65292;&#22914;ResNet18&#12289;DenseNet121&#21644;Xception&#20013;&#20998;&#21035;&#20351;&#29992;&#20102;CBAM&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#36825;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#29305;&#21035;&#26159;&#22312;DenseNet121&#27169;&#22411;&#20013;&#65292;CBAM&#22312;&#27599;&#20010;&#21367;&#31215;&#22359;&#20013;&#22343;&#24471;&#21040;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;</title><link>https://arxiv.org/abs/2408.02692</link><description>&lt;p&gt;
Attention is all you need for an improved CNN-based flash flood susceptibility modeling. The case of the ungauged Rheraya watershed, Morocco
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#22359;&#27880;&#24847;&#27169;&#22359;&#65288;CBAM&#65289;&#65292;&#26469;&#25913;&#36827;&#20102;&#20351;&#29992;CNNs&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#23545;&#25705;&#27931;&#21733;&#40065;&#25289;&#20122;&#27700;&#31995;&#26410;&#23450;&#29702;&#27969;&#22495;&#30340;&#27946;&#27700;&#25935;&#24863;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;CNN&#27169;&#22411;&#65292;&#22914;ResNet18&#12289;DenseNet121&#21644;Xception&#20013;&#20998;&#21035;&#20351;&#29992;&#20102;CBAM&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#36825;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#29305;&#21035;&#26159;&#22312;DenseNet121&#27169;&#22411;&#20013;&#65292;CBAM&#22312;&#27599;&#20010;&#21367;&#31215;&#22359;&#20013;&#22343;&#24471;&#21040;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02692v1 Announce Type: cross  Abstract: Effective flood hazard management requires evaluating and predicting flash flood susceptibility. Convolutional neural networks (CNNs) are commonly used for this task but face issues like gradient explosion and overfitting. This study explores the use of an attention mechanism, specifically the convolutional block attention module (CBAM), to enhance CNN models for flash flood susceptibility in the ungauged Rheraya watershed, a flood prone region. We used ResNet18, DenseNet121, and Xception as backbone architectures, integrating CBAM at different locations. Our dataset included 16 conditioning factors and 522 flash flood inventory points. Performance was evaluated using accuracy, precision, recall, F1-score, and the area under the curve (AUC) of the receiver operating characteristic (ROC). Results showed that CBAM significantly improved model performance, with DenseNet121 incorporating CBAM in each convolutional block achieving the best 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Symmetric Graph Contrastive Learning&#65288;SGCL&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#30830;&#20445;&#20102;&#25968;&#25454;&#22686;&#24191;&#30340;&#23545;&#27604;&#35270;&#22270;&#19981;&#20250;&#22240;&#20026;&#22833;&#21435;&#21407;&#26377;&#30340;&#36830;&#25509;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02691</link><description>&lt;p&gt;
Symmetric Graph Contrastive Learning against Noisy Views for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Symmetric Graph Contrastive Learning&#65288;SGCL&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#30830;&#20445;&#20102;&#25968;&#25454;&#22686;&#24191;&#30340;&#23545;&#27604;&#35270;&#22270;&#19981;&#20250;&#22240;&#20026;&#22833;&#21435;&#21407;&#26377;&#30340;&#36830;&#25509;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02691v1 Announce Type: cross  Abstract: Graph Contrastive Learning (GCL) leverages data augmentation techniques to produce contrasting views, enhancing the accuracy of recommendation systems through learning the consistency between contrastive views. However, existing augmentation methods, such as directly perturbing interaction graph (e.g., node/edge dropout), may interfere with the original connections and generate poor contrasting views, resulting in sub-optimal performance. In this paper, we define the views that share only a small amount of information with the original graph due to poor data augmentation as noisy views (i.e., the last 20% of the views with a cosine similarity value less than 0.1 to the original view). We demonstrate through detailed experiments that noisy views will significantly degrade recommendation performance. Further, we propose a model-agnostic Symmetric Graph Contrastive Learning (SGCL) method with theoretical guarantees to address this issue. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STPS&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#20256;&#24863;&#30340;&#38271;&#26399;&#20132;&#36890;&#39044;&#25253;&#38382;&#39064;&#65292;&#26088;&#22312;&#38477;&#20302;&#20132;&#36890;&#31649;&#29702;&#20013;&#20256;&#24863;&#22120;&#30340;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#35813;&#27169;&#22411;&#21019;&#26032;&#22320;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#23884;&#20837;&#25216;&#26415;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;&#21644;&#19981;&#21487;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#23436;&#20840;&#20256;&#24863;&#30340;&#29615;&#22659;&#20013;&#39044;&#27979;&#22797;&#26434;&#26102;&#31354;&#20851;&#32852;&#30340;&#38271;&#26399;&#20132;&#36890;&#27969;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#22122;&#22768;&#21644;&#19981;&#35268;&#24459;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#22914;&#36947;&#36335;&#23553;&#38381;&#12290;

&#20197;&#19978;&#24635;&#32467;&#21253;&#25324;&#20102;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#20013;&#30340;&#8220;&#37096;&#20998;&#20256;&#24863;&#8221;&#12289;&#8220;&#38271;&#26399;&#20132;&#36890;&#39044;&#25253;&#8221;&#12289;&#8220;&#27169;&#22411;&#8221;&#12289;&#8220;&#23884;&#20837;&#25216;&#26415;&#8221;&#31561;&#20851;&#38190;&#35789;&#12290;</title><link>https://arxiv.org/abs/2408.02689</link><description>&lt;p&gt;
Spatio-Temporal Partial Sensing Forecast for Long-term Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STPS&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#20256;&#24863;&#30340;&#38271;&#26399;&#20132;&#36890;&#39044;&#25253;&#38382;&#39064;&#65292;&#26088;&#22312;&#38477;&#20302;&#20132;&#36890;&#31649;&#29702;&#20013;&#20256;&#24863;&#22120;&#30340;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#35813;&#27169;&#22411;&#21019;&#26032;&#22320;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#23884;&#20837;&#25216;&#26415;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;&#21644;&#19981;&#21487;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#23436;&#20840;&#20256;&#24863;&#30340;&#29615;&#22659;&#20013;&#39044;&#27979;&#22797;&#26434;&#26102;&#31354;&#20851;&#32852;&#30340;&#38271;&#26399;&#20132;&#36890;&#27969;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#22122;&#22768;&#21644;&#19981;&#35268;&#24459;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#22914;&#36947;&#36335;&#23553;&#38381;&#12290;

&#20197;&#19978;&#24635;&#32467;&#21253;&#25324;&#20102;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#20013;&#30340;&#8220;&#37096;&#20998;&#20256;&#24863;&#8221;&#12289;&#8220;&#38271;&#26399;&#20132;&#36890;&#39044;&#25253;&#8221;&#12289;&#8220;&#27169;&#22411;&#8221;&#12289;&#8220;&#23884;&#20837;&#25216;&#26415;&#8221;&#31561;&#20851;&#38190;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02689v1 Announce Type: cross  Abstract: Traffic forecasting uses recent measurements by sensors installed at chosen locations to forecast the future road traffic. Existing work either assumes all locations are equipped with sensors or focuses on short-term forecast. This paper studies partial sensing traffic forecast of long-term traffic, assuming sensors only at some locations. The study is important in lowering the infrastructure investment cost in traffic management since deploying sensors at all locations could incur prohibitively high cost. However, the problem is challenging due to the unknown distribution at unsensed locations, the intricate spatio-temporal correlation in long-term forecasting, as well as noise in data and irregularities in traffic patterns (e.g., road closure). We propose a Spatio-Temporal Partial Sensing (STPS) forecast model for long-term traffic prediction, with several novel contributions, including a rank-based embedding technique to capture irr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20013;&#38388;&#34701;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02686</link><description>&lt;p&gt;
A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02686
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20013;&#38388;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02686v1 Announce Type: cross  Abstract: Deep learning has revolutionized biomedical research by providing sophisticated methods to handle complex, high-dimensional data. Multimodal deep learning (MDL) further enhances this capability by integrating diverse data types such as imaging, textual data, and genetic information, leading to more robust and accurate predictive models. In MDL, differently from early and late fusion methods, intermediate fusion stands out for its ability to effectively combine modality-specific features during the learning process. This systematic review aims to comprehensively analyze and formalize current intermediate fusion methods in biomedical applications. We investigate the techniques employed, the challenges faced, and potential future directions for advancing intermediate fusion methods. Additionally, we introduce a structured notation to enhance the understanding and application of these methods beyond the biomedical domain. Our findings are 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24635;&#32467;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#20809;&#30005;&#24212;&#29992;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#65292;&#20027;&#35201;&#21253;&#25324;&#20102;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#19982;&#20809;&#30005;&#30828;&#20214;&#23454;&#29616;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#27169;&#22411;&#35774;&#35745;&#20197;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#20219;&#21153;&#25191;&#34892;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#20809;&#30005;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#22810;&#20010;&#30456;&#20851;&#24212;&#29992;&#65292;&#36825;&#23558;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#29702;&#35770;&#21644;&#25216;&#26415;&#19978;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.02685</link><description>&lt;p&gt;
Artificial Neural Networks for Photonic Applications: From Algorithms to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24635;&#32467;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#20809;&#30005;&#24212;&#29992;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#65292;&#20027;&#35201;&#21253;&#25324;&#20102;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#19982;&#20809;&#30005;&#30828;&#20214;&#23454;&#29616;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#27169;&#22411;&#35774;&#35745;&#20197;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#20219;&#21153;&#25191;&#34892;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#20809;&#30005;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#22810;&#20010;&#30456;&#20851;&#24212;&#29992;&#65292;&#36825;&#23558;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#29702;&#35770;&#21644;&#25216;&#26415;&#19978;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02685v1 Announce Type: cross  Abstract: This tutorial-review on applications of artificial neural networks in photonics targets a broad audience, ranging from optical research and engineering communities to computer science and applied mathematics. We focus here on the research areas at the interface between these disciplines, attempting to find the right balance between technical details specific to each domain and overall clarity. First, we briefly recall key properties and peculiarities of some core neural network types, which we believe are the most relevant to photonics, also linking the layer's theoretical design to some photonics hardware realizations. After that, we elucidate the question of how to fine-tune the selected model's design to perform the required task with optimized accuracy. Then, in the review part, we discuss recent developments and progress for several selected applications of neural networks in photonics, including multiple aspects relevant to optic
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#24405;&#38899;&#35013;&#32622;&#26469;&#35760;&#24405;&#20329;&#25140;&#32773;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20307;&#39564;&#65292;&#24182;&#29992;&#20110;&#26500;&#24314;&#26032;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#25509;&#36817;&#30495;&#23454;&#20154;&#29289;&#24773;&#24863;&#21644;&#29983;&#29702;&#21453;&#24212;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;AI&#25216;&#26415;&#30340;&#36924;&#30495;&#24230;&#21644;&#33258;&#28982;&#20132;&#20114;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02680</link><description>&lt;p&gt;
Recording First-person Experiences to Build a New Type of Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#24405;&#38899;&#35013;&#32622;&#26469;&#35760;&#24405;&#20329;&#25140;&#32773;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20307;&#39564;&#65292;&#24182;&#29992;&#20110;&#26500;&#24314;&#26032;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#25509;&#36817;&#30495;&#23454;&#20154;&#29289;&#24773;&#24863;&#21644;&#29983;&#29702;&#21453;&#24212;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;AI&#25216;&#26415;&#30340;&#36924;&#30495;&#24230;&#21644;&#33258;&#28982;&#20132;&#20114;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02680v1 Announce Type: new  Abstract: Foundation models have had a big impact in recent years and billions of dollars are being invested in them in the current AI boom. The more popular ones, such as Chat-GPT, are trained on large amounts of Internet data. However, it is becoming apparent that this data is likely to be exhausted soon, and technology companies are looking for new sources of data to train the next generation of foundation models.   Reinforcement learning, RAG, prompt engineering and cognitive modelling are often used to fine-tune and augment the behaviour of foundation models. These techniques have been used to replicate people, such as Caryn Marjorie. These chatbots are not based on people's actual emotional and physiological responses to their environment, so they are, at best, a surface-level approximation to the characters they are imitating.   To address these issues, we have developed a recording rig that captures what the wearer is seeing and hearing as
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24739;&#32773;&#20013;&#24515;&#30340;&#36328;&#23398;&#31185;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#20102;&#20020;&#24202;&#25968;&#25454;&#12289;&#24739;&#32773;&#25253;&#21578;&#25968;&#25454;&#21644;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#21033;&#29992;&#22810;&#20195;&#29702;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21644;&#20248;&#21270;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#19981;&#26029;&#23398;&#20064;&#30340;&#21307;&#30103;&#31995;&#32479;&#65292;&#20197;&#20419;&#36827;&#25968;&#23383;&#20581;&#24247;&#21019;&#26032;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26356;&#22909;&#22320;&#26381;&#21153;&#20110;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2408.02677</link><description>&lt;p&gt;
Patient-centered data science: an integrative framework for evaluating and predicting clinical outcomes in the digital health era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24739;&#32773;&#20013;&#24515;&#30340;&#36328;&#23398;&#31185;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#20102;&#20020;&#24202;&#25968;&#25454;&#12289;&#24739;&#32773;&#25253;&#21578;&#25968;&#25454;&#21644;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#21033;&#29992;&#22810;&#20195;&#29702;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21644;&#20248;&#21270;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#19981;&#26029;&#23398;&#20064;&#30340;&#21307;&#30103;&#31995;&#32479;&#65292;&#20197;&#20419;&#36827;&#25968;&#23383;&#20581;&#24247;&#21019;&#26032;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26356;&#22909;&#22320;&#26381;&#21153;&#20110;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02677v1 Announce Type: cross  Abstract: This study proposes a novel, integrative framework for patient-centered data science in the digital health era. We developed a multidimensional model that combines traditional clinical data with patient-reported outcomes, social determinants of health, and multi-omic data to create comprehensive digital patient representations. Our framework employs a multi-agent artificial intelligence approach, utilizing various machine learning techniques including large language models, to analyze complex, longitudinal datasets. The model aims to optimize multiple patient outcomes simultaneously while addressing biases and ensuring generalizability. We demonstrate how this framework can be implemented to create a learning healthcare system that continuously refines strategies for optimal patient care. This approach has the potential to significantly improve the translation of digital health innovations into real-world clinical benefits, addressing 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#25968;&#25454;&#24211;&#20013;&#22522;&#20110;&#30524;&#24213;&#22270;&#20687;&#30340;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23384;&#22312;&#30340;&#24040;&#22823;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#25972;&#20307;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#35780;&#20272;&#20013;&#24515;&#30340;&#20010;&#20307;&#20173;&#28982;&#38754;&#20020;&#26174;&#33879;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25581;&#31034;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#36807;&#31243;&#20013;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#32780;&#19988;&#23545;&#27604;&#20102;&#22810;&#31181;&#29616;&#26377;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38024;&#23545;&#19981;&#21516;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#32531;&#35299;&#26041;&#27861;&#25928;&#26524;&#24046;&#24322;&#29978;&#22823;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#23450;&#21046;&#21270;&#30340;&#32531;&#35299;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26368;&#32456;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32531;&#35299;&#25163;&#27573;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#25928;&#26524;&#26377;&#38480;&#65292;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#23545;&#20844;&#24179;&#24615;&#31639;&#27861;&#30340;&#39640;&#35201;&#27714;&#21644;&#32487;&#32493;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02676</link><description>&lt;p&gt;
On Biases in a UK Biobank-based Retinal Image Classification Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#25968;&#25454;&#24211;&#20013;&#22522;&#20110;&#30524;&#24213;&#22270;&#20687;&#30340;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23384;&#22312;&#30340;&#24040;&#22823;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#25972;&#20307;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#35780;&#20272;&#20013;&#24515;&#30340;&#20010;&#20307;&#20173;&#28982;&#38754;&#20020;&#26174;&#33879;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25581;&#31034;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#36807;&#31243;&#20013;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#32780;&#19988;&#23545;&#27604;&#20102;&#22810;&#31181;&#29616;&#26377;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38024;&#23545;&#19981;&#21516;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#32531;&#35299;&#26041;&#27861;&#25928;&#26524;&#24046;&#24322;&#29978;&#22823;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#23450;&#21046;&#21270;&#30340;&#32531;&#35299;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26368;&#32456;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32531;&#35299;&#25163;&#27573;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#25928;&#26524;&#26377;&#38480;&#65292;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#23545;&#20844;&#24179;&#24615;&#31639;&#27861;&#30340;&#39640;&#35201;&#27714;&#21644;&#32487;&#32493;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02676v1 Announce Type: cross  Abstract: Recent work has uncovered alarming disparities in the performance of machine learning models in healthcare. In this study, we explore whether such disparities are present in the UK Biobank fundus retinal images by training and evaluating a disease classification model on these images. We assess possible disparities across various population groups and find substantial differences despite strong overall performance of the model. In particular, we discover unfair performance for certain assessment centres, which is surprising given the rigorous data standardisation protocol. We compare how these differences emerge and apply a range of existing bias mitigation methods to each one. A key insight is that each disparity has unique properties and responds differently to the mitigation methods. We also find that these methods are largely unable to enhance fairness, highlighting the need for better bias mitigation methods tailored to the specif
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counterfactual Shapley Values&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36879;&#26126;&#24230;&#65292;&#36890;&#36807;&#32467;&#21512;&#21453;&#20107;&#23454;&#20998;&#26512;&#19982;Shapley&#20540;&#26469;&#37327;&#21270;&#21644;&#27604;&#36739;&#19981;&#21516;&#29366;&#24577;&#32500;&#24230;&#23545;&#19981;&#21516;&#21160;&#20316;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29305;&#24449;&#20540;&#20989;&#25968;&#65292;&#22914;&#8220;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#21644;&#8220;&#24179;&#22343;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#26102;&#30340;&#38590;&#39064;&#65292;&#24182;&#35745;&#31639;&#20102;Shapley&#20540;&#20197;&#35780;&#20272;&#26368;&#20248;&#21160;&#20316;&#21644;&#38750;&#26368;&#20248;&#21160;&#20316;&#20043;&#38388;&#30340;&#36129;&#29486;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;GridWorld&#12289;FrozenLake&#21644;Taxi&#31561;&#22810;&#20010;RL&#39046;&#22495;&#20869;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;CSV&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#22797;&#26434;RL&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#24230;&#65292;&#36824;&#37327;&#21270;&#20102;&#19981;&#21516;&#20915;&#31574;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2408.02529</link><description>&lt;p&gt;
Counterfactual Shapley Values for Explaining Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counterfactual Shapley Values&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36879;&#26126;&#24230;&#65292;&#36890;&#36807;&#32467;&#21512;&#21453;&#20107;&#23454;&#20998;&#26512;&#19982;Shapley&#20540;&#26469;&#37327;&#21270;&#21644;&#27604;&#36739;&#19981;&#21516;&#29366;&#24577;&#32500;&#24230;&#23545;&#19981;&#21516;&#21160;&#20316;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29305;&#24449;&#20540;&#20989;&#25968;&#65292;&#22914;&#8220;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#21644;&#8220;&#24179;&#22343;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#26102;&#30340;&#38590;&#39064;&#65292;&#24182;&#35745;&#31639;&#20102;Shapley&#20540;&#20197;&#35780;&#20272;&#26368;&#20248;&#21160;&#20316;&#21644;&#38750;&#26368;&#20248;&#21160;&#20316;&#20043;&#38388;&#30340;&#36129;&#29486;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;GridWorld&#12289;FrozenLake&#21644;Taxi&#31561;&#22810;&#20010;RL&#39046;&#22495;&#20869;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;CSV&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#22797;&#26434;RL&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#24230;&#65292;&#36824;&#37327;&#21270;&#20102;&#19981;&#21516;&#20915;&#31574;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02529v1 Announce Type: new  Abstract: This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the ``Counterfactual Difference Characteristic Value" and the ``Average Counterfactual Difference Characteristic Value." These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#23454;&#38469;shellcodes&#30340;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#24694;&#24847;&#36719;&#20214;&#20195;&#30721;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#24341;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19981;&#21516;&#32780;&#26377;&#25152;&#24046;&#24322;&#65292;&#34920;&#26126;&#20102;&#36866;&#24403;&#19978;&#19979;&#25991;&#23545;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#22686;&#21152;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26410;&#24102;&#26469;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#19968;&#20010;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36824;&#33021;&#35782;&#21035;&#24182;&#25490;&#38500;&#19981;&#24517;&#35201;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#26356;&#31934;&#30830;&#22320;&#29983;&#25104;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2408.02402</link><description>&lt;p&gt;
Enhancing AI-based Generation of Software Exploits with Contextual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#23454;&#38469;shellcodes&#30340;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#24694;&#24847;&#36719;&#20214;&#20195;&#30721;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#24341;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19981;&#21516;&#32780;&#26377;&#25152;&#24046;&#24322;&#65292;&#34920;&#26126;&#20102;&#36866;&#24403;&#19978;&#19979;&#25991;&#23545;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#22686;&#21152;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26410;&#24102;&#26469;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#19968;&#20010;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36824;&#33021;&#35782;&#21035;&#24182;&#25490;&#38500;&#19981;&#24517;&#35201;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#26356;&#31934;&#30830;&#22320;&#29983;&#25104;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02402v1 Announce Type: cross  Abstract: This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;DiReCT&#30340;&#20020;&#24202;&#31508;&#35760;&#35786;&#26029;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#23427;&#36890;&#36807;521&#20221;&#20020;&#24202;&#31508;&#35760;&#30340;&#32454;&#33268;&#26631;&#27880;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#23545;&#20154;&#31867;&#21307;&#29983;&#30340;&#35299;&#37322;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#35786;&#26029;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;LLMs&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2408.01933</link><description>&lt;p&gt;
DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;DiReCT&#30340;&#20020;&#24202;&#31508;&#35760;&#35786;&#26029;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#23427;&#36890;&#36807;521&#20221;&#20020;&#24202;&#31508;&#35760;&#30340;&#32454;&#33268;&#26631;&#27880;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#23545;&#20154;&#31867;&#21307;&#29983;&#30340;&#35299;&#37322;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#35786;&#26029;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;LLMs&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01933v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FBSDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#39057;&#29575;&#24102;&#26367;&#25442;&#21040;&#25193;&#25955;&#29305;&#24449;&#20013;&#65292;&#29992;&#20110;&#39640;&#24230;&#21487;&#25511;&#30340;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32763;&#35793;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#22312;&#32447;&#20248;&#21270;&#65292;&#30452;&#25509;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2408.00998</link><description>&lt;p&gt;
FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FBSDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#39057;&#29575;&#24102;&#26367;&#25442;&#21040;&#25193;&#25955;&#29305;&#24449;&#20013;&#65292;&#29992;&#20110;&#39640;&#24230;&#21487;&#25511;&#30340;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32763;&#35793;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#22312;&#32447;&#20248;&#21270;&#65292;&#30452;&#25509;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00998v2 Announce Type: replace  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I gener
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#22240;&#32032;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25237;&#36164;&#30456;&#32467;&#21512;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#19982;28&#23478;&#20844;&#21496;&#21512;&#20316;&#24320;&#21457;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#35780;&#20272;AI&#24212;&#29992;&#30340;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#21487;&#25345;&#32493;&#30340;AI&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00965</link><description>&lt;p&gt;
Integrating ESG and AI: A Comprehensive Responsible AI Assessment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#22240;&#32032;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25237;&#36164;&#30456;&#32467;&#21512;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#19982;28&#23478;&#20844;&#21496;&#21512;&#20316;&#24320;&#21457;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#35780;&#20272;AI&#24212;&#29992;&#30340;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#21487;&#25345;&#32493;&#30340;AI&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00965v2 Announce Type: replace  Abstract: Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;Segment Anything Model 2 (SAM 2)&#22312;2D&#21644;3D&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25193;&#23637;&#12290;&#36890;&#36807;&#23545;18&#31181;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;SAM 2&#22312;&#22810;&#24103;3D&#20998;&#21106;&#21644;&#21333;&#24103;2D&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;CT&#12289;MRI&#12289;PET&#31561;3D&#21644;X&#20809;&#12289;&#36229;&#22768;&#31561;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.00756</link><description>&lt;p&gt;
Segment anything model 2: an application to 2D and 3D medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;Segment Anything Model 2 (SAM 2)&#22312;2D&#21644;3D&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25193;&#23637;&#12290;&#36890;&#36807;&#23545;18&#31181;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;SAM 2&#22312;&#22810;&#24103;3D&#20998;&#21106;&#21644;&#21333;&#24103;2D&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;CT&#12289;MRI&#12289;PET&#31561;3D&#21644;X&#20809;&#12289;&#36229;&#22768;&#31561;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00756v2 Announce Type: replace  Abstract: Segment Anything Model (SAM) has gained significant attention because of its ability to segment varous objects in images given a prompt. The recently developed SAM 2 has extended this ability to video inputs. This opens an opportunity to apply SAM to 3D images, one of the fundamental tasks in the medical imaging field. In this paper, we extensively evaluate SAM 2's ability to segment both 2D and 3D medical images by first collecting 18 medical imaging datasets, including common 3D modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) as well as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of SAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are provided to one or multiple slice(s) selected from the volume, and (2) single-frame 2D segmentation, where prompts are provided to each slice. The former is only applicable to 3D modaliti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;SentenceVAE&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23618;&#38598;&#25104;&#20197;&#25552;&#21319;&#21477;&#23376;&#32423;&#21035;&#30340;LLMs&#65288;SLLMs&#65289;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#39044;&#27979;&#23454;&#29616;&#20102;&#36895;&#24230;&#25552;&#21319;&#21644;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#25903;&#25345;&#38271;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2408.00655</link><description>&lt;p&gt;
SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;SentenceVAE&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23618;&#38598;&#25104;&#20197;&#25552;&#21319;&#21477;&#23376;&#32423;&#21035;&#30340;LLMs&#65288;SLLMs&#65289;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#39044;&#27979;&#23454;&#29616;&#20102;&#36895;&#24230;&#25552;&#21319;&#21644;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#25903;&#25345;&#38271;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00655v3 Announce Type: replace  Abstract: Current large language models (LLMs) primarily utilize next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a Sentence Encoder and a Sentence Decoder. The Sentence Encoder can effectively condense the information within a sentence into a singular token, while the Sentence Decoder can reconstruct this compressed token back into sentence. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference method. In addition, the SentenceVAE module of SLLMs can maintain the integrity of the original semantic content by segmenting the context into sentences, thereby improving accuracy 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;MoMa&#26550;&#26500;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#24335;&#24863;&#30693;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20219;&#24847;&#24207;&#21015;&#30340;&#28151;&#21512;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#26089;&#26399;&#34701;&#21512;&#39044;&#35757;&#32451;&#12290;MoMa&#36890;&#36807;&#23558;&#19987;&#23478;&#27169;&#22359;&#20998;&#20026;&#19987;&#23646;&#22788;&#29702;&#29305;&#23450;&#27169;&#24335;&#32452;&#21035;&#30340;&#27169;&#24335;&#24863;&#30693;&#19987;&#23478;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.21770</link><description>&lt;p&gt;
MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;MoMa&#26550;&#26500;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#24335;&#24863;&#30693;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20219;&#24847;&#24207;&#21015;&#30340;&#28151;&#21512;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#26089;&#26399;&#34701;&#21512;&#39044;&#35757;&#32451;&#12290;MoMa&#36890;&#36807;&#23558;&#19987;&#23478;&#27169;&#22359;&#20998;&#20026;&#19987;&#23646;&#22788;&#29702;&#29305;&#23450;&#27169;&#24335;&#32452;&#21035;&#30340;&#27169;&#24335;&#24863;&#30693;&#19987;&#23478;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21770v2 Announce Type: replace  Abstract: We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7x overall, with 2.6x for text and 5.2x for image processing compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3x overall FLOPs savings (3x for text
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;eSPARQL&#30340;&#22235;&#20540;&#36923;&#36753;&#26597;&#35810;&#35821;&#35328;&#65292;&#29992;&#20110;&#22788;&#29702;RDF-star&#30693;&#35782;&#22270;&#20013;&#30340; agnostic&#65288;&#19981;&#21487;&#30693;&#35770;&#65289;&#21644;atheistic&#65288;&#26080;&#31070;&#35770;&#65289;&#20449;&#24565;&#12290;eSPARQL&#36890;&#36807;&#25193;&#23637;SPARQL-star&#65292;&#20801;&#35768;&#26597;&#35810;&#20197;&#19981;&#21516;&#30340;&#20449;&#24565;&#25191;&#34892;&#65292;&#20197;&#22788;&#29702;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#34920;&#36798;&#22235;&#20010;&#29992;&#20363;&#26597;&#35810;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#22810;&#28304;&#21644;&#20914;&#31361;&#20449;&#24565;&#26041;&#38754;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2407.21483</link><description>&lt;p&gt;
eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;eSPARQL&#30340;&#22235;&#20540;&#36923;&#36753;&#26597;&#35810;&#35821;&#35328;&#65292;&#29992;&#20110;&#22788;&#29702;RDF-star&#30693;&#35782;&#22270;&#20013;&#30340; agnostic&#65288;&#19981;&#21487;&#30693;&#35770;&#65289;&#21644;atheistic&#65288;&#26080;&#31070;&#35770;&#65289;&#20449;&#24565;&#12290;eSPARQL&#36890;&#36807;&#25193;&#23637;SPARQL-star&#65292;&#20801;&#35768;&#26597;&#35810;&#20197;&#19981;&#21516;&#30340;&#20449;&#24565;&#25191;&#34892;&#65292;&#20197;&#22788;&#29702;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#34920;&#36798;&#22235;&#20010;&#29992;&#20363;&#26597;&#35810;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#22810;&#28304;&#21644;&#20914;&#31361;&#20449;&#24565;&#26041;&#38754;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21483v3 Announce Type: replace  Abstract: Over the past few years, we have seen the emergence of large knowledge graphs combining information from multiple sources. Sometimes, this information is provided in the form of assertions about other assertions, defining contexts where assertions are valid. A recent extension to RDF which admits statements over statements, called RDF-star, is in revision to become a W3C standard. However, there is no proposal for a semantics of these RDF-star statements nor a built-in facility to operate over them. In this paper, we propose a query language for epistemic RDF-star metadata based on a four-valued logic, called eSPARQL. Our proposed query language extends SPARQL-star, the query language for RDF-star, with a new type of FROM clause to facilitate operating with multiple and sometimes conflicting beliefs. We show that the proposed query language can express four use case queries, including the following features: (i) querying the belief o
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PolyRAG&#30340;&#22810;&#23618;&#27425;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;RAG&#26694;&#26550;&#20013;&#26500;&#24314;&#30693;&#35782;&#37329;&#23383;&#22612;&#65292;&#21253;&#25324;&#22885;&#39039;&#23398;&#12289;&#30693;&#35782;&#22270;&#21644;&#25991;&#26412;&#23618;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36328;&#23618;&#30693;&#35782;&#26356;&#26032;&#30340;&#21160;&#24577;&#22788;&#29702;&#21644;&#32039;&#20945;&#30340;&#30693;&#35782;&#22270;&#36807;&#28388;&#26041;&#27861;&#65292;&#20026;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#26816;&#32034;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.21276</link><description>&lt;p&gt;
Multi-Level Querying using A Knowledge Pyramid
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PolyRAG&#30340;&#22810;&#23618;&#27425;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;RAG&#26694;&#26550;&#20013;&#26500;&#24314;&#30693;&#35782;&#37329;&#23383;&#22612;&#65292;&#21253;&#25324;&#22885;&#39039;&#23398;&#12289;&#30693;&#35782;&#22270;&#21644;&#25991;&#26412;&#23618;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36328;&#23618;&#30693;&#35782;&#26356;&#26032;&#30340;&#21160;&#24577;&#22788;&#29702;&#21644;&#32039;&#20945;&#30340;&#30693;&#35782;&#22270;&#36807;&#28388;&#26041;&#27861;&#65292;&#20026;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#26816;&#32034;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21276v2 Announce Type: replace  Abstract: This paper addresses the need for improved precision in existing Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effec
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#30340;&#22810;&#35299;&#30721;&#22120;&#22330;&#26223;&#34920;&#31034;&#32593;&#32476;&#65288;MDSRN&#65289;&#65292;&#33021;&#22815;&#20026;&#36755;&#20837;&#22352;&#26631;&#29983;&#25104;&#22810;&#20010;&#21487;&#33021;&#39044;&#27979;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#22343;&#20540;&#20316;&#20026;&#22810;&#35299;&#30721;&#22120;&#38598;&#21512;&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#26041;&#24046;&#20316;&#20026;&#21487;&#20449;&#24230;&#20998;&#25968;&#65292;&#20174;&#32780;&#25903;&#25345;&#25512;&#29702;&#26102;&#39044;&#27979;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2407.19082</link><description>&lt;p&gt;
Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#30340;&#22810;&#35299;&#30721;&#22120;&#22330;&#26223;&#34920;&#31034;&#32593;&#32476;&#65288;MDSRN&#65289;&#65292;&#33021;&#22815;&#20026;&#36755;&#20837;&#22352;&#26631;&#29983;&#25104;&#22810;&#20010;&#21487;&#33021;&#39044;&#27979;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#22343;&#20540;&#20316;&#20026;&#22810;&#35299;&#30721;&#22120;&#38598;&#21512;&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#26041;&#24046;&#20316;&#20026;&#21487;&#20449;&#24230;&#20998;&#25968;&#65292;&#20174;&#32780;&#25903;&#25345;&#25512;&#29702;&#26102;&#39044;&#27979;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19082v2 Announce Type: replace-cross  Abstract: Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN) ensemble architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#21160;&#24577;&#35821;&#35328;&#32452;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;DLG-MoE&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#22330;&#26223;&#19979;MoE&#26694;&#26550;&#30340;&#25928;&#29575;&#19982;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#35821;&#35328;&#36335;&#30001;&#21644;&#29420;&#31435;&#30340;&#20869;&#37096;&#20998;&#32452;&#36335;&#30001;&#26041;&#38754;&#21462;&#24471;&#20102;&#21019;&#26032;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.18581</link><description>&lt;p&gt;
Dynamic Language Group-Based MoE: Enhancing Efficiency and Flexibility for Code-Switching Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#21160;&#24577;&#35821;&#35328;&#32452;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;DLG-MoE&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#22330;&#26223;&#19979;MoE&#26694;&#26550;&#30340;&#25928;&#29575;&#19982;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#35821;&#35328;&#36335;&#30001;&#21644;&#29420;&#31435;&#30340;&#20869;&#37096;&#20998;&#32452;&#36335;&#30001;&#26041;&#38754;&#21462;&#24471;&#20102;&#21019;&#26032;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18581v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) approach is ideally suited for tackling multilingual and code-switching (CS) challenges due to its multi-expert architecture. This work introduces the DLG-MoE, which is optimized for bilingual and CS scenarios. Our novel Dynamic Language Group-based MoE layer features a language router with shared weights for explicit language modeling, while independent unsupervised routers within the language group handle attributes beyond language. This structure not only enhances expert extension capabilities but also supports dynamic top-k training, allowing for flexible inference across various top-k values and improving overall performance. The model requires no pre-training and supports streaming recognition, achieving state-of-the-art (SOTA) results with unmatched flexibility compared to other methods. The Code will be released.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CityX&#30340;&#20840;&#26032;&#22810;&#27169;&#24335;&#21487;&#25511;&#30340;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#22810;&#31181;&#24067;&#23616;&#25351;&#20196;&#29983;&#25104;&#29616;&#23454;&#24863;&#21313;&#36275;&#12289;&#35268;&#27169;&#19981;&#21463;&#38480;&#30340;3D&#34394;&#25311;&#22478;&#24066;&#65292;&#24182;&#33021;&#23454;&#29616;&#23545;&#22478;&#24066;&#24067;&#23616;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2407.17572</link><description>&lt;p&gt;
CityX: Controllable Procedural Content Generation for Unbounded 3D Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17572
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CityX&#30340;&#20840;&#26032;&#22810;&#27169;&#24335;&#21487;&#25511;&#30340;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#22810;&#31181;&#24067;&#23616;&#25351;&#20196;&#29983;&#25104;&#29616;&#23454;&#24863;&#21313;&#36275;&#12289;&#35268;&#27169;&#19981;&#21463;&#38480;&#30340;3D&#34394;&#25311;&#22478;&#24066;&#65292;&#24182;&#33021;&#23454;&#29616;&#23545;&#22478;&#24066;&#24067;&#23616;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17572v3 Announce Type: replace  Abstract: Generating a realistic, large-scale 3D virtual city remains a complex challenge due to the involvement of numerous 3D assets, various city styles, and strict layout constraints. Existing approaches provide promising attempts at procedural content generation to create large-scale scenes using Blender agents. However, they face crucial issues such as difficulties in scaling up generation capability and achieving fine-grained control at the semantic layout level. To address these problems, we propose a novel multi-modal controllable procedural content generation method, named CityX, which enhances realistic, unbounded 3D city generation guided by multiple layout conditions, including OSM, semantic maps, and satellite images. Specifically, the proposed method contains a general protocol for integrating various PCG plugins and a multi-agent framework for transforming instructions into executable Blender actions. Through this effective fra
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#25512;&#29702;&#30340;&#24490;&#29615;&#31185;&#23398;&#21457;&#29616;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#12290;&#25991;&#31456;&#36824;&#23450;&#20041;&#20102;&#19968;&#20010;&#20174;&#31038;&#20250;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#30340;&#35299;&#37322;&#36873;&#25321;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#20102;&#29616;&#26377;&#30340;&#27010;&#24565;&#24182;&#25193;&#23637;&#20102;&#26032;&#30340;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.17454</link><description>&lt;p&gt;
Automated Explanation Selection for Scientific Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#25512;&#29702;&#30340;&#24490;&#29615;&#31185;&#23398;&#21457;&#29616;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#12290;&#25991;&#31456;&#36824;&#23450;&#20041;&#20102;&#19968;&#20010;&#20174;&#31038;&#20250;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#30340;&#35299;&#37322;&#36873;&#25321;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#20102;&#29616;&#26377;&#30340;&#27010;&#24565;&#24182;&#25193;&#23637;&#20102;&#26032;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17454v2 Announce Type: replace  Abstract: Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explanability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;AIR-Bench 2024&#65292;&#19968;&#20010;&#19982;&#26368;&#26032;&#25919;&#24220;&#27861;&#35268;&#21644;&#20225;&#19994;&#25919;&#31574;&#30456;&#19968;&#33268;&#30340;AI&#23433;&#20840;&#22522;&#20934;&#65292;&#23558;&#39118;&#38505;&#20998;&#20026;&#22235;&#20010;&#23618;&#27425;&#65292;&#24182;&#35814;&#32454;&#21015;&#20986;&#20102;314&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;AI&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.17436</link><description>&lt;p&gt;
AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24320;&#21457;&#20102;AIR-Bench 2024&#65292;&#19968;&#20010;&#19982;&#26368;&#26032;&#25919;&#24220;&#27861;&#35268;&#21644;&#20225;&#19994;&#25919;&#31574;&#30456;&#19968;&#33268;&#30340;AI&#23433;&#20840;&#22522;&#20934;&#65292;&#23558;&#39118;&#38505;&#20998;&#20026;&#22235;&#20010;&#23618;&#27425;&#65292;&#24182;&#35814;&#32454;&#21015;&#20986;&#20102;314&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;AI&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17436v2 Announce Type: replace-cross  Abstract: Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 di
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#20998;&#26512;Chat GPT&#25910;&#38598;&#30340;&#26377;&#20851;&#8220;Spring&#8221;&#19968;&#35789;&#30340;&#22235;&#31181;&#19981;&#21516;&#21547;&#20041;&#30340;1000&#21477;&#25991;&#26412;&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#32454;&#32990;&#30340;&#24494;&#23567;&#21464;&#24322;&#22914;&#20309;&#23548;&#33268;&#35789;&#20041;&#30340;&#22810;&#20041;&#24615;&#65292;&#36825;&#26159;&#35813;&#35789;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#33719;&#24471;&#19981;&#21516;&#24847;&#20041;&#30340;&#36827;&#21270;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2407.16110</link><description>&lt;p&gt;
Analyzing the Polysemy Evolution using Semantic Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#20998;&#26512;Chat GPT&#25910;&#38598;&#30340;&#26377;&#20851;&#8220;Spring&#8221;&#19968;&#35789;&#30340;&#22235;&#31181;&#19981;&#21516;&#21547;&#20041;&#30340;1000&#21477;&#25991;&#26412;&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#32454;&#32990;&#30340;&#24494;&#23567;&#21464;&#24322;&#22914;&#20309;&#23548;&#33268;&#35789;&#20041;&#30340;&#22810;&#20041;&#24615;&#65292;&#36825;&#26159;&#35813;&#35789;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#33719;&#24471;&#19981;&#21516;&#24847;&#20041;&#30340;&#36827;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16110v2 Announce Type: replace-cross  Abstract: The senses of words evolve. The sense of the same word may change from today to tomorrow, and multiple senses of the same word may be the result of the evolution of each other, that is, they may be parents and children. If we view Juba as an evolving ecosystem, the paradigm of learning the correct answer, which does not move with the sense of a word, is no longer valid. This paper is a case study that shows that word polysemy is an evolutionary consequence of the modification of Semantic Cells, which has al-ready been presented by the author, by introducing a small amount of diversity in its initial state as an example of analyzing the current set of short sentences. In particular, the analysis of a sentence sequence of 1000 sentences in some order for each of the four senses of the word Spring, collected using Chat GPT, shows that the word acquires the most polysemy monotonically in the analysis when the senses are arranged in
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#30340;&#22810;&#20449;&#21495;&#31649;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#32593;&#32476;&#25216;&#26415;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#26657;&#22253;&#32593;&#32476;&#29615;&#22659;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.15520</link><description>&lt;p&gt;
Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#30340;&#22810;&#20449;&#21495;&#31649;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#32593;&#32476;&#25216;&#26415;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#26657;&#22253;&#32593;&#32476;&#29615;&#22659;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15520v2 Announce Type: replace-cross  Abstract: Digital Twins (DTs) are set to become a key enabling technology in future wireless networks, with their use in network management increasing significantly. We developed a DT framework that leverages the heterogeneity of network access technologies as a resource for enhanced network performance and management, enabling smart data handling in the physical network. Tested in a Campus Area Network environment, our framework integrates diverse data sources to provide real-time, holistic insights into network performance and environmental sensing. We also envision that traditional analytics will evolve to rely on emerging AI models, such as Generative AI (GenAI), while leveraging current analytics capabilities. This capacity can simplify analytics processes through advanced ML models, enabling descriptive, diagnostic, predictive, and prescriptive analytics in a unified fashion. Finally, we present specific research opportunities conc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CCVA-FL&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21307;&#30103;&#24433;&#20687;&#39046;&#22495;&#20013;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#23567;&#21270;&#36328;&#23458;&#25143;&#31471;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20351;&#29992;Scalable Diffusion Models with Transformers&#65288;DiT&#65289;&#29983;&#25104;&#21453;&#26144;&#30446;&#26631;&#23458;&#25143;&#31471;&#22270;&#20687;&#25968;&#25454;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#34987;&#29992;&#20110;&#20849;&#20139;&#20197;&#24110;&#21161;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#36827;&#20837;&#30446;&#26631;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#24335;&#21307;&#30103;&#24433;&#20687;&#25968;&#25454;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CCVA-FL&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21307;&#30103;&#24433;&#20687;&#39046;&#22495;&#20013;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#23567;&#21270;&#36328;&#23458;&#25143;&#31471;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20351;&#29992;Scalable Diffusion Models with Transformers&#65288;DiT&#65289;&#29983;&#25104;&#21453;&#26144;&#30446;&#26631;&#23458;&#25143;&#31471;&#22270;&#20687;&#25968;&#25454;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#34987;&#29992;&#20110;&#20849;&#20139;&#20197;&#24110;&#21161;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#25968;&#25454;&#36827;&#20837;&#30446;&#26631;&#23458;&#25143;&#31471;&#30340;&#22270;&#20687;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#24335;&#21307;&#30103;&#24433;&#20687;&#25968;&#25454;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v4 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;ToG2.0&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#26679;&#21270;&#26597;&#35810;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#20449;&#24687;&#31934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#30693;&#35782;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.10805</link><description>&lt;p&gt;
Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;ToG2.0&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#26679;&#21270;&#26597;&#35810;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#20449;&#24687;&#31934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#30693;&#35782;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10805v2 Announce Type: replace-cross  Abstract: Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG${2.0}$ not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential o
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;Autoverse&#65292;&#19968;&#31181;&#21487;&#36827;&#21270;&#12289;&#38024;&#23545;&#24615;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#40065;&#26834;&#36523;&#20307;&#29305;&#24449;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#36890;&#36807;&#20854;&#20316;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#31639;&#27861;&#21487;&#25193;&#23637;&#22521;&#35757;&#24179;&#21488;&#30340;&#23454;&#38469;&#26696;&#20363;&#12290;Autoverse&#20351;&#29992;&#31867;&#20284;&#20110;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#25551;&#36848;&#28216;&#25103;&#26426;&#21046;&#65292;&#25903;&#25345;&#21508;&#31181;&#28216;&#25103;&#29615;&#22659;&#30340;&#25551;&#36848;&#65292;&#22914;&#36855;&#23467;&#12289;&#22320;&#19979;&#23460;&#31561;&#65292;&#36825;&#20123;&#29615;&#22659;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27979;&#35797;&#38750;&#24120;&#26377;&#29992;&#12290;&#27599;&#26465;&#35268;&#21017;&#21487;&#20197;&#34987;&#31616;&#21270;&#30340;&#21367;&#31215;&#25805;&#20316;&#25152;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#22312;GPU&#19978;&#24182;&#34892;&#22788;&#29702;&#29615;&#22659;&#65292;&#22823;&#24133;&#24230;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#20351;&#29992;Autoverse&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#21644;&#25628;&#32034;&#26469;&#36805;&#36895;&#21551;&#21160;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#31574;&#30053;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#36827;&#21270;&#36873;&#25321;Autoverse&#29615;&#22659;&#30340;&#35268;&#21017;&#21644;&#21021;&#22987;&#22320;&#22270;&#25299;&#25169;&#26469;&#26368;&#22823;&#21270;&#36138;&#23146;&#25628;&#32034;&#21457;&#29616;&#26032;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35838;&#31243;&#12290;</title><link>https://arxiv.org/abs/2407.04221</link><description>&lt;p&gt;
Autoverse: An Evolvable Game Language for Learning Robust Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.04221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;Autoverse&#65292;&#19968;&#31181;&#21487;&#36827;&#21270;&#12289;&#38024;&#23545;&#24615;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#40065;&#26834;&#36523;&#20307;&#29305;&#24449;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#36890;&#36807;&#20854;&#20316;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#31639;&#27861;&#21487;&#25193;&#23637;&#22521;&#35757;&#24179;&#21488;&#30340;&#23454;&#38469;&#26696;&#20363;&#12290;Autoverse&#20351;&#29992;&#31867;&#20284;&#20110;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#25551;&#36848;&#28216;&#25103;&#26426;&#21046;&#65292;&#25903;&#25345;&#21508;&#31181;&#28216;&#25103;&#29615;&#22659;&#30340;&#25551;&#36848;&#65292;&#22914;&#36855;&#23467;&#12289;&#22320;&#19979;&#23460;&#31561;&#65292;&#36825;&#20123;&#29615;&#22659;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27979;&#35797;&#38750;&#24120;&#26377;&#29992;&#12290;&#27599;&#26465;&#35268;&#21017;&#21487;&#20197;&#34987;&#31616;&#21270;&#30340;&#21367;&#31215;&#25805;&#20316;&#25152;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#22312;GPU&#19978;&#24182;&#34892;&#22788;&#29702;&#29615;&#22659;&#65292;&#22823;&#24133;&#24230;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#20351;&#29992;Autoverse&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#21644;&#25628;&#32034;&#26469;&#36805;&#36895;&#21551;&#21160;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#31574;&#30053;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#36827;&#21270;&#36873;&#25321;Autoverse&#29615;&#22659;&#30340;&#35268;&#21017;&#21644;&#21021;&#22987;&#22320;&#22270;&#25299;&#25169;&#26469;&#26368;&#22823;&#21270;&#36138;&#23146;&#25628;&#32034;&#21457;&#29616;&#26032;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.04221v2 Announce Type: replace  Abstract: We introduce Autoverse, an evolvable, domain-specific language for single-player 2D grid-based games, and demonstrate its use as a scalable training ground for Open-Ended Learning (OEL) algorithms. Autoverse uses cellular-automaton-like rewrite rules to describe game mechanics, allowing it to express various game environments (e.g. mazes, dungeons, sokoban puzzles) that are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite rule can be expressed as a series of simple convolutions, allowing for environments to be parallelized on the GPU, thereby drastically accelerating RL training. Using Autoverse, we propose jump-starting open-ended learning by imitation learning from search. In such an approach, we first evolve Autoverse environments (their rules and initial map topology) to maximize the number of iterations required by greedy tree search to discover a new best solution, producing a curriculum of increasingly com
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;VCHAR&#26694;&#26550;&#65292;&#19968;&#31181;&#22522;&#20110;variances&#39537;&#21160;&#30340;&#22797;&#26434;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#34920;&#31034;&#26469;&#26377;&#25928;&#35782;&#21035;&#21644;&#35299;&#37322;&#35270;&#39057;&#20013;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20256;&#32479;&#30340;&#26631;&#31614;&#21270;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2407.03291</link><description>&lt;p&gt;
VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.03291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;VCHAR&#26694;&#26550;&#65292;&#19968;&#31181;&#22522;&#20110;variances&#39537;&#21160;&#30340;&#22797;&#26434;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#34920;&#31034;&#26469;&#26377;&#25928;&#35782;&#21035;&#21644;&#35299;&#37322;&#35270;&#39057;&#20013;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20256;&#32479;&#30340;&#26631;&#31614;&#21270;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.03291v2 Announce Type: replace-cross  Abstract: Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evalu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31354;&#38388;&#20851;&#31995;&#30340;&#22810;&#35270;&#22270;&#22270;&#21464;&#25442;&#22120;&#65288;MVGT&#65289;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26102;&#38388;&#12289;&#39057;&#29575;&#21644;&#31354;&#38388;&#22495;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#20960;&#20309;&#21644;&#35299;&#21078;&#32467;&#26500;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;EEG&#36890;&#36947;&#30340;&#31354;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#24863;&#30693;&#36890;&#36947;&#31354;&#38388;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;EEG&#24773;&#32490;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2407.03131</link><description>&lt;p&gt;
MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.03131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31354;&#38388;&#20851;&#31995;&#30340;&#22810;&#35270;&#22270;&#22270;&#21464;&#25442;&#22120;&#65288;MVGT&#65289;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26102;&#38388;&#12289;&#39057;&#29575;&#21644;&#31354;&#38388;&#22495;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#20960;&#20309;&#21644;&#35299;&#21078;&#32467;&#26500;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;EEG&#36890;&#36947;&#30340;&#31354;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#24863;&#30693;&#36890;&#36947;&#31354;&#38388;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;EEG&#24773;&#32490;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.03131v3 Announce Type: replace-cross  Abstract: Electroencephalography (EEG), a medical imaging technique that captures scalp electrical activity of brain structures via electrodes, has been widely used in affective computing. The spatial domain of EEG is rich in affective information. However, few of the existing studies have simultaneously analyzed EEG signals from multiple perspectives of geometric and anatomical structures in spatial domain. In this paper, we propose a multi-view Graph Transformer (MVGT) based on spatial relations, which integrates information from the temporal, frequency and spatial domains, including geometric and anatomical structures, so as to enhance the expressive power of the model comprehensively. We incorporate the spatial information of EEG channels into the model as encoding, thereby improving its ability to perceive the spatial structure of the channels. Meanwhile, experimental results based on publicly available datasets demonstrate that our
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Logic-LM++&#65292;&#36825;&#26159;&#23545;Logic-LM &#30340;&#19968;&#39033;&#25913;&#36827;&#65292;&#23427;&#22312;&#29983;&#25104;&#21644;&#31934;&#28860;&#24418;&#24335;&#21270;&#34920;&#36798;&#26041;&#38754;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#30340;&#33021;&#21147;&#26469;&#35780;&#20272;&#25152;&#24314;&#35758;&#30340;&#25913;&#36827;&#12290;Logic-LM++&#22312;&#19977;&#20010;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;Logic-LM&#21644;&#20854;&#20182;&#24403;&#21069;&#26041;&#27861;&#65306;FOLIO&#12289;ProofWriter&#21644;AR-LSAT&#65292;&#22312;&#26631;&#20934;&#25552;&#31034;&#12289;&#38142;&#24335;&#24819;&#27861;&#25552;&#31034;&#20197;&#21450;Logic-LM&#19978;&#30340;&#24179;&#22343;&#25913;&#36827;&#29575;&#20998;&#21035;&#20026;18.5%&#12289;12.3%&#21644;5%&#12290;</title><link>https://arxiv.org/abs/2407.02514</link><description>&lt;p&gt;
LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.02514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Logic-LM++&#65292;&#36825;&#26159;&#23545;Logic-LM &#30340;&#19968;&#39033;&#25913;&#36827;&#65292;&#23427;&#22312;&#29983;&#25104;&#21644;&#31934;&#28860;&#24418;&#24335;&#21270;&#34920;&#36798;&#26041;&#38754;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#30340;&#33021;&#21147;&#26469;&#35780;&#20272;&#25152;&#24314;&#35758;&#30340;&#25913;&#36827;&#12290;Logic-LM++&#22312;&#19977;&#20010;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;Logic-LM&#21644;&#20854;&#20182;&#24403;&#21069;&#26041;&#27861;&#65306;FOLIO&#12289;ProofWriter&#21644;AR-LSAT&#65292;&#22312;&#26631;&#20934;&#25552;&#31034;&#12289;&#38142;&#24335;&#24819;&#27861;&#25552;&#31034;&#20197;&#21450;Logic-LM&#19978;&#30340;&#24179;&#22343;&#25913;&#36827;&#29575;&#20998;&#21035;&#20026;18.5%&#12289;12.3%&#21644;5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02514v3 Announce Type: replace-cross  Abstract: In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. Although recent works have started to employ formal languages as an intermediate representation for reasoning tasks, they often face challenges in accurately generating and refining these formal specifications to ensure correctness. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and other contemporary techniques across natural language reasoning tasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;InterleavedBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#38543;&#24847;&#20132;&#32455;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#24037;&#20855;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24320;&#21457;&#20102;InterleavedEval&#65292;&#19968;&#20010;&#22522;&#20110;GPT-4o&#30340;&#24378;&#22823;&#26080;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2406.14643</link><description>&lt;p&gt;
Holistic Evaluation for Interleaved Text-and-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.14643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;InterleavedBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#38543;&#24847;&#20132;&#32455;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#24037;&#20855;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#24320;&#21457;&#20102;InterleavedEval&#65292;&#19968;&#20010;&#22522;&#20110;GPT-4o&#30340;&#24378;&#22823;&#26080;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.14643v2 Announce Type: replace  Abstract: Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Soft Prompting for Unlearning&#65288;SPUL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#26597;&#35810;&#20013;&#38468;&#21152;&#29305;&#23450;&#30340;&#25552;&#31034; tokens &#26469;&#26080;&#30417;&#30563;&#22320;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#35757;&#32451;&#20363;&#23376;&#30340;&#36951;&#24536;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#19981;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#21069;&#25552;&#19979;&#23436;&#32654;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#12290;</title><link>https://arxiv.org/abs/2406.12038</link><description>&lt;p&gt;
Soft Prompting for Unlearning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.12038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Soft Prompting for Unlearning&#65288;SPUL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#26597;&#35810;&#20013;&#38468;&#21152;&#29305;&#23450;&#30340;&#25552;&#31034; tokens &#26469;&#26080;&#30417;&#30563;&#22320;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#35757;&#32451;&#20363;&#23376;&#30340;&#36951;&#24536;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#19981;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#21069;&#25552;&#19979;&#23436;&#32654;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.12038v2 Announce Type: replace-cross  Abstract: The widespread popularity of Large Language Models (LLMs), partly due to their unique ability to perform in-context learning, has also brought to light the importance of ethical and safety considerations when deploying these pre-trained models. In this work, we focus on investigating machine unlearning for LLMs motivated by data protection regulations. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize the unlearning of a subset of training data. With losses designed to enforce forgetting as well as utility preservation, our framework \textbf{S}oft \textbf{P}rompting for \textbf{U}n\textbf{l}earning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed met
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;STAR&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#20154;&#31867;&#32418;&#38431;&#20219;&#21153;&#20013;&#21442;&#25968;&#21270;&#25351;&#20196;&#30340;&#29983;&#25104;&#21644;&#20105;&#35758;&#20210;&#35009;&#26426;&#21046;&#65292;&#25552;&#21319;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39118;&#38505;&#30340;&#35782;&#21035;&#35206;&#30422;&#19982;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#30417;&#27979;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2406.11757</link><description>&lt;p&gt;
STAR: SocioTechnical Approach to Red Teaming Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.11757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;STAR&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#20154;&#31867;&#32418;&#38431;&#20219;&#21153;&#20013;&#21442;&#25968;&#21270;&#25351;&#20196;&#30340;&#29983;&#25104;&#21644;&#20105;&#35758;&#20210;&#35009;&#26426;&#21046;&#65292;&#25552;&#21319;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39118;&#38505;&#30340;&#35782;&#21035;&#35206;&#30422;&#19982;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#30417;&#27979;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.11757v3 Announce Type: replace  Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2406.04485</link><description>&lt;p&gt;
GenAI Arena: An Open Evaluation Platform for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04485v2 Announce Type: replace-cross  Abstract: Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source g
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hummer&#8221;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20943;&#23569;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#23545;&#40784;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20174;&#32780;&#25552;&#39640;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2405.11647</link><description>&lt;p&gt;
Hummer: Towards Limited Competitive Preference Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hummer&#8221;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20943;&#23569;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#23545;&#40784;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20174;&#32780;&#25552;&#39640;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11647v3 Announce Type: replace  Abstract: Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present \texttt{Hummer} and its fine-grained variant, \texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives. \texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment object
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#23545;&#40784;&#26469;&#25913;&#36827;&#24178;&#39044;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#23545;&#40784;&#27169;&#22359;&#65292;&#20316;&#32773;&#21457;&#29616;&#21487;&#20197;&#25913;&#21892;&#20154;&#31867;&#19987;&#23478;&#22312;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23545;&#27599;&#20010;&#22270;&#20687;&#30340;&#24178;&#39044;&#27425;&#25968;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2405.01531</link><description>&lt;p&gt;
Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#23545;&#40784;&#26469;&#25913;&#36827;&#24178;&#39044;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#23545;&#40784;&#27169;&#22359;&#65292;&#20316;&#32773;&#21457;&#29616;&#21487;&#20197;&#25913;&#21892;&#20154;&#31867;&#19987;&#23478;&#22312;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23545;&#27599;&#20010;&#22270;&#20687;&#30340;&#24178;&#39044;&#27425;&#25968;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01531v2 Announce Type: replace-cross  Abstract: Concept Bottleneck Models (CBMs) ground image classification on human-understandable concepts to allow for interpretable model decisions. Crucially, the CBM design inherently allows for human interventions, in which expert users are given the ability to modify potentially misaligned concept choices to influence the decision behavior of the model in an interpretable fashion. However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human feedback is expensive. In this paper, we find that this is noticeably driven by an independent treatment of concepts during intervention, wherein a change of one concept does not influence the use of other ones in the model's final decision. To address this issue, we introduce a trainable concept intervention realignment module, which leverages concept relations to realign concept ass
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMM-PCQA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32473;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#25552;&#20379;&#25991;&#26412;&#30417;&#30563;&#26469;&#23454;&#29616;&#23545;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#30340;&#36741;&#21161;&#12290;&#36890;&#36807;&#23558;&#36136;&#37327;&#26631;&#31614;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#65292;LMMs&#33021;&#22815;&#20174;2D&#28857;&#20113;&#25237;&#24433;&#20013;&#25512;&#23548;&#20986;&#36136;&#37327;&#35780;&#20998;logits&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#21462;&#20102;&#32467;&#26500;&#29305;&#24449;&#26469;&#34917;&#20607;3D&#39046;&#22495;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#36136;&#37327;logits&#21644;&#32467;&#26500;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#20998;&#25968;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#29616;&#20102;&#23558;LMMs&#38598;&#25104;&#21040;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35780;&#20272;&#28857;&#20113;&#36136;&#37327;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.18203</link><description>&lt;p&gt;
LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMM-PCQA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32473;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#25552;&#20379;&#25991;&#26412;&#30417;&#30563;&#26469;&#23454;&#29616;&#23545;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#30340;&#36741;&#21161;&#12290;&#36890;&#36807;&#23558;&#36136;&#37327;&#26631;&#31614;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#65292;LMMs&#33021;&#22815;&#20174;2D&#28857;&#20113;&#25237;&#24433;&#20013;&#25512;&#23548;&#20986;&#36136;&#37327;&#35780;&#20998;logits&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#21462;&#20102;&#32467;&#26500;&#29305;&#24449;&#26469;&#34917;&#20607;3D&#39046;&#22495;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#36136;&#37327;logits&#21644;&#32467;&#26500;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#20998;&#25968;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#29616;&#20102;&#23558;LMMs&#38598;&#25104;&#21040;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35780;&#20272;&#28857;&#20113;&#36136;&#37327;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18203v2 Announce Type: replace  Abstract: Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model under
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24341;&#20837;&#20102;Oak Ridge Base Foundation Model for Earth System Predictability&#65288;ORBIT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;113&#20159;&#21442;&#25968;&#30340;&#20808;&#36827;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#28151;&#21512;&#24352;&#37327;&#25968;&#25454;&#24182;&#34892;&#31574;&#30053;&#12290;ORBIT&#36890;&#36807;&#36825;&#19968;&#31574;&#30053;&#36229;&#36234;&#20102;&#24403;&#21069;&#27668;&#20505;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#22823;&#23567;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#29615;&#22659;&#21160;&#24577;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#25972;&#21512;&#30340;&#25913;&#21892;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.14712</link><description>&lt;p&gt;
ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.14712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24341;&#20837;&#20102;Oak Ridge Base Foundation Model for Earth System Predictability&#65288;ORBIT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;113&#20159;&#21442;&#25968;&#30340;&#20808;&#36827;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#28151;&#21512;&#24352;&#37327;&#25968;&#25454;&#24182;&#34892;&#31574;&#30053;&#12290;ORBIT&#36890;&#36807;&#36825;&#19968;&#31574;&#30053;&#36229;&#36234;&#20102;&#24403;&#21069;&#27668;&#20505;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#22823;&#23567;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#29615;&#22659;&#21160;&#24577;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#25972;&#21512;&#30340;&#25913;&#21892;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.14712v2 Announce Type: replace-cross  Abstract: Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved. Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges. To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique. As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold. Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling efficiency maintai
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#22823;&#35268;&#27169;&#26696;&#20363;&#30740;&#31350;Twitter&#29992;&#25143;&#36164;&#26009;&#22270;&#29255;&#65292;&#30830;&#35748;&#20102;AI&#29983;&#25104;&#22270;&#20687;&#22312;&#24179;&#21488;&#19978;&#30340;&#26174;&#33879;&#23384;&#22312;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2404.14244</link><description>&lt;p&gt;
AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.14244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#22823;&#35268;&#27169;&#26696;&#20363;&#30740;&#31350;Twitter&#29992;&#25143;&#36164;&#26009;&#22270;&#29255;&#65292;&#30830;&#35748;&#20102;AI&#29983;&#25104;&#22270;&#20687;&#22312;&#24179;&#21488;&#19978;&#30340;&#26174;&#33879;&#23384;&#22312;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.14244v2 Announce Type: replace-cross  Abstract: Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of thes
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#38416;&#36848;&#20102;&#20351;&#29992;&#28151;&#21512;&#29616;&#23454;(XR)&#25216;&#26415;&#20351;&#26222;&#36890;&#29289;&#29702;&#23545;&#35937;&#20855;&#22791;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#23545;&#35937;&#35782;&#21029;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#29289;&#29702;&#23545;&#35937;&#26469;&#35302;&#21457;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#20851;&#30340;&#25968;&#23383;&#20132;&#20114;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#30340;&#20307;&#39564;&#24182;&#24320;&#36767;&#20102;&#26032;&#30340;&#20132;&#20114;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2404.13274</link><description>&lt;p&gt;
Augmented Object Intelligence with XR-Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.13274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#38416;&#36848;&#20102;&#20351;&#29992;&#28151;&#21512;&#29616;&#23454;(XR)&#25216;&#26415;&#20351;&#26222;&#36890;&#29289;&#29702;&#23545;&#35937;&#20855;&#22791;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#23545;&#35937;&#35782;&#21029;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#29289;&#29702;&#23545;&#35937;&#26469;&#35302;&#21457;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#20851;&#30340;&#25968;&#23383;&#20132;&#20114;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#30340;&#20307;&#39564;&#24182;&#24320;&#36767;&#20102;&#26032;&#30340;&#20132;&#20114;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.13274v3 Announce Type: replace-cross  Abstract: Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper explores Artificial Object Intelligence (AOI) in the context of XR, an interaction paradigm that aims to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to digital functionalities. Our approach utilizes real-time object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions without the need for object pre-registration. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in contextually relevant ways using object-based context menus. This system enables analog objects to not on
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#27491;&#35268;&#21270;&#12289;&#21069;&#25552;&#36873;&#25321;&#12289;&#35777;&#26126;&#27493;&#39588;&#29983;&#25104;&#21644;&#35777;&#26126;&#25628;&#32034;&#31561;&#20219;&#21153;&#12290;&#25991;&#20013;&#19981;&#20165;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#36824;&#35752;&#35770;&#20102;&#35780;&#20215;&#25351;&#26631;&#21644;&#24403;&#21069;&#25216;&#26415;&#22312;&#22788;&#29702;&#36825;&#20123;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#20840;&#38754;&#27010;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25552;&#21319;&#23450;&#29702;&#35777;&#26126;&#25928;&#29575;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.09939</link><description>&lt;p&gt;
A Survey on Deep Learning for Theorem Proving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.09939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#27491;&#35268;&#21270;&#12289;&#21069;&#25552;&#36873;&#25321;&#12289;&#35777;&#26126;&#27493;&#39588;&#29983;&#25104;&#21644;&#35777;&#26126;&#25628;&#32034;&#31561;&#20219;&#21153;&#12290;&#25991;&#20013;&#19981;&#20165;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#36824;&#35752;&#35770;&#20102;&#35780;&#20215;&#25351;&#26631;&#21644;&#24403;&#21069;&#25216;&#26415;&#22312;&#22788;&#29702;&#36825;&#20123;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#20840;&#38754;&#27010;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25552;&#21319;&#23450;&#29702;&#35777;&#26126;&#25928;&#29575;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.09939v2 Announce Type: replace  Abstract: Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundation
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#20013;&#27880;&#20837;&#19981;&#33391;&#20559;&#22909;&#23545;&#65292;&#26469;&#25915;&#20987;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#33391;&#20559;&#22909;&#23545;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#30340;&#27745;&#26579;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#22909;&#27745;&#26579;&#26159;&#39640;&#24230;&#26377;&#25928;&#30340;&#65306;&#21363;&#20351;&#27880;&#20837;&#30340;&#19981;&#33391;&#25968;&#25454;&#20165;&#21344;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;1%-5%&#65289;&#65292;&#20063;&#21487;&#20197;&#25104;&#21151;&#25805;&#32437;LM&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#24182;&#24102;&#26377;&#30446;&#26631;&#24773;&#24863;&#12290;&#36825;&#35828;&#26126;&#24694;&#24847;&#34892;&#20026;&#32773;&#26377;&#21487;&#33021;&#36890;&#36807;&#27745;&#26579;&#20559;&#22909;&#25968;&#25454;&#26469;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.05530</link><description>&lt;p&gt;
Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.05530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#20013;&#27880;&#20837;&#19981;&#33391;&#20559;&#22909;&#23545;&#65292;&#26469;&#25915;&#20987;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#33391;&#20559;&#22909;&#23545;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#30340;&#27745;&#26579;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#22909;&#27745;&#26579;&#26159;&#39640;&#24230;&#26377;&#25928;&#30340;&#65306;&#21363;&#20351;&#27880;&#20837;&#30340;&#19981;&#33391;&#25968;&#25454;&#20165;&#21344;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;1%-5%&#65289;&#65292;&#20063;&#21487;&#20197;&#25104;&#21151;&#25805;&#32437;LM&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#24182;&#24102;&#26377;&#30446;&#26631;&#24773;&#24863;&#12290;&#36825;&#35828;&#26126;&#24694;&#24847;&#34892;&#20026;&#32773;&#26377;&#21487;&#33021;&#36890;&#36807;&#27745;&#26579;&#20559;&#22909;&#25968;&#25454;&#26469;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.05530v2 Announce Type: replace-cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or ne
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#31243;&#24230;&#35854;&#35328;&#21644;&#35686;&#21578;&#20449;&#24687;&#23545;&#31038;&#20250;&#25509;&#21463;&#24230;&#21450;&#25152;&#20316;&#21453;&#24212;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#32039;&#24613;&#35686;&#21578;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.03745</link><description>&lt;p&gt;
Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.03745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#31243;&#24230;&#35854;&#35328;&#21644;&#35686;&#21578;&#20449;&#24687;&#23545;&#31038;&#20250;&#25509;&#21463;&#24230;&#21450;&#25152;&#20316;&#21453;&#24212;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#32039;&#24613;&#35686;&#21578;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.03745v2 Announce Type: replace-cross  Abstract: The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Participants ranked content as truthful in the order of genuine, minor hallucination, and major hallucination, and user engagement behaviors mirrored this pattern. More importantly, we observed that warning im
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#19982;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#26512;&#20986;LLM&#22312;&#22788;&#29702;&#35821;&#21477;&#26102;&#25152;&#20381;&#36182;&#30340;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#22312;&#27169;&#22411;&#20869;&#37096;&#20174;&#30690;&#37327;&#24418;&#24335;&#21040;&#35859;&#35789;&#38598;&#21512;&#30340;&#36716;&#21270;&#36807;&#31243;&#12290;&#25991;&#31456;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#35843;&#21046;&#30340;&#24037;&#20855;&#65292;&#23427;&#20801;&#35768;&#23454;&#26102;&#25913;&#21464;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#21333;&#20010;&#35789;&#20803;&#30340;&#21709;&#24212;&#65292;&#20197;&#25552;&#21462;&#38544;&#34255;&#22312;&#36825;&#20123;&#35789;&#20803;&#20869;&#37096;&#30340;&#23454;&#20307;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#23454;&#20307;&#30693;&#35782;&#24182;&#19981;&#30452;&#25509;&#30001;&#35757;&#32451;&#33719;&#24471;&#65292;&#32780;&#26159;&#22312;&#19982;&#36755;&#20837;&#20449;&#21495;&#20132;&#20114;&#30340;&#36807;&#31243;&#20013;&#36880;&#27493;&#26500;&#24314;&#65292;&#20294;&#25991;&#31456;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36825;&#31181;&#35843;&#21046;&#30340;&#25163;&#27573;&#25552;&#21462;&#24182;&#35299;&#26512;&#36825;&#20123;&#30693;&#35782;&#65292;&#24182;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#21477;&#32423;&#20107;&#23454;&#21028;&#26029;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#20102;&#27979;&#35797;&#65292;&#25506;&#32034;&#20102;&#30693;&#35782;&#29255;&#27573;&#22312;&#27169;&#22411;&#19981;&#21516;&#23618;&#27425;&#19978;&#30340;&#20307;&#29616;&#12290;

&#31616;&#32780;&#35328;&#20043;&#65292;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22788;&#29702;&#35821;&#21477;&#26102;&#25152;&#26681;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#20102;&#20107;&#23454;&#20449;&#24687;&#30340;&#25552;&#21462;&#35299;&#26512;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#28608;&#27963;&#35843;&#21046;&#26041;&#27861;&#21487;&#20197;&#23545;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#36827;&#34892;&#26377;&#25928;&#30340;&#25366;&#25496;&#12290;</title><link>https://arxiv.org/abs/2404.03623</link><description>&lt;p&gt;
Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.03623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#19982;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#26512;&#20986;LLM&#22312;&#22788;&#29702;&#35821;&#21477;&#26102;&#25152;&#20381;&#36182;&#30340;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#22312;&#27169;&#22411;&#20869;&#37096;&#20174;&#30690;&#37327;&#24418;&#24335;&#21040;&#35859;&#35789;&#38598;&#21512;&#30340;&#36716;&#21270;&#36807;&#31243;&#12290;&#25991;&#31456;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#35843;&#21046;&#30340;&#24037;&#20855;&#65292;&#23427;&#20801;&#35768;&#23454;&#26102;&#25913;&#21464;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#21333;&#20010;&#35789;&#20803;&#30340;&#21709;&#24212;&#65292;&#20197;&#25552;&#21462;&#38544;&#34255;&#22312;&#36825;&#20123;&#35789;&#20803;&#20869;&#37096;&#30340;&#23454;&#20307;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#23454;&#20307;&#30693;&#35782;&#24182;&#19981;&#30452;&#25509;&#30001;&#35757;&#32451;&#33719;&#24471;&#65292;&#32780;&#26159;&#22312;&#19982;&#36755;&#20837;&#20449;&#21495;&#20132;&#20114;&#30340;&#36807;&#31243;&#20013;&#36880;&#27493;&#26500;&#24314;&#65292;&#20294;&#25991;&#31456;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36825;&#31181;&#35843;&#21046;&#30340;&#25163;&#27573;&#25552;&#21462;&#24182;&#35299;&#26512;&#36825;&#20123;&#30693;&#35782;&#65292;&#24182;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#21477;&#32423;&#20107;&#23454;&#21028;&#26029;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#20102;&#27979;&#35797;&#65292;&#25506;&#32034;&#20102;&#30693;&#35782;&#29255;&#27573;&#22312;&#27169;&#22411;&#19981;&#21516;&#23618;&#27425;&#19978;&#30340;&#20307;&#29616;&#12290;

&#31616;&#32780;&#35328;&#20043;&#65292;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22788;&#29702;&#35821;&#21477;&#26102;&#25152;&#26681;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#20102;&#20107;&#23454;&#20449;&#24687;&#30340;&#25552;&#21462;&#35299;&#26512;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#28608;&#27963;&#35843;&#21046;&#26041;&#27861;&#21487;&#20197;&#23545;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#36827;&#34892;&#26377;&#25928;&#30340;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.03623v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis hi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#26356;&#20026;&#20840;&#38754;&#30340;&#35748;&#30693;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;&#25991;&#31456;&#36890;&#36807;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#23545;LLMs&#30340;&#25512;&#29702;&#34892;&#20026;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#36235;&#21183;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#24635;&#32467;&#21644;&#35780;&#35770;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#23545;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#65292;&#25581;&#31034;&#20102;&#22312;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#35748;&#30693;&#33021;&#21147;&#26102;&#24212;&#35813;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;</title><link>https://arxiv.org/abs/2404.01869</link><description>&lt;p&gt;
Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#26356;&#20026;&#20840;&#38754;&#30340;&#35748;&#30693;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;&#25991;&#31456;&#36890;&#36807;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#23545;LLMs&#30340;&#25512;&#29702;&#34892;&#20026;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#36235;&#21183;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#24635;&#32467;&#21644;&#35780;&#35770;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#23545;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#65292;&#25581;&#31034;&#20102;&#22312;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#35748;&#30693;&#33021;&#21147;&#26102;&#24212;&#35813;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01869v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CR3DT&#30340;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#25668;&#20687;&#22836;&#21644;&#38647;&#36798;&#25216;&#26415;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#36890;&#36807;&#23558;&#38647;&#36798;&#25968;&#25454;&#30340;&#21152;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CR3DT&#30340;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#25668;&#20687;&#22836;&#21644;&#38647;&#36798;&#25216;&#26415;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#36890;&#36807;&#23558;&#38647;&#36798;&#25968;&#25454;&#30340;&#21152;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v2 Announce Type: replace  Abstract: To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34013;&#22270;&#22270;&#30340;&#36777;&#35770;&#33539;&#24335;&#65288;BDoG&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#24847;&#35265;&#31616;&#21270;&#38382;&#39064;&#21644;&#22270;&#20687;&#24341;&#20837;&#30340;&#27880;&#24847;&#21147;&#20998;&#25955;&#38382;&#39064;&#12290;BDoG&#36890;&#36807;&#23558;&#36777;&#35770;&#38480;&#21046;&#22312;&#34013;&#22270;&#22270;&#20013;&#26469;&#38450;&#27490;&#36890;&#36807;&#19990;&#30028;&#32423;&#24635;&#32467;&#23548;&#33268;&#30340;&#35266;&#28857;&#31616;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#22270;&#20998;&#25903;&#20013;&#23384;&#20648;&#35777;&#25454;&#26469;&#20943;&#23569;&#39057;&#32321;&#20294;&#26080;&#20851;&#27010;&#24565;&#30340;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;BDoG&#22312;ScienceQA&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14972</link><description>&lt;p&gt;
A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34013;&#22270;&#22270;&#30340;&#36777;&#35770;&#33539;&#24335;&#65288;BDoG&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#24847;&#35265;&#31616;&#21270;&#38382;&#39064;&#21644;&#22270;&#20687;&#24341;&#20837;&#30340;&#27880;&#24847;&#21147;&#20998;&#25955;&#38382;&#39064;&#12290;BDoG&#36890;&#36807;&#23558;&#36777;&#35770;&#38480;&#21046;&#22312;&#34013;&#22270;&#22270;&#20013;&#26469;&#38450;&#27490;&#36890;&#36807;&#19990;&#30028;&#32423;&#24635;&#32467;&#23548;&#33268;&#30340;&#35266;&#28857;&#31616;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#22270;&#20998;&#25903;&#20013;&#23384;&#20648;&#35777;&#25454;&#26469;&#20943;&#23569;&#39057;&#32321;&#20294;&#26080;&#20851;&#27010;&#24565;&#30340;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;BDoG&#22312;ScienceQA&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14972v2 Announce Type: replace  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate that BDoG is able to achieve state-of-the-art results in ScienceQA and MMBench with significant improvements over previous methods. The source code can be accessed at
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#35774;&#35745;&#20102;&#19968;&#27454;&#21517;&#20026;ClarifAI&#30340;&#33258;&#21160;&#21270; propaganda&#26816;&#27979;&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#28608;&#27963;&#8220;&#21452;&#24605;&#32771;&#31995;&#32479;&#8221;&#29702;&#35770;&#65292;&#20419;&#20351;&#35835;&#32773;&#23545;&#26032;&#38395;&#20869;&#23481;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#24605;&#32771;&#12290;&#36890;&#36807;&#36825;&#39033;&#21019;&#26032;&#65292;ClarifAI&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#25991;&#31456;&#20013;&#30340; propaganda&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#20182;&#20204;&#22312;&#38405;&#35835;&#26102;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#65292;&#35813;&#30740;&#31350;&#36824;&#35777;&#26126;&#20102;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35299;&#37322;&#24615;&#20869;&#23481;&#22312;&#25552;&#21319;&#25209;&#21028;&#24615;&#24605;&#32500;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19135</link><description>&lt;p&gt;
Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#35774;&#35745;&#20102;&#19968;&#27454;&#21517;&#20026;ClarifAI&#30340;&#33258;&#21160;&#21270; propaganda&#26816;&#27979;&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#28608;&#27963;&#8220;&#21452;&#24605;&#32771;&#31995;&#32479;&#8221;&#29702;&#35770;&#65292;&#20419;&#20351;&#35835;&#32773;&#23545;&#26032;&#38395;&#20869;&#23481;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#24605;&#32771;&#12290;&#36890;&#36807;&#36825;&#39033;&#21019;&#26032;&#65292;ClarifAI&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#25991;&#31456;&#20013;&#30340; propaganda&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#20182;&#20204;&#22312;&#38405;&#35835;&#26102;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#65292;&#35813;&#30740;&#31350;&#36824;&#35777;&#26126;&#20102;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35299;&#37322;&#24615;&#20869;&#23481;&#22312;&#25552;&#21319;&#25209;&#21028;&#24615;&#24605;&#32500;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19135v2 Announce Type: replace-cross  Abstract: In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#22996;&#25176;&#21338;&#24328;&#30340;&#27169;&#22411;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#65288;&#22914;&#20010;&#20154;&#34394;&#25311;&#21161;&#25163;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65289;&#20043;&#38388;&#30340;&#24037;&#20316;&#20851;&#31995;&#12290;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22996;&#25176;&#28216;&#25103;&#20013;&#30340;&#20004;&#31181;&#37325;&#35201;&#22833;&#36133;&#27169;&#24335;&#65306;&#25511;&#21046;&#38382;&#39064;&#21644;&#21512;&#20316;&#38382;&#39064;&#12290;&#25511;&#21046;&#38382;&#39064;&#28041;&#21450;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#19982;&#20854;&#22996;&#25176;&#20154;&#20559;&#22909;&#19981;&#19968;&#33268;&#65292;&#32780;&#21512;&#20316;&#38382;&#39064;&#21017;&#25351;&#20195;&#29702;&#20154;&#20043;&#38388;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#21327;&#20316;&#19981;&#20339;&#12290;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23558;&#23427;&#20204;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;&#20559;&#22909;&#19968;&#33268;&#24615;&#21644;&#25191;&#34892;&#33021;&#21147;&#20004;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#22996;&#25176;&#20154;&#30340;&#31119;&#21033;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#29616;&#26377;&#20449;&#24687;&#26469;&#20272;&#35745;&#36825;&#20123;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#20026;&#29702;&#35299;&#20154;&#31867;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24037;&#20316;&#21327;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#21487;&#33021;&#23545;&#28041;&#21450;&#22810;&#26041;&#20132;&#20114;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#35774;&#35745;&#26377;&#37325;&#35201;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.15821</link><description>&lt;p&gt;
Cooperation and Control in Delegation Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#22996;&#25176;&#21338;&#24328;&#30340;&#27169;&#22411;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#65288;&#22914;&#20010;&#20154;&#34394;&#25311;&#21161;&#25163;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65289;&#20043;&#38388;&#30340;&#24037;&#20316;&#20851;&#31995;&#12290;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22996;&#25176;&#28216;&#25103;&#20013;&#30340;&#20004;&#31181;&#37325;&#35201;&#22833;&#36133;&#27169;&#24335;&#65306;&#25511;&#21046;&#38382;&#39064;&#21644;&#21512;&#20316;&#38382;&#39064;&#12290;&#25511;&#21046;&#38382;&#39064;&#28041;&#21450;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#19982;&#20854;&#22996;&#25176;&#20154;&#20559;&#22909;&#19981;&#19968;&#33268;&#65292;&#32780;&#21512;&#20316;&#38382;&#39064;&#21017;&#25351;&#20195;&#29702;&#20154;&#20043;&#38388;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#21327;&#20316;&#19981;&#20339;&#12290;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23558;&#23427;&#20204;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;&#20559;&#22909;&#19968;&#33268;&#24615;&#21644;&#25191;&#34892;&#33021;&#21147;&#20004;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#22996;&#25176;&#20154;&#30340;&#31119;&#21033;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#29616;&#26377;&#20449;&#24687;&#26469;&#20272;&#35745;&#36825;&#20123;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#20026;&#29702;&#35299;&#20154;&#31867;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24037;&#20316;&#21327;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#21487;&#33021;&#23545;&#28041;&#21450;&#22810;&#26041;&#20132;&#20114;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#35774;&#35745;&#26377;&#37325;&#35201;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15821v2 Announce Type: replace-cross  Abstract: Many settings of interest involving humans and machines -- from virtual personal assistants to autonomous vehicles -- can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf. We refer to these multi-principal, multi-agent scenarios as delegation games. In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together). In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?). We show -- theoretically and empirically -- how these measures determine the principals' welfare, how they can be estimated using limited observatio
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#25991;&#39640;&#32771;&#30340;&#20840;&#26032;&#22810;&#27169;&#24577;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#35201;&#27714;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#22270;&#29255;&#12289;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#21644;&#20854;&#20182;10&#31181;&#31867;&#22411;&#22270;&#29255;&#30340;&#27491;&#30830;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#21516;&#26102;&#35201;&#27714;&#27169;&#22411;&#20855;&#26377;&#25512;&#29702;&#21644;&#22788;&#29702;&#39640;&#32423;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;GAOKAO-MM&#22522;&#20934;&#20013;&#65292;10&#20010;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#19981;&#20339;&#65292;&#23427;&#20204;&#30340;&#27491;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;LVLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#36739;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#25991;&#39640;&#32771;&#30340;&#20840;&#26032;&#22810;&#27169;&#24577;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#35201;&#27714;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#22270;&#29255;&#12289;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#21644;&#20854;&#20182;10&#31181;&#31867;&#22411;&#22270;&#29255;&#30340;&#27491;&#30830;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#21516;&#26102;&#35201;&#27714;&#27169;&#22411;&#20855;&#26377;&#25512;&#29702;&#21644;&#22788;&#29702;&#39640;&#32423;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;GAOKAO-MM&#22522;&#20934;&#20013;&#65292;10&#20010;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#19981;&#20339;&#65292;&#23427;&#20204;&#30340;&#27491;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;LVLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#36739;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v2 Announce Type: replace-cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs ha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#31995;&#32479;&#24615;&#25991;&#29486;&#22238;&#39038;&#65288;SLR&#65289;&#36827;&#34892;&#32508;&#21512;&#23457;&#26597;&#65292;&#25351;&#20986;&#20102;AI&#22312;&#33258;&#21160;&#21270;SLR&#36807;&#31243;&#20013;&#30340;&#28508;&#22312;&#26426;&#20250;&#19982;&#25361;&#25112;&#12290;&#36890;&#36807;21&#31181;SLR&#24037;&#20855;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20998;&#26512;&#20102;AI&#25216;&#26415;&#22312;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#30340;&#24212;&#29992;&#65292;&#24182;&#29305;&#21035;&#25506;&#35752;&#20102;11&#27454;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#23398;&#26415;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#26032;&#24037;&#20855;&#65292;&#20174;&#32780;&#20026;&#25552;&#21319;SLR&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.08565</link><description>&lt;p&gt;
Artificial Intelligence for Literature Reviews: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#31995;&#32479;&#24615;&#25991;&#29486;&#22238;&#39038;&#65288;SLR&#65289;&#36827;&#34892;&#32508;&#21512;&#23457;&#26597;&#65292;&#25351;&#20986;&#20102;AI&#22312;&#33258;&#21160;&#21270;SLR&#36807;&#31243;&#20013;&#30340;&#28508;&#22312;&#26426;&#20250;&#19982;&#25361;&#25112;&#12290;&#36890;&#36807;21&#31181;SLR&#24037;&#20855;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20998;&#26512;&#20102;AI&#25216;&#26415;&#22312;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#30340;&#24212;&#29992;&#65292;&#24182;&#29305;&#21035;&#25506;&#35752;&#20102;11&#27454;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#23398;&#26415;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#26032;&#24037;&#20855;&#65292;&#20174;&#32780;&#20026;&#25552;&#21319;SLR&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08565v2 Announce Type: replace  Abstract: This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#20915;&#31574;&#25928;&#26524;&#65292;&#20197;&#21450;&#19968;&#31181;&#26088;&#22312;&#25551;&#36848;&#24615;&#39044;&#27979;&#24182;&#27169;&#25311;&#20154;&#31867;&#20915;&#31574;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21518;&#32773;&#26356;&#22909;&#22320;&#36866;&#24212;&#20102;&#21253;&#21547;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#22797;&#26434;&#20915;&#31574;&#29615;&#22659;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.11044</link><description>&lt;p&gt;
Preservation of Feature Stability in Machine Learning Under Data Uncertainty for Decision Support in Critical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#20915;&#31574;&#25928;&#26524;&#65292;&#20197;&#21450;&#19968;&#31181;&#26088;&#22312;&#25551;&#36848;&#24615;&#39044;&#27979;&#24182;&#27169;&#25311;&#20154;&#31867;&#20915;&#31574;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21518;&#32773;&#26356;&#22909;&#22320;&#36866;&#24212;&#20102;&#21253;&#21547;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#22797;&#26434;&#20915;&#31574;&#29615;&#22659;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11044v2 Announce Type: replace-cross  Abstract: In a world where Machine Learning (ML) is increasingly deployed to support decision-making in critical domains, providing decision-makers with explainable, stable, and relevant inputs becomes fundamental. Understanding how machine learning works under missing data and how this affects feature variability is paramount. This is even more relevant as machine learning approaches focus on standardising decision-making approaches that rely on an idealised set of features. However, decision-making in human activities often relies on incomplete data, even in critical domains. This paper addresses this gap by conducting a set of experiments using traditional machine learning methods that look for optimal decisions in comparison to a recently deployed machine learning method focused on a classification that is more descriptive and mimics human decision making, allowing for the natural integration of explainability. We found that the ML d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#21407;&#22987;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#35821;&#35328;&#30340;&#37325;&#26032;&#35757;&#32451;&#21644;&#19968;&#20010;&#39564;&#35777;&#27493;&#39588;&#65292;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32599;&#39532;&#23383;&#27597;&#35821;&#35328;&#20013;&#36896;&#25104;&#30340;&#25991;&#26412;&#36807;&#24230;&#30862;&#29255;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.10660</link><description>&lt;p&gt;
Accelerating Multilingual Language Model for Excessively Tokenized Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.10660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#21407;&#22987;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#35821;&#35328;&#30340;&#37325;&#26032;&#35757;&#32451;&#21644;&#19968;&#20010;&#39564;&#35777;&#27493;&#39588;&#65292;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32599;&#39532;&#23383;&#27597;&#35821;&#35328;&#20013;&#36896;&#25104;&#30340;&#25991;&#26412;&#36807;&#24230;&#30862;&#29255;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.10660v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) have remarkably enhanced performances on a variety of tasks in multiple languages. However, tokenizers in LLMs trained primarily on English-centric corpora often overly fragment a text into character or Unicode-level tokens in non-Roman alphabetic languages, leading to inefficient text generation. We introduce a simple yet effective framework to accelerate text generation in such languages. Our approach involves employing a new language model head with a vocabulary set tailored to a specific target language for a pre-trained LLM. This is followed by fine-tuning the new head while incorporating a verification step to ensure the model's performance is preserved. We show that this targeted fine-tuning, while freezing other model parameters, effectively reduces token fragmentation for the target language. Our extensive experiments demonstrate that the proposed framework increases 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#20195;&#34920;&#24037;&#31243;&#23398;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#36234;&#29425;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#25552;&#39640;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;LLMs&#20135;&#29983;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#35782;&#21035;&#21644;&#35843;&#33410;&#29305;&#23450;&#23433;&#20840;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#22686;&#24378;&#25110;&#20943;&#24369;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.06824</link><description>&lt;p&gt;
Rethinking Jailbreaking through the Lens of Representation Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06824
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#20195;&#34920;&#24037;&#31243;&#23398;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#36234;&#29425;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#25552;&#39640;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;LLMs&#20135;&#29983;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#35782;&#21035;&#21644;&#35843;&#33410;&#29305;&#23450;&#23433;&#20840;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#22686;&#24378;&#25110;&#20943;&#24369;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06824v3 Announce Type: replace-cross  Abstract: The recent surge in jailbreaking methods has revealed the vulnerability of Large Language Models (LLMs) to malicious inputs. While earlier research has primarily concentrated on increasing the success rates of jailbreaking attacks, the underlying mechanism for safeguarding LLMs remains underexplored. This study investigates the vulnerability of safety-aligned LLMs by uncovering specific activity patterns within the representation space generated by LLMs. Such ``safety patterns'' can be identified with only a few pairs of contrastive queries in a simple method and function as ``keys'' (used as a metaphor for security defense capability) that can be used to open or lock Pandora's Box of LLMs. Extensive experiments demonstrate that the robustness of LLMs against jailbreaking can be lessened or augmented by attenuating or strengthening the identified safety patterns. These findings deepen our understanding of jailbreaking phenomena
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PathoDuet&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;H&amp;E&#21644;IHC&#26579;&#33394;&#30340;&#30149;&#29702;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29305;&#23450;&#20110;&#30149;&#29702;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#22914;&#19981;&#21516;&#20493;&#29575;&#30340;&#22270;&#20687;&#21644;&#19981;&#21516;&#26579;&#33394;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#8212;&#8212;&#36328;&#23610;&#24230;&#23450;&#20301;&#21644;&#36328;&#26579;&#33394;&#36716;&#25442;&#65292;&#30740;&#31350;&#32773;&#33021;&#22815;&#22312;H&amp;E&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#26377;&#25928;&#36801;&#31227;&#21040;IHC&#22270;&#20687;&#20219;&#21153;&#19978;&#12290;</title><link>https://arxiv.org/abs/2312.09894</link><description>&lt;p&gt;
PathoDuet: Foundation Models for Pathological Slide Analysis of H&amp;E and IHC Stains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PathoDuet&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;H&amp;E&#21644;IHC&#26579;&#33394;&#30340;&#30149;&#29702;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29305;&#23450;&#20110;&#30149;&#29702;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#22914;&#19981;&#21516;&#20493;&#29575;&#30340;&#22270;&#20687;&#21644;&#19981;&#21516;&#26579;&#33394;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#8212;&#8212;&#36328;&#23610;&#24230;&#23450;&#20301;&#21644;&#36328;&#26579;&#33394;&#36716;&#25442;&#65292;&#30740;&#31350;&#32773;&#33021;&#22815;&#22312;H&amp;E&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#26377;&#25928;&#36801;&#31227;&#21040;IHC&#22270;&#20687;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09894v2 Announce Type: replace  Abstract: Large amounts of digitized histopathological data display a promising future for developing pathological foundation models via self-supervised learning methods. Foundation models pretrained with these methods serve as a good basis for downstream tasks. However, the gap between natural and histopathological images hinders the direct application of existing methods. In this work, we present PathoDuet, a series of pretrained models on histopathological images, and a new self-supervised learning framework in histopathology. The framework is featured by a newly-introduced pretext token and later task raisers to explicitly utilize certain relations between images, like multiple magnifications and multiple stains. Based on this, two pretext tasks, cross-scale positioning and cross-stain transferring, are designed to pretrain the model on Hematoxylin and Eosin (H&amp;E) images and transfer the model to immunohistochemistry (IHC) images, respecti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#21435;&#23398;&#20064;&#65288;Deep Unlearning&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#20381;&#36182;&#26799;&#24230;&#20449;&#24687;&#22320;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#35299;&#20915;&#31867;&#21035;&#36951;&#24536;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#38656;&#37325;&#35757;&#27169;&#22411;&#21363;&#21487;&#20026;&#27599;&#20010;&#21024;&#38500;&#35831;&#27714;&#36827;&#34892;&#31934;&#30830;&#30340;&#31867;&#21035;&#26356;&#26032;&#65292;&#22312;&#19981;&#36829;&#32972;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.00761</link><description>&lt;p&gt;
Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#21435;&#23398;&#20064;&#65288;Deep Unlearning&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#20381;&#36182;&#26799;&#24230;&#20449;&#24687;&#22320;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#35299;&#20915;&#31867;&#21035;&#36951;&#24536;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#38656;&#37325;&#35757;&#27169;&#22411;&#21363;&#21487;&#20026;&#27599;&#20010;&#21024;&#38500;&#35831;&#27714;&#36827;&#34892;&#31934;&#30830;&#30340;&#31867;&#21035;&#26356;&#26032;&#65292;&#22312;&#19981;&#36829;&#32972;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00761v4 Announce Type: replace-cross  Abstract: Machine unlearning is a prominent and challenging field, driven by regulatory demands for user data deletion and heightened privacy awareness. Existing approaches involve retraining model or multiple finetuning steps for each deletion request, often constrained by computational limits and restricted data access. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate specific classes from the learned model. Our algorithm first estimates the Retain and the Forget Spaces using Singular Value Decomposition on the layerwise activations for a small subset of samples from the retain and unlearn classes, respectively. We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space. Finally, we obtain the unlearned model by updating the weights to suppress the class discriminatory features from the activation spaces. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;(RF)&#39044;&#27979;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20026;&#20854;&#35757;&#32451;&#30446;&#26631;&#21152;&#26435;&#21644;&#30340;&#26032;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;RF&#27169;&#22411;&#39044;&#27979;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;RF&#39044;&#27979;&#30340;&#26412;&#22320;&#21270;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#26435;&#37325;&#21644;&#35757;&#32451;&#38598;&#20013;&#30340;&#20219;&#20309;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#29305;&#24449;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;SHAP&#12290;</title><link>https://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
Enhanced Local Explainability and Trust Scores with Random Forest Proximities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;(RF)&#39044;&#27979;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20026;&#20854;&#35757;&#32451;&#30446;&#26631;&#21152;&#26435;&#21644;&#30340;&#26032;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;RF&#27169;&#22411;&#39044;&#27979;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;RF&#39044;&#27979;&#30340;&#26412;&#22320;&#21270;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#26435;&#37325;&#21644;&#35757;&#32451;&#38598;&#20013;&#30340;&#20219;&#20309;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#29305;&#24449;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;SHAP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12428v3 Announce Type: replace-cross  Abstract: We initiate a novel approach to explain the predictions and out of sample performance of random forest (RF) regression and classification models by exploiting the fact that any RF can be mathematically formulated as an adaptive weighted K nearest-neighbors model. Specifically, we employ a recent result that, for both regression and classification tasks, any RF prediction can be rewritten exactly as a weighted sum of the training targets, where the weights are RF proximities between the corresponding pairs of data points. We show that this linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established feature-based methods like SHAP, which generate attributions for a model prediction across input features. We show how this proximity-based approach to explainability can be used in conjunction
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;GPT-4&#21644;GPT-3.5&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#31934;&#30830;&#30340;&#32534;&#31243;&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#23548;&#24072;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#31995;&#32479;&#20351;&#29992;GPT-4&#29983;&#25104;&#32534;&#31243;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;GPT-3.5&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#20026;&#23398;&#29983;&#22312;&#35299;&#20915;&#32534;&#31243;&#38169;&#35823;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2310.03780</link><description>&lt;p&gt;
Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;GPT-4&#21644;GPT-3.5&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#31934;&#30830;&#30340;&#32534;&#31243;&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#23548;&#24072;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#31995;&#32479;&#20351;&#29992;GPT-4&#29983;&#25104;&#32534;&#31243;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;GPT-3.5&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#20026;&#23398;&#29983;&#22312;&#35299;&#20915;&#32534;&#31243;&#38169;&#35823;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03780v4 Announce Type: replace  Abstract: Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model,
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#20165;&#20381;&#36182;&#26597;&#35810;&#28857;&#26631;&#27880;&#30340;&#24369; supervision&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#20165;&#38656;&#23569;&#37327;&#30340;&#26597;&#35810;&#28857;&#26631;&#27880;&#21363;&#21487;&#36827;&#34892;&#21355;&#26143;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39640;&#31934;&#24230;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2309.05490</link><description>&lt;p&gt;
Learning Semantic Segmentation with Query Points Supervision on Aerial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#20165;&#20381;&#36182;&#26597;&#35810;&#28857;&#26631;&#27880;&#30340;&#24369; supervision&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#20165;&#38656;&#23569;&#37327;&#30340;&#26597;&#35810;&#28857;&#26631;&#27880;&#21363;&#21487;&#36827;&#34892;&#21355;&#26143;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39640;&#31934;&#24230;&#20687;&#32032;&#32423;&#26631;&#27880;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05490v2 Announce Type: replace  Abstract: Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models supervised with imag
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCoherentFactorModelNeuralNetwork&#65288;DeepCFMNN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;MQForecaster&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#21152;&#20837;&#19968;&#20010;&#22522;&#20110;&#28145;&#23618;&#39640;&#26031;&#20998;&#35299;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#28385;&#36275;&#23618;&#32423;&#32467;&#26500;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#33021;&#28304;&#31649;&#29702;&#12289;&#27668;&#20505;&#39044;&#27979;&#31561;&#38656;&#35201;&#23618;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#24212;&#29992;&#22330;&#26223;&#20855;&#26377;&#37325;&#35201;&#21019;&#26032;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCoherentFactorModelNeuralNetwork&#65288;DeepCFMNN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;MQForecaster&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#21152;&#20837;&#19968;&#20010;&#22522;&#20110;&#28145;&#23618;&#39640;&#26031;&#20998;&#35299;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#28385;&#36275;&#23618;&#32423;&#32467;&#26500;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#33021;&#28304;&#31649;&#29702;&#12289;&#27668;&#20505;&#39044;&#27979;&#31561;&#38656;&#35201;&#23618;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#24212;&#29992;&#22330;&#26223;&#20855;&#26377;&#37325;&#35201;&#21019;&#26032;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.09797v2 Announce Type: replace-cross  Abstract: Obtaining accurate probabilistic forecasts is an important operational challenge in many applications, perhaps most obviously in energy management, climate forecasting, supply chain planning, and resource allocation. In many of these applications, there is a natural hierarchical structure over the forecasted quantities; and forecasting systems that adhere to this hierarchical structure are said to be coherent. Furthermore, operational planning benefits from accuracy at all levels of the aggregation hierarchy. Building accurate and coherent forecasting systems, however, is challenging: classic multivariate time series tools and neural network methods are still being adapted for this purpose. In this paper, we augment an MQForecaster neural network architecture with a novel deep Gaussian factor forecasting model that achieves coherence by construction, yielding a method we call the Deep Coherent Factor Model Neural Network (DeepC
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#38416;&#36848;&#20102;&#24037;&#20855;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#23545;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#30340;&#20511;&#37492;&#24847;&#20041;&#12290;&#22522;&#20110;&#36825;&#31867;&#24212;&#29992;&#23454;&#36341;&#65292;&#25991;&#31456;&#27010;&#36848;&#20102;&#24037;&#20855;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#21319;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#27700;&#24179;&#12290;&#25991;&#31456;&#36890;&#36807;&#31995;&#32479;&#22320;&#22238;&#39038;&#29616;&#26377;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24037;&#20855;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#36935;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
Tool Learning with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#38416;&#36848;&#20102;&#24037;&#20855;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#23545;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#30340;&#20511;&#37492;&#24847;&#20041;&#12290;&#22522;&#20110;&#36825;&#31867;&#24212;&#29992;&#23454;&#36341;&#65292;&#25991;&#31456;&#27010;&#36848;&#20102;&#24037;&#20855;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#21319;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#27700;&#24179;&#12290;&#25991;&#31456;&#36890;&#36807;&#31995;&#32479;&#22320;&#22238;&#39038;&#29616;&#26377;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24037;&#20855;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#36935;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08354v3 Announce Type: replace-cross  Abstract: Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmente
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#20998;&#26512;&#20102;&#26680;&#24515;&#21442;&#29031;&#35299;&#20915;&#25216;&#26415;&#65292;&#35752;&#35770;&#20102;&#22810;&#31181;&#19981;&#21516;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#26009;&#24211;&#19978;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#26032;&#31639;&#27861;&#21644;&#26032;&#26041;&#27861;&#23545;&#20110;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26680;&#24515;&#21442;&#29031;&#35299;&#20915;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.04428</link><description>&lt;p&gt;
Review of coreference resolution in English and Persian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#20998;&#26512;&#20102;&#26680;&#24515;&#21442;&#29031;&#35299;&#20915;&#25216;&#26415;&#65292;&#35752;&#35770;&#20102;&#22810;&#31181;&#19981;&#21516;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#26009;&#24211;&#19978;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#26032;&#31639;&#27861;&#21644;&#26032;&#26041;&#27861;&#23545;&#20110;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26680;&#24515;&#21442;&#29031;&#35299;&#20915;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04428v2 Announce Type: replace-cross  Abstract: Coreference resolution (CR), identifying expressions referring to the same real-world entity, is a fundamental challenge in natural language processing (NLP). This paper explores the latest advancements in CR, spanning coreference and anaphora resolution. We critically analyze the diverse corpora that have fueled CR research, highlighting their strengths, limitations, and suitability for various tasks. We examine the spectrum of evaluation metrics used to assess CR systems, emphasizing their advantages, disadvantages, and the need for more nuanced, task-specific metrics. Tracing the evolution of CR algorithms, we provide a detailed overview of methodologies, from rule-based approaches to cutting-edge deep learning architectures. We delve into mention-pair, entity-based, cluster-ranking, sequence-to-sequence, and graph neural network models, elucidating their theoretical foundations and performance on benchmark datasets. Recogni
&lt;/p&gt;</description></item></channel></rss>