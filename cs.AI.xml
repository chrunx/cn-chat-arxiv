<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#25552;&#31034;&#38598;&#24182;&#35843;&#25972;&#25552;&#31034;&#35843;&#29992;&#65292;&#20197;&#20248;&#21270;&#25552;&#31034;&#20351;&#29992;&#25928;&#29575;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#26631;&#20934;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01423</link><description>&lt;p&gt;
&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#65306;&#22312;LLM&#33258;&#21160;&#25552;&#31034;&#20013;&#20855;&#26377;&#33258;&#36866;&#24212;&#22686;&#38271;&#30340;&#29983;&#21629;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#25552;&#31034;&#38598;&#24182;&#35843;&#25972;&#25552;&#31034;&#35843;&#29992;&#65292;&#20197;&#20248;&#21270;&#25552;&#31034;&#20351;&#29992;&#25928;&#29575;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#26631;&#20934;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01423v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#25191;&#34892;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#20219;&#21153;&#65292;&#20854;&#20013;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#26174;&#33879;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#26412;&#36523;&#23384;&#22312;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#26377;&#20004;&#31181;&#65306;&#31532;&#19968;&#31181;&#65292;&#20363;&#22914;&#38142;&#24335;&#24605;&#24819;&#65288;CoT&#65289;&#65292;&#28041;&#21450;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#65292;&#22240;&#27492;&#31216;&#20026;&#19987;&#23478;&#35774;&#35745;&#25552;&#31034;&#65288;EDPs&#65289;&#12290;&#19968;&#26086;&#36825;&#20123;&#25552;&#31034;&#30830;&#31435;&#65292;&#23427;&#20204;&#23601;&#26159;&#19981;&#21487;&#25913;&#21464;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25928;&#26524;&#19978;&#38480;&#30001;&#20154;&#31867;&#35774;&#35745;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#20915;&#23450;&#12290;&#24403;&#23558;&#36825;&#20123;&#38745;&#24577;EDPs&#24212;&#29992;&#20110;LLMs&#26102;&#65292;&#23545;&#20110;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#30340;&#31616;&#21333;&#21644;&#22797;&#26434;&#38382;&#39064;&#65292;&#37117;&#20250;&#37319;&#21462;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#25991;&#26412;&#27169;&#24335;&#23545;&#31616;&#21333;&#38382;&#39064;&#30340;&#20351;&#29992;&#25928;&#29575;&#20302;&#19979;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#28041;&#21450;&#30001;LLM&#33258;&#20027;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#27492;&#31867;&#25552;&#31034;&#34987;&#31216;&#20026;&#33258;&#21161;&#24335;&#29983;&#25104;&#25552;&#31034;&#65288;AGPs&#65289;&#12290;&#20256;&#32479;&#30340;AGPs&#36890;&#36807;&#33258;&#36866;&#24212;&#24615;&#32500;&#25345;LLMs&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#28789;&#27963;&#24615;&#20173;&#28982;&#21463;&#21040;&#23616;&#38480;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#26631;&#20934;&#27169;&#24335;&#20559;&#24046;&#30340;&#32858;&#31867;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;AGPs&#19981;&#21306;&#20998;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#23548;&#33268;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#25552;&#31034;&#38271;&#24230;&#27604;&#23454;&#38469;&#38656;&#27714;&#35201;&#38271;&#65292;&#32780;&#22312;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#21457;&#25381;&#20316;&#29992;&#36739;&#24369;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#65288;RRS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#35843;&#29992;&#26469;&#36866;&#24212;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#25552;&#31034;&#38598;&#12290;&#36882;&#24402;&#25628;&#32034;&#19968;&#26041;&#38754;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#21442;&#25968;&#65292;&#25552;&#20379;&#23545;LLM&#23616;&#37096;&#33021;&#21147;&#22686;&#24378;&#30340;&#21160;&#24577;&#25552;&#31034;&#26426;&#21046;&#12290;&#36825;&#31181;&#26426;&#21046;&#30340;&#20316;&#29992;&#26159;&#20248;&#21270;&#25552;&#31034;&#20351;&#29992;&#25928;&#29575;&#65292;&#24182;&#22686;&#21152;LLM&#22312;&#38750;&#26631;&#20934;&#38382;&#39064;&#19978;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RRS&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#35777;&#26126;&#20854;&#22312;&#31616;&#21270;&#22797;&#26434;&#30340;&#38382;&#39064;&#22788;&#29702;&#21644;&#24378;&#21270;&#23545;&#38750;&#26631;&#20934;&#38382;&#39064;&#30340;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#19987;&#23478;&#35774;&#35745;&#25552;&#31034;&#21644;&#33258;&#21161;&#24335;&#29983;&#25104;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01423v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit remarkable proficiency in addressing a diverse array of tasks within the Natural Language Processing (NLP) domain, with various prompt design strategies significantly augmenting their capabilities. However, these prompts, while beneficial, each possess inherent limitations. The primary prompt design methodologies are twofold: The first, exemplified by the Chain of Thought (CoT), involves manually crafting prompts specific to individual datasets, hence termed Expert-Designed Prompts (EDPs). Once these prompts are established, they are unalterable, and their effectiveness is capped by the expertise of the human designers. When applied to LLMs, the static nature of EDPs results in a uniform approach to both simple and complex problems within the same dataset, leading to the inefficient use of tokens for straightforward issues. The second method involves prompts autonomously generated by the LLM, known 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#20559;&#22909;&#23545;&#40784;&#31574;&#30053;&#19979;&#20173;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#29702;&#35299;&#21644;&#36991;&#20813;&#36825;&#31181;&#8220;&#36867;&#36920;&#8221;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01420</link><description>&lt;p&gt;
&#12298;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#65306;&#20174;&#32479;&#35745;&#35270;&#35282;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36867;&#36920;&#12299;
&lt;/p&gt;
&lt;p&gt;
Mission Impossible: A Statistical Perspective on Jailbreaking LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#20559;&#22909;&#23545;&#40784;&#31574;&#30053;&#19979;&#20173;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#29702;&#35299;&#21644;&#36991;&#20813;&#36825;&#31181;&#8220;&#36867;&#36920;&#8221;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01420v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#21313;&#23383;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26377;&#38480;&#30340;&#36136;&#25511;&#26465;&#20214;&#19979;&#25509;&#21463;&#20102;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#24847;&#26009;&#20043;&#22806;&#30340;&#25110;&#29978;&#33267;&#26159;&#26377;&#23475;&#30340;&#34892;&#20026;&#65292;&#20363;&#22914;&#27844;&#38706;&#20449;&#24687;&#12289;&#25955;&#25773;&#34394;&#20551;&#26032;&#38395;&#25110;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#12290;&#25269;&#24481;&#25514;&#26045;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#20559;&#22909;&#23545;&#40784;&#65292;&#21253;&#25324;&#20351;&#29992;&#31934;&#24515;&#32534;&#20889;&#30340;&#25991;&#26412;&#23454;&#20363;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#23454;&#20363;&#20307;&#29616;&#20102;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#21363;&#20351;&#36825;&#26679;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#20559;&#22909;&#23545;&#40784;&#30340;LLMs&#21487;&#33021;&#20250;&#34987;&#24341;&#35825;&#20174;&#20107;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36867;&#36920;&#29616;&#35937;&#36890;&#24120;&#26159;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#30340;&#25552;&#31034;&#25991;&#26412;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#31181;&#25552;&#31034;&#25991;&#26412;&#20855;&#26377;&#23545;LLM&#30340;&#24694;&#24847;&#30340;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20174;&#32479;&#35745;&#23398;&#35270;&#35282;&#20026;&#20559;&#22909;&#23545;&#40784;&#21644;&#36867;&#36920;&#29616;&#35937;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#12290;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#39318;&#20808;&#35777;&#26126;&#20102;&#22914;&#26524;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26377;&#23475;&#34892;&#20026;&#65292;&#39044;&#35757;&#32451;&#30340;LLMs&#23558;&#27169;&#20223;&#36825;&#20123;&#34892;&#20026;&#12290;&#22522;&#20110;&#21516;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#40784;&#34892;&#20026;&#30340;&#32479;&#35745;&#27010;&#24565;&#65292;&#24182;&#23545;&#36867;&#36920;&#34892;&#20026;&#36827;&#34892;&#20102;&#19979;&#30028;&#20272;&#35745;&#12290;&#22312;&#38754;&#23545;&#23545;&#40784;&#31574;&#30053;&#30340;&#28508;&#22312;&#24369;&#28857;&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#32479;&#35745;&#26041;&#27861;&#24050;&#32463;&#21487;&#20197;&#24456;&#22909;&#22320;&#29702;&#35299;&#24182;&#19988;&#23581;&#35797;&#36991;&#20813;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36825;&#20123;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01420v1 Announce Type: cross  Abstract: Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jai
&lt;/p&gt;</description></item><item><title>"&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32570;&#20047;&#33258;&#36866;&#24212;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#27807;&#36890;&#25928;&#29575;&#25552;&#39640;&#30340;&#20851;&#38190;&#12290;"</title><link>https://arxiv.org/abs/2408.01417</link><description>&lt;p&gt;
"&#23569;&#35828;&#35805;&#65292;&#22810;&#20114;&#21160;&#65306;&#22312;&#22810;&#27169;&#24577;LLM&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#23545;&#35805;&#36866;&#24212;&#24615;"
&lt;/p&gt;
&lt;p&gt;
Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01417
&lt;/p&gt;
&lt;p&gt;
"&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32570;&#20047;&#33258;&#36866;&#24212;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#27807;&#36890;&#25928;&#29575;&#25552;&#39640;&#30340;&#20851;&#38190;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#33258;&#21457;&#22320;&#20351;&#29992;&#36234;&#26469;&#36234;&#39640;&#25928;&#30340;&#35328;&#35821;&#65292;&#36890;&#36807;&#35843;&#25972;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#12290;&#36825;&#31181;&#29616;&#35937;&#24050;&#32463;&#22312;&#21442;&#32771;&#28216;&#25103;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#35821;&#35328;&#30340;&#19968;&#20123;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#36229;&#20986;&#20102;&#20256;&#36798;&#24847;&#22270;&#30340;&#33539;&#22260;&#12290;&#33267;&#20170;&#23578;&#26410;&#25506;&#35752;&#30340;&#26159;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#26159;&#21542;&#20250;&#20687;&#20154;&#31867;&#19968;&#26679;&#22312;&#20114;&#21160;&#20013;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#37319;&#29992;&#21738;&#20123;&#26426;&#21046;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;ICCA&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#20114;&#21160;&#20013;&#30340;&#23545;&#35805;&#36866;&#24212;&#24615;&#20316;&#20026;&#19968;&#31181;&#20869;&#22312;&#34892;&#20026;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#29702;&#35299;&#20182;&#20204;&#30340;&#23545;&#35805;&#32773;&#30340;&#35328;&#35821;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#25928;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20687;&#20154;&#31867;&#37027;&#26679;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#33258;&#21457;&#22320;&#20351;&#33258;&#24049;&#30340;&#35821;&#35328;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#21518;&#32773;&#33021;&#21147;&#21482;&#33021;&#22312;&#26576;&#20123;&#27169;&#22411;(&#20363;&#22914;GPT-4)&#20013;&#36890;&#36807;&#34987;&#21160;&#30340;&#25552;&#31034;&#26041;&#24335;&#28608;&#21457;&#20986;&#26469;&#12290;&#36825;&#34920;&#26126;&#20102;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#29305;&#24615;&#36824;&#27809;&#26377;&#23436;&#20840;&#34987;&#24403;&#21069;&#30340;&#27169;&#22411;&#25152;&#25484;&#25569;&#12290;"
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01417v1 Announce Type: cross  Abstract: Humans spontaneously use increasingly efficient language as interactions progress, by adapting and forming ad-hoc conventions. This phenomenon has been studied extensively using reference games, showing properties of human language that go beyond relaying intents. It remains unexplored whether multimodal large language models (MLLMs) similarly increase communication efficiency during interactions, and what mechanisms they may adopt for this purpose. We introduce ICCA, an automated framework to evaluate such conversational adaptation as an in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and observe that while they may understand the increasingly efficient language of their interlocutor, they do not spontaneously make their own language more efficient over time. This latter ability can only be elicited in some models (e.g., GPT-4) with heavy-handed prompting. This shows that this property of linguistic interacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#35270;&#35282;&#65292;&#20998;&#31867;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#35843;&#35299;&#32773;&#31867;&#22411;&#12289;&#25628;&#32034;&#26041;&#27861;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#26356;&#22909;&#29702;&#35299;&#20854;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#26102;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01416</link><description>&lt;p&gt;
&#23547;&#25214;&#21512;&#36866;&#30340;&#35843;&#35299;&#32773;&#65306;&#22240;&#26524;&#35299;&#37322;&#24615;&#30740;&#31350;&#30340;&#21382;&#21490;&#12289;&#32508;&#36848;&#21644;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#35270;&#35282;&#65292;&#20998;&#31867;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#35843;&#35299;&#32773;&#31867;&#22411;&#12289;&#25628;&#32034;&#26041;&#27861;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#26356;&#22909;&#29702;&#35299;&#20854;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#26102;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01416v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#35299;&#37322;&#24615;&#20026;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20197;&#21450;&#20026;&#20160;&#20040;&#22312;&#29305;&#23450;&#26041;&#24335;&#19979;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#32479;&#19968;&#24615;&#65306;&#22823;&#22810;&#25968;&#30740;&#31350;&#37319;&#29992;&#19987;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#20849;&#20139;&#29702;&#35770;&#22522;&#30784;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#34913;&#37327;&#36827;&#23637;&#24182;&#19982;&#19981;&#21516;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26426;&#21046;&#29702;&#35299;&#32463;&#24120;&#34987;&#35752;&#35770;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#30340;&#22522;&#30784;&#22240;&#26524;&#21333;&#20301;&#36890;&#24120;&#27809;&#26377;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#35266;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35299;&#37322;&#24615;&#30740;&#31350;&#30340;&#21382;&#21490;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#36825;&#20123;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;&#30340;&#22240;&#26524;&#21333;&#20301;&#65288;&#35843;&#35299;&#32773;&#65289;&#30340;&#31867;&#22411;&#20197;&#21450;&#29992;&#20110;&#25628;&#32034;&#35843;&#35299;&#32773;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27599;&#31181;&#35843;&#35299;&#32773;&#30340;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#35828;&#26126;&#22312;&#26681;&#25454;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#20102;&#35299;&#21644;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20309;&#26102;&#26368;&#36866;&#24212;&#29992;&#29305;&#23450;&#31867;&#22411;&#30340;&#35843;&#35299;&#32773;&#21644;&#25628;&#32034;&#26041;&#27861;&#21462;&#20915;&#20110;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#12289;&#24037;&#20316;&#26041;&#24335;&#21450;&#20854;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01416v1 Announce Type: cross  Abstract: Interpretability provides a toolset for understanding how and why neural networks behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this paper, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate depending on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#26465;&#20214;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;LoRA&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#20102;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01415</link><description>&lt;p&gt;
&#26465;&#20214;LoRA&#21442;&#25968;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Conditional LoRA Parameter Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#26465;&#20214;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;LoRA&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#20102;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COND P-DIFF&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20102;&#23545;&#29305;&#23450;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#39640;&#24615;&#33021;LoRA&#65288;&#20302;&#31209;&#36866;&#24212;&#65289;&#21442;&#25968;&#36827;&#34892;&#25511;&#21046;&#21487;&#29983;&#25104;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#21462;&#21442;&#25968;&#30340;&#25928;&#29575;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#26465;&#20214;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#20855;&#26377;&#39640;&#34920;&#29616;&#21147;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26080;&#35770;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#36824;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23454;&#39564;&#32467;&#26524;&#22343;&#19968;&#33268;&#34920;&#26126;COND P-DIFF&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#39640;&#25928;&#29575;&#21442;&#25968;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01415v1 Announce Type: new  Abstract: Generative models have achieved remarkable success in image, video, and text domains. Inspired by this, researchers have explored utilizing generative models to generate neural network parameters. However, these efforts have been limited by the parameter size and the practicality of generating high-performance parameters. In this paper, we propose COND P-DIFF, a novel approach that demonstrates the feasibility of controllable high-performance parameter generation, particularly for LoRA (Low-Rank Adaptation) weights, during the fine-tuning process. Specifically, we employ an autoencoder to extract efficient latent representations for parameters. We then train a conditional latent diffusion model to synthesize high-performing model parameters from random noise based on specific task conditions. Experimental results in both computer vision and natural language processing domains consistently demonstrate that COND P-DIFF can generate high-pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;PC&#178;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20266;&#20998;&#31867;&#21644;&#20266;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#26469;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01349</link><description>&lt;p&gt;
PC&#178;&#65306;&#22522;&#20110;&#20266;&#20998;&#31867;&#30340;&#20266;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;PC&#178;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20266;&#20998;&#31867;&#21644;&#20266;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#26469;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#30340;&#26080;&#32541;&#38598;&#25104;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;(NCL)&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#22122;&#22768;&#36890;&#24120;&#28304;&#20110;&#25968;&#25454;&#23545;&#30340;&#38169;&#37197;&#65292;&#36825;&#26159;&#19982;&#20256;&#32479;&#26377;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#30456;&#27604;&#30340;&#19968;&#20010;&#26174;&#33879;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20266;&#20998;&#31867;&#30340;&#20266;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;(PC&#178;)&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;PC&#178;&#25552;&#20379;&#20102;&#19968;&#20010;&#19977;&#37325;&#31574;&#30053;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#8220;&#20266;&#20998;&#31867;&#8221;&#20219;&#21153;&#65292;&#23558;&#25551;&#36848;&#35299;&#37322;&#20026;&#20998;&#31867;&#26631;&#31614;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#26426;&#21046;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#22270;&#20687;-&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;PC&#178;&#30340;&#20266;&#20998;&#31867;&#33021;&#21147;&#65292;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20266;&#25551;&#36848;&#30340;&#25391;&#33633;&#26469;&#36827;&#19968;&#27493;&#22686;&#21152;&#25439;&#22833;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#21319;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21516;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01349v1 Announce Type: cross  Abstract: In the realm of cross-modal retrieval, seamlessly integrating diverse modalities within multimedia remains a formidable challenge, especially given the complexities introduced by noisy correspondence learning (NCL). Such noise often stems from mismatched data pairs, which is a significant obstacle distinct from traditional noisy labels. This paper introduces Pseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address this challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an auxiliary "pseudo-classification" task that interprets captions as categorical labels, steering the model to learn image-text semantic similarity through a non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques, capitalizing on PC$^2$'s pseudo-classification capability, we generate pseudo-captions to provide more informative and tangible supervision for each mismatched pair. Thirdly, the oscillation of pse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#23427;&#33021;&#22815;&#23558;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#26377;&#25928;&#34701;&#21512;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01343</link><description>&lt;p&gt;
StitchFusion: &#19968;&#31181;&#34701;&#21512;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#23427;&#33021;&#22815;&#23558;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#26377;&#25928;&#34701;&#21512;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01343v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#21253;&#21547;&#19987;&#38376;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#36825;&#20123;&#27169;&#22359;&#29305;&#22320;&#20026;&#29305;&#23450;&#30340;&#27169;&#24577;&#35774;&#35745;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#36755;&#20837;&#30340;&#28789;&#27963;&#24615;&#21644;&#22686;&#21152;&#20102;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#19988;&#26377;&#25928;&#30340;&#22823;&#27169;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#35813;&#26694;&#26550;&#30452;&#25509;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#29305;&#24449;&#34701;&#21512;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#34701;&#21512;&#65292;&#36890;&#36807;&#20849;&#20139;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20449;&#24687;&#27969;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#30340;&#21452;&#21521;&#20256;&#36755;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#30340;&#36755;&#20837;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#34701;&#21512;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#24577;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;&#36866;&#37197;&#22120;&#27169;&#22359;&#65288;MultiAdapter&#65289;&#65292;&#20197;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#21033;&#29992;MultiAdapter&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#26377;&#25928;&#34701;&#21512;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01343v1 Announce Type: new  Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#23478;&#31995;&#32479;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#20132;&#20114;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01342</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Leveraging Knowledge Graph Embedding for Effective Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#23478;&#31995;&#32479;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#20132;&#20114;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01342v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26368;&#36817;&#65292;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#65292;&#23427;&#32467;&#21512;&#20102;&#23545;&#35805;&#31995;&#32479;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#27604;&#65292;CRS&#36890;&#36807;&#20114;&#21160;&#65288;&#21363;&#23545;&#35805;&#65289;&#26356;&#22909;&#22320;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CRS&#30740;&#31350;&#24573;&#35270;&#20102;&#26377;&#25928;&#22788;&#29702;&#23646;&#24615;&#12289;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25552;&#20986;&#19981;&#24403;&#30340;&#38382;&#39064;&#21644;&#25512;&#33616;&#19981;&#24403;&#30340;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;&#31216;&#20026;KG-CRS&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#29992;&#25143;-&#39033;&#30446;&#22270;&#21644;&#39033;&#30446;-&#23646;&#24615;&#22270;&#25972;&#21512;&#21040;&#19968;&#20010;&#21160;&#24577;&#22270;&#20013;&#65292;&#21363;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#21160;&#24577;&#21464;&#21270;&#65292;&#36890;&#36807;&#31227;&#38500;&#36127;&#38754;&#39033;&#30446;&#25110;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#37051;&#25509;&#33410;&#28857;&#20043;&#38388;&#30340;&#20256;&#25773;&#65292;&#23398;&#20064;&#20102;&#29992;&#25143;&#12289;&#39033;&#30446;&#21644;&#23646;&#24615;&#30340;&#20449;&#24687;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#23478;&#31995;&#32479;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#29983;&#25104;&#20505;&#36873;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#35821;&#35328;&#30340;&#34920;&#31034;&#26469;&#35299;&#37322;&#29992;&#25143;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#20132;&#20114;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#21160;&#24577;&#22270;&#23884;&#20837;&#26041;&#27861;&#22312;&#23545;&#35805;&#25512;&#33616;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#25512;&#33616;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;KG-CRS&#22312;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#20919;&#21551;&#21160;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#23545;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#37117;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01342v1 Announce Type: cross  Abstract: Conversational recommender system (CRS), which combines the techniques of dialogue system and recommender system, has obtained increasing interest recently. In contrast to traditional recommender system, it learns the user preference better through interactions (i.e. conversations), and then further boosts the recommendation performance. However, existing studies on CRS ignore to address the relationship among attributes, users, and items effectively, which might lead to inappropriate questions and inaccurate recommendations. In this view, we propose a knowledge graph based conversational recommender system (referred as KG-CRS). Specifically, we first integrate the user-item graph and item-attribute graph into a dynamic graph, i.e., dynamically changing during the dialogue process by removing negative items or attributes. We then learn informative embedding of users, items, and attributes by also considering propagation through neighbo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01334</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#38271;&#26399;&#20219;&#21153;&#29702;&#35299;&#30340;&#39592;&#24178;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Backbone for Long-Horizon Robot Task Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#31471;&#21040;&#31471;&#26426;&#22120;&#20154;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#20219;&#21153;&#39046;&#22495;&#65292;&#24120;&#24120;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#32467;&#26524;&#21644;&#19981;&#33391;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#21363;TBBF (Therblig-based Backbone Framework)&#65292;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;&#21644;&#36716;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;&#20351;&#29992;therbligs&#65288;&#22522;&#26412;&#21160;&#20316;&#20803;&#32032;&#65289;&#20316;&#20026;&#25903;&#25745;&#65292;&#24182;&#19982;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#29702;&#35299;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#12290;&#22312;&#31163;&#32447;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Meta-RGate SynerFusion (MGSF)&#32593;&#32476;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#21508;&#31181;&#20219;&#21153;&#30340;therbligs&#12290;&#22312;&#32447;&#27979;&#35797;&#38454;&#27573;&#65292;&#22312;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#28436;&#31034;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;MGSF&#32593;&#32476;&#25552;&#21462;&#39640;&#38454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;ActionREG&#65288;&#21160;&#20316;&#27880;&#20876;&#65289;&#23558;&#20854;&#32534;&#30721;&#25104;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;Meta-Learner&#65292;&#23427;&#21487;&#20197;&#20174;&#21333;&#20010;&#20219;&#21153;&#30340;&#34920;&#29616;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#27867;&#21270;&#21040;&#19981;&#21516;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#20013;&#36827;&#34892;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#25552;&#21319;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 Announce Type: new  Abstract: End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Additionally
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25195;&#25551;&#36335;&#24452;&#31639;&#27861;&#65292;&#23427;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#23545;&#35937;&#32447;&#32034;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#24341;&#23548;&#35270;&#32447;&#34892;&#20026;&#65292;&#24182;&#27169;&#25311;&#20102;&#35266;&#23519;&#32773;&#30340;&#23454;&#38469;&#35266;&#30475;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2408.01322</link><description>&lt;p&gt;
&#21551;&#21457;&#33258;&#26426;&#22120;&#20154;&#30340;&#25195;&#25551;&#36335;&#24452;&#27169;&#22411;&#25581;&#31034;&#21160;&#24577;&#22330;&#26223;&#20013;&#19981;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#23545;&#35937;&#32447;&#32034;&#23545;&#20110;&#35270;&#32447;&#24341;&#23548;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty and Semantic Object Cues for Gaze Guidance in Dynamic Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25195;&#25551;&#36335;&#24452;&#31639;&#27861;&#65292;&#23427;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#23545;&#35937;&#32447;&#32034;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#24341;&#23548;&#35270;&#32447;&#34892;&#20026;&#65292;&#24182;&#27169;&#25311;&#20102;&#35266;&#23519;&#32773;&#30340;&#23454;&#38469;&#35266;&#30475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01322v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#25105;&#20204;&#21608;&#22260;&#19990;&#30028;&#24863;&#30693;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#25105;&#20204;&#25152;&#31215;&#26497;&#20851;&#27880;&#30340;&#20869;&#23481;&#65292;&#20294;&#25105;&#20204;&#30524;&#29699;&#30340;&#31227;&#21160;&#21448;&#20381;&#36182;&#20110;&#24863;&#30693;&#21040;&#30340;&#20107;&#29289;&#12290;&#28982;&#32780;&#65292;&#29289;&#20307;&#20998;&#21106;&#21644;&#30524;&#29699;&#36816;&#21160;&#30340;&#36890;&#24120;&#34987;&#30475;&#20316;&#26159;&#20004;&#20010;&#29420;&#31435;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26426;&#21046;&#27169;&#22411;&#65292;&#23427;&#23545;&#20110;&#21160;&#24577;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#27169;&#25311;&#36825;&#20123;&#36807;&#31243;&#65292;&#24182;&#20511;&#37492;&#20102;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#20449;&#24687;&#22788;&#29702;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#22270;&#20687;&#21487;&#35745;&#31639;&#27169;&#22411;&#22312;&#21160;&#24577;&#22330;&#26223;&#19979;&#65292;&#19981;&#20165;&#20351;&#29992;&#24403;&#21069;&#30340;&#29289;&#20307;&#20998;&#21106;&#36827;&#34892;&#22522;&#20110;&#29289;&#20307;&#30340;&#36339;&#36291;&#20915;&#31574;&#65292;&#32780;&#19988;&#36824;&#20351;&#29992;&#32858;&#28966;&#28857;&#29289;&#20307;&#26469;&#21453;&#22797;&#25913;&#36827;&#20854;&#22330;&#26223;&#20998;&#21106;&#32467;&#26524;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#25913;&#36827;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#29992;&#20110;&#25351;&#23548;&#20027;&#21160;&#22330;&#26223;&#25506;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#27169;&#25311;&#35266;&#23519;&#32773;&#33258;&#30001;&#35266;&#30475;&#34892;&#20026;&#26102;&#65292;&#19982;&#25195;&#25551;&#36335;&#24452;&#32479;&#35745;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#65292;&#21253;&#25324;&#29992;&#20110;&#21442;&#25968;&#25311;&#21512;&#30340;&#32858;&#28966;&#25345;&#32493;&#26102;&#38388;&#21644;&#36339;&#36291;&#24133;&#24230;&#20998;&#24067;&#20197;&#21450;&#26356;&#39640;&#30340;&#23618;&#38754;&#19978;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#19981;&#20165;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#27169;&#25311;&#35266;&#23519;&#32773;&#33258;&#30001;&#35266;&#30475;&#34892;&#20026;&#26102;&#65292;&#19982;&#25195;&#25551;&#36335;&#24452;&#32479;&#35745;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#65292;&#21253;&#25324;&#29992;&#20110;&#21442;&#25968;&#25311;&#21512;&#30340;&#32858;&#28966;&#25345;&#32493;&#26102;&#38388;&#21644;&#36339;&#36291;&#24133;&#24230;&#20998;&#24067;&#20197;&#21450;&#26356;&#39640;&#30340;&#23618;&#38754;&#19978;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#27169;&#25311;&#35266;&#23519;&#32773;&#33258;&#30001;&#35266;&#30475;&#34892;&#20026;&#26102;&#65292;&#19982;&#25195;&#25551;&#36335;&#24452;&#32479;&#35745;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#65292;&#21253;&#25324;&#29992;&#20110;&#21442;&#25968;&#25311;&#21512;&#30340;&#32858;&#28966;&#25345;&#32493;&#26102;&#38388;&#21644;&#36339;&#36291;&#24133;&#24230;&#20998;&#24067;&#20197;&#21450;&#26356;&#39640;&#30340;&#23618;&#38754;&#19978;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01322v1 Announce Type: new  Abstract: How we perceive objects around us depends on what we actively attend to, yet our eye movements depend on the perceived objects. Still, object segmentation and gaze behavior are typically treated as two independent processes. Drawing on an information processing pattern from robotics, we present a mechanistic model that simulates these processes for dynamic real-world scenes. Our image-computable model uses the current scene segmentation for object-based saccadic decision-making while using the foveated object to refine its scene segmentation recursively. To model this refinement, we use a Bayesian filter, which also provides an uncertainty estimate for the segmentation that we use to guide active scene exploration. We demonstrate that this model closely resembles observers' free viewing behavior, measured by scanpath statistics, including foveation duration and saccade amplitude distributions used for parameter fitting and higher-level s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;en_tldr: This paper provides a comprehensive review of the performance and challenges of multimodal large language models across various tasks, suggesting future research directions.</title><link>https://arxiv.org/abs/2408.01319</link><description>&lt;p&gt;
&#20840;&#38754;&#22238;&#39038;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;en_tldr: This paper provides a comprehensive review of the performance and challenges of multimodal large language models across various tasks, suggesting future research directions.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25968;&#25454;&#29190;&#28856;&#21644;&#31185;&#25216;&#24555;&#36895;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22788;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21069;&#27839;&#12290;&#36825;&#31181;&#27169;&#22411;&#35774;&#35745;&#29992;&#20110;&#26080;&#32541;&#25972;&#21512;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#24207;&#21015;&#65292;&#36828;&#36229;&#21333;&#19968;&#27169;&#24335;&#31995;&#32479;&#30340;&#22797;&#26434;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;MLLM&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#26803;&#29702;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#35782;&#21035;&#21644;&#38899;&#39057;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19981;&#21516;MLLM&#22312;&#29305;&#23450;&#20219;&#21153;&#20043;&#38388;&#30340;&#37325;&#28857;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;MLLM&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#36825;&#20123;&#35752;&#35770;&#65292;&#26412;&#25991;&#24076;&#26395;&#33021;&#22815;&#20026;MLLM&#30340;&#26410;&#26469;&#21457;&#23637;&#21644;&#24212;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01319v1 Announce Type: new  Abstract: In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26088;&#22312;&#25552;&#39640;AI&#39044;&#27979;&#36879;&#26126;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#19968;&#31995;&#21015;&#24037;&#20855;&#21644;&#26041;&#27861;&#65292;&#20026;AI&#22914;&#20309;&#33258;&#25105;&#35780;&#20272;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2408.01301</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#30340;&#20026;&#35774;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;AI&#33258;&#25105;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Decision-driven Methodology for Designing Uncertainty-aware AI Self-Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26088;&#22312;&#25552;&#39640;AI&#39044;&#27979;&#36879;&#26126;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#19968;&#31995;&#21015;&#24037;&#20855;&#21644;&#26041;&#27861;&#65292;&#20026;AI&#22914;&#20309;&#33258;&#25105;&#35780;&#20272;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01301v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;(AI)&#24050;&#32463;&#38761;&#21629;&#21270;&#20102;&#31038;&#20250;&#30340;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#21147;&#30340;&#22330;&#26223;&#20013;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;AI&#22312;&#21463;&#25511;&#29615;&#22659;&#19979;&#30340;&#39044;&#27979;&#33021;&#21147;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#22312;&#21508;&#31181;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#23427;&#20173;&#38754;&#20020;&#19968;&#31995;&#21015;&#23454;&#38469;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#32780;&#35328;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#32473;&#23450;&#30340;AI&#31995;&#32479;&#22312;&#20854;&#19979;&#28216;&#24212;&#29992;&#20013;&#26159;&#21542;&#21487;&#20449;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#24212;&#23545;&#23545;&#26356;&#20855;&#36879;&#26126;&#24230;&#12289;&#38887;&#24615;&#21644;&#21487;&#20449;&#24230;AI&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#20351;AI&#33021;&#22815;&#8220;&#33258;&#25105;&#35780;&#20272;&#8221;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;AI&#33258;&#25105;&#35780;&#20272;&#30340;&#26041;&#27861;&#25353;&#29031;&#20960;&#20010;&#20851;&#38190;&#32500;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#25321;&#21644;&#35774;&#35745;&#36866;&#24403;&#26041;&#27861;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01301v1 Announce Type: cross  Abstract: Artificial intelligence (AI) has revolutionized decision-making processes and systems throughout society and, in particular, has emerged as a significant technology in high-impact scenarios of national interest. Yet, despite AI's impressive predictive capabilities in controlled settings, it still suffers from a range of practical setbacks preventing its widespread use in various critical scenarios. In particular, it is generally unclear if a given AI system's predictions can be trusted by decision-makers in downstream applications. To address the need for more transparent, robust, and trustworthy AI systems, a suite of tools has been developed to quantify the uncertainty of AI predictions and, more generally, enable AI to "self-assess" the reliability of its predictions. In this manuscript, we categorize methods for AI self-assessment along several key dimensions and provide guidelines for selecting and designing the appropriate method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#35780;&#20272;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#31639;&#27861;&#24605;&#32500;&#33021;&#21147;&#30340;&#34394;&#25311;CAT&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24179;&#21488;&#38477;&#20302;&#20102;&#20154;&#21147;&#25104;&#26412;&#24182;&#19988;&#25552;&#20379;&#20102;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2408.01263</link><description>&lt;p&gt;
&#34394;&#25311;CAT&#65306;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#31639;&#27861;&#24605;&#32500;&#35780;&#20272;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
The virtual CAT: A tool for algorithmic thinking assessment in Swiss compulsory education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#35780;&#20272;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#31639;&#27861;&#24605;&#32500;&#33021;&#21147;&#30340;&#34394;&#25311;CAT&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24179;&#21488;&#38477;&#20302;&#20102;&#20154;&#21147;&#25104;&#26412;&#24182;&#19988;&#25552;&#20379;&#20102;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01263v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#65306;&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#25484;&#25569;&#31639;&#27861;&#24605;&#32500;&#65288;AT&#65289;&#25216;&#33021;&#19981;&#20165;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#30456;&#20851;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20010;&#20154;&#33021;&#22815;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#21487;&#31649;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#34892;&#21160;&#24207;&#21015;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#25945;&#32946;&#29615;&#22659;&#20013;&#31639;&#27861;&#24605;&#32500;&#35780;&#20272;&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#22788;&#29702;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#25311;Cross Array Task&#65288;CAT&#65289;&#30340;&#25968;&#23383;&#35780;&#20272;&#24037;&#20855;&#65292;&#23427;&#26159;&#23545;&#20256;&#32479;&#26080;&#25554;&#30005;&#35780;&#20272;&#27963;&#21160;&#30340;&#25913;&#36827;&#35774;&#35745;&#65292;&#26088;&#22312;&#35780;&#20272;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#30340;&#31639;&#27861;&#33021;&#21147;&#12290;&#36825;&#27454;&#24037;&#20855;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#21644;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#20154;&#21147;&#21442;&#19982;&#24182;&#32531;&#35299;&#20102;&#28508;&#22312;&#30340;&#25968;&#25454;&#25910;&#38598;&#38169;&#35823;&#12290;&#35813;&#24179;&#21488;&#25317;&#26377;&#22522;&#20110;&#25163;&#21183;&#30340;&#32534;&#31243;&#30028;&#38754;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#31215;&#26408;&#32534;&#31243;&#25509;&#21475;&#65292;&#30830;&#20445;&#20102;&#23545;&#19981;&#21516;&#23398;&#20064;&#32773;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25903;&#25345;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#34394;&#25311;CAT&#24179;&#21488;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#35797;&#28857;&#35780;&#20272;&#22312;&#29790;&#22763;&#25490;&#21517;&#21069;100&#30340;&#39640;&#20013;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#24179;&#21488;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#23398;&#29983;&#30340;&#31639;&#27861;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01263v1 Announce Type: cross  Abstract: In today's digital era, holding algorithmic thinking (AT) skills is crucial, not only in computer science-related fields. These abilities enable individuals to break down complex problems into more manageable steps and create a sequence of actions to solve them. To address the increasing demand for AT assessments in educational settings and the limitations of current methods, this paper introduces the virtual Cross Array Task (CAT), a digital adaptation of an unplugged assessment activity designed to evaluate algorithmic skills in Swiss compulsory education. This tool offers scalable and automated assessment, reducing human involvement and mitigating potential data collection errors. The platform features gesture-based and visual block-based programming interfaces, ensuring its usability for diverse learners, further supported by multilingual capabilities. To evaluate the virtual CAT platform, we conducted a pilot evaluation in Switzer
&lt;/p&gt;</description></item><item><title>TrIM &#19977;&#35282;&#24418;&#36755;&#20837;&#36816;&#21160;&#21516;&#27493;&#38453;&#21015;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#23450;&#25968;&#25454;&#27969;&#20943;&#23569;&#23545;&#22823;&#35268;&#27169;&#20869;&#23384;&#30340;&#35775;&#38382;&#27425;&#25968;&#65292;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#35745;&#31639;&#25552;&#20379;&#26356;&#39640;&#33021;&#25928;&#12290;</title><link>https://arxiv.org/abs/2408.01254</link><description>&lt;p&gt;
TrIM: &#19977;&#35282;&#24418;&#36755;&#20837;&#36816;&#21160;&#21516;&#27493;&#38453;&#21015;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; -- &#31532;&#19968;&#37096;&#20998;&#65306;&#25968;&#25454;&#27969;&#21644;&#20998;&#26512;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks -- Part I: Dataflow and Analytical Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01254
&lt;/p&gt;
&lt;p&gt;
TrIM &#19977;&#35282;&#24418;&#36755;&#20837;&#36816;&#21160;&#21516;&#27493;&#38453;&#21015;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#23450;&#25968;&#25454;&#27969;&#20943;&#23569;&#23545;&#22823;&#35268;&#27169;&#20869;&#23384;&#30340;&#35775;&#38382;&#27425;&#25968;&#65292;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#35745;&#31639;&#25552;&#20379;&#26356;&#39640;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01254v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#23450;&#26631;&#39064;&#65306;&#20026;&#20102;&#36319;&#19978;&#39640;&#32423;AI&#27169;&#22411;&#26085;&#30410;&#22797;&#26434;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#25968;&#25454;&#24378;&#24230;&#65292;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#27491;&#22312;&#34987;&#25552;&#20986;&#12290;&#36825;&#20123;&#33539;&#24335;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#20174;&#22788;&#29702;&#26680;&#24515;&#21040;&#20869;&#23384;&#30340;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#26469;&#23454;&#29616;&#39640;&#33021;&#25928;&#65292;&#20174;&#32780;&#20943;&#36731;&#20911;&#35834;&#20381;&#26364;&#29942;&#39048;&#65292;&#36825;&#26159;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#19982;&#22788;&#29702;&#26680;&#24515;&#21040;&#20869;&#23384;&#33021;&#32791;&#30456;&#20851;&#30340;&#19968;&#31181;&#29616;&#35937;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#12290;&#21516;&#27493;&#38453;&#21015;&#65288;SA&#65289;&#26159;&#20943;&#36731;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#30340;&#28508;&#22312;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#29305;&#23450;&#30340;&#25968;&#25454;&#27969;&#65288;&#22914;&#26435;&#37325;&#22266;&#23450;&#21644;&#25490;&#22266;&#23450;&#65289;&#20943;&#23569;&#23545;&#20027;&#20869;&#23384;&#30340;&#35775;&#38382;&#27425;&#25968;&#12290;&#36825;&#20123;PEs&#19981;&#26029;&#22312;&#29305;&#23450;&#25968;&#25454;&#27969;&#19979;&#20132;&#25442;&#21644;&#22788;&#29702;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#22823;&#35268;&#27169;&#20869;&#23384;&#30340;&#35775;&#38382;&#27425;&#25968;&#12290;SA&#30340;&#30828;&#20214;&#29305;&#27530;&#21270;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20174;&#30697;&#38453;&#20056;&#27861;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01254v1 Announce Type: new  Abstract: In order to follow the ever-growing computational complexity and data intensity of state-of-the-art AI models, new computing paradigms are being proposed. These paradigms aim at achieving high energy efficiency, by mitigating the Von Neumann bottleneck that relates to the energy cost of moving data between the processing cores and the memory. Convolutional Neural Networks (CNNs) are particularly susceptible to this bottleneck, given the massive data they have to manage. Systolic Arrays (SAs) are promising architectures to mitigate the data transmission cost, thanks to high data utilization carried out by an array of Processing Elements (PEs). These PEs continuously exchange and process data locally based on specific dataflows (like weight stationary and row stationary), in turn reducing the number of memory accesses to the main memory. The hardware specialization of SAs can meet different workloads, ranging from matrix multiplications to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20195;&#29702;&#20154;&#19981;&#23436;&#20840;&#20102;&#35299;&#25152;&#38754;&#20020;&#30340;MDP&#30340;&#27010;&#29575;&#20998;&#24067;&#26102;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#20248;&#21270;&#25512;&#29702;&#36807;&#31243;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#20154;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01253</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#30340;&#20803;&#25512;&#29702;&#65306;&#22522;&#20110;BAMDP&#26694;&#26550;&#30340;&#20803;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Metareasoning in uncertain environments: a meta-BAMDP framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20195;&#29702;&#20154;&#19981;&#23436;&#20840;&#20102;&#35299;&#25152;&#38754;&#20020;&#30340;MDP&#30340;&#27010;&#29575;&#20998;&#24067;&#26102;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#20248;&#21270;&#25512;&#29702;&#36807;&#31243;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#20154;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01253v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#20915;&#31574;&#21046;&#23450;&#22330;&#26223;&#20013;&#65292;&#21487;&#20197;&#35748;&#20026;&#8220;&#25512;&#29702;&#8221;&#26159;&#19968;&#31181;&#31639;&#27861;$P$&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;$a^* \in \mathcal{A}$&#65292;&#26088;&#22312;&#20248;&#21270;&#19968;&#20123;&#32467;&#26524;&#65292;&#22914;&#26368;&#22823;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25191;&#34892;$P$&#26412;&#36523;&#21487;&#33021;&#28041;&#21450;&#21040;&#19968;&#20123;&#25104;&#26412;&#65288;&#26102;&#38388;&#12289;&#33021;&#37327;&#12289;&#26377;&#38480;&#30340;&#33021;&#21147;&#31561;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#32771;&#34385;&#19982;&#25191;&#34892;&#36873;&#25321;&#22312;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#30452;&#25509;&#33719;&#24471;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#36825;&#26679;&#30340;&#25104;&#26412;&#38656;&#35201;&#22312;&#20934;&#30830;&#24314;&#27169;&#20154;&#31867;&#34892;&#20026;&#20197;&#21450;&#20248;&#21270;&#20154;&#24037;A&#35745;&#21010;&#26102;&#34987;&#32771;&#34385;&#36827;&#21435;&#65292;&#22240;&#20026;&#25152;&#26377;&#29289;&#29702;&#31995;&#32479;&#37117;&#38754;&#20020;&#30528;&#36164;&#28304;&#38480;&#21046;&#12290;&#25214;&#21040;&#27491;&#30830;&#30340;$P$&#26412;&#36523;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;$P$&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#34987;&#31216;&#20316;&#8220;&#20803;&#25512;&#29702;&#8221;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#20803;&#25512;&#29702;&#27169;&#22411;&#20551;&#35774;&#20195;&#29702;&#30693;&#36947;&#24213;&#23618;MDP&#30340;&#36716;&#31227;&#21644;&#22870;&#21169;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20195;&#29702;&#20154;&#23545;&#25152;&#38754;&#20020;&#30340;MDP&#30340;&#19968;&#38454;&#27010;&#29575;&#20998;&#24067;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#20248;&#21270;&#25512;&#29702;&#36807;&#31243;&#30340;&#36873;&#25321;&#12290;&#35813;&#26694;&#26550;&#20197;&#36125;&#21494;&#26031;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(BAMDP)&#20026;&#22522;&#30784;&#65292;&#20294;&#21435;&#25481;&#20102;&#20195;&#29702;&#20154;&#23545;MDP&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#21069;&#25552;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;\textit{meta-BAMDP}&#65288;&#20803;BAMDP&#65289;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#30340;&#20248;&#21270;&#20026;&#20195;&#29702;&#20154;&#25552;&#20379;&#20102;&#25191;&#34892;&#31574;&#30053;&#65292;&#35813;&#25191;&#34892;&#31574;&#30053;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#20102;&#35299;&#29615;&#22659;&#27010;&#29575;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01253v1 Announce Type: new  Abstract: In decision-making scenarios, \textit{reasoning} can be viewed as an algorithm $P$ that makes a choice of an action $a^* \in \mathcal{A}$, aiming to optimize some outcome such as maximizing the value function of a Markov decision process (MDP). However, executing $P$ itself may bear some costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making the choice in the underlying decision problem. Such costs need to be taken into account in order to accurately model human behavior, as well as optimizing AI planning, as all physical systems are bound to face resource constraints. Finding the right $P$ can itself be framed as an optimization problem over the space of reasoning processes $P$, generally referred to as \textit{metareasoning}. Conventionally, human metareasoning models assume that the agent knows the transition and reward distributions of the underlying MDP. This paper gener
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#21464;&#36164;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;IRS&#21644;UAV&#36741;&#21161;&#30340;MEC&#31995;&#32479;&#20013;&#30340;&#33021;&#32791;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#20195;&#29702;&#21644;&#28176;&#36827;&#26102;&#38388;&#35843;&#24230;&#22120;&#35299;&#20915;&#20102;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01248</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#21464;&#36164;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;IRS&#21644;UAV&#36741;&#21161;&#30340;MEC&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep progressive reinforcement learning-based flexible resource scheduling framework for IRS and UAV-assisted MEC system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01248
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#21464;&#36164;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;IRS&#21644;UAV&#36741;&#21161;&#30340;MEC&#31995;&#32479;&#20013;&#30340;&#33021;&#32791;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#20195;&#29702;&#21644;&#28176;&#36827;&#26102;&#38388;&#35843;&#24230;&#22120;&#35299;&#20915;&#20102;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01248v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;&#65306;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#65288;IRS&#65289;&#21644;&#26080;&#20154;&#39550;&#39542;&#33322;&#31354;&#22120;&#65288;UAV&#65289;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#31995;&#32479;&#22312;&#20020;&#26102;&#21644;&#32039;&#24613;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;UAV&#20301;&#32622;&#12289;IRS&#30456;&#20301;&#20559;&#31227;&#12289;&#20219;&#21153;&#21368;&#36733;&#21644;&#36164;&#28304;&#30340;&#20998;&#37197;&#26469;&#26368;&#23567;&#21270;MEC&#31995;&#32479;&#20013;&#30340;&#33021;&#32791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Flexible REsource Scheduling (FRES)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#25216;&#26415;&#21253;&#25324;&#20197;&#19979;&#21019;&#26032;&#28857;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#20219;&#21153;&#20195;&#29702;&#20197;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#12290;&#22810;&#20219;&#21153;&#20195;&#29702;&#26377;&#20004;&#20010;&#36755;&#20986;&#22836;&#65292;&#20998;&#21035;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#31867;&#22836;&#34987;&#29992;&#20110;&#24102;&#26377;&#25972;&#25968;&#21464;&#37327;&#30340;&#21368;&#36733;&#20915;&#31574;&#65292;&#32780;&#21478;&#19968;&#31867;&#22836;&#21017;&#34987;&#29992;&#20110;&#24102;&#26377;&#36830;&#32493;&#21464;&#37327;&#30340;&#36164;&#28304;&#20998;&#37197;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28176;&#36827;&#30340;&#26102;&#38388;&#35843;&#24230;&#22120;&#20197;&#20248;&#21270;&#20219;&#21153;&#23398;&#20064;&#36807;&#31243;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#36880;&#27493;&#25366;&#25496;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#39118;&#30005;&#32806;&#21512;&#20316;&#29992;&#30340;&#20869;&#22312;&#36923;&#36753;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#22522;&#20110;&#39118;&#30005;&#30340;MEC&#31995;&#32479;&#20248;&#21270;&#35843;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01248v1 Announce Type: cross  Abstract: The intelligent reflection surface (IRS) and unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) system is widely used in temporary and emergency scenarios. Our goal is to minimize the energy consumption of the MEC system by jointly optimizing UAV locations, IRS phase shift, task offloading, and resource allocation with a variable number of UAVs. To this end, we propose a Flexible REsource Scheduling (FRES) framework by employing a novel deep progressive reinforcement learning which includes the following innovations: Firstly, a novel multi-task agent is presented to deal with the mixed integer nonlinear programming (MINLP) problem. The multi-task agent has two output heads designed for different tasks, in which a classified head is employed to make offloading decisions with integer variables while a fitting head is applied to solve resource allocation with continuous variables. Secondly, a progressive scheduler is intro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#23454;&#20102; GPT-4 &#22312;&#33258;&#21160;&#21270;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;&#26041;&#38754;&#36229;&#36234;&#20102; GPT-3.5-Turbo&#65292;&#20026;&#31934;&#30830;&#21307;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.01214</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36890;&#37327;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-Throughput Phenotyping of Clinical Text Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#23454;&#20102; GPT-4 &#22312;&#33258;&#21160;&#21270;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;&#26041;&#38754;&#36229;&#36234;&#20102; GPT-3.5-Turbo&#65292;&#20026;&#31934;&#30830;&#21307;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01214v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#39640;&#36890;&#37327;&#34920;&#22411;&#33258;&#21160;&#21270;&#33258;&#21160;&#23558;&#24739;&#32773;&#30151;&#29366;&#26144;&#23556;&#21040;&#26631;&#20934;&#21270;&#27010;&#24565; ontology&#65292;&#36825;&#23545;&#20110;&#31934;&#30830;&#21307;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#22312;&#32447;&#23391;&#24503;&#23572;&#36951;&#20256;&#65288;OMIM&#65289;&#25968;&#25454;&#24211;&#20013;&#20020;&#24202;&#24635;&#32467;&#34920;&#22411;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#23427;&#20204;&#20016;&#23500;&#30340;&#34920;&#22411;&#25968;&#25454;&#65292;&#36825;&#20123;&#24635;&#32467;&#21487;&#20197;&#20316;&#20026;&#21307;&#29983;&#35760;&#24405;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102; GPT-4 &#21644; GPT-3.5-Turbo &#20043;&#38388;&#30340;&#24615;&#33021;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#26631;&#20934;&#21270;&#30151;&#29366;&#26041;&#38754;&#65292;GPT-4&#36229;&#36807;&#20102; GPT-3.5-Turbo&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20869;&#37096;&#35780;&#20998;&#32773;&#30340;&#20849;&#35782;&#30456;&#24403;&#12290;&#23613;&#31649;&#22312;&#30151;&#29366;&#26631;&#20934;&#21270;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294; GPT-4 &#30340;&#24191;&#27867;&#39044;&#35757;&#32451;&#22312;&#36328;&#22810;&#20010;&#34920;&#22411;&#20219;&#21153;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#26222;&#36941;&#36866;&#29992;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35745;&#23558;&#25104;&#20026;&#33258;&#21160;&#21270;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;&#30340;&#20027;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01214v1 Announce Type: cross  Abstract: High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20102;&#21517;&#20026;Deep W-Learning&#65288;DWN&#65289;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#20852;Web&#26381;&#21153;&#22120;&#31034;&#20363;&#65292;&#20197;&#22312;&#36816;&#34892;&#26102;&#25214;&#21040;&#26368;&#20339;&#24615;&#33021;&#20248;&#21270;&#37197;&#32622;&#65292;&#24182;&#19982;&#20004;&#31181;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#23454;&#29616;&#65288;&#949;-&#36138;&#23146;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65289;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2408.01188</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Deep Reinforcement Learning for Optimisation in Autonomous Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20102;&#21517;&#20026;Deep W-Learning&#65288;DWN&#65289;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#20852;Web&#26381;&#21153;&#22120;&#31034;&#20363;&#65292;&#20197;&#22312;&#36816;&#34892;&#26102;&#25214;&#21040;&#26368;&#20339;&#24615;&#33021;&#20248;&#21270;&#37197;&#32622;&#65292;&#24182;&#19982;&#20004;&#31181;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#23454;&#29616;&#65288;&#949;-&#36138;&#23146;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65289;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01188v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22312;&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#33021;&#22312;&#36816;&#34892;&#26102;&#23398;&#20064;&#32780;&#19981;&#38656;&#35201;&#29615;&#22659;&#27169;&#22411;&#25110;&#39044;&#23450;&#20041;&#21160;&#20316;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;AS&#20013;RL&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21482;&#33021;&#20248;&#21270;&#19968;&#20010;&#30446;&#26631;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#31995;&#32479;&#65288;&#22914;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#20013;&#65292;&#38656;&#35201;&#23558;&#22810;&#20010;&#30446;&#26631;&#22312;&#21333;&#20010;&#30446;&#26631;&#20989;&#25968;&#20013;&#20197;&#39044;&#20808;&#23450;&#20041;&#30340;&#26435;&#37325;&#32467;&#21512;&#12290;&#23384;&#22312;&#22810;&#31181;MORL&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#25968;&#21482;&#22312;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#24212;&#29992;&#65292;&#32780;&#19981;&#26159;&#22312;&#30495;&#23454;&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21517;&#20026;Deep W-Learning&#65288;DWN&#65289;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#22312;&#35832;&#22914;&#33258;&#36866;&#24212;&#26381;&#21153;&#22120;&#36825;&#26679;&#30340;&#26032;&#20852;Web&#26381;&#21153;&#22120;&#31034;&#20363;&#20013;&#23545;&#20854;&#36827;&#34892;&#24212;&#29992;&#65292;&#20197;&#25214;&#21040;&#22312;&#36816;&#34892;&#26102;&#24615;&#33021;&#20248;&#21270;&#26041;&#38754;&#30340;&#29702;&#24819;&#37197;&#32622;&#12290;&#25105;&#20204;&#23558;DWN&#19982;&#20004;&#31181;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#949;-&#36138;&#23146;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;DW
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01188v1 Announce Type: new  Abstract: Reinforcement Learning (RL) is used extensively in Autonomous Systems (AS) as it enables learning at runtime without the need for a model of the environment or predefined actions. However, most applications of RL in AS, such as those based on Q-learning, can only optimize one objective, making it necessary in multi-objective systems to combine multiple objectives in a single objective function with predefined weights. A number of Multi-Objective Reinforcement Learning (MORL) techniques exist but they have mostly been applied in RL benchmarks rather than real-world AS systems. In this work, we use a MORL technique called Deep W-Learning (DWN) and apply it to the Emergent Web Servers exemplar, a self-adaptive server, to find the optimal configuration for runtime performance optimization. We compare DWN to two single-objective optimization implementations: {\epsilon}-greedy algorithm and Deep Q-Networks. Our initial evaluation shows that DW
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25972;&#21512;&#20102;&#22810;&#31181;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#65288;Particle Swarm Optimization, Ant Colony Optimization&#31561;&#65289;&#20110;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#36864;&#28779;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#19979;&#33719;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01187</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#31574;&#30053;&#20248;&#21270; variational &#37327;&#23376;&#30005;&#36335;&#30340;&#24378;&#21270;&#23398;&#20064; Metaheuristic &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Variational Quantum Circuits Using Metaheuristic Strategies in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25972;&#21512;&#20102;&#22810;&#31181;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#65288;Particle Swarm Optimization, Ant Colony Optimization&#31561;&#65289;&#20110;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#36864;&#28779;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#19979;&#33719;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01187v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30456;&#36739;&#20110;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#31616;&#27905;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#38469;&#30340;&#22909;&#22788;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;QRL&#38754;&#20020;&#35832;&#22914;&#35299;&#20915;&#26041;&#26696;&#26223;&#35266;&#24179;&#22374;&#31561;&#25361;&#25112;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#25928;&#29575;&#20302;&#19979;&#65292;&#36825;&#38656;&#35201;&#20351;&#29992;&#26080;&#26799;&#24230;&#31639;&#27861;&#12290;&#26412;&#24037;&#20316;&#25506;&#35752;&#23558;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#12289;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#12289;&#31105;&#24524;&#25628;&#32034;&#65288;TS&#65289;&#12289;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#21644;&#21644;&#35856;&#25628;&#32034;&#65288;HS&#65289;&#65292;&#19982;QRL&#38598;&#25104;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#21442;&#25968;&#20248;&#21270;&#26041;&#38754;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;$5\times5$ MiniGrid&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#25152;&#26377;&#31639;&#27861;&#22343;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#32467;&#26524;&#65292;&#20854;&#20013;&#27169;&#25311;&#36864;&#28779;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#34920;&#29616;&#26368;&#20339;&#12290;&#22312;Cart Pole&#29615;&#22659;&#20013;&#65292;&#27169;&#25311;&#36864;&#28779;&#21644;&#36951;&#20256;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#21040;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#20013;&#65292;&#20197;&#39564;&#35777;&#20854;&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01187v1 Announce Type: cross  Abstract: Quantum Reinforcement Learning (QRL) offers potential advantages over classical Reinforcement Learning, such as compact state space representation and faster convergence in certain scenarios. However, practical benefits require further validation. QRL faces challenges like flat solution landscapes, where traditional gradient-based methods are inefficient, necessitating the use of gradient-free algorithms. This work explores the integration of metaheuristic algorithms -- Particle Swarm Optimization, Ant Colony Optimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony Search -- into QRL. These algorithms provide flexibility and efficiency in parameter optimization. Evaluations in $5\times5$ MiniGrid Reinforcement Learning environments show that, all algorithms yield near-optimal results, with Simulated Annealing and Particle Swarm Optimization performing best. In the Cart Pole environment, Simulated Annealing, Geneti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#23427;&#20204;&#23384;&#22312;&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#21644;&#22312;&#30830;&#20445;&#20449;&#24687;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2408.01168</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35823;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#28431;&#27934;&#12289;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Misinforming LLMs: vulnerabilities, challenges and opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#23427;&#20204;&#23384;&#22312;&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#21644;&#22312;&#30830;&#20445;&#20449;&#24687;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;arXiv:2408.01168v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#20173;&#24120;&#24120;&#34987;&#35823;&#35299;&#12290;&#23613;&#31649;&#34920;&#29616;&#20986;&#36830;&#36143;&#30340;&#22238;&#31572;&#21644;&#26126;&#26174;&#30340;&#25512;&#29702;&#34892;&#20026;&#65292;LLMs&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#35789;&#27719;&#23884;&#20837;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;LLMs&#20381;&#36182;&#20110;&#35789;&#27719;&#23884;&#20837;&#21521;&#37327;&#24207;&#21015;&#27169;&#24335;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#30340;&#20869;&#32622;&#26426;&#21046;&#23384;&#22312;&#26681;&#26412;&#30340;&#19981;&#20449;&#20219;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#32467;&#21512;&#22522;&#20110;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19982;&#20107;&#23454;&#22522;&#30784;&#21644;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#30340;&#30740;&#31350;&#21487;&#33021;&#20250;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#20449;&#36182;&#30340;LLM&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22522;&#20110;&#25552;&#20379;&#30340;&#20107;&#23454;&#29983;&#25104;&#35821;&#21477;&#24182;&#35299;&#37322;&#20854;&#33258;&#25105;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01168v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant advances in natural language processing, but their underlying mechanisms are often misunderstood. Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely on statistical patterns in word embeddings rather than true cognitive processes. This leads to vulnerabilities such as "hallucination" and misinformation. The paper argues that current LLM architectures are inherently untrustworthy due to their reliance on correlations of sequential patterns of word embedding vectors. However, ongoing research into combining generative transformer-based models with fact bases and logic programming languages may lead to the development of trustworthy LLMs capable of generating statements based on given truth and explaining their self-reasoning process.
&lt;/p&gt;</description></item><item><title>TCR-GPT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;T&#32454;&#32990;&#21463;&#20307;&#24207;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20154;&#28304;&#21270;TCRs&#30340;&#24207;&#21015;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01156</link><description>&lt;p&gt;
TCR-GPT: &#32467;&#21512;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;T&#32454;&#32990;&#21463;&#20307;&#24211;
&lt;/p&gt;
&lt;p&gt;
TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for T-Cell Receptor Repertoires Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01156
&lt;/p&gt;
&lt;p&gt;
TCR-GPT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;T&#32454;&#32990;&#21463;&#20307;&#24207;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20154;&#28304;&#21270;TCRs&#30340;&#24207;&#21015;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01156v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;T&#32454;&#32990;&#21463;&#20307;&#65288;TCRs&#65289;&#22312;&#20813;&#30123;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#35782;&#21035;&#24182;&#32467;&#21512;&#30001;&#24863;&#26579;&#25110;&#30284;&#24615;&#32454;&#32990;&#21576;&#29616;&#30340;&#29305;&#23450;&#25239;&#21407;&#12290;&#20102;&#35299;TCR&#30340;&#24207;&#21015;&#27169;&#24335;&#23545;&#20110;&#24320;&#21457;&#38024;&#23545;&#20813;&#30123;&#27835;&#30103;&#30340;&#31574;&#30053;&#21644;&#35774;&#35745;&#26377;&#25928;&#30340;&#30123;&#33495;&#33267;&#20851;&#37325;&#35201;&#12290;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#33258;&#22238;&#24402;&#36716;&#25442;&#22120;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;TCR&#24211;&#30340;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;TCR&#24207;&#21015;&#65292;&#36825;&#20123;&#24207;&#21015;&#32487;&#25215;&#20102;&#24211;&#20013;&#28508;&#22312;&#30340;&#24207;&#21015;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCR-GPT&#65292;&#19968;&#20010;&#22522;&#20110;&#20165;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#32467;&#26500;&#30340;&#32463;&#27982;&#27169;&#22411;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#22797;&#21046;TCR&#24211;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#12290;TCR-GPT&#22312;&#36890;&#36807;&#30382;&#23572;&#26862;&#30456;&#20851;&#31995;&#25968;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#25512;&#26029;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#31934;&#30830;&#24230;&#36798;&#21040;&#20102;0.953&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#24050;&#32463;&#35843;&#25972;&#20102;TCR&#24207;&#21015;&#30340;&#20998;&#24067;&#65292;&#20197;&#20445;&#35777;&#22312;&#20154;&#28304;&#21270;TCRs&#65288;Hu-TCRs&#65289;&#30340;&#24207;&#21015;&#29983;&#25104;&#20013;&#25512;&#24191;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;TCR-GPT&#33021;&#22815;&#38024;&#23545;&#29305;&#23450;&#30340;&#24207;&#21015;&#31354;&#38388;&#35774;&#35745;&#20986;&#26356;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;TCR&#24207;&#21015;&#21644;&#23454;&#38469;Hu-TCRs&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#22810;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;TCR-GPT&#27169;&#22411;&#22312;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;TCR&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01156v1 Announce Type: cross  Abstract: T-cell receptors (TCRs) play a crucial role in the immune system by recognizing and binding to specific antigens presented by infected or cancerous cells. Understanding the sequence patterns of TCRs is essential for developing targeted immune therapies and designing effective vaccines. Language models, such as auto-regressive transformers, offer a powerful solution to this problem by learning the probability distributions of TCR repertoires, enabling the generation of new TCR sequences that inherit the underlying patterns of the repertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only transformer architecture, designed to uncover and replicate sequence patterns in TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring sequence probability distributions measured by Pearson correlation coefficient. Furthermore, by leveraging Reinforcement Learning(RL), we adapted the distribution of TCR sequences t
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26032;&#30340;&#23494;&#38598;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#32534;&#30721;&#36328;&#30693;&#35782;&#22270;&#23454;&#20307;&#30340;&#21508;&#31181;&#20449;&#24687;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#65292;&#20419;&#36827;&#20102;&#36328;KG&#31561;&#25928;&#23454;&#20307;&#30340;&#24555;&#36895;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2408.01154</link><description>&lt;p&gt;
DERA: &#23494;&#38598;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01154
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26032;&#30340;&#23494;&#38598;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#32534;&#30721;&#36328;&#30693;&#35782;&#22270;&#23454;&#20307;&#30340;&#21508;&#31181;&#20449;&#24687;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#65292;&#20419;&#36827;&#20102;&#36328;KG&#31561;&#25928;&#23454;&#20307;&#30340;&#24555;&#36895;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01154v1 &#20844;&#21578;&#31867;&#22411;: cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01154v1 Announce Type: cross  Abstract: Entity Alignment (EA) aims to match equivalent entities in different Knowledge Graphs (KGs), which is essential for knowledge fusion and integration. Recently, embedding-based EA has attracted significant attention and many approaches have been proposed. Early approaches primarily focus on learning entity embeddings from the structural features of KGs, defined by relation triples. Later methods incorporated entities' names and attributes as auxiliary information to enhance embeddings for EA. However, these approaches often used different techniques to encode structural and attribute information, limiting their interaction and mutual enhancement. In this work, we propose a dense entity retrieval framework for EA, leveraging language models to uniformly encode various features of entities and facilitate nearest entity search across KGs. Alignment candidates are first generated through entity retrieval, which are subsequently reranked to 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#22312;&#20840;&#29699;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#38543;&#39057;&#29575;&#30340;&#25351;&#25968;&#19979;&#38477;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20302;&#39057;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#27491;&#38754;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#39640;&#39057;&#29575;&#20449;&#21495;&#30340;&#36129;&#29486;&#19982;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#36127;&#30456;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2408.01139</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#29702;&#35889;&#37325;&#35201;&#24615;&#20998;&#35299;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#30340;&#20840;&#23616;&#25200;&#21160;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01139
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#22312;&#20840;&#29699;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#38543;&#39057;&#29575;&#30340;&#25351;&#25968;&#19979;&#38477;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20302;&#39057;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#27491;&#38754;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#39640;&#39057;&#29575;&#20449;&#21495;&#30340;&#36129;&#29486;&#19982;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#36127;&#30456;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01139v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25200;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#20102;&#27169;&#22411;&#23545;&#21508;&#31181;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#27745;&#26579;&#21644; adversarial&#25915;&#20987;&#12290;&#29702;&#35299;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#26426;&#21046;&#23545;&#20110;&#20840;&#23616;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#26426;&#21046;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#30340;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#21551;&#21457;&#12290;&#39318;&#20808;&#65292;&#20197;&#21069;&#30340;&#20840;&#29699;&#35299;&#37322;&#24615;&#24037;&#20316;&#19982;&#40065;&#26834;&#24615;&#22522;&#20934;&#65288;&#20363;&#22914;&#24179;&#22343;&#27745;&#26579;&#38169;&#35823;mCE&#65289;&#21516;&#26102;&#36827;&#34892;&#65292;&#24182;&#19981;&#26159;&#20026;&#20102;&#30452;&#25509;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#20013;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#38543;&#39057;&#29575;&#25351;&#25968;&#19979;&#38477;&#12290;&#36825;&#31181;&#24130;&#24459;&#31867;&#20284;&#30340;&#19979;&#38477;&#34920;&#26126;&#65306;&#20302;&#39057;&#20449;&#21495;&#36890;&#24120;&#27604;&#39640;&#39057;&#20449;&#21495;&#26356;&#40065;&#26834;&#8212;&#8212;&#28982;&#32780;&#65292;&#39640;&#20998;&#31867;&#31934;&#24230;&#24182;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#27934;&#23519;&#21040;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;-mCE&#21644;&#39640;&#39057;&#20449;&#21495;&#30340;&#36129;&#29486;&#26377;&#36127;&#30456;&#20851;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#39640;&#39057;&#20449;&#21495;&#36739;&#23567;&#30340;&#22270;&#20687;&#21306;&#22495;&#20013;&#65292;&#21363;&#20351;&#23384;&#22312;&#39640;&#22122;&#22768;&#27700;&#24179;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20063;&#24456;&#39640;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#39640;&#39057;&#29575;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#36127;&#38754;&#20316;&#29992;&#65292;&#24182;&#20026;&#27169;&#22411;&#32467;&#26500;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#23545;&#20110;&#36731;&#24230;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#20063;&#20542;&#21521;&#20110;&#22312;&#20302;&#39057;&#20449;&#21495;&#26356;&#22823;&#30340;&#31354;&#38388;&#21306;&#22495;&#20013;&#20445;&#25345;&#26356;&#39640;&#30340;SNR&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#23545;&#22270;&#20687;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20840;&#29699;&#35299;&#37322;&#24615;&#29702;&#35299;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21518;&#32493;&#30340;&#27169;&#22411;&#35774;&#35745;&#12289;&#29702;&#35299;&#21644;&#20248;&#21270;&#24037;&#20316;&#12290;&#19979;&#26041;&#26159;&#35813;&#35770;&#25991;&#30340;&#33521;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#65292;&#35831;&#27880;&#24847;&#65292;&#23545;&#20110;&#20197;&#19979;&#30340;tldr&#21644;en_tldr&#37096;&#20998;&#65292;&#25105;&#20250;&#24635;&#32467;&#20986;&#19968;&#20010;&#20013;&#25991;&#21644;&#33521;&#25991;&#29256;&#30340;&#27010;&#35201;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01139v1 Announce Type: cross  Abstract: Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals -- yet high classification accuracy can not be ac
&lt;/p&gt;</description></item><item><title>Mamba&#26550;&#26500;&#20197;&#32463;&#20856;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#28789;&#24863;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#36817;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#19982;Transformer&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#26377;&#26395;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01129</link><description>&lt;p&gt;
Mamba&#26550;&#26500;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01129
&lt;/p&gt;
&lt;p&gt;
Mamba&#26550;&#26500;&#20197;&#32463;&#20856;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#28789;&#24863;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#36817;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#19982;Transformer&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#26377;&#26395;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#38376;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24341;&#21457;&#20102;&#19968;&#22330;&#26174;&#33879;&#30340;&#38761;&#21629;&#12290;&#20316;&#20026;&#26368;&#20856;&#22411;&#30340;&#26550;&#26500;&#65292;Transformer&#24050;&#32463;&#36171;&#33021;&#20102;&#22823;&#37327;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#21253;&#21547;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30707;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#23601;&#65292;&#20294;Transformer&#20173;&#28982;&#38754;&#20020;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#23548;&#33268;&#30340;&#32791;&#26102;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;Mamba&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#20511;&#37492;&#20102;&#32463;&#20856;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#21457;&#23637;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#26367;&#20195;&#26041;&#26696;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#22312;&#20445;&#25345;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#36817;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;Transformer&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#28608;&#21169;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#31215;&#26497;&#25506;&#32034;Mamba&#22312;&#21508;&#31181;&#39046;&#22495;&#23454;&#29616;&#21331;&#36234;&#34920;&#29616;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01129v1 Announce Type: cross  Abstract: Deep learning, as a vital technique, has sparked a notable revolution in artificial intelligence. As the most representative architecture, Transformers have empowered numerous advanced models, especially the large language models that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models, has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domain
&lt;/p&gt;</description></item><item><title>BioRAG&#26159;&#19968;&#31181;&#37319;&#29992;RAG-LLM&#25216;&#26415;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20179;&#24211;&#32500;&#25252;&#21644;&#20449;&#24687;&#26816;&#32034;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01107</link><description>&lt;p&gt;
BioRAG: &#22522;&#20110;RAG-LLM&#26694;&#26550;&#30340;&#29983;&#29289;&#23398;&#38382;&#39064;&#25512;&#29702;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BioRAG: A RAG-LLM Framework for Biological Question Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01107
&lt;/p&gt;
&lt;p&gt;
BioRAG&#26159;&#19968;&#31181;&#37319;&#29992;RAG-LLM&#25216;&#26415;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20179;&#24211;&#32500;&#25252;&#21644;&#20449;&#24687;&#26816;&#32034;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01107v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#30001;&#20110;&#21457;&#29616;&#30340;&#36895;&#24230;&#21152;&#24555;&#12289;&#27934;&#23519;&#21147;&#30340;&#28436;&#21464;&#20197;&#21450;&#30693;&#35782;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#32500;&#25252;&#19968;&#20010;&#20840;&#38754;&#30340;&#36164;&#26009;&#20179;&#24211;&#21644;&#20934;&#30830;&#30340;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01107v1 Announce Type: cross  Abstract: The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BioRAG deconstructs the question and employs an it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;CoLoRA&#65292;&#36890;&#36807;&#38543;&#26426;&#22833;&#30495;&#39044;&#35757;&#32451;&#65288;PROD&#65289;&#21644;&#36129;&#29486;&#24615;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#38024;&#23545;&#22810;&#20010;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#65292;&#22823;&#24133;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#31934;&#24230;&#19982;&#20869;&#23384;&#38656;&#27714;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01099</link><description>&lt;p&gt;
&#22522;&#20110;&#36129;&#29486;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;CoLoRA&#65292;&#36890;&#36807;&#38543;&#26426;&#22833;&#30495;&#39044;&#35757;&#32451;&#65288;PROD&#65289;&#21644;&#36129;&#29486;&#24615;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#38024;&#23545;&#22810;&#20010;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#65292;&#22823;&#24133;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#31934;&#24230;&#19982;&#20869;&#23384;&#38656;&#27714;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01099v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#39640;&#23618;&#27425;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#20511;&#21161;&#25513;&#30721;&#24314;&#27169;&#21644;&#25552;&#31034;&#35843;&#20248;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39640;&#25928;&#21442;&#25968;&#35843;&#20248;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#23618;&#27425;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65288;&#22914;&#20943;&#36731;AI&#36793;&#32536;&#35774;&#22791;&#19978;&#26032;&#20219;&#21153;&#26102;&#30340;&#20869;&#23384;&#33192;&#32960;&#38382;&#39064;&#65289;&#20013;&#30340;&#37325;&#35201;&#24615;&#19982;&#25910;&#30410;&#22791;&#21463;&#20851;&#27880;&#65292;&#39640;&#25928;&#30340;&#23567;&#25209;&#37327;&#21442;&#25968;&#35843;&#20248;&#31574;&#30053;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26377;&#25928;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#31216;&#20026;&#36129;&#29486;&#24615;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;CoLoRA&#65289;&#65292;&#29992;&#20110;&#22810;&#20010;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#65292;&#20197;&#21450;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#38543;&#26426;&#39034;&#24207;&#22833;&#30495;&#65288;PROD&#65289;&#12290;&#19982;&#20043;&#21069;&#25152;&#26377;&#32593;&#32476;&#21442;&#25968;&#35843;&#20248;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;CoLoRA&#36890;&#36807;&#21033;&#29992;LoRA&#65288;&#20302;&#31209;&#36866;&#24212;&#24615;&#65289;&#38024;&#23545;&#27599;&#20010;&#26032;&#30340;&#35270;&#35273;&#20219;&#21153;&#26469;&#26377;&#25928;&#22320;&#35843;&#20248;&#23569;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#36129;&#29486;&#27979;&#37327;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#30340;&#24863;&#30693;&#21464;&#21270;&#26435;&#37325;&#26469;&#25351;&#23548;&#32593;&#32476;&#21442;&#25968;&#30340;&#36873;&#23450;&#21644;&#20248;&#21270;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;PROD&#21644;CoLoRA&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20016;&#23500;&#20102;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#24182;&#20026;AI&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#26032;&#20219;&#21153;&#38598;&#25104;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01099v1 Announce Type: cross  Abstract: Recently, pre-trained model and efficient parameter tuning have achieved remarkable success in natural language processing and high-level computer vision with the aid of masked modeling and prompt tuning. In low-level computer vision, however, there have been limited investigations on pre-trained models and even efficient fine-tuning strategy has not yet been explored despite its importance and benefit in various real-world tasks such as alleviating memory inflation issue when integrating new tasks on AI edge devices. Here, we propose a novel efficient parameter tuning approach dubbed contribution-based low-rank adaptation (CoLoRA) for multiple image restorations along with effective pre-training method with random order degradations (PROD). Unlike prior arts that tune all network parameters, our CoLoRA effectively fine-tunes small amount of parameters by leveraging LoRA (low-rank adaptation) for each new vision task with our contribut
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.01091</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01091
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01091v1 Announce Type: new  Abstract: Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. The dataset and code are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#35758;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;AI&#35780;&#20215;&#23610;&#24230;&#65292;&#19987;&#38376;&#20026;&#33521;&#35821;&#23398;&#26415;&#29992;&#36884;&#35774;&#35745;&#65292;&#20197;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#26356;&#26377;&#25928;&#22320;&#24212;&#29992;&#65292;&#30830;&#20445;&#35780;&#20272;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01075</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;AI&#35780;&#20272;&#23610;&#24230;&#65306;&#20026;&#33521;&#35821;&#23398;&#26415;&#29992;&#36884;&#30340;AI&#35780;&#20272;&#25913;&#36827;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The EAP-AIAS: Adapting the AI Assessment Scale for English for Academic Purposes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#35758;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;AI&#35780;&#20215;&#23610;&#24230;&#65292;&#19987;&#38376;&#20026;&#33521;&#35821;&#23398;&#26415;&#29992;&#36884;&#35774;&#35745;&#65292;&#20197;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#26356;&#26377;&#25928;&#22320;&#24212;&#29992;&#65292;&#30830;&#20445;&#35780;&#20272;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01075v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01075v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) presents both opportunities and challenges for English for Academic Purposes (EAP) instruction. This paper proposes an adaptation of the AI Assessment Scale (AIAS) specifically tailored for EAP contexts, termed the EAP-AIAS.   This framework aims to provide a structured approach for integrating GenAI tools into EAP assessment practices while maintaining academic integrity and supporting language development. The EAP-AIAS consists of five levels, ranging from "No AI" to "Full AI", each delineating appropriate GenAI usage in EAP tasks. We discuss the rationale behind this adaptation, considering the unique needs of language learners and the dual focus of EAP on language proficiency and academic acculturation.   This paper explores potential applications of the EAP-AIAS across various EAP assessment types, including writing tasks, presentations, and research projects. By 
&lt;/p&gt;</description></item><item><title>&#33258;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24110;&#21161;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#33258;&#36523;&#22797;&#21046;&#25110;&#21382;&#21490;&#29256;&#26412;&#30340;&#23545;&#24328;&#20013;&#23398;&#20064;&#65292;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2408.01072</link><description>&lt;p&gt;
&#33258;&#23545;&#24328;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-play Methods in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01072
&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24110;&#21161;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#33258;&#36523;&#22797;&#21046;&#25110;&#21382;&#21490;&#29256;&#26412;&#30340;&#23545;&#24328;&#20013;&#23398;&#20064;&#65292;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#33258;&#23545;&#24328;&#65288;Self-play&#65289;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#26032;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#23545;&#24328;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21253;&#25324;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#22522;&#26412;&#30340;&#21338;&#24328;&#35770;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#23545;&#24328;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#33258;&#23545;&#24328;&#31639;&#27861;&#24402;&#31867;&#21040;&#36825;&#20010;&#26694;&#26550;&#20043;&#19979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#33258;&#23545;&#24328;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24357;&#21512;&#20102;&#31639;&#27861;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#24328;&#38754;&#20020;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#20026;&#29702;&#35299;&#33258;&#23545;&#24328;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22810;&#23618;&#38754;&#29305;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01072v1 Announce Type: new  Abstract: Self-play, characterized by agents' interactions with copies or past versions of itself, has recently gained prominence in reinforcement learning. This paper first clarifies the preliminaries of self-play, including the multi-agent reinforcement learning framework and basic game theory concepts. Then it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different scenarios. Finally, the survey highlights open challenges and future research directions in self-play. This paper is an essential guide map for understanding the multifaceted landscape of self-play in RL.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#22788;&#29702;&#36719;&#20214;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#65292;&#25552;&#39640;&#31995;&#32479;&#30340;&#33258;&#25105;&#24674;&#22797;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01055</link><description>&lt;p&gt;
&#22522;&#20110;LLMs&#30340;&#36719;&#20214;&#31995;&#32479;&#33258;&#24840;&#33021;&#21147;&#65306;&#24212;&#23545;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LLM as Runtime Error Handler: A Promising Pathway to Adaptive Self-Healing of Software Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01055
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#22788;&#29702;&#36719;&#20214;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#65292;&#25552;&#39640;&#31995;&#32479;&#30340;&#33258;&#25105;&#24674;&#22797;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01055v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#36719;&#20214;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#65292;&#26080;&#27861;&#39044;&#26009;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#23548;&#33268;&#31243;&#24207;&#31361;&#28982;&#32456;&#27490;&#25191;&#34892;&#65292;&#20174;&#32780;&#24341;&#21457;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#25968;&#25454;&#20002;&#22833;&#25110;&#31995;&#32479;&#23849;&#28291;&#12290;&#23613;&#31649;&#24320;&#21457;&#38454;&#27573;&#20250;&#23581;&#35797;&#35782;&#21035;&#28508;&#22312;&#38169;&#35823;&#65292;&#20294;&#27492;&#31867;&#39044;&#26009;&#20043;&#22806;&#30340;&#38169;&#35823;&#20173;&#38590;&#20197;&#23436;&#20840;&#28040;&#38500;&#65292;&#22240;&#27492;&#65292;&#25191;&#34892;&#26102;&#37319;&#29992;&#32531;&#35299;&#25514;&#26045;&#20173;&#26159;&#20943;&#23569;&#20854;&#24433;&#21709;&#30340;&#20851;&#38190;&#12290;&#33258;&#21160;&#21270;&#30340;&#33258;&#24840;&#25216;&#26415;&#65292;&#22914;&#37325;&#29992;&#29616;&#26377;&#30340;&#38169;&#35823;&#22788;&#29702;&#22120;&#65292;&#24050;&#34987;&#30740;&#31350;&#29992;&#20110;&#38477;&#20302;&#25191;&#34892;&#32456;&#27490;&#24102;&#26469;&#30340;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20351;&#29992;&#24615;&#21463;&#38480;&#20110;&#23427;&#20204;&#30340;&#39044;&#35774;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#23427;&#20204;&#22312;&#24212;&#23545;&#22810;&#26679;&#21270;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;&#21463;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#26469;&#22788;&#29702;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01055v1 Announce Type: cross  Abstract: Unanticipated runtime errors, lacking predefined handlers, can abruptly terminate execution and lead to severe consequences, such as data loss or system crashes. Despite extensive efforts to identify potential errors during the development phase, such unanticipated errors remain a challenge to to be entirely eliminated, making the runtime mitigation measurements still indispensable to minimize their impact. Automated self-healing techniques, such as reusing existing handlers, have been investigated to reduce the loss coming through with the execution termination. However, the usability of existing methods is retained by their predefined heuristic rules and they fail to handle diverse runtime errors adaptively. Recently, the advent of Large Language Models (LLMs) has opened new avenues for addressing this problem. Inspired by their remarkable capabilities in understanding and generating code, we propose to deal with the runtime errors i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SemGro&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#20998;&#35299;&#25216;&#33021;&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#38024;&#23545;&#19981;&#21516;&#39046;&#22495;&#35268;&#21010;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.01024</link><description>&lt;p&gt;
&#36328;&#22495;&#29615;&#22659;&#20013;&#22522;&#20110;&#36523;&#20307;&#25351;&#20196;&#36981;&#24490;&#30340;&#35821;&#20041;&#25216;&#33021;&#25509;&#22320;
&lt;/p&gt;
&lt;p&gt;
Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SemGro&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#20998;&#35299;&#25216;&#33021;&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#38024;&#23545;&#19981;&#21516;&#39046;&#22495;&#35268;&#21010;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01024v1 &#26032;&#38395;&#31867;&#22411;&#65306;&#26032;&#28040;&#24687; &#25688;&#35201;&#65306;&#22312;&#22522;&#20110;&#36523;&#20307;&#25351;&#20196;&#36981;&#24490;&#65288;EIF&#65289;&#20013;&#65292;&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#22120;&#25972;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20986;&#29616;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#20219;&#21153;&#22312;&#25216;&#33021;&#23618;&#32423;&#19978;&#34987;&#39044;&#35757;&#32451;&#25216;&#33021;&#21644;&#29992;&#25143;&#25351;&#20196;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#25152;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25509;&#22320;&#36825;&#20123;&#39044;&#35757;&#32451;&#25216;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#32039;&#23494;&#32416;&#32544;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#25216;&#33021;&#25509;&#22320;&#65288;SemGro&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#35821;&#20041;&#25216;&#33021;&#30340;&#23618;&#27425;&#24615;&#12290;SemGro&#35748;&#35782;&#21040;&#20102;&#36825;&#20123;&#25216;&#33021;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#20174;&#22312;&#19981;&#21516;&#39046;&#22495;&#37117;&#26222;&#36941;&#36866;&#29992;&#30340;&#20302;&#35821;&#20041;&#25216;&#33021;&#21040;&#39640;&#24230;&#19987;&#38376;&#21270;&#21644;&#29305;&#23450;&#20110;&#26576;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#21069;&#26223;&#20016;&#23500;&#35821;&#20041;&#25216;&#33021;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#36845;&#20195;&#30340;&#25216;&#33021;&#20998;&#35299;&#26041;&#27861;&#65292;&#20174;&#20855;&#26377;&#36739;&#39640;&#35821;&#20041;&#25216;&#33021;&#23618;&#27425;&#30340;&#26356;&#24191;&#27867;&#33539;&#22260;&#24320;&#22987;&#65292;&#28982;&#21518;&#20877;&#36880;&#27493;&#21521;&#19979;&#65292;&#23558;&#35821;&#20041;&#25216;&#33021;&#20998;&#35299;&#20026;&#26356;&#20026;&#32454;&#31890;&#24230;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#20934;&#30830;&#22320;&#35268;&#21010;&#20986;&#36866;&#24212;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01024v1 Announce Type: new  Abstract: In embodied instruction-following (EIF), the integration of pretrained language models (LMs) as task planners emerges as a significant branch, where tasks are planned at the skill level by prompting LMs with pretrained skills and user instructions. However, grounding these pretrained skills in different domains remains challenging due to their intricate entanglement with the domain-specific knowledge. To address this challenge, we present a semantic skill grounding (SemGro) framework that leverages the hierarchical nature of semantic skills. SemGro recognizes the broad spectrum of these skills, ranging from short-horizon low-semantic skills that are universally applicable across domains to long-horizon rich-semantic skills that are highly specialized and tailored for particular domains. The framework employs an iterative skill decomposition approach, starting from the higher levels of semantic skill hierarchy and then moving downwards, s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;KAN&#26550;&#26500;&#30340;GNNs&#65292;GNN-MolKAN&#21644;GNN-MolKAN+&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#29305;&#24615;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01018</link><description>&lt;p&gt;
GNN-MolKAN: &#32467;&#21512;KAN&#25552;&#21319;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;GNNs
&lt;/p&gt;
&lt;p&gt;
GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular Representation Learning with GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;KAN&#26550;&#26500;&#30340;GNNs&#65292;GNN-MolKAN&#21644;GNN-MolKAN+&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#29305;&#24615;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01018v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;&#35774;&#35745;&#20013;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#26631;&#27880;&#19981;&#36275;&#21644;&#26550;&#26500;&#35774;&#35745;&#19981;&#20339;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20363;&#22914;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#36807;&#24230;&#21387;&#32553;&#32780;&#23548;&#33268;&#20998;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#32454;&#33410;&#20002;&#22833;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20998;&#23376;&#34920;&#24449;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#31867;&#65292;&#21363;GNN-MolKAN&#21450;&#20854;&#22686;&#24378;&#21464;&#31181;GNN-MolKAN+&#65292;&#23427;&#20204;&#23558;&#20154;&#24037;&#26234;&#33021;+&#31185;&#23398;&#39046;&#22495;&#30340;Kolmogorov-Arnold Networks&#65288;KAN&#65289;&#26550;&#26500;&#34701;&#20837;GNNs&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#24555;&#36895;KAN&#65288;AdFastKAN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#32423;KAN&#65292;&#25552;&#20379;&#20102;&#22686;&#21152;&#30340;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#26631;&#20934;GNN&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;1) &#21331;&#36234;&#24615;&#33021;&#65306;GNN-MolKAN&#21644;GNN-MolKAN+&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;2) &#32467;&#26500;&#25484;&#25569;&#65306;KAN&#32467;&#21512;&#20102;GNN&#30340;&#32467;&#26500;&#21160;&#24577;&#24863;&#30693;&#33021;&#21147;&#65307;3) &#39640;&#25928;&#23398;&#20064;&#65306;AdFastKAN&#23454;&#29616;&#20102;&#24555;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#31283;&#23450;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;GNN-MolKAN&#21644;GNN-MolKAN+&#22312;&#22810;&#26679;&#21270;&#20998;&#23376;&#29305;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20102;KAN&#26694;&#26550;&#22312;GNNs&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01018v1 Announce Type: cross  Abstract: Effective molecular representation learning is crucial for molecular property prediction and drug design. However, existing approaches struggle with limitations in insufficient annotations and suboptimal architecture design. For instance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the loss of important structural details in molecules, thus impairing molecular representations. In this work, we propose a new class of GNNs, GNN-MolKAN and its augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold Networks (KAN) architecture from AI + Science into GNNs to address these challenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an advanced KAN that offers increased stability and speed, further enhancing the performance of standard GNNs. Notably, our approach holds three key benefits: 1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior prediction ability, robust generalizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#38598;&#22312;&#36866;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01016</link><description>&lt;p&gt;
&#36947;&#36335;&#20132;&#36890;&#22270;&#25968;&#25454;&#65306;&#22522;&#20934;&#27979;&#35797;&#19982;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#38598;&#22312;&#36866;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;: &#36947;&#36335;&#20132;&#36890;&#22581;&#22622;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#33021;&#20351;&#24471;&#20132;&#36890;&#31649;&#29702;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#25552;&#39640;&#37066;&#21306;&#20307;&#39564;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#24182;&#25972;&#20307;&#25552;&#39640;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#34429;&#28982;&#26377;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#23588;&#20854;&#26159;&#22312;&#37117;&#24066;&#22320;&#21306;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#24182;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#65292;&#22240;&#20026;&#25968;&#25454;&#37327;&#19981;&#22815;&#65288;&#21363;&#20256;&#24863;&#22120;&#21644;&#36947;&#36335;&#36830;&#25509;&#25968;&#37327;&#19981;&#36275;&#65289;&#20197;&#21450;&#19968;&#20123;&#22806;&#37096;&#22240;&#32032;&#65292;&#22914;&#30446;&#26631;&#22320;&#21306;&#30340;&#29305;&#24615;&#24046;&#24322;&#65292;&#22914;&#22478;&#24066;&#12289;&#39640;&#36895;&#20844;&#36335;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#20301;&#32622;&#31561;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#20016;&#23500;&#20855;&#26377;&#26032;&#22320;&#29702;&#29305;&#24449;&#30340;&#25991;&#29486;&#12290;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22312;2451&#20010;&#19981;&#21516;&#22320;&#28857;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01016v1 Announce Type: cross  Abstract: Road traffic congestion prediction is a crucial component of intelligent transportation systems, since it enables proactive traffic management, enhances suburban experience, reduces environmental impact, and improves overall safety and efficiency. Although there are several public datasets, especially for metropolitan areas, these datasets may not be applicable to practical scenarios due to insufficiency in the scale of data (i.e. number of sensors and road links) and several external factors like different characteristics of the target area such as urban, highways and the data collection location. To address this, this paper introduces a novel IBB Traffic graph dataset as an alternative benchmark dataset to mitigate these limitations and enrich the literature with new geographical characteristics. IBB Traffic graph dataset covers the sensor data collected at 2451 distinct locations. Moreover, we propose a novel Road Traffic Prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TT-LoRA&#30340;&#20302;&#31209;&#24352;&#37327;&#26463;&#36817;&#20284;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26356;&#24555;&#30340;LLMs&#24494;&#35843;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.01008</link><description>&lt;p&gt;
&#24352;&#37327;&#26463;&#20302;&#31209;&#36817;&#20284; (TT-LoRA): &#21152;&#36895; LLMs &#20197;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TT-LoRA&#30340;&#20302;&#31209;&#24352;&#37327;&#26463;&#36817;&#20284;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26356;&#24555;&#30340;LLMs&#24494;&#35843;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01008v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#38382;&#31572;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;LLMs&#26085;&#30410;&#22797;&#26434;&#30340;&#38656;&#27714;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#65292;&#38459;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22914;Low-Rank Approximation&#65288;LoRA&#65289;&#21644;Adapters&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#21487;&#21387;&#32553;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LoRA&#22312;&#29616;&#20195;&#22823;&#22411;LLM&#20013;&#36234;&#26469;&#36234;&#22686;&#22810;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19978;&#38590;&#20197;&#26377;&#25928;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Low-Rank Economic Tensor-Train Adaptation&#65288;LoRETTA&#65289;&#21033;&#29992;&#24352;&#37327;&#26463;&#20998;&#35299;&#65292;&#20294;&#23427;&#23578;&#26410;&#23454;&#29616;&#23545;&#38750;&#24120;&#22823;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#24517;&#35201;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01008v1 Announce Type: cross  Abstract: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks, such as question-answering, sentiment analysis, text summarization, and machine translation. However, the ever-growing complexity of LLMs demands immense computational resources, hindering the broader research and application of these models. To address this, various parameter-efficient fine-tuning strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been developed. Despite their potential, these methods often face limitations in compressibility. Specifically, LoRA struggles to scale effectively with the increasing number of trainable parameters in modern large scale LLMs. Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which utilizes tensor train decomposition, has not yet achieved the level of compression necessary for fine-tuning very large scale mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#22320;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#28789;&#27963;&#22320;&#25511;&#21046;&#22270;&#20687;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2408.00998</link><description>&lt;p&gt;
FBSDiff: &#25554;&#25300;&#24335;&#39057;&#29575;&#24102;&#26367;&#20195;&#24335;&#25193;&#25955;&#29305;&#24449;&#21464;&#25442;&#30340;&#39640;&#21487;&#25511;&#21046;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#22320;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#28789;&#27963;&#22320;&#25511;&#21046;&#22270;&#20687;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00998v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: &#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687; diffusion &#27169;&#22411;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#30340;&#21457;&#23637;&#20013;&#26159;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#37324;&#31243;&#30865;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#38750;&#20961;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#32570;&#20047;&#21487;&#25511;&#24615;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#22240;&#20026;&#27880;&#24847;&#21147;&#24050;&#32463;&#38598;&#20013;&#22312;&#21033;&#29992;&#21442;&#32771;&#22270;&#20687;&#26469;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;&#30001;&#20110;&#21442;&#32771;&#22270;&#20687;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26681;&#25454;&#25991;&#26412;&#25805;&#32437;&#65288;&#25110;&#32773;&#32534;&#36753;&#65289;&#21442;&#32771;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#21363;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#12290;&#26412;&#25991;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#32451;&#12289;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#22270;&#20687;&#21040;&#22270;&#20687;&#65288;I2I&#65289;&#33539;&#24335;&#20013;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#21464;&#30340;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00998v1 Announce Type: new  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing extraordinary image generation based on natural-language text prompts. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation, for which attention has been focused on leveraging a reference image to control text-to-image synthesis. Due to the close correlation between the reference image and the generated image, this problem can also be regarded as the task of manipulating (or editing) the reference image as per the text, namely text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts the pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#36981;&#23432;&#23433;&#20840;&#38480;&#21046;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20010;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26410;&#30693;&#26465;&#20214;&#21644;&#38556;&#30861;&#29305;&#21035;&#26377;&#21033;&#65292;&#22914;&#21487;&#33021;&#21253;&#25324;&#30340;&#38556;&#30861;&#29289;&#12290;</title><link>https://arxiv.org/abs/2408.00997</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#23433;&#20840;&#32422;&#26463;&#32593;&#26684;&#29615;&#22659;&#20013;&#23454;&#29616;&#26080;&#27169;&#22411;&#20219;&#21153;&#36866;&#24212;&#30340;&#23433;&#20840;&#25506;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Safe Exploration Strategy for Model-free Task Adaptation in Safety-constrained Grid Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00997
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#36981;&#23432;&#23433;&#20840;&#38480;&#21046;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20010;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26410;&#30693;&#26465;&#20214;&#21644;&#38556;&#30861;&#29305;&#21035;&#26377;&#21033;&#65292;&#22914;&#21487;&#33021;&#21253;&#25324;&#30340;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#23433;&#20840;&#32422;&#26463;&#30340;&#32593;&#26684;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#21516;&#26102;&#25506;&#32034;&#29615;&#22659;&#24182;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#12290;Our safety-aware exploration strategy allows agents to learn new tasks without compromising the environment's safety, thereby facilitating learning in practical environments. This approach is particularly advantageous for robotic navigation in complex environments that may include obstacles and unpredictable conditions.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00997v1 Announce Type: new  Abstract: Training a model-free reinforcement learning agent requires allowing the agent to sufficiently explore the environment to search for an optimal policy. In safety-constrained environments, utilizing unsupervised exploration or a non-optimal policy may lead the agent to undesirable states, resulting in outcomes that are potentially costly or hazardous for both the agent and the environment. In this paper, we introduce a new exploration framework for navigating the grid environments that enables model-free agents to interact with the environment while adhering to safety constraints. Our framework includes a pre-training phase, during which the agent learns to identify potentially unsafe states based on both observable features and specified safety constraints in the environment. Subsequently, a binary classification model is trained to predict those unsafe states in new environments that exhibit similar dynamics. This trained classifier emp
&lt;/p&gt;</description></item><item><title>IncidentNet&#26159;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31232;&#30095;&#25918;&#32622;&#22312;&#22478;&#24066;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#35780;&#20272;&#20132;&#36890;&#20107;&#20214;&#30340;&#20005;&#37325;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00996</link><description>&lt;p&gt;
&#12298;IncidentNet: &#31232;&#30095;&#20256;&#24863;&#19979;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#20005;&#37325;&#24615;&#20272;&#35745;&#12299;
&lt;/p&gt;
&lt;p&gt;
IncidentNet: Traffic Incident Detection, Localization and Severity Estimation with Sparse Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00996
&lt;/p&gt;
&lt;p&gt;
IncidentNet&#26159;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31232;&#30095;&#25918;&#32622;&#22312;&#22478;&#24066;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#35780;&#20272;&#20132;&#36890;&#20107;&#20214;&#30340;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20256;&#24863;&#22120;&#35206;&#30422;&#65292;&#24182;&#19988;&#20027;&#35201;&#22522;&#20110;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#33021;&#21147;&#19978;&#26377;&#38480;&#65292;&#22240;&#27492;&#26080;&#27861;&#20934;&#30830;&#26816;&#27979;&#20107;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IncidentNet&#8212;&#8212;&#19968;&#31181;&#20351;&#29992;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#31232;&#30095;&#25918;&#32622;&#30340;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20998;&#31867;&#12289;&#23450;&#20301;&#21644;&#20272;&#35745;&#20132;&#36890;&#20107;&#20214;&#30340;&#20005;&#37325;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#22312;&#24494;&#35266;&#20132;&#36890;&#25968;&#25454;&#20043;&#19978;&#65292;&#35813;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#23433;&#35013;&#22312;&#20132;&#36890;&#20132;&#21449;&#36335;&#21475;&#30340;&#25668;&#20687;&#22836;&#25910;&#38598;&#12290;&#30001;&#20110;&#32570;&#20047;&#21516;&#26102;&#25552;&#20379;&#24494;&#35266;&#20132;&#36890;&#32454;&#33410;&#21644;&#20132;&#36890;&#20107;&#20214;&#32454;&#33410;&#30340;&#21512;&#25104;&#20132;&#36890;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#23439;&#35266;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#24494;&#35266;&#20132;&#36890;&#25968;&#25454;&#38598;&#12290;&#22312;&#20351;&#29992;IncidentNet&#25216;&#26415;&#26102;&#65292;&#26816;&#27979;&#20986;&#30340;&#20107;&#25925;&#29575;&#36798;&#21040;&#20102;98%&#65292;&#35823;&#25253;&#29575;&#20302;&#20110;7%&#65292;&#24182;&#19988;&#22312;197&#20010;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;95%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00996v1 Announce Type: cross  Abstract: Prior art in traffic incident detection relies on high sensor coverage and is primarily based on decision-tree and random forest models that have limited representation capacity and, as a result, cannot detect incidents with high accuracy. This paper presents IncidentNet - a novel approach for classifying, localizing, and estimating the severity of traffic incidents using deep learning models trained on data captured from sparsely placed sensors in urban environments. Our model works on microscopic traffic data that can be collected using cameras installed at traffic intersections. Due to the unavailability of datasets that provide microscopic traffic details and traffic incident details simultaneously, we also present a methodology to generate a synthetic microscopic traffic dataset that matches given macroscopic traffic data. IncidentNet achieves a traffic incident detection rate of 98%, with false alarm rates of less than 7% in 197 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ARCHCODE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#32452;&#32455;&#24182;&#25512;&#26029;&#36719;&#20214;&#38656;&#27714;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.00994</link><description>&lt;p&gt;
ArchCode&#65306;&#23558;&#36719;&#20214;&#38656;&#27714;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;
&lt;/p&gt;
&lt;p&gt;
ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ARCHCODE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#32452;&#32455;&#24182;&#25512;&#26029;&#36719;&#20214;&#38656;&#27714;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00994v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26412;&#25991;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#25193;&#23637;&#21040;&#33021;&#22815;&#33258;&#21160;&#22788;&#29702;&#20174;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#20013;&#32473;&#20986;&#30340;&#20840;&#38754;&#36719;&#20214;&#38656;&#27714;&#12290;&#36825;&#20123;&#38656;&#27714;&#21253;&#25324;&#21151;&#33021;&#24615;&#65288;&#21363;&#23545;&#36755;&#20837;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#65289;&#21644;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;/&#31354;&#38388;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#12289;&#21487;&#32500;&#25252;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#25551;&#36848;&#35201;&#20040;&#21487;&#33021;&#20887;&#38271;&#22320;&#34920;&#36798;&#35201;&#27714;&#65292;&#35201;&#20040;&#29978;&#33267;&#21487;&#33021;&#30465;&#30053;&#19968;&#20123;&#35201;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;ARCHCODE&#65292;&#19968;&#20010;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#8220;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#8221;&#30340;&#21407;&#29702;&#26469;&#32452;&#32455;&#20174;&#25551;&#36848;&#20013;&#35266;&#23519;&#21040;&#30340;&#38656;&#27714;&#65292;&#24182;&#20174;&#36825;&#20123;&#25551;&#36848;&#20013;&#25512;&#26029;&#20986;&#26410;&#34920;&#36798;&#30340;&#38656;&#27714;&#12290;ARCHCODE&#20174;&#32473;&#20986;&#30340;&#25551;&#36848;&#20013;&#29983;&#25104;&#38656;&#27714;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#20135;&#29983;&#20195;&#30721;&#29255;&#27573;&#21644;&#27979;&#35797;&#29992;&#20363;&#12290;&#27599;&#20010;&#27979;&#35797;&#29992;&#20363;&#37117;&#38024;&#23545;&#19968;&#20010;&#35201;&#27714;&#65292;&#20801;&#35768;&#26681;&#25454;&#20195;&#30721;&#29255;&#27573;&#25191;&#34892;&#32467;&#26524;&#19982;&#35201;&#27714;&#30340;&#31526;&#21512;&#24615;&#23545;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#25490;&#21517;&#12290;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;ARCHCODE&#22312;&#22788;&#29702;&#22797;&#26434;&#36719;&#20214;&#38656;&#27714;&#21644;&#29983;&#25104;&#39640;&#36136;&#37327;&#20195;&#30721;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#38598;&#20013;&#20110;&#23454;&#29616;ARCHCODE&#22312;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#24037;&#31243;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00994v1 Announce Type: cross  Abstract: This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;multi-agent&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#21152;&#31995;&#32479;&#25269;&#24481;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.00989</link><description>&lt;p&gt;
&#20855;&#26377;&#24694;&#24847;&#20195;&#29702;&#30340;multi-agent&#31995;&#32479;&#30340;&#24377;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Resilience of Multi-Agent Systems with Malicious Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;multi-agent&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#21152;&#31995;&#32479;&#25269;&#24481;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00989v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;&#65306;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#20381;&#38752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#19987;&#23478;&#20195;&#29702;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#34987;&#21333;&#29420;&#37096;&#32626;&#26102;&#65292;&#23384;&#22312;&#19968;&#20010;&#39118;&#38505;&#65292;&#21363;&#24694;&#24847;&#29992;&#25143;&#21487;&#33021;&#20250;&#24341;&#20837;&#24694;&#24847;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#29983;&#25104;&#30340;&#32467;&#26524;&#26159;&#38169;&#35823;&#30340;&#25110;&#19981;&#30456;&#20851;&#30340;&#65292;&#20197;&#33267;&#20110;&#20854;&#20182;&#38750;&#19987;&#38376;&#20195;&#29702;&#38590;&#20197;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#20195;&#29702;&#31995;&#32479;&#32467;&#26500;&#65288;&#20363;&#22914;A-&gt;B-&gt;C&#65292;A&lt;-&gt;B&lt;-&gt;C&#65289;&#19979;&#65292;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;&#65292;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24377;&#24615;&#26159;&#22810;&#23569;&#65311;&#65288;2&#65289;&#25105;&#20204;&#22914;&#20309;&#33021;&#22686;&#21152;&#31995;&#32479;&#25269;&#24481;&#24694;&#24847;&#20195;&#29702;&#30340;&#33021;&#21147;&#65311;&#20026;&#20102;&#27169;&#25311;&#24694;&#24847;&#20195;&#29702;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;AutoTransform&#21644;AutoInject&#65292;&#23558;&#20219;&#20309;&#20195;&#29702;&#36716;&#25442;&#25104;&#24694;&#24847;&#20195;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21151;&#33021;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#30693;&#35782;&#25277;&#21462;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#31995;&#32479;&#22312;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;multi-agent&#31995;&#32479;&#22312;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;&#34920;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#33030;&#24369;&#24615;&#65292;&#20294;&#36890;&#36807;&#36866;&#24403;&#30340;&#32467;&#26500;&#35774;&#35745;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#21046;&#23450;&#65292;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#26159;&#21487;&#20197;&#24471;&#21040;&#22686;&#24378;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#25581;&#31034;&#20102;multi-agent&#31995;&#32479;&#22312;&#38754;&#23545;&#24694;&#24847;&#25915;&#20987;&#26102;&#30340;&#25239;&#24615;&#65292;&#20063;&#20026;&#26500;&#24314;&#26356;&#23433;&#20840;&#12289;&#26356;&#20581;&#22766;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00989v1 Announce Type: new  Abstract: Multi-agent systems, powered by large language models, have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, when agents are deployed separately, there is a risk that malicious users may introduce malicious agents who generate incorrect or irrelevant results that are too stealthy to be identified by other non-specialized agents. Therefore, this paper investigates two essential questions: (1) What is the resilience of various multi-agent system structures (e.g., A$\rightarrow$B$\rightarrow$C, A$\leftrightarrow$B$\leftrightarrow$C) under malicious agents, on different downstream tasks? (2) How can we increase system resilience to defend against malicious agents? To simulate malicious agents, we devise two methods, AutoTransform and AutoInject, to transform any agent into a malicious one while preserving its functional integrity. We run comprehensive experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;ESG-AI&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#25237;&#36164;&#32773;&#35780;&#20272;&#21644;&#31649;&#29702;AI&#25237;&#36164;&#65292;&#21516;&#26102;&#25512;&#21160;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00965</link><description>&lt;p&gt;
&#29615;&#22659;&#12289;&#31038;&#20250;&#12289;&#27835;&#29702;(ESG)&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#65306;&#20840;&#38754;&#30340;&#36131;&#20219;AI&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Integrating ESG and AI: A Comprehensive Responsible AI Assessment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;ESG-AI&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#25237;&#36164;&#32773;&#35780;&#20272;&#21644;&#31649;&#29702;AI&#25237;&#36164;&#65292;&#21516;&#26102;&#25512;&#21160;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00965v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#21508;&#20010;&#34892;&#19994;&#37096;&#38376;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#23558;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#32771;&#34385;&#22240;&#32032;&#19982;AI&#25237;&#36164;&#30456;&#32467;&#21512;&#65292;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#25216;&#26415;&#19978;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#20174;&#25237;&#36164;&#32773;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#25972;&#21512;&#19981;&#20165;&#33021;&#22815;&#20943;&#36731;&#39118;&#38505;&#65292;&#36824;&#33021;&#22815;&#36890;&#36807;&#19982;&#26356;&#24191;&#27867;&#30340;&#20840;&#29699;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#25552;&#21319;&#38271;&#26399;&#30340;&#20215;&#20540;&#21019;&#36896;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#25506;&#35752;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;ESG-AI&#26694;&#26550;&#65292;&#36825;&#20010;&#26694;&#26550;&#26159;&#22522;&#20110;&#19982;28&#23478;&#20844;&#21496;&#21512;&#20316;&#32463;&#39564;&#20013;&#30340;&#27934;&#23519;&#32780;&#24320;&#21457;&#30340;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#12290;&#36890;&#36807;&#19982;&#19994;&#30028;&#23454;&#36341;&#32773;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#12290;ESG-AI&#26694;&#26550;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#24110;&#21161;&#20351;&#29992;&#32773;&#65292;&#22914;&#25237;&#36164;&#32773;&#65292;&#21435;&#35780;&#20272;&#21644;&#31649;&#29702;AI&#25237;&#36164;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#25512;&#21160;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#30446;&#26631;&#21644;&#30408;&#21033;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#35780;&#20272;&#24037;&#20855;&#65292;&#20197;&#30830;&#20445;&#35813;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00965v1 Announce Type: new  Abstract: Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors asse
&lt;/p&gt;</description></item><item><title>PERSOMA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#33021;&#39640;&#25928;&#22788;&#29702;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#12290;</title><link>https://arxiv.org/abs/2408.00960</link><description>&lt;p&gt;
PERSOMA&#20010;&#24615;&#21270;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#22312;&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00960
&lt;/p&gt;
&lt;p&gt;
PERSOMA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#33021;&#39640;&#25928;&#22788;&#29702;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PERSOMA&#30340;&#20010;&#24615;&#21270;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#29992;&#25143;&#30340;&#24191;&#27867;&#20132;&#20114;&#21382;&#21490;&#36827;&#34892;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#29702;&#35299;&#12290;&#19982;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;PERSOMA&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#25429;&#33719;&#29992;&#25143;&#21382;&#21490;&#30340;&#26041;&#24335;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#20132;&#20114;&#30340;&#25805;&#20316;&#20943;&#23569;&#21644;&#21387;&#32553;&#31639;&#27861;&#65292;&#23558;&#29992;&#25143;&#20132;&#20114;&#20449;&#24687;&#36716;&#21270;&#20026;&#29305;&#24449;&#20016;&#23500;&#30340;&#36719;&#25552;&#31034;&#23884;&#20837;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#36890;&#36807;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#39564;&#35777;&#20102;PERSOMA&#26550;&#26500;&#65292;&#21253;&#25324;&#21028;&#26029;&#21508;&#31867;&#21442;&#25968;&#21387;&#32553;&#21644;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#22914;&#20302;&#31209;&#27880;&#24847;&#21147;&#65288;LoRA&#65289;&#31561;&#26041;&#27861;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#27979;&#35797;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;PERSOMA&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#22823;&#37327;&#30340;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#23884;&#20837;&#21644;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00960v1 Announce Type: cross  Abstract: Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;WarpSci&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22312;GPU&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38656;&#35201;&#22797;&#26434;&#29615;&#22659;&#27169;&#22411;&#25968;&#25454;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2408.00930</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;GPU&#19978;&#23454;&#29616;&#39640;&#25968;&#25454;&#21534;&#21520;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#39046;&#22495;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain Agnostic Framework for Data-Driven Scientific Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00930
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;WarpSci&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22312;GPU&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38656;&#35201;&#22797;&#26434;&#29615;&#22659;&#27169;&#22411;&#25968;&#25454;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#25105;&#20204;&#20171;&#32461;WarpSci&#65292;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#20811;&#26381;&#22312;&#20855;&#26377;&#22823;&#37327;&#39640;&#32500;&#35266;&#27979;&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#31995;&#32479;&#29942;&#39048;&#30340;&#39046;&#22495;&#26080;&#20851;&#26694;&#26550;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204; framework eliminates the need for data transfer between the CPU and GPU&#65292;&#20351;&#21333;&#20010;&#25110;&#22810;&#20010;GPU&#19978;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#25968;&#21315;&#20010;&#27169;&#25311;&#12290;&#36825;&#23545;&#20110;&#30740;&#31350;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#30740;&#31350;&#29305;&#21035;&#26377;&#21033;&#65292;&#22240;&#20026;&#22312;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#27169;&#22411;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00930v1 Announce Type: cross  Abstract: We introduce WarpSci, a domain agnostic framework designed to overcome crucial system bottlenecks encountered in the application of reinforcement learning to intricate environments with vast datasets featuring high-dimensional observation or action spaces. Notably, our framework eliminates the need for data transfer between the CPU and GPU, enabling the concurrent execution of thousands of simulations on a single or multiple GPUs. This high data throughput architecture proves particularly advantageous for data-driven scientific research, where intricate environment models are commonly essential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#39033;&#26032;&#30340;&#25968;&#25454;&#27844;&#38706;&#25216;&#26415;&#65292;&#21363;&#22312;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#26102;&#65292;&#36890;&#36807;&#27880;&#20837;&#24694;&#24847;&#25351;&#20196;&#24182;&#20351;&#29992;GCG&#21518;&#32512;&#25552;&#39640;&#25968;&#25454;&#27844;&#38706;&#30340;&#25104;&#21151;&#29575;&#65292;&#23588;&#20854;&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#23545;&#19994;&#21153;&#36896;&#25104;&#32422;450&#19975;&#32654;&#20803;&#30340;&#25439;&#22833;&#12290;&#34429;&#28982;&#23384;&#22312;&#39118;&#38505;&#65292;&#20294;&#36890;&#36807;&#31649;&#29702;&#21644;&#23433;&#20840;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#38477;&#20302;&#27492;&#31867;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.00925</link><description>&lt;p&gt;
&#30333;&#30382;&#20070;&#65306;&#21033;&#29992;GCG&#21518;&#32512;&#30340;&#25968;&#25454;&#27844;&#38706;&#31616;&#35201;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#39033;&#26032;&#30340;&#25968;&#25454;&#27844;&#38706;&#25216;&#26415;&#65292;&#21363;&#22312;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#26102;&#65292;&#36890;&#36807;&#27880;&#20837;&#24694;&#24847;&#25351;&#20196;&#24182;&#20351;&#29992;GCG&#21518;&#32512;&#25552;&#39640;&#25968;&#25454;&#27844;&#38706;&#30340;&#25104;&#21151;&#29575;&#65292;&#23588;&#20854;&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#23545;&#19994;&#21153;&#36896;&#25104;&#32422;450&#19975;&#32654;&#20803;&#30340;&#25439;&#22833;&#12290;&#34429;&#28982;&#23384;&#22312;&#39118;&#38505;&#65292;&#20294;&#36890;&#36807;&#31649;&#29702;&#21644;&#23433;&#20840;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#38477;&#20302;&#27492;&#31867;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00925v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20132;&#21449;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65288;XPIA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#25968;&#25454;&#27844;&#38706;&#30340;&#25216;&#26415;&#65292;&#36817;&#26469;&#22312;&#35813;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;XPIA&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#33021;&#22815;&#22312;&#31532;&#19977;&#26041;&#25968;&#25454;&#20013;&#27880;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#24456;&#21487;&#33021;&#20250;&#34987;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36741;&#21161;&#29992;&#25143;&#26102;&#28040;&#36153;&#65292;&#29992;&#25143;&#25104;&#20026;&#21463;&#23475;&#32773;&#12290;XPIA&#36890;&#24120;&#34987;&#29992;&#20316;&#25968;&#25454;&#27844;&#38706;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20272;&#35745;&#19968;&#27425;&#24179;&#22343;&#25968;&#25454;&#27844;&#38706;&#30340;&#25104;&#26412;&#23545;&#20225;&#19994;&#26469;&#35828;&#25509;&#36817;450&#19975;&#32654;&#20803;&#65292;&#36825;&#19968;&#25104;&#26412;&#21253;&#25324;&#20102;&#22914;&#20225;&#19994;&#20973;&#35777;&#34987;&#31713;&#25913;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#27844;&#38706;&#12290;&#38543;&#30528;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#65292;&#22914;GCG&#21518;&#32512;&#25915;&#20987;&#30340;&#20852;&#36215;&#65292;&#20351;&#29992;GCG&#21518;&#32512;&#30340;XPIA&#21457;&#29983;&#30340;&#27010;&#29575;&#20196;&#20154;&#25285;&#24551;&#12290;&#22312;&#25105;&#20316;&#20026;Microsoft&#20154;&#24037;&#26234;&#33021;&#32418;&#38431;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#30340;XPIA&#22330;&#26223;&#20013;&#20351;&#29992;GCG&#21518;&#32512;&#37197;&#21512;&#27880;&#20837;&#28431;&#27934;&#30340;&#21487;&#34892;&#24615;&#25915;&#20987;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23384;&#22312;GCG&#21518;&#32512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#25968;&#25454;&#27844;&#38706;&#30340;&#27010;&#29575;&#20960;&#20046;&#22686;&#21152;&#20102;20%&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#27844;&#38706;&#30340;&#21487;&#33021;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#23454;&#38469;&#30340;&#25104;&#21151;&#29575;&#65292;&#22240;&#20026;&#38500;&#20102;&#25216;&#26415;&#22240;&#32032;&#22806;&#65292;&#36824;&#26377;&#31649;&#29702;&#21644;&#23433;&#20840;&#31574;&#30053;&#31561;&#38750;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20026;&#25968;&#25454;&#20445;&#25252;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#21644;&#24605;&#32771;&#26041;&#21521;&#65292;&#21516;&#26102;&#20063;&#20026;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#25552;&#20379;&#21442;&#32771;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00925v1 Announce Type: cross  Abstract: The cross-prompt injection attack (XPIA) is an effective technique that can be used for data exfiltration, and that has seen increasing use. In this attack, the attacker injects a malicious instruction into third party data which an LLM is likely to consume when assisting a user, who is the victim. XPIA is often used as a means for data exfiltration, and the estimated cost of the average data breach for a business is nearly $4.5 million, which includes breaches such as compromised enterprise credentials. With the rise of gradient-based attacks such as the GCG suffix attack, the odds of an XPIA occurring which uses a GCG suffix are worryingly high. As part of my work in Microsoft's AI Red Team, I demonstrated a viable attack model using a GCG suffix paired with an injection in a simulated XPIA scenario. The results indicate that the presence of a GCG suffix can increase the odds of successful data exfiltration by nearly 20%, with some c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20107;&#20214;&#26816;&#27979;&#20013;&#20351;&#29992;GPT-4&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#20026;GPT-4&#25552;&#20379;&#30340;&#8220;&#35768;&#21487;&#35777;&#21644;&#26426;&#20250;&#8221;&#65288;L&amp;O&#65289;&#65292;&#23454;&#29616;&#20102;0.759 AUC&#30340;&#32622;&#20449;&#24230;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2408.00914</link><description>&lt;p&gt;
&#25480;&#20104; GPT-4 &#35768;&#21487;&#35777;&#21644;&#26426;&#20250;&#65306;&#22686;&#24378;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Granting GPT-4 License and Opportunity: Enhancing Accuracy and Confidence Estimation for Few-Shot Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20107;&#20214;&#26816;&#27979;&#20013;&#20351;&#29992;GPT-4&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#20026;GPT-4&#25552;&#20379;&#30340;&#8220;&#35768;&#21487;&#35777;&#21644;&#26426;&#20250;&#8221;&#65288;L&amp;O&#65289;&#65292;&#23454;&#29616;&#20102;0.759 AUC&#30340;&#32622;&#20449;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00914v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914; GPT-4&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#20013;&#26174;&#31034;&#20986;&#36275;&#22815;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#8220;&#38134;&#33394;&#8221;&#25968;&#25454;&#21644;&#26032;&#30693;&#35782;&#24211;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#21644;&#23457;&#26597;&#65292;&#36825;&#26679;&#30340;&#24037;&#20316;&#27969;&#31243;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#12290;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26159;&#27492;&#31867;&#27169;&#22411;&#22914; GPT-4&#30340;&#24050;&#25253;&#21578;&#24369;&#28857;&#65292;&#32780;&#34917;&#20607;&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102; GPT-4&#22312;&#22522;&#20110; BETTER &#30693;&#35782;&#24211;&#30340;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26377;&#25928;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#20851;&#38190;&#21019;&#26032;&#26159;&#23558;&#25552;&#20379;&#32473; GPT-4&#30340;&#25552;&#31034;&#21644;&#20219;&#21153;&#25193;&#23637;&#20026;&#25552;&#20379;&#19968;&#20010;&#20801;&#35768;&#29468;&#27979;&#32780;&#19981;&#30830;&#23450;&#30340;&#35768;&#21487;&#35777;&#21644;&#37327;&#21270;&#24182;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#20250;&#65288;L&amp;O&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#39069;&#22806;&#35774;&#22791;&#21363;&#21487;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#21487;&#29992;&#30340;&#32622;&#20449;&#24230;&#25351;&#26631;&#65288;0.759 AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00914v1 Announce Type: new  Abstract: Large Language Models (LLMs) such as GPT-4 have shown enough promise in the few-shot learning context to suggest use in the generation of "silver" data and refinement of new ontologies through iterative application and review. Such workflows become more effective with reliable confidence estimation. Unfortunately, confidence estimation is a documented weakness of models such as GPT-4, and established methods to compensate require significant additional complexity and computation. The present effort explores methods for effective confidence estimation with GPT-4 with few-shot learning for event detection in the BETTER ontology as a vehicle. The key innovation is expanding the prompt and task presented to GPT-4 to provide License to speculate when unsure and Opportunity to quantify and explain its uncertainty (L&amp;O). This approach improves accuracy and provides usable confidence measures (0.759 AUC) with no additional machinery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38745;&#24687;&#29366;&#24577;EEG&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#26799;&#24230;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#35299;&#37322;&#26469;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#22797;&#26434;&#22823;&#33041;&#36830;&#25509;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00906</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#22836;&#22270;&#32467;&#26500;&#23398;&#20064;&#19982;&#26799;&#24230;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#35299;&#37322;&#30340;&#38745;&#24687;&#29366;&#24577;EEG&#22810;&#22836;&#22270;&#32467;&#26500;&#23398;&#20064;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Parkinson's Disease Detection from Resting State EEG using Multi-Head Graph Structure Learning with Gradient Weighted Graph Attention Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38745;&#24687;&#29366;&#24577;EEG&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#26799;&#24230;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#35299;&#37322;&#26469;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#22797;&#26434;&#22823;&#33041;&#36830;&#25509;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00906v1 &#23459;&#24067;&#31867;&#22411;: &#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00906v1 Announce Type: cross  Abstract: Parkinson's disease (PD) is a debilitating neurodegenerative disease that has severe impacts on an individual's quality of life. Compared with structural and functional MRI-based biomarkers for the disease, electroencephalography (EEG) can provide more accessible alternatives for clinical insights. While deep learning (DL) techniques have provided excellent outcomes, many techniques fail to model spatial information and dynamic brain connectivity, and face challenges in robust feature learning, limited data sizes, and poor explainability. To address these issues, we proposed a novel graph neural network (GNN) technique for explainable PD detection using resting state EEG. Specifically, we employ structured global convolutions with contrastive learning to better model complex features with limited data, a novel multi-head graph structure learner to capture the non-Euclidean structure of EEG data, and a head-wise gradient-weighted graph 
&lt;/p&gt;</description></item><item><title>AnoT&#36890;&#36807;&#23558;&#26102;&#24207;&#30693;&#35782;&#22270;&#36716;&#25442;&#25104;&#35268;&#21017;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36866;&#24212;&#30693;&#35782;&#26356;&#26032;&#30340;&#27169;&#24335;&#21464;&#21270;&#21644;&#35821;&#20041;&#28418;&#31227;&#12290;</title><link>https://arxiv.org/abs/2408.00872</link><description>&lt;p&gt;
&#26102;&#24207;&#30693;&#35782;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Online Detection of Anomalies in Temporal Knowledge Graphs with Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00872
&lt;/p&gt;
&lt;p&gt;
AnoT&#36890;&#36807;&#23558;&#26102;&#24207;&#30693;&#35782;&#22270;&#36716;&#25442;&#25104;&#35268;&#21017;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36866;&#24212;&#30693;&#35782;&#26356;&#26032;&#30340;&#27169;&#24335;&#21464;&#21270;&#21644;&#35821;&#20041;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#30693;&#35782;&#22270;&#65288;TKGs&#65289;&#23545;&#20110;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#28436;&#21464;&#20851;&#31995;&#38750;&#24120;&#23453;&#36149;&#65292;&#20294;&#24448;&#24448;&#20805;&#26021;&#30528;&#22122;&#22768;&#65292;&#22240;&#27492;&#38656;&#35201;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22312;&#25429;&#25417;TKGs&#20013;&#33410;&#28857;&#21644;&#36793;&#31867;&#30340;&#20016;&#23500;&#35821;&#20041;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#32780;TKG&#23884;&#20837;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#21066;&#24369;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36866;&#24212;&#22240;&#30693;&#35782;&#26356;&#26032;&#32780;&#23548;&#33268;&#30340;&#27169;&#24335;&#21464;&#21270;&#21644;&#35821;&#20041;&#28418;&#31227;&#26041;&#38754;&#20063;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AnoT&#65292;&#19968;&#31181;&#38024;&#23545;TKG&#30340;&#39640;&#25928;&#21487;&#35299;&#37322;&#24615;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;AnoT&#39318;&#20808;&#23558;TKG&#24635;&#32467;&#20026;&#19968;&#31181;&#26032;&#30340;&#35268;&#21017;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;TKGs&#20013;&#28789;&#27963;&#22320;&#25512;&#26029;&#20986;&#22797;&#26434;&#30340;&#27169;&#24335;&#12290;&#24403;&#26032;&#30340;&#30693;&#35782;&#20986;&#29616;&#26102;&#65292;AnoT&#23558;&#23427;&#26144;&#23556;&#21040;&#35268;&#21017;&#22270;&#20013;&#30340;&#19968;&#20010;&#33410;&#28857;&#19978;&#65292;&#24182;&#36882;&#24402;&#22320;&#36941;&#21382;&#35268;&#21017;&#22270;&#26469;&#25512;&#23548;&#20986;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00872v1 Announce Type: new  Abstract: Temporal knowledge graphs (TKGs) are valuable resources for capturing evolving relationships among entities, yet they are often plagued by noise, necessitating robust anomaly detection mechanisms. Existing dynamic graph anomaly detection approaches struggle to capture the rich semantics introduced by node and edge categories within TKGs, while TKG embedding methods lack interpretability, undermining the credibility of anomaly detection. Moreover, these methods falter in adapting to pattern changes and semantic drifts resulting from knowledge updates. To tackle these challenges, we introduce AnoT, an efficient TKG summarization method tailored for interpretable online anomaly detection in TKGs. AnoT begins by summarizing a TKG into a novel rule graph, enabling flexible inference of complex patterns in TKGs. When new knowledge emerges, AnoT maps it onto a node in the rule graph and traverses the rule graph recursively to derive the anomaly
&lt;/p&gt;</description></item><item><title>UniMoT&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#23376;-&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;tokenizer&#65292;&#23427;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#23558;&#20998;&#23376;&#36716;&#25442;&#25104;&#24207;&#21015;&#30340;&#20998;&#23376;&#20196;&#29260;&#65292;&#23454;&#29616;&#20998;&#23376;&#19982;&#25991;&#26412;&#30340;&#39640;&#25928;&#38598;&#25104;&#12290;</title><link>https://arxiv.org/abs/2408.00863</link><description>&lt;p&gt;
UniMoT&#65306;&#20855;&#26377;&#31163;&#25955;&#20196;&#29260;&#34920;&#31034;&#30340;&#32479;&#19968;&#20998;&#23376;-&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00863
&lt;/p&gt;
&lt;p&gt;
UniMoT&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#23376;-&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;tokenizer&#65292;&#23427;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#23558;&#20998;&#23376;&#36716;&#25442;&#25104;&#24207;&#21015;&#30340;&#20998;&#23376;&#20196;&#29260;&#65292;&#23454;&#29616;&#20998;&#23376;&#19982;&#25991;&#26412;&#30340;&#39640;&#25928;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00863v1 Announce Type: cross  &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#25512;&#21160;&#30740;&#31350;&#31038;&#21306;&#25193;&#23637;&#20854;&#33021;&#21147;&#21040;&#20998;&#23376;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#23376;LLMs&#20351;&#29992;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#19981;&#24179;&#31561;&#22320;&#23545;&#24453;&#20998;&#23376;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#20998;&#23376;&#27169;&#24577;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniMoT&#65292;&#19968;&#20010;&#32479;&#19968;&#20998;&#23376;-&#25991;&#26412;LLM&#65292;&#37319;&#29992;&#22522;&#20110;tokenizer&#30340;&#26550;&#26500;&#65292;&#23558;&#20998;&#23376;&#20196;&#29260;&#25193;&#23637;&#21040;LLM&#30340;&#35789;&#27719;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#65288;Vector Quantization&#65289;&#30340;tokenizer&#65292;&#20854;&#37319;&#29992;Q-Former&#26469;&#24357;&#21512;&#20998;&#23376;&#19982;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;tokenizer&#23558;&#20998;&#23376;&#36716;&#25442;&#20026;&#20855;&#26377;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#30340;&#20998;&#23376;&#20196;&#29260;&#24207;&#21015;&#65292;&#23553;&#35013;&#20102;&#20998;&#23376;&#21644;&#25991;&#26412;&#30340;&#39640;&#23618;&#20449;&#24687;&#12290;&#35013;&#22791;&#20102;&#36825;&#20010;tokenizer&#65292;UniMoT&#21487;&#20197;&#23558;&#20998;&#23376;&#21644;&#25991;&#26412;&#27169;&#24577;&#32479;&#19968;&#21040;&#20849;&#20139;&#20196;&#29260;&#34920;&#31034;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#30495;&#27491;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00863v1 Announce Type: cross  Abstract: The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive
&lt;/p&gt;</description></item><item><title>&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#19982;&#31070;&#32463;&#28210;&#26579;&#32467;&#21512;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D&#36229;&#22768;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00860</link><description>&lt;p&gt;
UlRe-NeRF: &#20351;&#29992;&#31070;&#32463;&#28210;&#26579;&#30340;3D&#36229;&#22768;&#25104;&#20687;&#65292;&#36890;&#36807;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#19982;&#31070;&#32463;&#28210;&#26579;&#32467;&#21512;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D&#36229;&#22768;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25552;&#20132;  &#25688;&#35201;&#65306;&#19977;&#32500;&#36229;&#22768;&#25104;&#20687;&#22312;&#21307;&#30103;&#35786;&#26029;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#19977;&#32500;&#36229;&#22768;&#25104;&#20687;&#26041;&#27861;&#23384;&#22312;&#22266;&#23450;&#20998;&#36776;&#29575;&#12289;&#23384;&#20648;&#25928;&#29575;&#20302;&#12289;&#19978;&#19979;&#25991;&#36830;&#25509;&#19981;&#36275;&#31561;&#38382;&#39064;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#24322;&#24120;&#21644;&#21453;&#23556;&#29305;&#24615;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;NeRF&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#30340;&#25216;&#26415;&#22312;&#35270;&#35282;&#21512;&#25104;&#21644;&#19977;&#32500;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#65292;&#20294;&#22312;&#39640;&#28165;&#26224;&#24230;&#36229;&#22768;&#25104;&#20687;&#26041;&#38754;&#20173;&#26377;&#30740;&#31350;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;UlRe-NeRF&#65292;&#23427;&#23558;&#38544;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26174;&#24335;&#30340;&#36229;&#22768;&#20307;&#32472;&#21046;&#21151;&#33021;&#38598;&#25104;&#21040;&#19968;&#20010;&#36229;&#22768;&#31070;&#32463;&#28210;&#26579;&#26550;&#26500;&#20013;&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#20102;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#65292;&#20351;&#29992;&#26041;&#21521;&#24615;MLP&#27169;&#22359;&#29983;&#25104;&#35270;&#35282;&#20381;&#36182;&#30340;&#39640;&#39057;&#21453;&#23556;&#24378;&#24230;&#20272;&#35745;&#65292;&#20197;&#21450;&#19968;&#20010;&#31354;&#38388;MLP&#27169;&#22359;&#26469;&#20272;&#35745;&#25972;&#20010;&#31354;&#38388;&#20869;&#30340;&#21453;&#23556;&#24378;&#24230;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#25216;&#26415;&#65292;UlRe-NeRF&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D ultrasound&#28210;&#26579;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#23384;&#20648;&#21644;&#26356;&#20248;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#21644;&#36879;&#35270;&#38382;&#39064;&#26102;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UlRe-NeRF&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#36229;&#22768;&#25104;&#20687;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v1 Announce Type: new  Abstract: Three-dimensional ultrasound imaging is a critical technology widely used in medical diagnostics. However, traditional 3D ultrasound imaging methods have limitations such as fixed resolution, low storage efficiency, and insufficient contextual connectivity, leading to poor performance in handling complex artifacts and reflection characteristics. Recently, techniques based on NeRF (Neural Radiance Fields) have made significant progress in view synthesis and 3D reconstruction, but there remains a research gap in high-quality ultrasound imaging. To address these issues, we propose a new model, UlRe-NeRF, which combines implicit neural networks and explicit ultrasound volume rendering into an ultrasound neural rendering architecture. This model incorporates reflection direction parameterization and harmonic encoding, using a directional MLP module to generate view-dependent high-frequency reflection intensity estimates, and a spatial MLP mod
&lt;/p&gt;</description></item><item><title>Y &#31038;&#20132;&#26159;&#19968;&#20010;&#20351;&#29992;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21452;&#32990;&#32974;&#65292;&#23427;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#22312;&#32447;&#24179;&#21488;&#30340;&#21160;&#24577;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.00818</link><description>&lt;p&gt;
Y &#31038;&#20132;&#65306;&#22522;&#20110; LLMs &#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21452;&#32990;&#32974;
&lt;/p&gt;
&lt;p&gt;
Y Social: an LLM-powered Social Media Digital Twin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00818
&lt;/p&gt;
&lt;p&gt;
Y &#31038;&#20132;&#26159;&#19968;&#20010;&#20351;&#29992;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21452;&#32990;&#32974;&#65292;&#23427;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#22312;&#32447;&#24179;&#21488;&#30340;&#21160;&#24577;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#21452;&#32990;&#32974; Y&#65292;&#23427;&#26088;&#22312;&#27169;&#25311;&#19968;&#20010;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#12290;&#25968;&#23383;&#21452;&#32990;&#32974;&#26159;&#29289;&#29702;&#31995;&#32479;&#30340;&#34394;&#25311;&#22797;&#21046;&#21697;&#65292;&#29992;&#20110;&#36827;&#34892;&#39640;&#32423;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20687; Y &#36825;&#26679;&#30340;&#25968;&#23383;&#21452;&#32990;&#32974;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#27169;&#25311;&#21644;&#29702;&#35299;&#22797;&#26434;&#22312;&#32447;&#20114;&#21160;&#12290;Y &#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21487;&#20197;&#22797;&#29616;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#20934;&#30830;&#22320;&#27169;&#25311;&#29992;&#25143;&#20114;&#21160;&#12289;&#20869;&#23481;&#20256;&#25773;&#21644;&#32593;&#32476;&#21160;&#24577;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#26041;&#38754;&#65292;Y &#25552;&#20379;&#26377;&#20851;&#29992;&#25143;&#21442;&#19982;&#24230;&#12289;&#20449;&#24687;&#20256;&#25773;&#21644;&#24179;&#21488;&#25919;&#31574;&#24433;&#21709;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38598;&#25104; LLMs&#65292;Y &#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#30340;&#25991;&#26412;&#20869;&#23481;&#21644;&#39044;&#27979;&#29992;&#25143;&#21453;&#24212;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00818v1 Announce Type: new  Abstract: In this paper we introduce Y, a new-generation digital twin designed to replicate an online social media platform. Digital twins are virtual replicas of physical systems that allow for advanced analyses and experimentation. In the case of social media, a digital twin such as Y provides a powerful tool for researchers to simulate and understand complex online interactions. {\tt Y} leverages state-of-the-art Large Language Models (LLMs) to replicate sophisticated agent behaviors, enabling accurate simulations of user interactions, content dissemination, and network dynamics. By integrating these aspects, Y offers valuable insights into user engagement, information spread, and the impact of platform policies. Moreover, the integration of LLMs allows Y to generate nuanced textual content and predict user responses, facilitating the study of emergent phenomena in online environments.   To better characterize the proposed digital twin, in this
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#30899;&#25490;&#25918;&#65292;&#29305;&#21035;&#26159;&#22312;&#21160;&#24577;&#20132;&#36890;&#26465;&#20214;&#19979;&#24615;&#33021;&#26356;&#20339;&#65292;&#26174;&#31034;&#20102;&#20854;&#24191;&#38420;&#30340;&#23454;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2408.00814</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36866;&#24212;&#24615;&#20132;&#36890;&#20449;&#21495;&#23433;&#20840;&#19982;&#25928;&#29575;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal safety and efficiency improvement by multi objective deep reinforcement learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#30899;&#25490;&#25918;&#65292;&#29305;&#21035;&#26159;&#22312;&#21160;&#24577;&#20132;&#36890;&#26465;&#20214;&#19979;&#24615;&#33021;&#26356;&#20339;&#65292;&#26174;&#31034;&#20102;&#20854;&#24191;&#38420;&#30340;&#23454;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;ATSC&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#26696;&#26088;&#22312;&#25552;&#39640;&#36335;&#21475;&#25511;&#21046;&#31574;&#30053;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#33073;&#30899;&#21270;&#19977;&#20010;&#30446;&#26631;&#12290;&#20256;&#32479;&#30340;ATSC&#26041;&#27861;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#20132;&#36890;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#36866;&#24212;&#23454;&#26102;&#21160;&#24577;&#20132;&#36890;&#26465;&#20214;&#26102;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dueling Double Deep Q Network&#65288;D3QN&#65289;&#26694;&#26550;&#30340;DRL-based ATSC&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#20013;&#22269;&#30340;&#38271;&#27801;&#30340;&#19968;&#20010;&#27169;&#25311;&#36335;&#21475;&#30340;&#34920;&#29616;&#34987;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#20256;&#32479;&#30340;ATSC&#21644;&#21333;&#32431;&#20248;&#21270;&#25928;&#29575;&#30340;ATSC&#31639;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;ATSC&#31639;&#27861;&#36890;&#36807;&#23454;&#29616;&#20132;&#36890;&#20107;&#25925;&#20943;&#23569;&#20102;16%&#21644;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#20943;&#23569;&#20102;4%&#65292;&#31361;&#20986;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#36824;&#33021;&#22312;&#21464;&#21270;&#22810;&#31471;&#30340;&#20132;&#36890;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00814v1 Announce Type: cross  Abstract: This research introduces an innovative method for adaptive traffic signal control (ATSC) through the utilization of multi-objective deep reinforcement learning (DRL) techniques. The proposed approach aims to enhance control strategies at intersections while simultaneously addressing safety, efficiency, and decarbonization objectives. Traditional ATSC methods typically prioritize traffic efficiency and often struggle to adapt to real-time dynamic traffic conditions. To address these challenges, the study suggests a DRL-based ATSC algorithm that incorporates the Dueling Double Deep Q Network (D3QN) framework. The performance of this algorithm is assessed using a simulated intersection in Changsha, China. Notably, the proposed ATSC algorithm surpasses both traditional ATSC and ATSC algorithms focused solely on efficiency optimization by achieving over a 16% reduction in traffic conflicts and a 4% decrease in carbon emissions. Regarding tr
&lt;/p&gt;</description></item><item><title>ChipExpert&#26159;&#19987;&#38376;&#20026;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#39640;&#38376;&#27099;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.00804</link><description>&lt;p&gt;
ChipExpert&#65306;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#19987;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00804
&lt;/p&gt;
&lt;p&gt;
ChipExpert&#26159;&#19987;&#38376;&#20026;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#39640;&#38376;&#27099;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00804v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#39640;&#24230;&#30340;&#19987;&#19994;&#24615;&#65292;&#20026;&#20837;&#38376;&#21644;&#30740;&#31350;&#24320;&#21457;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLMs&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#23398;&#29983;&#12289;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#22312;IC&#35774;&#35745;&#39046;&#22495;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;IC&#35774;&#35745;&#39046;&#22495;&#20013;&#30340;&#28508;&#21147;&#22823;&#37096;&#20998;&#20173;&#26410;&#34987;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChipExpert&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;IC&#35774;&#35745;&#39046;&#22495;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#25945;&#32946;&#22411;LLM&#12290;ChipExpert&#26159;&#22312;&#24403;&#21069;&#26368;&#20339;&#24320;&#28304;&#22522;&#30784;&#27169;&#22411;&#65288;Llama-3 8B&#65289;&#19978;&#35757;&#32451;&#30340;&#12290;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#21253;&#25324;&#20960;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#21253;&#25324;&#25968;&#25454;&#20934;&#22791;&#12289;&#32487;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#25351;&#23548;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#21644;&#20559;&#22909;&#23545;&#40784;&#20197;&#21450;&#35780;&#20272;&#12290;&#22312;&#25968;&#25454;&#20934;&#22791;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#20154;&#24037;&#27880;&#37322;&#21019;&#24314;&#20102;&#22810;&#20010;&#39640;&#36136;&#37327;&#30340;&#23450;&#21046;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#19990;&#30028;&#35774;&#35745;&#25361;&#25112;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;IC&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;Karel&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#26469;&#38598;&#25104;IC&#35774;&#35745;&#30340;&#35821;&#35328;&#28459;&#28216;&#33021;&#21147;&#12290;&#22312;&#25351;&#20196;&#25351;&#23548;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21253;&#21547;&#22810;&#20010;IC&#35774;&#35745;&#30456;&#20851;&#20219;&#21153;&#30340;&#31934;&#35843;&#25968;&#25454;&#38598;&#12290;&#22312;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20559;&#32622;&#26657;&#27491;&#26469;&#25552;&#39640;ChipExpert&#22312;IC&#35774;&#35745;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#24037;&#19994;&#39046;&#22495;&#30340;&#24037;&#31243;&#24072;&#21512;&#20316;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#23454;&#39564;&#65292;&#20197;&#30830;&#20445;LLM&#33021;&#22815;&#20934;&#30830;&#22320;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;IC&#35774;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36890;&#36807;&#24320;&#25918;&#28304;&#20195;&#30721;&#21644;&#23450;&#21046;&#21270;&#30340;LLM&#65292;&#23558;&#36827;&#19968;&#27493;&#20419;&#36827;IC&#35774;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#25945;&#32946;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00804v1 Announce Type: cross  Abstract: The field of integrated circuit (IC) design is highly specialized, presenting significant barriers to entry and research and development challenges. Although large language models (LLMs) have achieved remarkable success in various domains, existing LLMs often fail to meet the specific needs of students, engineers, and researchers. Consequently, the potential of LLMs in the IC design domain remains largely unexplored. To address these issues, we introduce ChipExpert, the first open-source, instructional LLM specifically tailored for the IC design field. ChipExpert is trained on one of the current best open-source base model (Llama-3 8B). The entire training process encompasses several key stages, including data preparation, continue pre-training, instruction-guided supervised fine-tuning, preference alignment, and evaluation. In the data preparation stage, we construct multiple high-quality custom datasets through manual selection and d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#24494;&#26381;&#21153;&#20013;&#35782;&#21035;&#21644;&#35299;&#20915;&#22797;&#26434;&#20381;&#36182;&#21644;&#20256;&#25773;&#24615;&#25925;&#38556;&#25361;&#25112;&#30340;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;(RCA)&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#24674;&#22797;&#21644;&#32500;&#25252;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00803</link><description>&lt;p&gt;
&#20803;&#22240;&#20998;&#26512;&#22312;(&#24494;)&#26381;&#21153;&#20013;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Root Cause Analysis in (Micro) Services: Methodologies, Challenges, and Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#24494;&#26381;&#21153;&#20013;&#35782;&#21035;&#21644;&#35299;&#20915;&#22797;&#26434;&#20381;&#36182;&#21644;&#20256;&#25773;&#24615;&#25925;&#38556;&#25361;&#25112;&#30340;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;(RCA)&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#24674;&#22797;&#21644;&#32500;&#25252;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00803v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#24494;&#26381;&#21153;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#21644;&#20256;&#25773;&#24615;&#25925;&#38556;&#29305;&#24615;&#65292;&#36825;&#20123;&#26381;&#21153;&#20197;&#20854;&#23494;&#38598;&#30340;&#32593;&#32476;&#20114;&#32852;&#29305;&#24615;&#20026;&#26631;&#24535;&#65292;&#22312;&#30830;&#23450;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36805;&#36895;&#35782;&#21035;&#21644;&#35299;&#20915;&#30772;&#22351;&#24615;&#30340;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#24555;&#36895;&#24674;&#22797;&#21644;&#32500;&#25252;&#31995;&#32479;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#30151;&#29366;&#25968;&#25454;&#35786;&#26029;&#25925;&#38556;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;&#24494;&#26381;&#21153;&#20013;&#20803;&#22240;&#20998;&#26512;(RCA)&#25216;&#26415;&#30340;&#20840;&#38754;&#12289;&#32467;&#26500;&#21270;&#22238;&#39038;&#65292;&#25506;&#32034;&#21253;&#25324;&#24230;&#37327;&#12289;&#36319;&#36394;&#12289;&#26085;&#24535;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#22312;&#20869;&#30340;&#26041;&#27861;&#35770;&#12290;&#23427;&#28145;&#20837;&#25506;&#35752;&#20102;&#24494;&#26381;&#21153;&#26550;&#26500;&#20013;&#30340;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#20301;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#21160;&#21270;&#36827;&#27493;&#30340;&#26368;&#21069;&#27839;&#65292;&#23427;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00803v1 Announce Type: cross  Abstract: The complex dependencies and propagative faults inherent in microservices, characterized by a dense network of interconnected services, pose significant challenges in identifying the underlying causes of issues. Prompt identification and resolution of disruptive problems are crucial to ensure rapid recovery and maintain system stability. Numerous methodologies have emerged to address this challenge, primarily focusing on diagnosing failures through symptomatic data. This survey aims to provide a comprehensive, structured review of root cause analysis (RCA) techniques within microservices, exploring methodologies that include metrics, traces, logs, and multi-model data. It delves deeper into the methodologies, challenges, and future trends within microservices architectures. Positioned at the forefront of AI and automation advancements, it offers guidance for future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;LLM&#25512;&#29702;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#22312;&#38646;&#23556;&#31243;&#21644;&#24494;&#35843;&#35774;&#32622;&#20013;&#20351;&#29992;LLM&#25512;&#29702;&#33021;&#22815;&#25552;&#39640;&#20219;&#21153;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;RecSAVER&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#35780;&#20215;LLM&#25512;&#29702;&#21709;&#24212;&#36136;&#37327;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.00802</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#25512;&#29702;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Leveraging LLM Reasoning Enhances Personalized Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;LLM&#25512;&#29702;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#22312;&#38646;&#23556;&#31243;&#21644;&#24494;&#35843;&#35774;&#32622;&#20013;&#20351;&#29992;LLM&#25512;&#29702;&#33021;&#22815;&#25552;&#39640;&#20219;&#21153;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;RecSAVER&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#35780;&#20215;LLM&#25512;&#29702;&#21709;&#24212;&#36136;&#37327;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00802v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#65306;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#26174;&#31034;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;CoT(Chain-of-Thought&#65292;&#24605;&#24819;&#38142;)&#25552;&#31034;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#20687;&#31639;&#26415;&#25512;&#29702;&#36825;&#26679;&#30340;&#20219;&#21153;&#28041;&#21450;&#26126;&#30830;&#30340;&#12289;&#26126;&#30830;&#30340;&#31572;&#26696;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;LLM&#25512;&#29702;&#22312;&#25512;&#33616;&#31995;&#32479;(RecSys)&#20013;&#30340;&#24212;&#29992;&#26159;&#19968;&#20010;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20960;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;RecSys&#20013;&#30340;&#25512;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38646;&#23556;&#31243;&#21644;&#24494;&#35843;&#35774;&#32622;&#20013;&#20351;&#29992;LLM&#25512;&#29702;&#26159;&#22914;&#20309;&#25552;&#39640;&#20219;&#21153;&#36136;&#37327;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;RecSAVER&#65288;&#25512;&#33616;&#31995;&#32479;&#33258;&#21160;&#39564;&#35777;&#21644;&#35780;&#20215;&#25512;&#29702;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;LLM&#25512;&#29702;&#21709;&#24212;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#32463; curated &#40644;&#37329;&#21442;&#32771;&#25110;&#20154;&#31867;&#35780;&#20998;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00802v1 Announce Type: cross  Abstract: Recent advancements have showcased the potential of Large Language Models (LLMs) in executing reasoning tasks, particularly facilitated by Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve clear, definitive answers and logical chains of thought, the application of LLM reasoning in recommendation systems (RecSys) presents a distinct challenge. RecSys tasks revolve around subjectivity and personalized preferences, an under-explored domain in utilizing LLMs' reasoning capabilities. Our study explores several aspects to better understand reasoning for RecSys and demonstrate how task quality improves by utilizing LLM reasoning in both zero-shot and finetuning settings. Additionally, we propose RecSAVER (Recommender Systems Automatic Verification and Evaluation of Reasoning) to automatically assess the quality of LLM reasoning responses without the requirement of curated gold references or human raters. We show 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#24335;&#26412;&#20307;&#20132;&#20114;&#27010;&#24565;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#25552;&#38382;&#20934;&#30830;&#36716;&#21270;&#20026;SPARQL&#26597;&#35810;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#34394;&#20551;&#20449;&#24687;&#30340;&#20135;&#29983;&#12290;</title><link>https://arxiv.org/abs/2408.00800</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#24335;&#26412;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#24335;&#26412;&#20307;&#20132;&#20114;&#27010;&#24565;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#25552;&#38382;&#20934;&#30830;&#36716;&#21270;&#20026;SPARQL&#26597;&#35810;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#34394;&#20551;&#20449;&#24687;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#36129;&#29486;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#65292;&#23427;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#30028;&#38754;&#26469;&#22686;&#24378;&#26412;&#20307;&#35770;&#30340;SPARQL&#26597;&#35810;&#29983;&#25104;&#65292;&#20174;&#32780;&#20026;&#27491;&#24335;&#30693;&#35782;&#25552;&#20379;&#30452;&#35266;&#30340;&#35775;&#38382;&#36884;&#24452;&#12290;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25552;&#38382;&#65292;&#31995;&#32479;&#23558;&#36825;&#20123;&#25552;&#38382;&#36716;&#25442;&#25104;&#20934;&#30830;&#30340;SPARQL&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20005;&#26684;&#26597;&#35810;&#26412;&#20307;&#35770;&#30340;&#20869;&#23481;&#65292;&#38450;&#27490;&#30001;LLM&#24102;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#34394;&#26500;&#20869;&#23481;&#12290;&#20026;&#20102;&#25552;&#39640;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#65292;&#23558;&#26469;&#33258;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#39069;&#22806;&#25991;&#26412;&#20449;&#24687;&#25972;&#21512;&#21040;&#26412;&#20307;&#35770;&#20013;&#65292;&#20197;&#20415;&#23545;&#23427;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#31934;&#30830;&#25551;&#36848;&#12290;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;SPARQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;LLMs&#26597;&#35810;&#26412;&#20307;&#35770;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00800v1 Announce Type: cross  Abstract: The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21517;&#20026;Golden-Retriever&#30340;&#31995;&#32479;&#34987;&#35774;&#35745;&#29992;&#26469;&#22686;&#24378;&#24037;&#19994;&#30693;&#35782;&#24211;&#30340;&#26816;&#32034;&#25928;&#29575;&#65292;&#36890;&#36807;&#25512;&#25970;&#24335;&#30340;&#26597;&#35810;&#22686;&#24378;&#21644;&#26126;&#30830;&#30340;&#19978;&#19979;&#25991;&#35299;&#37322;&#65292;&#25552;&#21319;&#20102;RAG&#26694;&#26550;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00798</link><description>&lt;p&gt;
Golden-Retriever: &#39640;&#20445;&#30495;&#33021;&#21160;&#24615;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22312;&#24037;&#19994;&#30693;&#35782;&#24211;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00798
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21517;&#20026;Golden-Retriever&#30340;&#31995;&#32479;&#34987;&#35774;&#35745;&#29992;&#26469;&#22686;&#24378;&#24037;&#19994;&#30693;&#35782;&#24211;&#30340;&#26816;&#32034;&#25928;&#29575;&#65292;&#36890;&#36807;&#25512;&#25970;&#24335;&#30340;&#26597;&#35810;&#22686;&#24378;&#21644;&#26126;&#30830;&#30340;&#19978;&#19979;&#25991;&#35299;&#37322;&#65292;&#25552;&#21319;&#20102;RAG&#26694;&#26550;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00798v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26412;&#25991;&#20171;&#32461;&#20102;Golden-Retriever&#65292;&#26088;&#22312;&#39640;&#25928;&#22320;&#23548;&#33322;&#24222;&#22823;&#30340;&#24037;&#19994;&#30693;&#35782;&#24211;&#65292;&#20811;&#26381;&#20102;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;(RAG)&#26694;&#26550;&#22312;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#21644;&#19978;&#19979;&#25991;&#35299;&#37322;&#26041;&#38754;&#30340;&#20256;&#32479;&#25361;&#25112;&#12290;Golden-Retriever&#22312;&#25991;&#26723;&#26816;&#32034;&#21069;&#23454;&#26045;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24605;&#30340;&#26597;&#35810;&#22686;&#24378;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#28041;&#21450;&#35782;&#21035;&#36755;&#20837;&#38382;&#39064;&#20013;&#30340;&#26415;&#35821;&#12289;&#22522;&#20110;&#19978;&#19979;&#25991;&#28548;&#28165;&#20854;&#24847;&#20041;&#65292;&#24182;&#22312;&#22686;&#24378;&#26597;&#35810;&#20043;&#21069;&#21015;&#20986;&#25152;&#26377;&#26415;&#35821;&#21644;&#32553;&#20889;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#28041;&#21450;&#20174;&#36755;&#20837;&#38382;&#39064;&#20013;&#25552;&#21462;&#21644;&#21015;&#20986;&#25152;&#26377;&#30340;&#26415;&#35821;&#21644;&#32553;&#20889;&#12289;&#30830;&#23450;&#19982;&#20043;&#30456;&#31526;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20174;&#39044;&#23450;&#20041;&#30340;&#26415;&#35821;&#20856;&#20013;&#20026;&#27599;&#39033;&#23547;&#27714;&#25193;&#23637;&#30340;&#23450;&#20041;&#21644;&#25551;&#36848;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;&#22686;&#24378;&#30830;&#20445;&#20102;RAG&#26694;&#26550;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#21644;&#35299;&#37322;&#36855;&#24785;&#24773;&#22659;&#65292;&#33021;&#22815;&#26816;&#32034;&#21040;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#19977;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#26679;&#26412;&#38598;&#19978;&#30340;&#25628;&#32034;&#20934;&#30830;&#24615;&#24471;&#20998;&#19978;&#33719;&#24471;&#20102;&#26126;&#26174;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#20891;&#20107;&#21644;&#24037;&#19994;&#35821;&#35328;&#24212;&#29992;&#26041;&#38754;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#30417;&#25511;&#25968;&#25454;&#38598;&#20013;&#65292;&#26597;&#35810;&#24369;&#30340;&#39046;&#22495;&#24863;&#34987;&#30452;&#25509;&#36716;&#21270;&#20026;&#22686;qq&#24378;&#38382;&#39064;&#23545;&#21407;&#22987;&#26597;&#35810;&#30340;&#33391;&#22909;&#21709;&#24212;&#12290;&#25972;&#20307;&#19978;&#65292;Golden-Retriever&#22312;&#25968;&#37327;&#36739;&#22810;&#30340;&#26679;&#26412;&#38598;&#19978;&#20063;&#33719;&#24471;&#20102;&#26368;&#39640;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#20877;&#27425;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24037;&#19994;&#30693;&#35782;&#26816;&#32034;&#21644;&#38382;&#31572;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23427;&#22312;&#25345;&#32493;&#32435;&#20837;&#26032;&#21457;&#29616;&#30340;&#39046;&#22495;&#26415;&#35821;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00798v1 Announce Type: cross  Abstract: This paper introduces Golden-Retriever, designed to efficiently navigate vast industrial knowledge bases, overcoming challenges in traditional LLM fine-tuning and RAG frameworks with domain-specific jargon and context interpretation. Golden-Retriever incorporates a reflection-based question augmentation step before document retrieval, which involves identifying jargon, clarifying its meaning based on context, and augmenting the question accordingly. Specifically, our method extracts and lists all jargon and abbreviations in the input question, determines the context against a pre-defined list, and queries a jargon dictionary for extended definitions and descriptions. This comprehensive augmentation ensures the RAG framework retrieves the most relevant documents by providing clear context and resolving ambiguities, significantly improving retrieval accuracy. Evaluations using three open-source LLMs on a domain-specific question-answer d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21327;&#20316;&#20849;&#36827;&#21270;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;SNN&#21098;&#26525;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2408.00794</link><description>&lt;p&gt;
CCSRP: &#36890;&#36807;&#21327;&#20316;&#20849;&#36827;&#21270;&#27861;&#23454;&#29616;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#31283;&#20581;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
CCSRP: Robust Pruning of Spiking Neural Networks through Cooperative Coevolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#20849;&#36827;&#21270;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;SNN&#21098;&#26525;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00794v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#21508;&#31181;&#21160;&#24577;&#35270;&#35273;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#28508;&#21147;&#65292;&#20294;&#37027;&#20123;&#36866;&#21512;&#23454;&#38469;&#37096;&#32626;&#30340;&#36890;&#24120;&#32570;&#20047;&#22312;&#36164;&#28304;&#21463;&#38480;&#21644;&#20851;&#38190;&#23433;&#20840;&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#32039;&#20945;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#32593;&#32476;&#21098;&#26525;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#31561;&#31574;&#30053;&#26469;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#32039;&#20945;&#24615;&#25110;&#40065;&#26834;&#24615;&#65292;&#32780;&#23545;&#20110;SNNs&#30340;&#31867;&#20284;&#26041;&#27861;&#30740;&#31350;&#29978;&#23569;&#12290;&#31283;&#20581;&#20462;&#21098;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21450;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#31283;&#20581;&#20462;&#21098;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#21644;&#21453;&#22797;&#35797;&#39564;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#21098;&#26525;&#26631;&#20934;&#25110;&#36741;&#21161;&#27169;&#22359;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#20204;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#26469;&#33258;&#21160;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21098;&#26525;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#36229;&#20986;&#20102;&#21098;&#26525;&#26631;&#20934;&#25110;&#36741;&#21161;&#27169;&#22359;&#30340;&#36873;&#25321;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00794v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs) have shown promise in various dynamic visual tasks, yet those ready for practical deployment often lack the compactness and robustness essential in resource-limited and safety-critical settings. Prior research has predominantly concentrated on enhancing the compactness or robustness of artificial neural networks through strategies like network pruning and adversarial training, with little exploration into similar methodologies for SNNs. Robust pruning of SNNs aims to reduce computational overhead while preserving both accuracy and robustness. Current robust pruning approaches generally necessitate expert knowledge and iterative experimentation to establish suitable pruning criteria or auxiliary modules, thus constraining their broader application. Concurrently, evolutionary algorithms (EAs) have been employed to automate the pruning of artificial neural networks, delivering remarkable outcomes yet overloo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#26426;&#22330;&#36816;&#33829;&#35745;&#21010;&#30340;&#20248;&#21270;&#65292;&#20197;&#24212;&#23545;&#22825;&#27668;&#28798;&#23475;&#30340;&#32039;&#24613;&#30095;&#25955;&#38656;&#27714;&#65292;&#21363;&#20351;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#30095;&#25955;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.00790</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#36951;&#20256;&#31639;&#27861;&#25913;&#36827;&#39044;&#28798;&#35268;&#21010;&#30340;&#33322;&#31354;&#26426;&#21160;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Air Mobility for Pre-Disaster Planning with Neural Network Accelerated Genetic Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#26426;&#22330;&#36816;&#33829;&#35745;&#21010;&#30340;&#20248;&#21270;&#65292;&#20197;&#24212;&#23545;&#22825;&#27668;&#28798;&#23475;&#30340;&#32039;&#24613;&#30095;&#25955;&#38656;&#27714;&#65292;&#21363;&#20351;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#30095;&#25955;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00790v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22825;&#27668;&#28798;&#23475;&#30456;&#20851;&#30340;&#32039;&#24613;&#25805;&#20316;&#23545;&#33322;&#31354;&#26426;&#21160;&#24615;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24433;&#21709;&#36880;&#28176;&#20020;&#36817;&#30340;&#26102;&#20505;&#65292;&#23588;&#20854;&#26159;&#22312;&#24433;&#21709;&#36880;&#28176;&#20020;&#36817;&#30340;&#26102;&#20505;&#65292;&#23588;&#20854;&#26159;&#22312;&#24433;&#21709;&#36880;&#28176;&#20020;&#36817;&#30340;&#26102;&#20505;&#65292;&#23545;&#39134;&#26426;&#21644;&#26426;&#22330;&#25805;&#20316;&#23588;&#20854;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#26426;&#22330;&#36816;&#33829;&#35745;&#21010;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#36825;&#31181;&#39044;&#28798;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#26426;&#22330;&#27719;&#24635;&#20250;&#36816;&#33829;&#25968;&#25454;&#65292;&#28982;&#21518;&#30830;&#23450;&#26368;&#22810;&#25764;&#31163;&#33322;&#29677;&#30340;&#25968;&#37327;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#21463;&#24433;&#21709;&#26426;&#22330;&#30340;&#21457;&#20986;&#33021;&#21147;&#65292;&#21516;&#26102;&#19981;&#38459;&#30861;&#24120;&#35268;&#33322;&#31354;&#20132;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21152;&#36895;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#29992;&#20110;&#30095;&#25955;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25972;&#21512;&#34429;&#28982;&#25552;&#20379;&#20102;&#19982;&#36739;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#30456;&#20284;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#32553;&#23567;&#31181;&#32676;&#35268;&#27169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#36951;&#20256;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#26426;&#22330;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21518;&#20063;&#21516;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00790v1 Announce Type: cross  Abstract: Weather disaster related emergency operations pose a great challenge to air mobility in both aircraft and airport operations, especially when the impact is gradually approaching. We propose an optimized framework for adjusting airport operational schedules for such pre-disaster scenarios. We first, aggregate operational data from multiple airports and then determine the optimal count of evacuation flights to maximize the impacted airport's outgoing capacity without impeding regular air traffic. We then propose a novel Neural Network (NN) accelerated Genetic Algorithm(GA) for evacuation planning. Our experiments show that integration yielded comparable results but with smaller computational overhead. We find that the utilization of a NN enhances the efficiency of a GA, facilitating more rapid convergence even when operating with a reduced population size. This effectiveness persists even when the model is trained on data from airports d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36776;&#35782;&#21644;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#26102;&#30340;&#20449;&#20208;&#39134;&#36291;&#65292;&#30452;&#25509;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26500;&#24314;&#20869;&#22312;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2408.00786</link><description>&lt;p&gt;
&#26159;&#21542;&#20449;&#20219;&#65306;&#26426;&#22120;&#23398;&#20064;&#30340;&#20449;&#20208;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
Whether to trust: the ML leap of faith
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36776;&#35782;&#21644;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#26102;&#30340;&#20449;&#20208;&#39134;&#36291;&#65292;&#30452;&#25509;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26500;&#24314;&#20869;&#22312;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00786v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#37319;&#32435;&#26469;&#35828;&#65292;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20449;&#20219;&#36890;&#24120;&#34987;&#29702;&#35299;&#20026;&#19968;&#31181;&#24577;&#24230;&#65292;&#20294;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#27979;&#37327;&#23427;&#65292;&#20063;&#19981;&#33021;&#23545;&#20854;&#36827;&#34892;&#31649;&#29702;&#12290;&#25105;&#20204;&#23558;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20449;&#20219;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21450;&#20854;&#32452;&#25104;&#37096;&#20214;&#30340;&#20449;&#20219;&#28151;&#20026;&#19968;&#35848;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29992;&#25143;&#22312;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#26102;&#25152;&#20570;&#30340;&#20449;&#20208;&#39134;&#36291;&#24448;&#24448;&#19981;&#34987;&#29702;&#35299;&#12290;&#24403;&#21069;&#26500;&#24314;&#20449;&#20219;&#30340;&#21162;&#21147;&#35299;&#37322;&#20102;ML&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#38750;ML&#19987;&#23478;&#26469;&#35828;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#65292;&#22240;&#20026;&#36825;&#20010;&#36807;&#31243;&#24456;&#22797;&#26434;&#65292;&#32780;&#19988;&#35299;&#37322;&#19982;&#20182;&#20204;&#33258;&#24049;&#26410;&#34920;&#36798;&#30340;&#24605;&#32500;&#27169;&#22411;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;ML&#20013;&#26500;&#24314;&#20869;&#22312;&#20449;&#20219;&#65292;&#36890;&#36807;&#36776;&#35782;&#21644;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;ML&#26102;&#30340;&#20449;&#20208;&#39134;&#36291;&#65288;Leap of Faith&#65292;LoF&#65289;&#12290;&#25105;&#20204;&#30340;LoF&#30697;&#38453;&#35782;&#21035;&#20986;ML&#27169;&#22411;&#19982;&#29992;&#25143;&#33258;&#36523;&#24605;&#32500;&#27169;&#22411;&#30340;&#21305;&#37197;&#24773;&#20917;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#21407;&#22987;&#25968;&#25454;&#21644;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#36755;&#20837;&#21040;ML&#27169;&#22411;&#20013;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35268;&#21017;&#20026;&#22522;&#30784;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20005;&#26684;&#20294;&#23454;&#38469;&#22320;&#35782;&#21035;&#36825;&#31181;&#21305;&#37197;&#12290;&#36825;&#31181;&#21305;&#37197;&#26159;&#22522;&#20110;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#20869;&#37096;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#23457;&#26597;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#30830;&#23450;&#30340;&#35268;&#21017;&#20316;&#20026;&#26657;&#20934;&#28857;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#21453;&#39304;&#19982;&#36825;&#20123;&#26657;&#20934;&#28857;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#29992;&#25143;&#23545;ML&#27169;&#22411;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23450;&#20041;&#30340;&#65292;&#20449;&#20219;&#24847;&#21619;&#30528;&#22312;&#27809;&#26377;&#20805;&#20998;&#20102;&#35299;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24895;&#24847;&#25509;&#21463;&#27169;&#22411;&#36755;&#20986;&#20316;&#20026;&#20915;&#31574;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00786v1 Announce Type: cross  Abstract: Human trust is critical for trustworthy AI adoption. Trust is commonly understood as an attitude, but we cannot accurately measure this, nor manage it. We conflate trust in the overall system, ML, and ML's component parts; so most users do not understand the leap of faith they take when they trust ML. Current efforts to build trust explain ML's process, which can be hard for non-ML experts to comprehend because it is complex, and explanations are unrelated to their own (unarticulated) mental models. We propose an innovative way of directly building intrinsic trust in ML, by discerning and measuring the Leap of Faith (LoF) taken when a user trusts ML. Our LoF matrix identifies where an ML model aligns to a user's own mental model. This match is rigorously yet practically identified by feeding the user's data and objective function both into an ML model and an expert-validated rules-based AI model, a verified point of reference that can 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#32447;&#32034;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#24773;&#22659;&#30693;&#35782;&#26469;&#25552;&#39640;&#22312;&#31038;&#20132;&#24773;&#22659;&#20013;&#20998;&#26512;&#38754;&#37096;&#34920;&#24773;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22234;&#24466;&#22256;&#22659;&#36825;&#19968;&#31038;&#20250;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.00780</link><description>&lt;p&gt;
&#28145;&#24230;&#20998;&#26512;&#36890;&#36807;&#30693;&#35782;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
In-Depth Analysis of Emotion Recognition through Knowledge-Based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#32447;&#32034;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#24773;&#22659;&#30693;&#35782;&#26469;&#25552;&#39640;&#22312;&#31038;&#20132;&#24773;&#22659;&#20013;&#20998;&#26512;&#38754;&#37096;&#34920;&#24773;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22234;&#24466;&#22256;&#22659;&#36825;&#19968;&#31038;&#20250;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00780v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#31038;&#20132;&#24773;&#22659;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#24037;&#20316;&#65292;&#23427;&#38656;&#35201;&#25972;&#21512;&#38754;&#37096;&#34920;&#24773;&#21644;&#24773;&#22659;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#33258;&#21160;&#24773;&#32490;&#35782;&#21035;&#30340;&#20256;&#32479;&#26041;&#27861;&#19987;&#27880;&#20110;&#33073;&#24773;&#22659;&#30340;&#20449;&#21495;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#24773;&#22659;&#22312;&#22609;&#36896;&#24773;&#32490;&#24863;&#30693;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#23545;&#26032;&#20852;&#30340;&#31038;&#20250;&#24773;&#22659;&#19979;&#30340;&#24773;&#32490;&#35782;&#21035;&#36825;&#19968;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#30340;&#24773;&#32490;&#24863;&#30693;&#29702;&#35770;&#26469;&#25351;&#23548;&#33258;&#21160;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#32447;&#32034;&#25972;&#21512;&#65288;BCI&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#38754;&#37096;&#34920;&#24773;&#30340;&#33073;&#24773;&#22659;&#20449;&#24687;&#21644;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#24773;&#22659;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#22234;&#24466;&#22256;&#22659;&#36825;&#19968;&#31038;&#20250;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#21508;&#31181;&#20027;&#35266;&#24773;&#24863;&#35780;&#20272;&#27979;&#35797;&#20013;&#23545;BCI&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00780v1 Announce Type: cross  Abstract: Emotion recognition in social situations is a complex task that requires integrating information from both facial expressions and the situational context. While traditional approaches to automatic emotion recognition have focused on decontextualized signals, recent research emphasizes the importance of context in shaping emotion perceptions. This paper contributes to the emerging field of context-based emotion recognition by leveraging psychological theories of human emotion perception to inform the design of automated methods. We propose an approach that combines emotion recognition methods with Bayesian Cue Integration (BCI) to integrate emotion inferences from decontextualized facial expressions and contextual knowledge inferred via Large-language Models. We test this approach in the context of interpreting facial expressions during a social task, the prisoner's dilemma. Our results provide clear support for BCI across a range of au
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;Frontend Diffusion&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#30340;&#33609;&#22270;&#21644;&#31471;&#21040;&#31471;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#25277;&#35937;&#24847;&#22270;&#21040;&#35814;&#32454;&#32593;&#39029;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2408.00778</link><description>&lt;p&gt;
&#21069;&#31471;&#25193;&#25955;&#65306;&#36890;&#36807;&#25277;&#35937;&#21040;&#35814;&#32454;&#20219;&#21153;&#36807;&#28193;&#25506;&#32034;&#24847;&#22270;&#39537;&#21160;&#30340;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Frontend Diffusion: Exploring Intent-Based User Interfaces through Abstract-to-Detailed Task Transitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;Frontend Diffusion&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#30340;&#33609;&#22270;&#21644;&#31471;&#21040;&#31471;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#25277;&#35937;&#24847;&#22270;&#21040;&#35814;&#32454;&#32593;&#39029;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00778v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00778v1 Announce Type: cross  Abstract: The emergence of Generative AI is catalyzing a paradigm shift in user interfaces from command-based to intent-based outcome specification. In this paper, we explore abstract-to-detailed task transitions in the context of frontend code generation as a step towards intent-based user interfaces, aiming to bridge the gap between abstract user intentions and concrete implementations. We introduce Frontend Diffusion, an end-to-end LLM-powered tool that generates high-quality websites from user sketches. The system employs a three-stage task transition process: sketching, writing, and coding. We demonstrate the potential of task transitions to reduce human intervention and communication costs in complex tasks. Our work also opens avenues for exploring similar approaches in other domains, potentially extending to more complex, interdependent tasks such as video production.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#8220;&#26356;&#39640;&#30340;&#24635;&#23383;&#25968;&#12289;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#8221;&#25776;&#20889;&#32773;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#23637;&#31034;&#20102;AI&#22312;&#29305;&#23450;&#35821;&#35328;&#27169;&#24335;&#21644;&#21477;&#23376;&#32467;&#26500;&#19978;&#30340;&#29420;&#29305;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00769</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32479;&#35745;&#20998;&#26512;&#35299;&#30721;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#20889;&#20316;&#65306;&#25581;&#31034;&#30340;&#35821;&#35328;&#24494;&#22937;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Decoding AI and Human Authorship: Nuances Revealed Through NLP and Statistical Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#8220;&#26356;&#39640;&#30340;&#24635;&#23383;&#25968;&#12289;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#8221;&#25776;&#20889;&#32773;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#23637;&#31034;&#20102;AI&#22312;&#29305;&#23450;&#35821;&#35328;&#27169;&#24335;&#21644;&#21477;&#23376;&#32467;&#26500;&#19978;&#30340;&#29420;&#29305;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00769v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#30001;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#65292;&#26088;&#22312;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#22914;&#20309;&#34920;&#36798;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25968;&#25454;&#32479;&#35745;&#20998;&#26512;&#65292;&#30740;&#31350;&#35843;&#26597;&#20102;&#25991;&#26412;&#20013;&#30340;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#12289;&#21019;&#36896;&#21147;&#21644;&#28508;&#22312;&#30340;&#20154;&#31867;&#21644;AI&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#30340;&#20559;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#23427;&#23545;&#29702;&#35299;AI&#30340;&#21019;&#36896;&#33021;&#21147;&#21644;&#20854;&#23545;&#25991;&#23398;&#12289;&#20132;&#27969;&#21644;&#31038;&#20250;&#26694;&#26550;&#30340;&#24433;&#21709;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36328;&#36234;&#21508;&#31181;&#20027;&#39064;&#21644;&#27969;&#27966;&#30340;500K&#31687;&#35770;&#25991;&#65292;&#36825;&#20123;&#35770;&#25991;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#25110;&#30001;&#20154;&#31867;&#25776;&#20889;&#65292;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#34920;&#36798;&#30340;&#28145;&#23618;&#27425;&#20197;&#21450;AI&#21644;&#20154;&#31867;&#25991;&#26412;&#21019;&#20316;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#20998;&#26512;&#21457;&#29616;&#65292;&#20154;&#31867;&#25776;&#20889;&#30340;&#35770;&#25991;&#36890;&#24120;&#22312;&#24635;&#21333;&#35789;&#25968;&#19978;&#27604;AI&#29983;&#25104;&#30340;&#35201;&#22810;&#65292;&#24182;&#19988;&#23637;&#29616;&#20102;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#65292;AI&#22312;&#26576;&#20123;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#24335;&#21644;&#21477;&#24335;&#20351;&#29992;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#30340;&#20889;&#20316;&#39118;&#26684;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#22312;&#25991;&#23398;&#21019;&#20316;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#23427;&#22312;&#26410;&#26469;&#21487;&#33021;&#23545;&#20154;&#31867;&#20889;&#20316;&#26041;&#24335;&#20135;&#29983;&#30340;&#21464;&#38761;&#12290;&#35770;&#25991;&#30340;&#32467;&#35770;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;AI&#20889;&#20316;&#29305;&#24615;&#21644;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00769v1 Announce Type: cross  Abstract: This research explores the nuanced differences in texts produced by AI and those written by humans, aiming to elucidate how language is expressed differently by AI and humans. Through comprehensive statistical data analysis, the study investigates various linguistic traits, patterns of creativity, and potential biases inherent in human-written and AI- generated texts. The significance of this research lies in its contribution to understanding AI's creative capabilities and its impact on literature, communication, and societal frameworks. By examining a meticulously curated dataset comprising 500K essays spanning diverse topics and genres, generated by LLMs, or written by humans, the study uncovers the deeper layers of linguistic expression and provides insights into the cognitive processes underlying both AI and human-driven textual compositions. The analysis revealed that human-authored essays tend to have a higher total word count on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20809;&#27969;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#20132;&#36890;&#20107;&#20214;&#30340;&#23454;&#26102;&#12289;&#39640;&#25928;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#20026;&#39550;&#39542;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#65292;&#35782;&#21035;&#21069;&#26041;&#36947;&#36335;&#28508;&#22312;&#30340;&#23041;&#32961;&#25110;&#31361;&#21457;&#20107;&#20214;&#65292;&#25552;&#39640;&#39550;&#39542;&#24773;&#20917;&#24863;&#30693;&#65292;&#24182;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00768</link><description>&lt;p&gt;
&#20351;&#29992;&#31354;&#38388; filling curves &#23545;&#27604;&#20809;&#27969;&#21644;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20809;&#27969;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#20132;&#36890;&#20107;&#20214;&#30340;&#23454;&#26102;&#12289;&#39640;&#25928;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#20026;&#39550;&#39542;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#65292;&#35782;&#21035;&#21069;&#26041;&#36947;&#36335;&#28508;&#22312;&#30340;&#23041;&#32961;&#25110;&#31361;&#21457;&#20107;&#20214;&#65292;&#25552;&#39640;&#39550;&#39542;&#24773;&#20917;&#24863;&#30693;&#65292;&#24182;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38754;&#20020;&#30528;&#22312;&#35270;&#39057;&#12289;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#31561;&#20132;&#36890;&#25968;&#25454;&#20013;&#35782;&#21035;&#20107;&#20214;&#21644;&#25910;&#38598;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#35780;&#20215;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#32467;&#26500;&#30340;&#12289;&#22810;&#27169;&#24577;&#30340;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#65292;&#19988;&#32570;&#20047;&#20803;&#25968;&#25454;&#25110;&#27880;&#37322;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20809;&#27969;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#39057;&#25968;&#25454;&#65288;&#26469;&#33258;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#65289;&#20013;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36710;&#36742;&#21608;&#22260;&#20809;&#27969;&#22330;&#30340;&#24178;&#25200;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#20107;&#20214;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#39550;&#39542;&#21592;&#30340;&#35270;&#32447;&#65292;&#20197;&#39044;&#27979;&#28508;&#22312;&#20107;&#20214;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#36755;&#36865;&#21040;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#19978;&#65292;&#20197;&#38477;&#20302;&#32500;&#24230;&#24182;&#23454;&#29616;&#35745;&#31639;&#19978;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#26631;&#20934;&#30340;&#23454;&#39564;&#65292;&#38024;&#23545;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#26816;&#27979;&#20107;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#39564;&#35777;&#20102;&#26412;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#35745;&#31639;&#25928;&#29575;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#31639;&#27861;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24635;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#31354;&#38388;&#35299;&#26500;&#30340;&#26102;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#27010;&#24565;&#25152;&#23637;&#31034;&#30340;&#31639;&#27861;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#20809;&#27969;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36798;&#21040;&#33391;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#26102;&#65292;&#26356;&#24555;&#22320;&#23436;&#25104;&#35745;&#31639;&#23494;&#38598;&#22411;&#25805;&#20316;&#12290;Our approach is designed to serve as an early warning or an auxiliary system that can provide real-time feedback to drivers or autonomous vehicles by identifying potential hazards or sudden events in the road ahead, improving situational awareness and potentially enhancing safety. In summary, this paper presents a novel framework for computationally efficient traffic event detection, which relies on optical flow, deep learning, and space-filling curves, offering a promising solution for the autonomous driving industry to achieve real-time event detection with minimal computational resources.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00768v1 Announce Type: new  Abstract: Gathering data and identifying events in various traffic situations remains an essential challenge for the systematic evaluation of a perception system's performance. Analyzing large-scale, typically unstructured, multi-modal, time series data obtained from video, radar, and LiDAR is computationally demanding, particularly when meta-information or annotations are missing. We compare Optical Flow (OF) and Deep Learning (DL) to feed computationally efficient event detection via space-filling curves on video data from a forward-facing, in-vehicle camera. Our first approach leverages unexpected disturbances in the OF field from vehicle surroundings; the second approach is a DL model trained on human visual attention to predict a driver's gaze to spot potential event locations. We feed these results to a space-filling curve to reduce dimensionality and achieve computationally efficient event retrieval. We systematically evaluate our concept b
&lt;/p&gt;</description></item><item><title>SentenceVAE&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#25913;&#20026;&#30001;&#21477;&#23376;&#36880;&#20010;&#22788;&#29702;&#30340;&#31574;&#30053;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00655</link><description>&lt;p&gt;
SentenceVAE&#65306;&#36890;&#36807;&#19979;&#19968;&#20010;&#21477;&#23376;&#39044;&#27979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26356;&#24555;&#12289;&#26356;&#38271;&#21644;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00655
&lt;/p&gt;
&lt;p&gt;
SentenceVAE&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#25913;&#20026;&#30001;&#21477;&#23376;&#36880;&#20010;&#22788;&#29702;&#30340;&#31574;&#30053;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00655v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20027;&#35201;&#20381;&#38752;&#19979;&#19968;&#20010;token&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#19979;&#19968;&#20010;&#21477;&#23376;&#39044;&#27979;&#65292;&#26088;&#22312;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21477;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SentenceVAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#21477;&#24335;&#32534;&#30721;&#22120;&#21644;&#21477;&#24335;&#35299;&#30721;&#22120;&#32452;&#25104;&#30340;tiny&#27169;&#22411;&#12290;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#21477;&#23376;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20195;&#24065;&#65292;&#32780;&#35299;&#30721;&#22120;&#37325;&#24314;&#36825;&#20010;&#21387;&#32553;&#30340;&#25968;&#25454;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#21407;&#22987;&#30340;&#21477;&#23376;&#24418;&#24335;&#12290;&#36890;&#36807;&#23558;SentenceVAE&#38598;&#25104;&#21040;LLM&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#23618;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21477;&#23376;&#32423;&#21035;&#30340;LLM&#65288;Sentence-level LLMs, SLLMs&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#25353;&#21477;&#23376;&#22788;&#29702;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;SentenceVAE&#36824;&#36890;&#36807;&#23558;&#25991;&#26412;&#20998;&#21106;&#25104;&#21477;&#23376;&#65292;&#20445;&#25345;&#20102;&#21407;&#22987;&#35821;&#20041;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#65292;&#22240;&#27492;&#25552;&#39640;&#20102;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00655v2 Announce Type: replace  Abstract: Contemporary large language models (LLMs) primarily rely on next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a Sentence Encoder and a Sentence Decoder. The encoder effectively condenses the information within a sentence into a singular token, while the decoder reconstructs this compressed data back into its original sentential form. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference approach, markedly accelerating inference speeds. SentenceVAE also maintains the integrity of the original semantic content by segmenting the text into sentences, thereby improving a
&lt;/p&gt;</description></item><item><title>V2INet &#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#35282;&#24230;&#20449;&#24687;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#21333;&#19968;&#35270;&#35282;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#26657;&#27491;&#21518;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.00374</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#34701;&#21512;&#30340; conformal &#36712;&#36857;&#39044;&#27979;&#22312;&#21512;&#20316;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00374
&lt;/p&gt;
&lt;p&gt;
V2INet &#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#35282;&#24230;&#20449;&#24687;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#21333;&#19968;&#35270;&#35282;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#26657;&#27491;&#21518;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00374v2 Announce Type: replace-cross &#25688;&#35201;: &#30446;&#21069;&#20851;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#38543;&#30528;&#36830;&#25509;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22914;&#36710;&#23545;&#36710;&#65288;V2V&#65289;&#21644;&#36710;&#23545;&#22522;&#30784;&#35774;&#26045;&#65288;V2I&#65289;&#36890;&#20449;&#65292;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#25910;&#38598;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21464;&#24471;&#21487;&#29992;&#12290;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#38598;&#25104;&#26377;&#28508;&#21147;&#20811;&#26381;&#20165;&#20174;&#21333;&#19968;&#35270;&#35282;&#25910;&#38598;&#25968;&#25454;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#65292;&#22914;&#36974;&#25377;&#21644;&#26377;&#38480;&#35270;&#37326;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; V2INet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#21333;&#19968;&#35270;&#22270;&#27169;&#22411;&#26469;&#24314;&#27169;&#22810;&#35270;&#22270;&#25968;&#25454;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#24471;&#21040;&#20102;&#26657;&#27491;&#65292;&#20197;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00374v2 Announce Type: replace-cross  Abstract: Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated 
&lt;/p&gt;</description></item><item><title>Gemma 2&#26159;Gemma&#31995;&#21015;&#20013;&#26032;&#22411;&#36731;&#37327;&#32423;&#24320;&#25918;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#26550;&#26500;&#21644;&#24212;&#29992;&#33976;&#39311;&#30693;&#35782;&#22521;&#35757;&#65292;&#22312;20&#20159;&#21644;90&#20159;&#21442;&#25968;&#35268;&#27169;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2408.00118</link><description>&lt;p&gt;
Gemma 2: &#25552;&#39640;&#23454;&#29992;&#35268;&#27169;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Gemma 2: Improving Open Language Models at a Practical Size
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00118
&lt;/p&gt;
&lt;p&gt;
Gemma 2&#26159;Gemma&#31995;&#21015;&#20013;&#26032;&#22411;&#36731;&#37327;&#32423;&#24320;&#25918;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#26550;&#26500;&#21644;&#24212;&#29992;&#33976;&#39311;&#30693;&#35782;&#22521;&#35757;&#65292;&#22312;20&#20159;&#21644;90&#20159;&#21442;&#25968;&#35268;&#27169;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00118v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;&#25688;&#35201;&#65306;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemma 2&#65292;Gemma&#23478;&#26063;&#30340;&#19968;&#20010;&#26032;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#35268;&#27169;&#20174;20&#20159;&#21040;270&#20159;&#19981;&#31561;&#12290;&#22312;&#26412;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#21521;Transformer&#26550;&#26500;&#24212;&#29992;&#20102;&#20960;&#39033;&#24050;&#30693;&#30340;&#25216;&#26415;&#25913;&#36827;&#65292;&#22914;&#24067;&#38647;&#29305;&#21513;&#31561;&#20154;&#65288;2020a&#65289;&#25552;&#20986;&#30340;&#26412;&#22320;-&#20840;&#23616;&#27880;&#24847;&#21147;&#20132;&#21449;&#21644;&#22467;&#36763;&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#30340;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#33976;&#39311;&#30693;&#35782;&#65288;Hinton et al.&#65292;2015&#65289;&#32780;&#19981;&#26159;&#25509;&#19979;&#26469;&#39044;&#27979;&#30340;&#26041;&#24335;&#35757;&#32451;&#20102;2&#20159;&#21644;90&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#27169;&#22411;&#22312;&#23427;&#20204;&#30340;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#20026;&#27604;&#23427;&#20204;&#22823;2-3&#20493;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#31454;&#20105;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#21457;&#24067;&#32473;&#20102;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00118v2 Announce Type: replace-cross  Abstract: In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#32500;&#26694;&#26550;&#26469;&#20998;&#26512;&#26497;&#21491;&#32764;&#31038;&#20132;&#24179;&#21488;&#8220;&#35780;&#20998;&#32593;&#8221;&#19978;&#30340;&#29992;&#25143;&#31867;&#22411;&#21644;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#24179;&#21488;&#29992;&#25143;&#29305;&#24449;&#12289;&#27963;&#36291;&#24230;&#12289;&#35752;&#35770;&#20869;&#23481;&#21644;&#31038;&#21306;&#25104;&#21592;&#30340;&#34892;&#20026;</title><link>https://arxiv.org/abs/2407.21753</link><description>&lt;p&gt;
&#30740;&#31350;&#29992;&#25143;&#21407;&#22411;&#20197;&#21450;&#22312;&#35780;&#20998;&#32593;&#19978;&#30340;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Characterizing User Archetypes and Discussions on Scored.co
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#32500;&#26694;&#26550;&#26469;&#20998;&#26512;&#26497;&#21491;&#32764;&#31038;&#20132;&#24179;&#21488;&#8220;&#35780;&#20998;&#32593;&#8221;&#19978;&#30340;&#29992;&#25143;&#31867;&#22411;&#21644;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#24179;&#21488;&#29992;&#25143;&#29305;&#24449;&#12289;&#27963;&#36291;&#24230;&#12289;&#35752;&#35770;&#20869;&#23481;&#21644;&#31038;&#21306;&#25104;&#21592;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21753v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#26222;&#21450;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#20010;&#20154;&#20114;&#21160;&#12289;&#32452;&#32455;&#21644;&#20998;&#20139;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#20114;&#21160;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#24613;&#21095;&#22686;&#21152;&#65292;&#21516;&#26102;&#23545;&#19968;&#20123;&#36793;&#32536;&#31038;&#20132;&#24179;&#21488;&#30340;&#30740;&#31350;&#21364;&#23569;&#20043;&#21448;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20851;&#27880;&#24230;&#19981;&#39640;&#30340;&#26497;&#21491;&#32764;&#24179;&#21488;&#8220;&#35780;&#20998;&#32593;&#8221;&#19978;&#30740;&#31350;&#33410;&#28857;&#21644;&#36229;&#36793;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25972;&#21512;&#20102;&#23545;&#26356;&#39640;&#38454;&#20114;&#21160;&#30340;&#21487;&#33021;&#24615;&#30740;&#31350;&#65292;&#24471;&#30410;&#20110;&#36229;&#32593;&#32476;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#33410;&#28857;&#29305;&#24449;&#65292;&#22914;&#29992;&#25143;&#27963;&#36291;&#24230;&#12289;&#24773;&#24863;&#21644;&#27602;&#24615;&#65292;&#26088;&#22312;&#23450;&#20041;&#19981;&#21516;&#31867;&#22411;&#30340;&#29992;&#25143;&#21407;&#22411;&#24182;&#20102;&#35299;&#20854;&#22312;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#21033;&#29992;&#35780;&#20998;&#32593;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#21407;&#22411;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#21160;&#21450;&#20854;&#24433;&#21709;&#21147;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#29305;&#24449;&#12289;&#27963;&#36291;&#24230;&#12289;&#35752;&#35770;&#20869;&#23481;&#21644;&#31038;&#21306;&#25104;&#21592;&#30340;&#34892;&#20026;&#65292;&#20026;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#20102;&#28145;&#21051;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21753v1 Announce Type: cross  Abstract: In recent years, the proliferation of social platforms has drastically transformed the way individuals interact, organize, and share information. In this scenario, we experience an unprecedented increase in the scale and complexity of interactions and, at the same time, little to no research about some fringe social platforms. In this paper, we present a multi-dimensional framework for characterizing nodes and hyperedges in social hypernetworks, with a focus on the understudied alt-right platform Scored.co. Our approach integrates the possibility of studying higher-order interactions, thanks to the hypernetwork representation, and various node features such as user activity, sentiment, and toxicity, with the aim to define distinct user archetypes and understand their roles within the network. Utilizing a comprehensive dataset from Scored.co, we analyze the dynamics of these archetypes over time and explore their interactions and influe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21152;&#25343;&#22823;&#38750;&#33829;&#21033;&#24615;&#34701;&#20837;&#39046;&#22495;&#38754;&#20020;&#30340;&#26032;&#26469;&#32773;&#38754;&#20020;&#30340;&#29983;&#25104;AI&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24314;&#35758;&#21152;&#24378;&#30456;&#20851;&#30740;&#31350;&#19982;&#24320;&#21457;&#20197;&#20419;&#36827;AI&#25945;&#32946;&#21644;&#23450;&#21046;&#21270;LLM&#25216;&#26415;&#19982;&#26381;&#21153;&#25552;&#20379;&#32773;&#30456;&#24212;&#22320;&#23545;&#25509;&#12290;</title><link>https://arxiv.org/abs/2407.20240</link><description>&lt;p&gt;
&#21152;&#25343;&#22823;&#38750;&#33829;&#21033;&#24615;&#34701;&#20837;&#39046;&#22495;&#38754;&#20020;&#30340;&#26032;&#26469;&#32773;&#38754;&#20020;&#30340;&#25972;&#21512;&#20316;&#29992;&#24191;&#20041;&#29992;&#36884;LLM&#30340;&#31038;&#20250;&#21644;&#20262;&#29702;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21152;&#25343;&#22823;&#38750;&#33829;&#21033;&#24615;&#34701;&#20837;&#39046;&#22495;&#38754;&#20020;&#30340;&#26032;&#26469;&#32773;&#38754;&#20020;&#30340;&#29983;&#25104;AI&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24314;&#35758;&#21152;&#24378;&#30456;&#20851;&#30740;&#31350;&#19982;&#24320;&#21457;&#20197;&#20419;&#36827;AI&#25945;&#32946;&#21644;&#23450;&#21046;&#21270;LLM&#25216;&#26415;&#19982;&#26381;&#21153;&#25552;&#20379;&#32773;&#30456;&#24212;&#22320;&#23545;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20240v2  &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25563;&#20132;&#21449;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20240v2 Announce Type: replace-cross  Abstract: The non-profit settlement sector in Canada supports newcomers in achieving successful integration. This sector faces increasing operational pressures amidst rising immigration targets, which highlights a need for enhanced efficiency and innovation, potentially through reliable AI solutions. The ad-hoc use of general-purpose generative AI, such as ChatGPT, might become a common practice among newcomers and service providers to address this need. However, these tools are not tailored for the settlement domain and can have detrimental implications for immigrants and refugees. We explore the risks that these tools might pose on newcomers to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25512;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#65292;&#20943;&#23569;&#26080;&#24110;&#21161;&#25110;&#35823;&#23548;&#24615;&#21709;&#24212;&#30340;&#20135;&#29983;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2407.19813</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064; &#65306; &#12298;&#36890;&#36807;&#33258;&#25105;&#25512;&#29702;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Improving Retrieval Augmented Language Model with Self-Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25512;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#65292;&#20943;&#23569;&#26080;&#24110;&#21161;&#25110;&#35823;&#23548;&#24615;&#21709;&#24212;&#30340;&#20135;&#29983;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19813v2 Announce Type: replace-cross  Abstract: The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#33258;&#35780;&#20272;&#31995;&#32479;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#31185;&#23398;&#23478;&#20204;&#23547;&#27714;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.19631</link><description>&lt;p&gt;
"&#19968;&#20010;&#22909;&#30340;&#26426;&#22120;&#20154;&#24635;&#26159;&#30693;&#36947;&#20854;&#23616;&#38480;&#24615;": &#36890;&#36807;&#22240;&#23376;&#21270;&#30340;&#33258;&#25105;&#33258;&#20449;&#37327;&#24230;&#35780;&#20272;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
"A Good Bot Always Knows Its Limitations": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19631
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#33258;&#35780;&#20272;&#31995;&#32479;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#31185;&#23398;&#23478;&#20204;&#23547;&#27714;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19631v2 &#23459;&#24067;&#31867;&#22411;: &#26367;&#25442;&#36328;&#26639;&#25688;&#35201;: &#26234;&#33021;&#26426;&#22120;&#22914;&#20309;&#35780;&#20272;&#20854;&#22312;&#23436;&#25104;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65311;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#31639;&#27861;&#24615; reasoning &#30340;&#33258;&#20027;&#31995;&#32479;&#26469;&#35828;&#26159;&#28966;&#28857;&#12290;&#22312;&#36825;&#37324;&#65292;&#25552;&#20986;&#26234;&#33021;&#26426;&#22120;&#30340;&#33258;&#25105;&#33258;&#20449;&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#23545;&#19990;&#30028;&#30340;&#29366;&#24577;&#12289;&#33258;&#24049;&#30340;&#30693;&#35782;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#30340;&#33258;&#25105;&#35780;&#20272;&#65292;&#26159;&#19968;&#31181;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#26377;&#29992;&#33021;&#21147;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#31216;&#20026;&#22240;&#23376;&#21270;&#30340;&#33258;&#25105;&#33258;&#20449;&#65288;Factorized Machine Self-confidence, FaMSeC&#65289;&#65292;&#20026;&#31639;&#27861;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#12289;&#24037;&#31243;&#23548;&#21521;&#30340;&#25551;&#36848;&#65292;&#21253;&#25324;&#65306;&#32467;&#26524;&#35780;&#20272;&#12289;&#27714;&#35299;&#22120;&#36136;&#37327;&#12289;&#27169;&#22411;&#36136;&#37327;&#12289;&#23545;&#40784;&#36136;&#37327;&#21644;&#36807;&#21435;&#30340;&#32463;&#39564;&#12290;&#22312;FaMSeC&#20013;&#65292;&#33258;&#25105;&#33258;&#20449;&#25351;&#26631;&#26159;&#20174;&#23618;&#27425;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#19981;&#20165;&#33021;&#22815;&#21578;&#30693;&#31995;&#32479;&#20869;&#37096;&#30340;&#30693;&#35782;&#21644;&#33021;&#21147;&#65292;&#20063;&#33021;&#22312;&#36328;&#31995;&#32479;&#20043;&#38388;&#36827;&#34892;&#33258;&#25105;&#27807;&#36890;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#30340;&#27807;&#36890;&#21644;&#25945;&#32946;&#65292;&#24182;&#23545;&#20110;&#35774;&#35745;&#21644;&#26500;&#24314;&#26356;&#21152;&#21487;&#20449;&#21644;&#21487;&#38752;&#30340;&#33258;&#20027;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19631v2 Announce Type: replace-cross  Abstract: How can intelligent machines assess their competencies in completing tasks? This question has come into focus for autonomous systems that algorithmically reason and make decisions under uncertainty. It is argued here that machine self-confidence - a form of meta-reasoning based on self-assessments of an agent's knowledge about the state of the world and itself, as well as its ability to reason about and execute tasks - leads to many eminently computable and useful competency indicators for such agents. This paper presents a culmination of work on this concept in the form of a computational framework called Factorized Machine Self-confidence (FaMSeC), which provides a holistic engineering-focused description of factors driving an algorithmic decision-making process, including: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self confidence indicators are derived from hierarch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;LLMs&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22840;&#22823;&#12290;&#34429;&#28982;LLMs&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#23545;&#35821;&#35328;&#30340;&#29702;&#35299;&#33021;&#21147;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#27979;&#35797;&#12290;Abstract&#20013;&#30340;&#25805;&#20316;&#34920;&#26126;&#65292;&#35780;&#20272;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#38656;&#35201;&#36776;&#35782;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#24182;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2407.19630</link><description>&lt;p&gt;
LLMs' Understanding of Natural Language Revealed
&lt;/p&gt;
&lt;p&gt;
LLMs' Understanding of Natural Language Revealed
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;LLMs&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22840;&#22823;&#12290;&#34429;&#28982;LLMs&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#23545;&#35821;&#35328;&#30340;&#29702;&#35299;&#33021;&#21147;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#27979;&#35797;&#12290;Abstract&#20013;&#30340;&#25805;&#20316;&#34920;&#26126;&#65292;&#35780;&#20272;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#38656;&#35201;&#36776;&#35782;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#24182;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19630v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#19968;&#39033;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#39537;&#21160;&#20498;&#32622;&#24037;&#31243;&#23454;&#39564;&#25104;&#26524;&#65292;&#26088;&#22312;&#20174;&#24213;&#37096;&#21521;&#19978;&#23545;&#35821;&#35328;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#39537;&#21160;&#21453;&#21521;&#24037;&#31243;&#12290;&#23613;&#31649;LLMs&#22312;&#35768;&#22810;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#25928;&#29992;&#65292;&#20294;&#22823;&#37327;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#25191;&#34892;&#38656;&#35201;&#23545;&#31526;&#21495;&#21464;&#37327;&#36827;&#34892;&#37327;&#21270;&#21644;&#25805;&#20316;&#30340;&#20219;&#21153;&#26041;&#38754;&#26159;&#26080;&#33021;&#20026;&#21147;&#30340;&#65292;&#20363;&#22914;&#35745;&#21010;&#21644;&#38382;&#39064;&#35299;&#20915;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#26723;&#20013;&#23558;&#37325;&#28857;&#27979;&#35797;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#36825;&#26159;&#23427;&#20204;&#30340;&#19987;&#38271;&#12290;&#27491;&#22914;&#25105;&#20204;&#23558;&#35201;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#24050;&#32463;&#34987;&#24191;&#27867;&#22320;&#22840;&#22823;&#20102;&#12290;&#34429;&#28982;LLMs&#24050;&#32463;&#34987;&#35777;&#26126;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#36830;&#36143;&#35821;&#35328;&#65288;&#22240;&#20026;&#36825;&#23601;&#26159;&#23427;&#20204;&#30340;&#35774;&#35745;&#26041;&#24335;&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#23578;&#26410;&#34987;&#27491;&#30830;&#27979;&#35797;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#36890;&#36807;&#25191;&#34892;&#19968;&#20010;&#25805;&#20316;&#26469;&#27979;&#35797;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#36825;&#20010;&#25805;&#20316;&#38656;&#35201;&#33021;&#22815;&#36776;&#35782;&#21644;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#21516;&#26102;&#20063;&#33021;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19630v2 Announce Type: replace  Abstract: Large language models (LLMs) are the result of a massive experiment in bottom-up, data-driven reverse engineering of language at scale. Despite their utility in a number of downstream NLP tasks, ample research has shown that LLMs are incapable of performing reasoning in tasks that require quantification over and the manipulation of symbolic variables (e.g., planning and problem solving); see for example [25][26]. In this document, however, we will focus on testing LLMs for their language understanding capabilities, their supposed forte. As we will show here, the language understanding capabilities of LLMs have been widely exaggerated. While LLMs have proven to generate human-like coherent language (since that's how they were designed), their language understanding capabilities have not been properly tested. In particular, we believe that the language understanding capabilities of LLMs should be tested by performing an operation that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#30431;AI&#27861;&#26696;&#21644;NIST&#26694;&#26550;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.17374</link><description>&lt;p&gt;
&#19982;AI&#20174;&#19994;&#32773;&#21644;AI&#21512;&#35268;&#19987;&#23478;&#20849;&#21516;&#35774;&#35745;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#30431;AI&#27861;&#26696;&#21644;NIST&#26694;&#26550;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#30417;&#31649;&#26085;&#30410;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#20844;&#21496;&#32780;&#35328;&#65292;&#36827;&#34892;&#24433;&#21709;&#35780;&#20272;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#25253;&#21578;&#25991;&#26723;&#20854;&#21512;&#35268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25253;&#21578;&#24448;&#24448;&#32570;&#20047;&#23545;&#27861;&#35268;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#19988;&#36890;&#24120;&#21482;&#20851;&#27880;AI&#31995;&#32479;&#30456;&#20851;&#30340;&#38544;&#31169;&#26041;&#38754;&#65292;&#32780;&#24573;&#30053;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#31995;&#32479;&#22320;&#35774;&#35745;&#24182;&#35780;&#20272;&#36825;&#20123;&#25253;&#21578;&#65292;&#21516;&#26102;&#32771;&#34385;AI&#20174;&#19994;&#32773;&#21644;AI&#21512;&#35268;&#19987;&#23478;&#30340;&#24847;&#35265;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#19982;14&#20301;AI&#20174;&#19994;&#32773;&#21644;6&#20301;AI&#21512;&#35268;&#19987;&#23478;&#32463;&#36807;&#21453;&#22797;&#30340;&#21327;&#21516;&#35774;&#35745;&#21644;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#30431;AI&#27861;&#26696;&#12289;NIST&#30340;AI&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#21644;ISO 42001 AI&#31649;&#29702;&#31995;&#32479;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#19968;&#23478;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#30340;&#22522;&#20110;AI&#30340;&#20250;&#35758;&#20276;&#20387;&#29983;&#20135;&#19968;&#20010;&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#65292;&#35780;&#20272;&#20102;&#35813;&#27169;&#26495;&#30340;&#25928;&#26524;&#12290;&#22312;&#35813;&#20844;&#21496;&#30340;8&#20301;AI&#20174;&#19994;&#32773;&#21644;&#20855;&#26377;&#30456;&#21516;&#32972;&#26223;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#25253;&#21578;&#30340;&#32467;&#26500;&#12289;&#20869;&#23481;&#21644;&#23454;&#29992;&#24615;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#29992;&#25143;&#23545;&#25253;&#21578;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#32473;&#20104;&#20102;&#31215;&#26497;&#35780;&#20215;&#65292;&#24182;&#19988;&#35748;&#20026;&#25253;&#21578;&#30340;&#23454;&#29992;&#24615;&#36739;&#39640;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#23558;&#26469;&#30340;&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21453;&#39304;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;AI&#25191;&#27861;&#32773;&#21644;&#21512;&#35268;&#19987;&#23478;&#30340;&#21512;&#20316;&#23545;&#20110;&#30830;&#20445;&#25253;&#21578;&#30340;&#23436;&#25972;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17374v2 Announce Type: replace-cross  Abstract: In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#8221;&#65288;CCVA-FL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#35299;&#20915;&#21307;&#30103;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861; CCVA-FL&#65306;&#24212;&#29992;&#20110;&#22522;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#8221;&#65288;CCVA-FL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#35299;&#20915;&#21307;&#30103;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v3 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;LoRA&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#26356;&#26032;&#20302;&#31209;&#30697;&#38453;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#25552;&#21319;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2407.11046</link><description>&lt;p&gt;
LoRA &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;&#24615;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on LoRA of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11046
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;LoRA&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#26356;&#26032;&#20302;&#31209;&#30697;&#38453;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#25552;&#21319;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11046v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449; &#25688;&#35201;: &#20302;&#31209;&#36866;&#24212;&#24615;(LoRA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36890;&#36807;&#26356;&#26032;&#23494;&#38598;&#22411;&#31070;&#32463;&#32593;&#32476;&#23618;&#19982;&#21487;&#25554;&#25300;&#30340;&#20302;&#31209;&#30697;&#38453;&#26469;&#36827;&#34892;&#30340;&#26368;&#26377;&#25928;&#30340;&#21442;&#25968;&#31934;&#31616;&#24494;&#35843;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#36328;&#20219;&#21153;&#27867;&#21270;&#20197;&#21450;&#22312;&#30830;&#20445;&#38544;&#31169;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;LoRA&#26368;&#36817;&#21463;&#21040;&#20102;&#22823;&#37327;&#30340;&#20851;&#27880;&#65292;&#19982;&#23427;&#30456;&#20851;&#30340;&#24037;&#20316;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#23545;LoRA&#30340;&#24403;&#21069;&#36827;&#23637;&#36827;&#34892;&#20840;&#38754;&#27010;&#36848;&#21464;&#24471;&#23588;&#20026;&#24517;&#35201;&#12290;&#26412;&#25991;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#23545;LoRA&#30340;&#21457;&#23637;&#36827;&#34892;&#20998;&#31867;&#21644;&#32508;&#36848;&#65306;(1) &#19979;&#28216;&#36866;&#24212;&#25913;&#36827;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#25552;&#39640;&#20102;LoRA&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;(2) &#36328;&#20219;&#21153;&#27867;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#28151;&#21512;&#20102;&#22810;&#20010;LoRA&#25554;&#20214;&#20197;&#23454;&#29616;&#36328;&#20219;&#21153;&#27867;&#21270;&#65307;(3) &#25928;&#29575;&#25913;&#36827;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;LoRA&#30340;&#35745;&#31639;&#25928;&#29575;&#65307;(4) &#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;LoRA&#65307;(5) &#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#31687;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;LoRA&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11046v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation~(LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this
&lt;/p&gt;</description></item><item><title>D-Rax &#26159;&#19968;&#20010;&#26032;&#30340;&#22495;&#29305;&#23450;&#25918;&#23556;&#23398;&#21161;&#25163;&#65292;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#27169;&#22411;&#39044;&#27979;&#65292;&#26356;&#31934;&#30830;&#22320;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#22788;&#29702;&#33016;&#37096;X&#20809;&#29255;&#12290;</title><link>https://arxiv.org/abs/2407.02604</link><description>&lt;p&gt;
D-Rax: &#22495;&#29305;&#23450;&#25918;&#23556;&#23398;&#21161;&#25163;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#27169;&#22411;&#39044;&#27979;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.02604
&lt;/p&gt;
&lt;p&gt;
D-Rax &#26159;&#19968;&#20010;&#26032;&#30340;&#22495;&#29305;&#23450;&#25918;&#23556;&#23398;&#21161;&#25163;&#65292;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#27169;&#22411;&#39044;&#27979;&#65292;&#26356;&#31934;&#30830;&#22320;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#22788;&#29702;&#33016;&#37096;X&#20809;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02604v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20174;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#21040;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#31243;&#24230;&#65292;&#36825;&#22312;&#21307;&#23398;&#22270;&#20687;&#21644;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;VLMs&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#39046;&#22495;&#25552;&#20379;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#24110;&#21161;&#35299;&#20915;&#21307;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#35813;&#39046;&#22495;&#26412;&#36523;&#23384;&#22312;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#30340;&#39044;&#27979;&#21644;&#24314;&#35758;&#31561;&#65292;VLMs&#22312;&#20020;&#24202;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#21463;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#33016;&#37096;X&#20809;&#29255;&#65288;CXR&#65289;&#20026;&#37325;&#28857;&#30340;&#22495;&#29305;&#23450;&#12289;&#20250;&#35805;&#24335;&#25918;&#23556;&#23398;&#21161;&#25163;&#8212;&#8212;D-Rax&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#19987;&#23478;&#30340;&#35265;&#35299;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#30340;&#20808;&#36827;&#24615;&#65292;D-Rax&#33021;&#22815;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35299;&#35835;CXR&#22270;&#20687;&#65292;&#20174;&#32780;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#25552;&#20379;&#19968;&#26465;&#20840;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#24182;&#26377;&#21161;&#20110;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#31934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#30446;&#21069;&#25105;&#20204;&#27491;&#22312;&#36880;&#27493;&#23436;&#21892;D-Rax&#30340;&#21151;&#33021;&#65292;&#24182;&#35745;&#21010;&#36827;&#34892;&#26356;&#22810;&#20020;&#24202;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20854;&#23545;&#21307;&#30103;&#23454;&#36341;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02604v2 Announce Type: replace  Abstract: Large vision language models (VLMs) have progressed incredibly from research to applicability for general-purpose use cases. LLaVA-Med, a pioneering large language and vision assistant for biomedicine, can perform multi-modal biomedical image and data analysis to provide a natural language interface for radiologists. While it is highly generalizable and works with multi-modal data, it is currently limited by well-known challenges that exist in the large language model space. Hallucinations and imprecision in responses can lead to misdiagnosis which currently hinder the clinical adaptability of VLMs. To create precise, user-friendly models in healthcare, we propose D-Rax -- a domain-specific, conversational, radiologic assistance tool that can be used to gain insights about a particular radiologic image. In this study, we enhance the conversational analysis of chest X-ray (CXR) images to support radiological reporting, offering compre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25972;&#21512;&#25193;&#25955;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39118;&#38505;&#31649;&#29702;&#21644;&#21327;&#21516;&#21160;&#20316;&#24314;&#27169;&#25552;&#39640;&#20102;&#22810;&#20010;&#20195;&#29702;&#34892;&#21160;&#30340;&#23433;&#20840;&#24615;&#12290;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23601;&#22320;&#12289;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#36712;&#36857;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DSRL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#33021;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2407.00741</link><description>&lt;p&gt;
&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#31163;&#32447;&#22810; agent &#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Offline Multi-agent Reinforcement Learning with Safety Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25972;&#21512;&#25193;&#25955;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39118;&#38505;&#31649;&#29702;&#21644;&#21327;&#21516;&#21160;&#20316;&#24314;&#27169;&#25552;&#39640;&#20102;&#22810;&#20010;&#20195;&#29702;&#34892;&#21160;&#30340;&#23433;&#20840;&#24615;&#12290;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23601;&#22320;&#12289;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#36712;&#36857;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DSRL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#33021;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00741v5 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#22810; agent &#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#65292;&#20854;&#24212;&#29992;&#24050;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#23433;&#20840;&#39118;&#38505;&#30340;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20391;&#37325;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040; MARL &#33539;&#24335;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#39118;&#38505;&#32531;&#35299;&#21644;&#21327;&#21516;&#21160;&#20316;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#34892;&#21160;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23601;&#22320;&#12289;&#20998;&#24067;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#26550;&#26500;&#65292;&#24182;&#22686;&#21152;&#20102;&#39044;&#27979;&#36712;&#36857;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#31639;&#27861;&#20197;&#30830;&#20445;&#25805;&#20316;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23545;DSRL&#22522;&#20934;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#20005;&#26684;&#36981;&#23432;&#20005;&#26684;&#30340;&#23433;&#20840;&#38480;&#21046;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#21644;&#34892;&#20026;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#38598;&#20307;&#24615;&#33021;&#65292;&#36824;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00741v5 Announce Type: replace  Abstract: In recent advancements in Multi-agent Reinforcement Learning (MARL), its application has extended to various safety-critical scenarios. However, most methods focus on online learning, which presents substantial risks when deployed in real-world settings. Addressing this challenge, we introduce an innovative framework integrating diffusion models within the MARL paradigm. This approach notably enhances the safety of actions taken by multiple agents through risk mitigation while modeling coordinated action. Our framework is grounded in the Centralized Training with Decentralized Execution (CTDE) architecture, augmented by a Diffusion Model for prediction trajectory generation. Additionally, we incorporate a specialized algorithm to further ensure operational safety. We evaluate our model against baselines on the DSRL benchmark. Experiment results demonstrate that our model not only adheres to stringent safety constraints but also achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;HPC&#38598;&#32676;&#19978;&#36816;&#34892;&#23454;&#26102;AI&#24212;&#29992;&#30340;&#25176;&#31649;&#21644;&#20113;VM&#26381;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#36991;&#20813;&#20102;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#32463;&#29992;&#25143;&#21516;&#24847;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2407.00110</link><description>&lt;p&gt;
Chat AI: &#19968;&#31181;&#26080;&#32541;&#25903;&#25345;Slurm&#30340;HPC&#26381;&#21153;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;HPC&#38598;&#32676;&#19978;&#36816;&#34892;&#23454;&#26102;AI&#24212;&#29992;&#30340;&#25176;&#31649;&#21644;&#20113;VM&#26381;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#36991;&#20813;&#20102;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#32463;&#29992;&#25143;&#21516;&#24847;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00110v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#25991;&#25688;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#20419;&#20351;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#12289;&#23433;&#20840;&#12289;&#31169;&#23494;&#30340;&#25176;&#31649;&#22522;&#30784;&#35774;&#26045;&#65292;&#36825;&#19981;&#20165;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#36816;&#34892;&#24320;&#28304;&#25110;&#33258;&#23450;&#20041;&#30340;&#31934;&#35843;LLM&#65292;&#32780;&#19988;&#30830;&#20445;&#29992;&#25143;&#30340;&#25968;&#25454;&#19981;&#20250;&#22312;&#27809;&#26377;&#29992;&#25143;&#21516;&#24847;&#30340;&#24773;&#20917;&#19979;&#34987;&#23384;&#20648;&#12290;&#34429;&#28982;&#37197;&#22791;&#20102;&#20808;&#36827;GPU&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#31995;&#32479;&#38750;&#24120;&#36866;&#21512;&#35757;&#32451;LLM&#65292;&#20294;&#23427;&#20204;&#30340;&#25209;&#27425;&#35843;&#24230;&#33539;&#24335;&#24182;&#19981;&#26159;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;AI&#24212;&#29992;&#30340;&#25176;&#31649;&#32780;&#35774;&#35745;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20113;&#31995;&#32479;&#38750;&#24120;&#36866;&#21512;&#25552;&#20379;Web&#26381;&#21153;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#35775;&#38382;HPC&#38598;&#32676;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#26114;&#36149;&#19988;&#31232;&#32570;&#30340;&#39640;&#31471;GPU&#65292;&#36825;&#20123;GPU&#23545;&#20110;&#33719;&#24471;&#26368;&#20339;&#25512;&#29702;&#24615;&#33021;&#26159;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#24182;&#22312;&#20113;VM&#19978;&#36816;&#34892;&#20102;&#19968;&#20010;Web&#26381;&#21153;&#65292;&#35813;&#26381;&#21153;&#20855;&#26377;&#23433;&#20840;&#35775;&#38382;&#22522;&#20110;HPC&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#21518;&#31471;&#30340;&#33021;&#21147;&#65292;&#35813;&#21518;&#31471;&#36816;&#34892;&#30528;&#22810;&#31181;LLM&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26550;&#26500;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#25968;&#25454;&#35775;&#38382;&#65292;&#29992;&#25143;&#25968;&#25454;&#19981;&#20250;&#34987;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00110v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of HPC clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of LLM models on HPC syste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#23545;&#39640;&#12289;&#20302;&#22826;&#38451;&#33021;&#27963;&#21160;&#27700;&#24179;&#36827;&#34892;&#24179;&#34913;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;SET&#25968;&#25454;&#38598;&#19978;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#30340;&#20934;&#30830;&#39044;&#25253;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#23792;&#26399;&#38388;&#12290;</title><link>https://arxiv.org/abs/2406.15847</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#21464;&#37327;&#21464;&#25442;&#22120;&#22686;&#24378;&#22826;&#38451;&#33021;&#39550;&#39542;&#21592;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Enhancing Solar Driver Forecasting with Multivariate Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.15847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#23545;&#39640;&#12289;&#20302;&#22826;&#38451;&#33021;&#27963;&#21160;&#27700;&#24179;&#36827;&#34892;&#24179;&#34913;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;SET&#25968;&#25454;&#38598;&#19978;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#30340;&#20934;&#30830;&#39044;&#25253;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#23792;&#26399;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.15847v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;PatchTST&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;F10.7&#12289;S10.7&#12289;M10.7&#21644;Y10.7&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#30340;&#39044;&#25253;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#22826;&#38451;&#33021;&#27963;&#21160;&#30340;&#20302;&#21644;&#39640;&#27700;&#24179;&#26377;&#24179;&#31561;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26681;&#25454;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#21382;&#21490;&#20998;&#24067;&#19982;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#21152;&#26435;&#26679;&#26412;&#12290;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#39044;&#25253;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;18&#22825;&#30340;&#22238;&#30475;&#31383;&#21475;&#65292;&#24182;&#33021;&#39044;&#27979;&#26410;&#26469;6&#22825;&#30340;&#20540;&#12290;&#24403;&#26412;&#27169;&#22411;&#22312;&#19982;Space Environment Technologies&#65288;SET&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#27169;&#22411;&#25552;&#20379;&#30340;&#39044;&#27979;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#26631;&#20934;&#22343;&#26041;&#35823;&#24046;&#37117;&#26356;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#22826;&#38451;&#33021;&#27963;&#21160;&#39640;&#23792;&#26399;&#65292;&#39044;&#27979;&#31934;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20174;Github&#30340;https://github.com/ARCLab-MIT/sw-driver-forecaster&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.15847v2 Announce Type: replace-cross  Abstract: In this work, we develop a comprehensive framework for F10.7, S10.7, M10.7, and Y10.7 solar driver forecasting with a time series Transformer (PatchTST). To ensure an equal representation of high and low levels of solar activity, we construct a custom loss function to weight samples based on the distance between the solar driver's historical distribution and the training set. The solar driver forecasting framework includes an 18-day lookback window and forecasts 6 days into the future. When benchmarked against the Space Environment Technologies (SET) dataset, our model consistently produces forecasts with a lower standard mean error in nearly all cases, with improved prediction accuracy during periods of high solar activity. All the code is available on Github https://github.com/ARCLab-MIT/sw-driver-forecaster.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#36825;&#26159;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#34394;&#25311;&#21161;&#25163;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#36825;&#20123;&#35760;&#24405;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;</title><link>https://arxiv.org/abs/2406.07867</link><description>&lt;p&gt;
&#29616;&#23454;&#23545;&#35805;&#65306;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.07867
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#36825;&#26159;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#34394;&#25311;&#21161;&#25163;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#36825;&#20123;&#35760;&#24405;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22788;&#29702;&#29992;&#25143;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#20316;&#20026;&#22238;&#24212;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#30340;&#34394;&#25311;&#21161;&#25163;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#65288;&#21363;&#22768;&#38899;&#21644;&#35270;&#35273;&#65289;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;MultiDialog&#21253;&#21547;&#20102;&#23545;&#35805;&#20249;&#20276;&#26681;&#25454;&#32473;&#23450;&#33050;&#26412;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#24182;&#36827;&#34892;&#24773;&#24863;&#26631;&#27880;&#30340;&#38899;&#39057;-&#35270;&#39057;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#26399;&#24453;&#36825;&#20123;&#35760;&#24405;&#23558;&#20026;&#22810;&#27169;&#24577;&#21512;&#25104;&#30740;&#31350;&#24320;&#36767;&#26032;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#32467;&#21512;&#20102;&#32463;&#36807;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#38899;&#39057;-&#35270;&#35273;&#21475;&#35821;&#23545;&#35805;&#39046;&#22495;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;-&#25991;&#26412;&#32852;&#21512;&#39044;&#35757;&#32451;&#32435;&#20837;&#27169;&#22411;&#20043;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.07867v2 Announce Type: replace  Abstract: In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20351;&#29305;&#23450;&#22320;&#21306;&#30340;&#22270;&#20687;&#34920;&#29616;&#19982;&#29616;&#23454;&#19990;&#30028;&#30456;&#31526;&#12290;</title><link>https://arxiv.org/abs/2406.04551</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#25913;&#36827;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20351;&#29305;&#23450;&#22320;&#21306;&#30340;&#22270;&#20687;&#34920;&#29616;&#19982;&#29616;&#23454;&#19990;&#30028;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04551v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04551v2 Announce Type: replace  Abstract: With the growing popularity of text-to-image generative models, there has been increasing focus on understanding their risks and biases. Recent work has found that state-of-the-art models struggle to depict everyday objects with the true diversity of the real world and have notable gaps between geographic regions. In this work, we aim to increase the diversity of generated images of common objects such that per-region variations are representative of the real world. We introduce an inference time intervention, contextualized Vendi Score Guidance (c-VSG), that guides the backwards steps of latent diffusion models to increase the diversity of a sample as compared to a "memory bank" of previously generated images while constraining the amount of variation within that of an exemplar set of real-world contextualizing images. We evaluate c-VSG with two geographically representative datasets and find that it substantially increases the dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RepCNN&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26032;&#26550;&#26500;&#21644;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2406.02652</link><description>&lt;p&gt;
RepCNN: &#24494;&#23567;&#27169;&#22411;&#20026;&#21796;&#37266;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RepCNN: Micro-sized, Mighty Models for Wakeword Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.02652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RepCNN&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26032;&#26550;&#26500;&#21644;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.02652v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#22987;&#32456;&#36816;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#26497;&#20302;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#24615;&#21442;&#25968;&#25968;&#37327;&#38480;&#21046;&#20102;&#27169;&#22411;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20256;&#32479;&#35757;&#32451;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#21442;&#25968;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21367;&#31215;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#23558;&#20854;&#35745;&#31639;&#37325;&#26032;&#26500;&#25104;&#20026;&#19968;&#31181;&#26356;&#22823;&#22411;&#19988;&#20998;&#25903;&#26356;&#22810;&#30340;&#26550;&#26500;&#26469;&#36827;&#34892;&#26356;&#22909;&#30340;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#36827;&#34892;&#25512;&#29702;&#26102;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#21333;&#20998;&#25903;&#24418;&#24335;&#65292;&#21442;&#25968;&#26356;&#23569;&#65292;&#20197;&#20415;&#20855;&#26377;&#26356;&#20302;&#30340;&#20869;&#23384;&#36275;&#36857;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22987;&#32456;&#22312;&#32447;&#21796;&#37266;&#35789;&#26816;&#27979;&#27169;&#22411;RepCNN&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;&#19982;&#20855;&#26377;&#30456;&#21516;&#36816;&#34892;&#26102;&#30340;&#21333;&#20998;&#25903;&#21367;&#31215;&#27169;&#22411;&#30456;&#27604;&#65292;RepCNN&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;43%&#12290;&#21516;&#26102;&#65292;RepCNN&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#22797;&#26434;&#26550;&#26500;&#22914;BC-ResNet&#30340;&#27700;&#24179;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.02652v2 Announce Type: replace-cross  Abstract: Always-on machine learning models require a very low memory and compute footprint. Their restricted parameter count limits the model's capacity to learn, and the effectiveness of the usual training algorithms to find the best parameters. Here we show that a small convolutional model can be better trained by first refactoring its computation into a larger redundant multi-branched architecture. Then, for inference, we algebraically re-parameterize the trained model into the single-branched form with fewer parameters for a lower memory footprint and compute cost. Using this technique, we show that our always-on wake-word detector model, RepCNN, provides a good trade-off between latency and accuracy during inference. RepCNN re-parameterized models are 43% more accurate than a uni-branch convolutional model while having the same runtime. RepCNN also meets the accuracy of complex architectures like BC-ResNet, while having 2x lesser p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#20985;&#25928;&#29992;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#30830;&#20445;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2405.19024</link><description>&lt;p&gt;
&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#36870;&#28216;&#25103;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.19024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#20985;&#25928;&#29992;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#30830;&#20445;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19024v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;&#65306;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20985;&#25928;&#29992;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;CURL&#65289;&#26159;&#26631;&#20934;RL&#30446;&#26631;&#30340;&#19968;&#33324;&#21270;&#65292;&#23427;&#20351;&#29992;&#20102;&#29366;&#24577;&#30340;&#21344;&#29992;&#24230;&#37327;&#20985;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#32447;&#24615;&#20989;&#25968;&#12290;CURL&#22240;&#20854;&#33021;&#22815;&#20195;&#34920;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#23454;&#20363;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;RL&#20363;&#22914;&#27169;&#20223;&#23398;&#20064;&#12289;&#32431;&#25506;&#32034;&#12289;&#21463;&#38480;MDP&#12289;&#31163;&#32447;RL&#12289;&#20154;&#36896;&#34892;&#20026;&#35268;&#33539;&#21270;RL&#31561;&#32780;&#26368;&#36817;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#23427;&#19987;&#27880;&#20110;&#24674;&#22797;&#19968;&#20010;&#26410;&#30693;&#30340;&#34892;&#20026;&#22870;&#21169;&#20989;&#25968;&#65292;&#23427;&#33021;&#22815;&#20026;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20851;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20854;&#20013;&#38382;&#39064;&#34987;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#21487;&#34892;&#30340;&#22870;&#21169;&#20989;&#25968;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CURL&#38382;&#39064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#23578;&#26410;&#34987;&#32771;&#34385;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#26631;&#20934;&#30340;IRL&#30740;&#31350;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#20985;&#25928;&#29992;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#30830;&#20445;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19024v3 Announce Type: replace-cross  Abstract: We consider inverse reinforcement learning problems with concave utilities. Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function. CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others. Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent. There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions. However, inverse RL for CURL problems has not been considered previously. In this paper we show that most of the standard IRL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20449;&#24687;&#20016;&#23500;&#25991;&#26412;&#35780;&#20272;&#26426;&#21046;&#30340;&#25928;&#21147;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25193;&#22823;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2405.15077</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Eliciting Informative Text Evaluations with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.15077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20449;&#24687;&#20016;&#23500;&#25991;&#26412;&#35780;&#20272;&#26426;&#21046;&#30340;&#25928;&#21147;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25193;&#22823;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#20197;&#26377;&#20445;&#35777;&#30340;&#20445;&#35777;&#28608;&#21169;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#30456;&#24403;&#31616;&#21333;&#30340;&#25253;&#21578;&#65292;&#22914;&#22810;&#39033;&#36873;&#25321;&#25110;&#26631;&#37327;&#25968;&#23383;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#25193;&#23637;&#21040;&#25991;&#26412;&#25253;&#21578;&#30340;&#22823;&#39046;&#22495;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#36825;&#22823;&#22823;&#25193;&#23637;&#20102;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#22240;&#20026;&#25991;&#26412;&#21453;&#39304;&#22312;&#22823;&#37327;&#21453;&#39304;&#28192;&#36947;&#20013;&#24456;&#24120;&#35265;&#65306;&#21516;&#34892;&#35780;&#23457;&#12289;&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#35780;&#20215;&#21644;&#31038;&#20250;&#23186;&#20307;&#19978;&#30340;&#35780;&#35770;&#12290;&#25105;&#20204;&#20171;&#32461;&#20004;&#31181;&#26426;&#21046;&#65292;&#21363;&#29983;&#25104;&#24615;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#65288;GPPM&#65289;&#21644;&#29983;&#25104;&#24615;&#27010;&#35201;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#65288;GSPPM&#65289;&#12290;&#36825;&#20123;&#26426;&#21046;&#21033;&#29992;LLM&#20316;&#20026;&#39044;&#27979;&#22120;&#65292;&#20174;&#19968;&#20010;&#20154;&#30340;&#25253;&#21578;&#20013;&#26144;&#23556;&#21040;&#23545;&#22905;&#30340;&#21516;&#20276;&#25253;&#21578;&#30340;&#39044;&#27979;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;LLM&#30340;&#39044;&#27979;&#36275;&#22815;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#21487;&#20197;&#28608;&#21169;&#39640;&#24378;&#24230;&#30340;&#21162;&#21147;&#24182;&#35828;&#23454;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.15077v3 Announce Type: replace-cross  Abstract: Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models. This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media.   We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32465;&#23450;&#22270;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#26088;&#22312;&#20026;&#22810;&#29289;&#29702;&#21644;&#22797;&#26434;&#22810;&#22495;&#29616;&#35937;&#20219;&#21153;&#25552;&#20379;&#22810;&#20449;&#24687;&#36755;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2405.13586</link><description>&lt;p&gt;
&#22810;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#32465;&#22270;&#27861;&#29992;&#20110;&#22810;&#21464;&#24322;&#24615;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Bond Graphs for multi-physics informed Neural Networks for multi-variate time series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.13586
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32465;&#23450;&#22270;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#26088;&#22312;&#20026;&#22810;&#29289;&#29702;&#21644;&#22797;&#26434;&#22810;&#22495;&#29616;&#35937;&#20219;&#21153;&#25552;&#20379;&#22810;&#20449;&#24687;&#36755;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13586v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#24341;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13586v2 Announce Type: replace-cross  Abstract: In the trend of hybrid Artificial Intelligence techniques, Physical-Informed Machine Learning has seen a growing interest. It operates mainly by imposing data, learning, or architecture bias with simulation data, Partial Differential Equations, or equivariance and invariance properties. While it has shown great success on tasks involving one physical domain, such as fluid dynamics, existing methods are not adapted to tasks with complex multi-physical and multi-domain phenomena. In addition, it is mainly formulated as an end-to-end learning scheme. To address these challenges, we propose to leverage Bond Graphs, a multi-physics modeling approach, together with Message Passing Graph Neural Networks. We propose a Neural Bond graph Encoder (NBgE) producing multi-physics-informed representations that can be fed into any task-specific model. It provides a unified way to integrate both data and architecture biases in deep learning. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#26032;&#38395;&#25991;&#31456;&#30340;&#23884;&#20837;&#29983;&#25104;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#21644;&#20027;&#39064;&#65292;&#24182;&#23545;&#23427;&#20204;&#19982;&#29305;&#23450;&#20107;&#20214;&#30340;&#21382;&#21490;&#32852;&#31995;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#23569;&#23186;&#20307;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2405.13071</link><description>&lt;p&gt;
&#26032;&#30340;&#22522;&#20110;&#20107;&#20214;&#23884;&#20837;&#26032;&#38395;&#25991;&#31456;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Method for News Article Event-Based Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.13071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#26032;&#38395;&#25991;&#31456;&#30340;&#23884;&#20837;&#29983;&#25104;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#21644;&#20027;&#39064;&#65292;&#24182;&#23545;&#23427;&#20204;&#19982;&#29305;&#23450;&#20107;&#20214;&#30340;&#21382;&#21490;&#32852;&#31995;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#23569;&#23186;&#20307;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13071v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#26032;&#38395;&#25991;&#31456;&#30340;&#23884;&#20837;&#26159;&#22810;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20363;&#22914;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#12289;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#21644;&#21046;&#20316;&#26032;&#38395;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26032;&#38395;&#23884;&#20837;&#26041;&#27861;&#24182;&#26410;&#20248;&#21270;&#20197;&#25429;&#25417;&#26032;&#38395;&#20107;&#20214;&#30340;&#38544;&#24615;&#19978;&#19979;&#25991;&#12290;&#22823;&#22810;&#25968;&#23884;&#20837;&#26041;&#27861;&#20381;&#36182;&#20110;&#20840;&#25991;&#26412;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#26102;&#38388;&#30456;&#20851;&#30340;&#23884;&#20837;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#21644;&#20027;&#39064;&#20197;&#21450;&#23427;&#20204;&#19982;&#29305;&#23450;&#20107;&#20214;&#30340;&#21382;&#21490;&#32852;&#31995;&#26469;&#20248;&#21270;&#26032;&#38395;&#23884;&#20837;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#24314;&#35758;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#32473;&#23450;&#30340;&#26032;&#38395;&#25991;&#31456;&#20013;&#22788;&#29702;&#21644;&#25552;&#21462;&#20107;&#20214;&#12289;&#23454;&#20307;&#21644;&#20027;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24403;&#21069;&#21644;&#21382;&#21490;&#25968;&#25454;&#19978;&#35757;&#32451;&#20998;&#26102;&#27573;&#30340;GloVe&#27169;&#22411;&#26469;&#20026;&#20027;&#39064;&#21644;&#23454;&#20307;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#26102;&#38388;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25340;&#25509;&#29983;&#25104;&#30340;&#26032;&#26032;&#38395;&#23884;&#20837;&#65306;&#24179;&#28369;&#20498;&#25968;&#39057;&#29575;&#65292;&#20197;&#21450;&#20027;&#39064;&#21644;&#23454;&#20307;&#30340;&#22522;&#20110;&#26102;&#38388;&#30340;&#20851;&#38190;&#35789;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#26032;&#38395;&#20027;&#39064;&#35782;&#21035;&#21644;&#30456;&#20284;&#26032;&#38395;&#25991;&#31456;&#25628;&#32034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#23186;&#20307;&#29615;&#22659;&#19979;&#20943;&#23569;&#23186;&#20307;&#20559;&#35265;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13071v2 Announce Type: replace-cross  Abstract: Embedding news articles is a crucial tool for multiple fields, such as media bias detection, identifying fake news, and making news recommendations. However, existing news embedding methods are not optimized to capture the latent context of news events. Most embedding methods rely on full-text information and neglect time-relevant embedding generation. In this paper, we propose a novel lightweight method that optimizes news embedding generation by focusing on entities and themes mentioned in articles and their historical connections to specific events. We suggest a method composed of three stages. First, we process and extract events, entities, and themes from the given news articles. Second, we generate periodic time embeddings for themes and entities by training time-separated GloVe models on current and historical data. Lastly, we concatenate the news embeddings generated by two distinct approaches: Smooth Inverse Frequency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25910;&#38598;&#29992;&#25143;&#23545;&#26410;&#32463;&#21382;&#30005;&#24433;&#30340;&#20449;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#32467;&#21512;&#29992;&#25143;&#35780;&#20998;&#12289;&#20449;&#24565;&#21644;&#25512;&#33616;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2405.11053</link><description>&lt;p&gt;
&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#21069;&#30340;&#29992;&#25143;&#20449;&#24565;&#25968;&#25454;&#38598;&#65306;&#25910;&#38598;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#21069;&#36873;&#25321;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25910;&#38598;&#29992;&#25143;&#23545;&#26410;&#32463;&#21382;&#30005;&#24433;&#30340;&#20449;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#32467;&#21512;&#29992;&#25143;&#35780;&#20998;&#12289;&#20449;&#24565;&#21644;&#25512;&#33616;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11053v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11053v3 Announce Type: replace-cross  Abstract: An increasingly important aspect of designing recommender systems involves considering how recommendations will influence consumer choices. This paper addresses this issue by introducing a method for collecting user beliefs about un-experienced items - a critical predictor of choice behavior. We implemented this method on the MovieLens platform, resulting in a rich dataset that combines user ratings, beliefs, and observed recommendations. We document challenges to such data collection, including selection bias in response and limited coverage of the product space. This unique resource empowers researchers to delve deeper into user behavior and analyze user choices absent recommendations, measure the effectiveness of recommendations, and prototype algorithms that leverage user belief data, ultimately leading to more impactful recommender systems. The dataset can be found at https://grouplens.org/datasets/movielens/ml_belief_2024
&lt;/p&gt;</description></item><item><title>FloorSet&#26159;&#19968;&#20010;&#21253;&#21547;&#24102;&#26377;&#30495;&#23454;&#19990;&#30028;SoC&#35774;&#35745;&#32422;&#26463;&#30340;VLSI&#24067;&#23616;&#35268;&#21010;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#19968;&#20010;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2405.05480</link><description>&lt;p&gt;
FloorSet --- &#19968;&#20010;&#24102;&#26377;&#30495;&#23454;&#19990;&#30028;SoC&#35774;&#35745;&#32422;&#26463;&#30340;VLSI&#24067;&#23616;&#35268;&#21010;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of Real-World SoCs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.05480
&lt;/p&gt;
&lt;p&gt;
FloorSet&#26159;&#19968;&#20010;&#21253;&#21547;&#24102;&#26377;&#30495;&#23454;&#19990;&#30028;SoC&#35774;&#35745;&#32422;&#26463;&#30340;VLSI&#24067;&#23616;&#35268;&#21010;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#19968;&#20010;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.05480v4 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#23545;&#20110;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoCs&#65289;&#21450;&#20854;&#23376;&#31995;&#32479;&#65292;&#24067;&#23616;&#35268;&#21010;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21644;&#38750;&#21516;&#23567;&#21487;&#30340;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#27493;&#39588;&#12290;&#23427;&#20195;&#34920;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#24102;&#26377;120&#20010;&#20998;&#21306;&#30340;&#22823;&#22411;SoC&#20135;&#29983;&#20102;&#22823;&#32422;10E250&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#38543;&#30528;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#20986;&#29616;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#23545;&#20110;&#19968;&#20010;&#33021;&#22815;&#28085;&#30422;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#24182;&#19988;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#32422;&#26463;&#21644;&#30446;&#26631;&#30340;&#29616;&#20195;&#22522;&#20934;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;FloorSet &#8212;&#8212;&#20004;&#20010;&#20840;&#38754;&#30340;&#21512;&#25104;&#22266;&#23450;&#36718;&#24275;&#24067;&#23616;&#35774;&#35745;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#21453;&#26144;&#20102;&#30495;&#23454;SoC&#30340;&#20998;&#24067;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;100&#19975;&#35757;&#32451;&#26679;&#26412;&#21644;100&#20010;&#27979;&#35797;&#26679;&#26412;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#19968;&#31181;&#21512;&#25104;&#24067;&#23616;&#12290;FloorSet-Prime&#21253;&#21547;&#20102;&#20840;&#36793;&#25509;&#30697;&#24418;&#20998;&#21306;&#30340;&#24067;&#23616;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#36208;&#32447;&#38271;&#24230;&#12290;&#19968;&#20010;&#31616;&#21270;&#29256;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21435;&#38500;&#31354;&#38386;&#21306;&#22495;&#21644;&#31616;&#21270;&#26012;&#21521;&#36830;&#25509;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;FloorSet-Duo&#21017;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;SoC&#20013;&#24120;&#35265;&#22797;&#26434;&#24230;&#30340;&#27169;&#25311;&#65292;&#21253;&#25324;&#19981;&#23545;&#31216;&#20998;&#21306;&#12289;&#23396;&#31435;&#20998;&#21306;&#21644;&#19981;&#35268;&#21017;&#24418;&#29366;&#12290;FloorSet&#30340;&#21457;&#24067;&#26631;&#24535;&#30528;&#24067;&#23616;&#35268;&#21010;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#24320;&#22987;&#65292;&#20026;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#26159;&#29702;&#35770;&#30740;&#31350;&#21644;&#24037;&#19994;&#23454;&#38469;&#30340;&#21452;&#37325;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.05480v4 Announce Type: replace-cross  Abstract: Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial and non-trivial step of the physical design flow. It represents a difficult combinatorial optimization problem. A typical large scale SoC with 120 partitions generates a search-space of nearly 10E250. As novel machine learning (ML) approaches emerge to tackle such problems, there is a growing need for a modern benchmark that comprises a large training dataset and performance metrics that better reflect real-world constraints and objectives compared to existing benchmarks. To address this need, we present FloorSet -- two comprehensive datasets of synthetic fixed-outline floorplan layouts that reflect the distribution of real SoCs. Each dataset has 1M training samples and 100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime comprises fully-abutted rectilinear partitions and near-optimal wire-length. A simplified dataset that reflec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26089;&#26399;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;&#30340;&#36807;&#31243;&#65292;&#30740;&#31350;&#20154;&#21592;&#20943;&#23569;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#26377;&#25928;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2405.03389</link><description>&lt;p&gt;
&#19981;&#35201;&#28010;&#36153;&#24744;&#30340;&#23453;&#36149;&#26102;&#38388;&#65306;&#26089;&#26399;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Don't Waste Your Time: Early Stopping Cross-Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.03389
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26089;&#26399;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;&#30340;&#36807;&#31243;&#65292;&#30740;&#31350;&#20154;&#21592;&#20943;&#23569;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#26377;&#25928;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.03389v2 Announce Type: &#26367;&#25442;&#26679;&#26412;&#25688;&#35201;&#65306;&#22312;&#34920;&#25968;&#25454;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#37319;&#29992;&#20132;&#21449;&#39564;&#35777;&#65307;&#30830;&#20445;&#27979;&#37327;&#30340;&#24615;&#33021;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#38543;&#21518;&#30340;&#38598;&#21512;&#23398;&#20064;&#19981;&#20250;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#19982;&#30041;&#20986;&#39564;&#35777;&#30456;&#27604;&#65292;&#20351;&#29992;k-&#25240;&#20132;&#21449;&#39564;&#35777;&#26174;&#33879;&#22686;&#21152;&#20102;&#39564;&#35777;&#21333;&#20010;&#37197;&#32622;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#34429;&#28982;&#30830;&#20445;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#27492;&#22686;&#24378;&#20102;&#24615;&#33021;&#65292;&#20294;&#39069;&#22806;&#30340;&#25104;&#26412;&#24448;&#24448;&#36229;&#36807;&#20102;&#22312;&#26102;&#38388;&#39044;&#31639;&#20869;&#36827;&#34892;&#26377;&#25928;&#27169;&#22411;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#24102;&#26377;&#20132;&#21449;&#39564;&#35777;&#30340;&#27169;&#22411;&#36873;&#25321;&#26356;&#21152;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#20013;&#23545;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#26089;&#26399;&#20572;&#39039;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#20004;&#31867;&#31639;&#27861;&#65288;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#38543;&#26426;&#26862;&#26519;&#65289;&#21644;36&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;&#38543;&#26426;&#25628;&#32034;&#30340;&#26089;&#26399;&#20572;&#39039;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;3-&#12289;5-&#21644;10-&#25240;&#20132;&#21449;&#39564;&#35777;&#26102;&#65292;&#25240;&#21472;&#25968;&#37327;&#23545;&#26089;&#26399;&#20572;&#39039;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24615;&#33021;&#25351;&#26631;&#30340;&#25935;&#24863;&#24615;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;&#26368;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#20013;&#37319;&#29992;&#26089;&#26399;&#20572;&#39039;&#30340;&#20132;&#21449;&#39564;&#35777;&#26159;&#21487;&#34892;&#30340;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.03389v2 Announce Type: replace-cross  Abstract: State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit. However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration. While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget. We aim to make model selection with cross-validation more effective. Therefore, we study early stopping the process of cross-validation during model selection. We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets. We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds. In addition, we inve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#19978;&#30340;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#26377;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#24635;&#20307;&#34920;&#29616;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#27169;&#22411;&#22312;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#22343;&#34913;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.17513</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20107;&#20214;&#25512;&#29702;&#30340;&#32508;&#21512;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation on Event Reasoning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#19978;&#30340;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#26377;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#24635;&#20307;&#34920;&#29616;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#27169;&#22411;&#22312;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#22343;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17513v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20107;&#20214;&#25512;&#29702;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#22522;&#30784;&#33021;&#21147;&#12290;&#23427;&#38656;&#35201;&#20107;&#20214;&#27169;&#24335;&#30693;&#35782;&#36827;&#34892;&#20840;&#23616;&#25512;&#29702;&#65292;&#24182;&#19988;&#38656;&#35201;&#22788;&#29702;&#21508;&#31181;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#12290;LLM&#22312;&#19981;&#21516;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#19978;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#20173;&#28982;&#26410;&#30693;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;LLM&#30340;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;EV2&#29992;&#20110;&#35780;&#20272;&#20107;&#20214;&#25512;&#29702;&#12290;EV2&#21253;&#21547;&#20004;&#20010;&#23618;&#27425;&#30340;&#35780;&#20215;&#65292;&#21363;&#27169;&#24335;&#21644;&#23454;&#20363;&#65292;&#24182;&#19988;&#22312;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#26041;&#38754;&#26159;&#20840;&#38754;&#30340;&#12290;&#25105;&#20204;&#22312;EV2&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#26410;&#36798;&#21040;&#28385;&#24847;&#12290;&#25105;&#20204;&#36824;&#27880;&#24847;&#21040;LLM&#22312;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#19981;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;LLM&#20855;&#26377;&#20107;&#20214;&#27169;&#24335;&#30693;&#35782;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#20107;&#20214;&#25512;&#29702;&#26041;&#38754;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17513v2 Announce Type: replace-cross  Abstract: Event reasoning is a fundamental ability that underlies many applications. It requires event schema knowledge to perform global reasoning and needs to deal with the diversity of the inter-event relations and the reasoning paradigms. How well LLMs accomplish event reasoning on various relations and reasoning paradigms remains unknown. To mitigate this disparity, we comprehensively evaluate the abilities of event reasoning of LLMs. We introduce a novel benchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of evaluation of schema and instance and is comprehensive in relations and reasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs have abilities to accomplish event reasoning but their performances are far from satisfactory. We also notice the imbalance of event reasoning abilities in LLMs. Besides, LLMs have event schema knowledge, however, they're not aligned with humans on how to
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#29983;&#29289;&#26234;&#33021;&#20013;&#31361;&#30772;&#20449;&#24687;&#29942;&#39048;&#65292;&#35753;&#26426;&#22120;&#23398;&#20064;&#33021;&#20687;&#29983;&#29289;&#26234;&#33021;&#19968;&#26679;&#39640;&#25928;&#22320;&#20174;&#22810;&#26679;&#38750;&#22870;&#21169;&#21050;&#28608;&#20449;&#24687;&#20013;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.12631</link><description>&lt;p&gt;
&#31361;&#30772;&#29942;&#39048;&#65306;&#31070;&#32463;&#35843;&#21046;&#31070;&#32463;&#32593;&#32476;&#20013;&#20174;&#22870;&#21169;&#39537;&#21160;&#23398;&#20064;&#21040;&#22870;&#21169; agnostic &#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#28436;&#21270;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
Breaching the Bottleneck: Evolutionary Transition from Reward-Driven Learning to Reward-Agnostic Domain-Adapted Learning in Neuromodulated Neural Nets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12631
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#29983;&#29289;&#26234;&#33021;&#20013;&#31361;&#30772;&#20449;&#24687;&#29942;&#39048;&#65292;&#35753;&#26426;&#22120;&#23398;&#20064;&#33021;&#20687;&#29983;&#29289;&#26234;&#33021;&#19968;&#26679;&#39640;&#25928;&#22320;&#20174;&#22810;&#26679;&#38750;&#22870;&#21169;&#21050;&#28608;&#20449;&#24687;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12631v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#24341;&#29992; &#25688;&#35201;&#65306;&#39640;&#32423;&#29983;&#29289;&#26234;&#33021;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#21050;&#28608;&#20449;&#24687;&#30340;&#20016;&#23500;&#27969;&#20013;&#23398;&#20064;&#65292;&#21363;&#20351;&#22312;&#34892;&#20026;&#36136;&#37327;&#21453;&#39304;&#31232;&#32570;&#25110;&#23436;&#20840;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#31181;&#23398;&#20064;&#21033;&#29992;&#20102;&#23545;&#20219;&#21153;&#39046;&#22495;&#30340;&#19968;&#20123;&#38544;&#21547;&#20551;&#35774;&#12290;&#25105;&#20204;&#25226;&#36825;&#31181;&#23398;&#20064;&#31216;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#65288;Domain-Adapted Learning, DAL&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#34892;&#20026;&#36136;&#37327;&#30340;&#22806;&#37096;&#26126;&#30830;&#25552;&#20379;&#25351;&#26631;&#26469;&#33719;&#24471;&#36866;&#24212;&#24615;&#34892;&#20026;&#12290;&#36825;&#31181;&#20570;&#27861;&#35774;&#31435;&#20102;&#19968;&#20010;&#20449;&#24687;&#29942;&#39048;&#65292;&#38459;&#30861;&#20102;&#23545;&#22810;&#26679;&#38750;&#22870;&#21169;&#21050;&#28608;&#20449;&#24687;&#30340;&#35748;&#30693;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#25105;&#20204;&#32771;&#34385;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#29983;&#29289;&#36827;&#21270;&#26159;&#22914;&#20309;&#32469;&#36807;&#36825;&#20010;&#29942;&#39048;&#26469;&#20135;&#29983;DAL&#30340;&#65311;&#25105;&#20204;&#25552;&#20986;&#29289;&#31181;&#39318;&#20808;&#36827;&#21270;&#20986;&#20174;&#22870;&#21169;&#20449;&#21495;&#20013;&#23398;&#20064;&#30340;&#26412;&#39046;&#65292;&#34429;&#28982;&#25928;&#29575;&#20302;&#19979;&#65288;&#26377;&#29942;&#39048;&#65289;&#20294;&#21364;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#24212;&#24615;&#12290;&#20174;&#37027;&#37324;&#65292;&#36890;&#36807;&#36880;&#28176;&#31215;&#32047;&#30340;&#38750;&#22870;&#21169;&#20449;&#24687;&#30340;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#21487;&#20197;&#36880;&#28176;&#36827;&#34892;&#65292;&#22240;&#27492;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#24341;&#23548;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12631v2 Announce Type: replace-cross  Abstract: Advanced biological intelligence learns efficiently from an information-rich stream of stimulus information, even when feedback on behaviour quality is sparse or absent. Such learning exploits implicit assumptions about task domains. We refer to such learning as Domain-Adapted Learning (DAL). In contrast, AI learning algorithms rely on explicit externally provided measures of behaviour quality to acquire fit behaviour. This imposes an information bottleneck that precludes learning from diverse non-reward stimulus information, limiting learning efficiency. We consider the question of how biological evolution circumvents this bottleneck to produce DAL. We propose that species first evolve the ability to learn from reward signals, providing inefficient (bottlenecked) but broad adaptivity. From there, integration of non-reward information into the learning process can proceed via gradual accumulation of biases induced by such infor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;&#26053;&#34892;&#37319;&#36141;&#38382;&#39064;&#20013;&#30340;&#36335;&#32447;&#26500;&#24314;&#21644;&#37319;&#36141;&#35745;&#21010;&#20998;&#24320;&#22788;&#29702;&#65292;&#24182;&#20174;&#20840;&#23616;&#35270;&#35282;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#65292;&#33021;&#22815;&#20026;&#26053;&#28216;&#37319;&#36141;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26053;&#28216;&#37319;&#36141;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;&#26053;&#34892;&#37319;&#36141;&#38382;&#39064;&#20013;&#30340;&#36335;&#32447;&#26500;&#24314;&#21644;&#37319;&#36141;&#35745;&#21010;&#20998;&#24320;&#22788;&#29702;&#65292;&#24182;&#20174;&#20840;&#23616;&#35270;&#35282;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#65292;&#33021;&#22815;&#20026;&#26053;&#28216;&#37319;&#36141;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#26053;&#34892;&#37319;&#36141;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#36335;&#32447;&#26500;&#24314;&#21644;&#37319;&#36141;&#35745;&#21010;&#20043;&#38388;&#23384;&#22312;&#32806;&#21512;&#65292;&#29616;&#26377;&#20851;&#20110;TPPs&#30340;&#24037;&#20316;&#36890;&#24120;&#21516;&#26102;&#35299;&#20915;&#36335;&#32447;&#21644;&#37319;&#36141;&#35745;&#21010;&#65292;&#36825;&#34429;&#28982;&#23548;&#33268;&#20102;&#35299;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#30830;&#20999;&#26041;&#27861;&#21644;&#35774;&#35745;&#22797;&#26434;&#20294;&#24615;&#33021;&#26377;&#38480;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#19982;&#27492;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36335;&#32447;&#26500;&#24314;&#21644;&#37319;&#36141;&#35745;&#21010;&#20998;&#24320;&#22788;&#29702;&#65292;&#24182;&#22312;&#20840;&#23616;&#35270;&#35282;&#19979;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;TPPs&#30340;&#21452;&#36793;&#22270;&#34920;&#31034;&#65292;&#20197;&#25429;&#33719;&#24066;&#22330;&#19982;&#20135;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19968;&#20010;&#25919;&#31574;&#32593;&#32476;&#65292;&#23427;&#20174;&#21452;&#36793;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#39034;&#24207;&#26500;&#24314;&#36335;&#32447;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#28857;&#26159;&#25105;&#20204;&#33021;&#22815;&#20197;&#36739;&#23569;&#30340;&#22788;&#29702;&#26102;&#38388;&#20026;&#36215;&#28857;&#65292;&#22312;&#36335;&#32447;&#19978;&#30340;&#25152;&#26377;&#21830;&#24215;&#20301;&#32622;&#31354;&#38388;&#20013;&#38543;&#26426;&#21021;&#22987;&#21270;&#19968;&#20010;&#28857;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31934;&#39635;&#26469;&#20248;&#21270;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#26053;&#28216;&#37319;&#36141;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;&#35774;&#35745;&#20102;&#19987;&#38376;&#30340;&#21452;&#36793;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#38454;&#27573;&#26053;&#34892;&#37319;&#36141;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v3 Announce Type: replace-cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficient
&lt;/p&gt;</description></item><item><title>DASA&#31639;&#27861;&#26159;&#38024;&#23545;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#38382;&#39064;&#35774;&#35745;&#30340;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#36890;&#20449;&#30340;&#24310;&#36831;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.17247</link><description>&lt;p&gt;
DASA: &#33258;&#36866;&#24212;&#24310;&#36831;&#30340;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17247
&lt;/p&gt;
&lt;p&gt;
DASA&#31639;&#27861;&#26159;&#38024;&#23545;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#38382;&#39064;&#35774;&#35745;&#30340;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#36890;&#20449;&#30340;&#24310;&#36831;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v3 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#25105;&#20204;&#32771;&#34385;&#36825;&#26679;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;N&#20010;&#20195;&#29702;&#35797;&#22270;&#36890;&#36807;&#24182;&#34892;&#34892;&#21160;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#26469;&#21152;&#36895;&#19968;&#20010;&#20849;&#21516;&#30340;&#38543;&#26426;&#36924;&#36817;(SA)&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#21521;&#19978;&#28216;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#30340;&#26102;&#38388;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19981;&#21464;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#20943;&#36731;&#24310;&#36831;&#21644;&#33853;&#21518;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\texttt{DASA}&#65292;&#23427;&#26159;&#22522;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#12290;&#25105;&#20204;&#20026;\texttt{DASA}&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#20998;&#26512;&#65292;&#20551;&#35774;&#20195;&#29702;&#30340;&#38543;&#26426;&#35266;&#27979;&#36807;&#31243;&#26159;&#29420;&#31435;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#26174;&#33879;&#22320;&#25512;&#36827;&#20102;&#29616;&#26377;&#30340;&#32467;&#26524;&#65292;\texttt{DASA}&#26159;&#31532;&#19968;&#20010;&#19982;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#21462;&#20915;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30456;&#20851;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;Markovian&#25277;&#26679;&#19979;&#30340;N&#20493;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21508;&#31181;SA&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#24403;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#29615;&#22659;&#26102;&#65292;&#20854;&#31639;&#27861;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v3 Announce Type: replace-cross  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tau_{mix}$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#32423;&#27979;&#35797;&#26041;&#27861;&#65292;&#20026;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#30340;&#26377;&#25928;&#27979;&#35797;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15857</link><description>&lt;p&gt;
&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#31995;&#32479;&#32423;&#33258;&#21160;&#27979;&#35797;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated System-level Testing of Unmanned Aerial Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#32423;&#27979;&#35797;&#26041;&#27861;&#65292;&#20026;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#30340;&#26377;&#25928;&#27979;&#35797;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#25688;&#35201;&#65306;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#65288;UAS&#65289;&#20381;&#36182;&#20110;&#21508;&#31181;&#33322;&#31354;&#30005;&#23376;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#23545;&#20110;&#23433;&#20840;&#24615;&#21644;&#20219;&#21153;&#25191;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#38469;&#23433;&#20840;&#26631;&#20934;&#30340;&#37325;&#22823;&#35201;&#27714;&#26159;&#23545;&#33322;&#31354;&#30005;&#23376;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#20005;&#26684;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#12290;&#24403;&#21069;&#24037;&#19994;&#20570;&#27861;&#26159;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#22330;&#26223;&#65292;&#20351;&#29992;&#27169;&#25311;&#22120;&#25163;&#21160;/&#33258;&#21160;&#25191;&#34892;&#36825;&#20123;&#22330;&#26223;&#65292;&#24182;&#25163;&#21160;&#35780;&#20272;&#32467;&#26524;&#12290;&#27979;&#35797;&#22330;&#26223;&#36890;&#24120;&#21253;&#25324;&#35774;&#32622;&#29305;&#23450;&#39134;&#34892;&#25110;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#21463;&#27979;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#27492;&#30446;&#30340;&#30340;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#20063;&#35201;&#27714;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#31995;&#32479;&#32423;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;AITester&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#26469;&#33258;&#21160;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#35780;&#20272;&#21508;&#31181;&#27979;&#35797;&#22330;&#26223;&#12290;&#27979;&#35797;&#22330;&#26223;&#36890;&#36807;AI&#25216;&#26415;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#65292;&#24182;&#27169;&#25311;&#21644;&#35780;&#20272;&#31995;&#32479;&#20013;&#19981;&#21516;&#32452;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27979;&#35797;&#30340;&#35206;&#30422;&#29575;&#21644;&#21487;&#38752;&#24230;&#12290;&#27492;&#22806;&#65292;AITester&#36824;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#20998;&#26512;&#21644;&#20248;&#21270;&#27979;&#35797;&#26696;&#20363;&#65292;&#20174;&#32780;&#22312;&#30701;&#26102;&#38388;&#20869;&#21457;&#29616;&#24182;&#38548;&#31163;&#31995;&#32479;&#28508;&#22312;&#30340;&#38169;&#35823;&#21644;&#38382;&#39064;&#12290;AITester&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#25104;&#26412;&#24182;&#25552;&#39640;UAS&#31995;&#32479;&#27979;&#35797;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v2 Announce Type: replace-cross  Abstract: Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems. The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes. The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings. The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation. In this paper, we propose a novel approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#65292;&#30830;&#20445;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;&#65292;&#20197;&#36798;&#21040;&#39640;&#25928;&#30340;3D&#32593;&#26684;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;</title><link>https://arxiv.org/abs/2403.15559</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#20445;&#23558;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#24212;&#29992;&#21040;3D&#32593;&#26684;&#30340;&#32441;&#29702;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#65292;&#30830;&#20445;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;&#65292;&#20197;&#36798;&#21040;&#39640;&#25928;&#30340;3D&#32593;&#26684;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15559v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#21270;&#26102;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#30830;&#20445;&#22810;&#35270;&#35282;&#30340;&#19968;&#33268;&#24615;&#12290;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#32858;&#21512;&#22810;&#35270;&#35282;&#36755;&#20837;&#65292;&#20854;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#22312;&#32858;&#21512;&#27493;&#39588;&#20013;&#30001;&#20110;&#24179;&#22343;&#25805;&#20316;&#24341;&#36215;&#30340;&#27169;&#31946;&#24615;&#65292;&#25110;&#32773;&#23616;&#37096;&#29305;&#24449;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20998;&#20026;&#22235;&#20010;&#38454;&#27573;&#26469;&#23454;&#29616;&#22810;&#35270;&#35282;&#30340;&#19968;&#33268;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#20174;&#39044;&#23450;&#20041;&#30340;&#22810;&#35270;&#35282;&#38598;&#29983;&#25104;&#19968;&#20010;2D&#32441;&#29702;&#30340;&#36807;&#23436;&#22791;&#38598;&#21512;&#12290;&#31532;&#20108;&#38454;&#27573;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#30340;&#35270;&#22270;&#65292;&#36825;&#20123;&#35270;&#22270;&#22312;&#35206;&#30422;&#19979;&#30340;3D&#27169;&#22411;&#20013;&#26159;&#30456;&#20114;&#19968;&#33268;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35299;&#20915;&#21322;&#27491;&#23450;&#35268;&#21010;&#26469;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#31532;&#19977;&#38454;&#27573;&#25191;&#34892;&#38750;&#21018;&#24615;&#23545;&#40784;&#65292;&#20197;&#22312;&#37325;&#21472;&#21306;&#22495;&#23545;&#36873;&#23450;&#30340;&#35270;&#22270;&#36827;&#34892;&#23545;&#40784;&#12290;&#31532;&#22235;&#38454;&#27573;&#35299;&#20915;&#19968;&#20010;MRF&#38382;&#39064;&#65292;&#23427;&#29992;&#20110;&#19968;&#33268;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#37325;&#24314;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#37319;&#20998;&#36873;&#37319;&#26679;&#26469;&#32454;&#21270;&#35270;&#35282;&#36873;&#25321;&#12290;&#25972;&#20307;&#26694;&#26550;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#65292;&#30452;&#21040;&#36798;&#21040;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;3D&#32593;&#26684;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15559v2 Announce Type: replace-cross  Abstract: A fundamental problem in the texturing of 3D meshes using pre-trained text-to-image models is to ensure multi-view consistency. State-of-the-art approaches typically use diffusion models to aggregate multi-view inputs, where common issues are the blurriness caused by the averaging operation in the aggregation step or inconsistencies in local features. This paper introduces an optimization framework that proceeds in four stages to achieve multi-view consistency. Specifically, the first stage generates an over-complete set of 2D textures from a predefined set of viewpoints using an MV-consistent diffusion process. The second stage selects a subset of views that are mutually consistent while covering the underlying 3D model. We show how to achieve this goal by solving semi-definite programs. The third stage performs non-rigid alignment to align the selected views across overlapping regions. The fourth stage solves an MRF problem t
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#32452;&#21453;&#20107;&#23454;&#35299;&#37322;&#20013;&#65292;&#30001;&#22810;&#31181;&#38598;&#25104;&#35299;&#37322;&#26041;&#27861;&#29983;&#25104;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#20943;&#23569;&#20914;&#31361;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#19968;&#20010;&#12290;</title><link>https://arxiv.org/abs/2403.13940</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26631;&#20934;&#26041;&#27861;&#20174;&#35299;&#37322;&#38598;&#21512;&#20013;&#36873;&#25321;&#38598;&#25104;&#30340;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13940
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#32452;&#21453;&#20107;&#23454;&#35299;&#37322;&#20013;&#65292;&#30001;&#22810;&#31181;&#38598;&#25104;&#35299;&#37322;&#26041;&#27861;&#29983;&#25104;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#20943;&#23569;&#20914;&#31361;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13940v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;&#25688;&#35201;: &#21453;&#20107;&#23454;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#25552;&#20379;&#33719;&#24471;&#26356;&#21463;&#27426;&#36814;&#39044;&#27979;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#29983;&#25104;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#19981;&#21516;&#30340;&#29978;&#33267;&#26159;&#20914;&#31361;&#30340;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#20135;&#29983;&#25130;&#28982;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29983;&#25104;&#21453;&#20107;&#23454;&#20013;&#30340;&#19968;&#20010;&#24182;&#38750;&#26131;&#20107;&#12290;&#32780;&#19981;&#26159;&#36843;&#20351;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#20307;&#21270;&#21453;&#20107;&#23454;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#27969;&#34892;&#36136;&#37327;&#26631;&#20934;&#19978;&#24471;&#20998;&#24456;&#22909;&#30340;&#22949;&#21327;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#25903;&#37197;&#20851;&#31995;&#21644;&#29702;&#24819;&#28857;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#36873;&#25321;&#19968;&#20010;&#21453;&#20107;&#23454;&#12290;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#32771;&#34385;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#24403;&#32771;&#34385;&#22810;&#20010;&#26631;&#20934;&#26102;&#65292;&#23427;&#33021;&#22815;&#20943;&#23569;&#22810;&#20934;&#21017;&#38382;&#39064;&#20013;&#30340;&#20914;&#31361;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#26131;&#20110;&#29702;&#35299;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20182;&#20204;&#36827;&#34892;&#35299;&#37322;&#36873;&#25321;&#26102;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13940v2 Announce Type: replace-cross  Abstract: Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front. The conducted experiments demonstrated that th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#21644;&#20248;&#21183;&#65292;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#24494;&#35843;&#65292;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#33719;&#24471;&#25509;&#36817;&#20174;&#38646;&#24320;&#22987;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25552;&#20379;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.02504</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#21644;&#20248;&#21183;&#65292;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#24494;&#35843;&#65292;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#33719;&#24471;&#25509;&#36817;&#20174;&#38646;&#24320;&#22987;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25552;&#20379;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v3 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;&#65306;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20805;&#24403;&#34920;&#36798;&#24605;&#24819;&#21644;&#24773;&#24863;&#30340;&#20027;&#35201;&#28192;&#36947;&#65292;&#25991;&#26412;&#20998;&#26512;&#24050;&#25104;&#20026;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#20351;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20986;&#23453;&#36149;&#35265;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#25903;&#25345;&#35832;&#22914;&#35780;&#20272;&#20010;&#24615;&#29305;&#24449;&#12289;&#30417;&#27979;&#24515;&#29702;&#20581;&#24247;&#21644;&#20154;&#38469;&#27807;&#36890;&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;&#31561;&#21162;&#21147;&#12290;&#22312;&#25991;&#26412;&#20998;&#26512;&#20013;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20250;&#37319;&#29992;&#25163;&#21160;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#32791;&#26102;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#20808;&#26500;&#24314;&#30340;&#35789;&#20856;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#24773;&#20917;&#65292;&#25110;&#32773;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20195;&#34920;&#20102;&#25991;&#26412;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#21464;&#38761;&#26041;&#27861;&#12290;&#36825;&#31181;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32780;&#19982;&#20247;&#19981;&#21516;&#65292;&#26174;&#31034;&#20986;&#22312;&#36127;&#36131;&#22810;&#31181;&#19981;&#21516;&#20219;&#21153;&#26102;&#30340;&#39640;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#24494;&#35843;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#33719;&#24471;&#20960;&#20046;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26102;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#27599;&#20010;&#26032;&#20219;&#21153;&#37117;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v3 Announce Type: replace-cross  Abstract: Given that natural language serves as the primary conduit for expressing thoughts and emotions, text analysis has become a key technique in psychological research. It enables the extraction of valuable insights from natural language, facilitating endeavors like personality traits assessment, mental health monitoring, and sentiment analysis in interpersonal communications. In text analysis, existing studies often resort to either human coding, which is time-consuming, using pre-built dictionaries, which often fails to cover all possible scenarios, or training models from scratch, which requires large amounts of labeled data. In this tutorial, we introduce the pretrain-finetune paradigm. The pretrain-finetune paradigm represents a transformative approach in text analysis and natural language processing. This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in f
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#21019;&#26032;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Routoo&#8221;&#30340;&#24314;&#31569;&#65292;&#26088;&#22312;&#26681;&#25454;&#24615;&#33021;&#12289;&#25104;&#26412;&#21644;&#25928;&#29575;&#26469;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#25552;&#31034;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#26377;&#25928;&#30340;&#36335;&#30001;&#23398;&#20064;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Routoo: Learning to Route to Large Language Models Effectively
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#21019;&#26032;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Routoo&#8221;&#30340;&#24314;&#31569;&#65292;&#26088;&#22312;&#26681;&#25454;&#24615;&#33021;&#12289;&#25104;&#26412;&#21644;&#25928;&#29575;&#26469;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#25552;&#31034;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;: arXiv:2401.13979v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#36328;&#23398;&#31185;&#25688;&#35201;: &#24320;&#21457;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25104;&#26412;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#65292;&#25928;&#29575;&#20063;&#36234;&#26469;&#36234;&#20302;&#12290;&#27492;&#22806;&#65292;&#38381;&#28304;&#21644;&#22823;&#22411;&#30340;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#21709;&#24212;&#36136;&#37327;&#65292;&#20294;&#20195;&#20215;&#26356;&#39640;&#65292;&#19981;&#22914;&#23567;&#35268;&#27169;&#27169;&#22411;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Routoo&#8221;&#30340;&#24314;&#31569;&#35774;&#35745;&#65292;&#26088;&#22312;&#26681;&#25454;&#24615;&#33021;&#12289;&#25104;&#26412;&#21644;&#25928;&#29575;&#26469;&#20248;&#21270;&#29305;&#23450;&#25552;&#31034;&#30340;LLM&#36873;&#25321;&#12290;Routoo&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#19968;&#20010;&#24615;&#33021;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25104;&#26412;&#33258;&#36866;&#24212;&#35299;&#30721;&#22120;&#12290;&#24615;&#33021;&#39044;&#27979;&#22120;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;LLM&#65292;&#23427;&#19981;&#38656;&#35201;&#25191;&#34892;&#21644;&#35780;&#20272;&#23601;&#33021;&#20272;&#35745;&#21508;&#31181;&#24213;&#23618;LLM&#30340;&#24615;&#33021;&#12290;&#25104;&#26412;&#33258;&#36866;&#24212;&#35299;&#30721;&#22120;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#21644;&#20854;&#20182;&#32422;&#26463;&#65288;&#22914;&#25104;&#26412;&#21644;&#24310;&#36831;&#65289;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;MMLU&#22522;&#20934;&#23545;57&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Routoo&#22312;&#21305;&#37197;MMLU&#22522;&#20934;&#28857;&#26041;&#38754;&#19982;&#24320;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.13979v2 Announce Type: replace-cross  Abstract: Developing foundational large language models (LLMs) is becoming increasingly costly and inefficient. Also, closed-source and larger open-source models generally offer better response quality but come with higher inference costs than smaller models. In this paper, we introduce Routoo, an architecture designed to optimize the selection of LLMs for specific prompts based on performance, cost, and efficiency. Routoo consists of two key components: a performance predictor and a cost-aware decoding. The performance predictor is a lightweight LLM that estimates the performance of various underlying LLMs without needing to execute and evaluate them. The cost-aware decoding then selects the most suitable model based on these predictions and other constraints like cost and latency. We evaluated Routoo using the MMLU benchmark across 57 domains employing open-source models. Our results show that Routoo matches the performance of the Mixt
&lt;/p&gt;</description></item><item><title>D-TSN&#21033;&#29992;&#21487;&#24494;&#26641;&#25628;&#32034;&#20248;&#21270;&#31574;&#30053;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#34892;&#21160;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2401.11660</link><description>&lt;p&gt;
&#21487;&#24494;&#26641;&#25628;&#32034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differentiable Tree Search Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11660
&lt;/p&gt;
&lt;p&gt;
D-TSN&#21033;&#29992;&#21487;&#24494;&#26641;&#25628;&#32034;&#20248;&#21270;&#31574;&#30053;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#34892;&#21160;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11660v2 &#21457;&#24067;&#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#24341;&#29992;&#25688;&#35201;&#65306;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30340;&#31574;&#30053;&#20989;&#25968;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#20174;&#26377;&#38480;&#30340;&#36164;&#26009;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#22312;&#32447;&#25628;&#32034;&#20013;&#30830;&#23450;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#19981;&#20934;&#30830;&#65292;&#38169;&#35823;&#20250;&#32047;&#31215;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#34429;&#28982;&#22914;TreeQN&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#24341;&#20837;&#31639;&#27861;&#30340;&#20559;&#35265;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#20559;&#35265;&#30340;&#24378;&#24230;&#24448;&#24448;&#19981;&#22815;&#24378;&#65292;&#19981;&#36275;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#24494;&#26641;&#25628;&#32034;&#32593;&#32476;&#65288;D-TSN&#65289;&#30340;&#20840;&#26032;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#36890;&#36807;&#22312;&#26368;&#20339;&#31532;&#19968;&#22312;&#32447;&#25628;&#32034;&#31639;&#27861;&#30340;&#32467;&#26500;&#20013;&#23884;&#20837;&#31639;&#27861;&#32467;&#26500;&#26469;&#26174;&#33879;&#21152;&#24378;&#20559;&#35265;&#12290;D-TSN&#20351;&#29992;&#39044;&#27979;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#30340;&#21487;&#24494;&#26641;&#25628;&#32034;&#65292;&#36825;&#20801;&#35768;&#23545;&#25628;&#32034;&#36807;&#31243;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#22797;&#26434;&#30340;&#36319;&#36394;&#20219;&#21153;&#21644;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#31034;&#20102;D-TSN&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#32570;&#20047;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#21547;&#22797;&#26434;&#24863;&#30693;&#21644;&#35268;&#21010;&#29305;&#24615;&#30340;&#38543;&#26426;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#21160;&#20316;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11660v2 Announce Type: replace-cross  Abstract: In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance. An alternative approach involves learning a world model from the limited data and determining actions through online search. However, the performance is adversely affected by compounding errors arising from inaccuracies in the learned world model. While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic inductive biases into the neural network architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks. In this work, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm. D-TSN employs a learned world model to conduct a fully d
&lt;/p&gt;</description></item><item><title>MERA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38754;&#21521;&#20420;&#35821;&#35821;&#35328;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#25351;&#20196;&#22522;&#20934;&#65292;&#23427;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12289;&#23616;&#38480;&#24615;&#21644;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.04531</link><description>&lt;p&gt;
MERA&#65306;&#19968;&#31181;&#20840;&#38754;&#30340;&#20420;&#35821;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MERA: A Comprehensive LLM Evaluation in Russian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.04531
&lt;/p&gt;
&lt;p&gt;
MERA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38754;&#21521;&#20420;&#35821;&#35821;&#35328;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#25351;&#20196;&#22522;&#20934;&#65292;&#23427;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12289;&#23616;&#38480;&#24615;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04531v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26368;&#26174;&#33879;&#30340;&#36827;&#27493;&#20043;&#19968;&#26159;&#22312;&#20110;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#23835;&#36215;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#12290;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;LMs&#22312;&#21487;&#37327;&#21270;&#30340;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#27493;&#65292;&#24182;&#22312;&#23450;&#24615;&#29305;&#24449;&#19978;&#26377;&#25152;&#26032;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#32473;&#20104;&#20102;&#20851;&#27880;&#65292;&#24182;&#19988;LM&#30340;&#24212;&#29992;&#36805;&#36895;&#22686;&#38271;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12289;&#23616;&#38480;&#24615;&#21644;&#30456;&#20851;&#39118;&#38505;&#30340;&#29702;&#35299;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#21152;&#28145;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;Multimodal Evaluation of Russian-language Architectures&#65288;MERA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#38754;&#21521;&#20420;&#35821;&#35821;&#35328;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#25351;&#20196;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19988;&#35774;&#35745;&#25104;&#20026;&#19968;&#20010;&#40657;&#30418;&#27979;&#35797;&#65292;&#20197;&#30830;&#20445;&#25490;&#38500;&#25968;&#25454;&#27844;&#28431;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#35770;&#65292;&#23588;&#20854;&#26159;&#22312;&#38646;&#26679;&#26412;&#21644;few-shot&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04531v3 Announce Type: replace-cross  Abstract: Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26696;&#20363;&#25512;&#29702;&#26469;&#25552;&#39640;&#22522;&#20110;OCL&#30340;MC/DC&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.03469</link><description>&lt;p&gt;
&#20351;&#29992;OCL&#21644;&#25628;&#32034;&#26041;&#27861;&#30340;&#39640;&#25928;MC/DC&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Test Data Generation for MC/DC with OCL and Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26696;&#20363;&#25512;&#29702;&#26469;&#25552;&#39640;&#22522;&#20110;OCL&#30340;MC/DC&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03469v3 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#33322;&#31354;&#30005;&#23376;&#36719;&#20214;&#31995;&#32479;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#38656;&#35201;&#31526;&#21512;&#22269;&#38469;&#23433;&#20840;&#26631;&#20934;&#22914;DO-178C&#30340;&#35201;&#27714;&#12290;&#33322;&#31354;&#30005;&#23376;&#24037;&#19994;&#30340;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#26159;&#25353;&#29031;&#23433;&#20840;&#26631;&#20934;&#25512;&#33616;&#30340;&#20934;&#21017;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#12290;DO-178C&#25512;&#33616;&#30340;&#20934;&#21017;&#20043;&#19968;&#26159;&#25913;&#36827;&#30340;&#26465;&#20214;/&#20915;&#31574;&#35206;&#30422;&#29575;(MC/DC)&#20934;&#21017;&#12290;&#24403;&#21069;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20351;&#29992;&#29992;&#20363;&#32422;&#26463;&#35821;&#35328;(OCL)&#32534;&#20889;&#30340;&#32422;&#26463;&#65292;&#24182;&#24212;&#29992;&#25628;&#32034;&#25216;&#26415;&#26469;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#19981;&#25903;&#25345;MC/DC&#20934;&#21017;&#65292;&#35201;&#20040;&#22312;&#20351;&#29992;&#22823;&#22411;&#33322;&#31354;&#30005;&#23376;&#31995;&#32479;&#26102;&#36935;&#21040;&#24615;&#33021;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#20013;&#33258;&#21160;&#29983;&#25104;MC/DC&#27979;&#35797;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;(CBR)&#21644;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;MC/DC&#23450;&#21046;OCL&#30340;&#32553;&#20943;&#33539;&#22260;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03469v3 Announce Type: replace-cross  Abstract: System-level testing of avionics software systems requires compliance with different international safety standards such as DO-178C. An important consideration of the avionics industry is automated test data generation according to the criteria suggested by safety standards. One of the recommended criteria by DO-178C is the modified condition/decision coverage (MC/DC) criterion. The current model-based test data generation approaches use constraints written in Object Constraint Language (OCL), and apply search techniques to generate test data. These approaches either do not support MC/DC criterion or suffer from performance issues while generating test data for large-scale avionics systems. In this paper, we propose an effective way to automate MC/DC test data generation during model-based testing. We develop a strategy that utilizes case-based reasoning (CBR) and range reduction heuristics designed to solve MC/DC-tailored OCL 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#21345;&#26041;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#21644;&#24191;&#20041;&#27779;&#23572;&#24503;-&#27779;&#23572;&#22827;&#29926;&#20857;&#20301;&#31227;&#26816;&#39564;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#24335;&#31526;&#21512;&#39044;&#26399;&#12290;</title><link>https://arxiv.org/abs/2312.10695</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#31574;&#30053;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Strategy Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10695
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#21345;&#26041;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#21644;&#24191;&#20041;&#27779;&#23572;&#24503;-&#27779;&#23572;&#22827;&#29926;&#20857;&#20301;&#31227;&#26816;&#39564;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#24335;&#31526;&#21512;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#65292;&#29992;&#20110;&#30830;&#23450;&#22312;&#37325;&#22797;&#30340;&#25112;&#30053;&#24418;&#24335;&#28216;&#25103;&#20013;&#65292;&#26681;&#25454;&#20195;&#29702;&#20154;&#34892;&#21160;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#36825;&#28041;&#21450;&#21040;&#20004;&#37096;&#20998;&#20869;&#23481;&#65306;&#30830;&#23450;&#20195;&#29702;&#21830;&#30340;&#32431;&#31574;&#30053;&#39057;&#29575;&#26159;&#21542;&#36275;&#22815;&#25509;&#36817;&#30446;&#26631;&#39057;&#29575;&#65292;&#20197;&#21450;&#30830;&#23450;&#22312;&#19981;&#21516;&#28216;&#25103;&#22238;&#21512;&#20013;&#36873;&#25321;&#30340;&#32431;&#31574;&#30053;&#26159;&#21542;&#29420;&#31435;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27979;&#35797;&#36890;&#36807;&#24212;&#29992;&#21345;&#26041;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26469;&#30830;&#23450;&#31532;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#24212;&#29992;&#24191;&#20041;&#30340;&#27779;&#23572;&#24503;-&#27779;&#23572;&#22827;&#29926;&#20857;&#20301;&#31227;&#26816;&#39564;&#26469;&#30830;&#23450;&#31532;&#20108;&#37096;&#20998;&#12290;&#20004;&#32773;&#30340;&#32467;&#26524;&#36890;&#36807;Bonferroni&#26657;&#27491;&#30456;&#32467;&#21512;&#65292;&#20026;&#32473;&#23450;&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;$\alpha$&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#35813;&#27979;&#35797;&#24212;&#29992;&#20110;&#20844;&#24320;&#30340;&#20154;&#31867;&#30707;&#22836;-&#21098;&#20992;-&#24067;&#28216;&#25103;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#21253;&#25324;500&#21517;&#20154;&#31867;&#29609;&#23478;&#30340;50&#27425;&#22238;&#21512;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#20551;&#35774;&#27979;&#35797;&#65292;&#35813;&#20551;&#35774;&#26159;&#21442;&#19982;&#32773;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10695v4 Announce Type: replace-cross  Abstract: We present a nonparametric statistical test for determining whether an agent is following a given mixed strategy in a repeated strategic-form game given samples of the agent's play. This involves two components: determining whether the agent's frequencies of pure strategies are sufficiently close to the target frequencies, and determining whether the pure strategies selected are independent between different game iterations. Our integrated test involves applying a chi-squared goodness of fit test for the first component and a generalized Wald-Wolfowitz runs test for the second component. The results from both tests are combined using Bonferroni correction to produce a complete test for a given significance level $\alpha.$ We applied the test to publicly available data of human rock-paper-scissors play. The data consists of 50 iterations of play for 500 human players. We test with a null hypothesis that the players are following
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#26469;&#25552;&#39640;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#30340;&#36710;&#27969;&#37327;&#21644;&#36895;&#24230;&#26465;&#20214;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#36890;&#36807;&#29575;&#24182;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.09436</link><description>&lt;p&gt;
&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#22312;&#31895;&#31890;&#24230;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Temporal Transfer Learning for Traffic Optimization with Coarse-grained Advisory Autonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#26469;&#25552;&#39640;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#30340;&#36710;&#27969;&#37327;&#21644;&#36895;&#24230;&#26465;&#20214;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#36890;&#36807;&#29575;&#24182;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09436v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09436v2 Announce Type: replace  Abstract: The recent development of connected and automated vehicle (CAV) technologies has spurred investigations to optimize dense urban traffic to maximize vehicle speed and throughput. This paper explores advisory autonomy, in which real-time driving advisories are issued to the human drivers, thus achieving near-term performance of automated vehicles. Due to the complexity of traffic systems, recent studies of coordinating CAVs have resorted to leveraging deep reinforcement learning (RL). Coarse-grained advisory is formalized as zero-order holds, and we consider a range of hold duration from 0.1 to 40 seconds. However, despite the similarity of the higher frequency tasks on CAVs, a direct application of deep RL fails to be generalized to advisory autonomy tasks. To overcome this, we utilize zero-shot transfer, training policies on a set of source tasks--specific traffic scenarios with designated hold durations--and then evaluating the effi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#31649;&#29702;&#31574;&#30053;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22914;&#20309;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.01700</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#31649;&#29702;&#27010;&#35272;
&lt;/p&gt;
&lt;p&gt;
Data Management For Training Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#31649;&#29702;&#31574;&#30053;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22914;&#20309;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01700v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01700v3 Announce Type: replace-cross  Abstract: Data plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the underlying mechanism of current prominent practices are still unknown. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey aims to provide a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various aspects of data management strategy design. Looking into the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Unreal Engine&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.04421</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#19968;&#20010;&#38024;&#23545;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39550;&#39542;&#24773;&#26223;
&lt;/p&gt;
&lt;p&gt;
SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.04421
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Unreal Engine&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.04421v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#27773;&#36710;&#39046;&#22495;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25163;&#21183;&#21160;&#20316;&#25968;&#25454;&#24211;&#21487;&#33021;&#20250;&#24456;&#33392;&#38590;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#30001;&#34394;&#25311;3D&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25163;&#21183;&#25968;&#25454;&#30340;&#26500;&#24819;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#25552;&#20379;&#33258;&#23450;&#20041;&#36873;&#39033;&#24182;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#30340;&#21361;&#38505;&#12290;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#22810;&#31181;&#21464;&#20307;&#65292;&#21253;&#25324;&#25163;&#21183;&#36895;&#24230;&#12289;&#34920;&#29616;&#21147;&#21644;&#25163;&#24418;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19981;&#21516;&#20301;&#32622;&#30340;&#25668;&#20687;&#22836;&#21644;&#31867;&#22411;&#30340;&#25668;&#20687;&#22836;&#65292;&#22914;RGB&#12289;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#33719;&#21462;&#36825;&#20123;&#25668;&#20687;&#22836;&#30340;&#39069;&#22806;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#8220;SynthoGestures&#65288;https://github.com/amrgomaaelhady/SynthoGestures&#65289;&#8221;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#31934;&#24230;&#65292;&#24182;&#21487;&#20197;&#20195;&#26367;&#25110;&#34917;&#20805;&#30495;&#23454;&#25163;&#21183;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#65292;&#25105;&#20204;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.04421v2 Announce Type: replace  Abstract: Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures (https://github.com/amrgomaaelhady/SynthoGestures), improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#35268;&#21017;&#22522;&#30784;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#36816;&#21160;&#36712;&#36857;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#12290;</title><link>https://arxiv.org/abs/2308.14250</link><description>&lt;p&gt;
&#35268;&#21017;&#22522;&#30784;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#23454;&#29616;&#36816;&#21160;&#36712;&#36857;&#20998;&#31867;&#30340;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rule-Based Error Detection and Correction to Operationalize Movement Trajectory Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#35268;&#21017;&#22522;&#30784;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#36816;&#21160;&#36712;&#36857;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14250v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#36816;&#21160;&#36712;&#36857;&#30340;&#20998;&#31867;&#22312;&#20132;&#36890;&#36816;&#36755;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#26159;&#22823;&#35268;&#27169;&#36816;&#21160;&#36712;&#36857;&#29983;&#25104;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#28798;&#38590;&#25110;&#20854;&#20182;&#22806;&#37096;&#20914;&#20987;&#21518;&#30340;&#23433;&#20840;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#20381;&#36182;&#20110;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65292;&#24403;&#36712;&#36857;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#36825;&#20250;&#36896;&#25104;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#22522;&#30784;&#26694;&#26550;&#65292;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#65292;&#20197;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#36816;&#21160;&#36712;&#36857;&#24179;&#21488;&#19978;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;&#39640;&#32423;&#25216;&#26415;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#20934;&#30830;&#30340;&#38169;&#35823;&#26816;&#27979;&#33021;&#21147;&#65292;&#20197;&#21450;&#23545;&#21464;&#21270;&#27979;&#35797;&#20998;&#24067;&#30340;&#20934;&#30830;&#24230;&#25913;&#36827;&#65292;&#20197;&#21450;&#23545;&#22522;&#32447;&#29992;&#20363;&#30340;&#20934;&#30830;&#24230;&#25913;&#36827;&#65292;&#20197;&#21450;&#23545;&#31639;&#27861;&#24320;&#21457;&#20855;&#26377;&#29702;&#35770;&#23646;&#24615;&#30340;&#23454;&#39564;&#22871;&#20214;&#12290;&#23588;&#20854;&#26159;&#22312;&#26174;&#31034;&#20102;F1&#35780;&#20998;&#22312;&#27979;&#35797;&#26102;&#30340;&#32489;&#25928;&#20043;&#21518;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;SOTA&#27169;&#22411;&#20013;&#36873;&#25321;&#20102;&#20845;&#20010;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#23454;&#38469;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#35268;&#21017;&#22522;&#30784;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#27861;&#22312;&#25152;&#26377;&#20845;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#30340;F1&#35780;&#20998;&#24179;&#22343;&#25552;&#39640;&#20102;5.45&#20010;&#30334;&#20998;&#28857;&#65292;&#34920;&#26126;&#20102;&#20854;&#22312;&#24212;&#23545;&#21464;&#21270;&#25968;&#25454;&#20998;&#24067;&#21644;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26032;&#30340;SOTA&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#24378;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#36798;&#21040;&#20102;21.62%&#30340;&#24179;&#22343;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;work&#36890;&#36807;&#22312;&#22797;&#26434;&#12289;&#26410;&#32463;&#35757;&#32451;&#30340;&#25968;&#25454;&#19978;&#26174;&#31034;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#36816;&#21160;&#36712;&#36857;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#35777;&#23454;&#20102;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#22522;&#30784;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14250v3 Announce Type: replace-cross  Abstract: Classification of movement trajectories has many applications in transportation and is a key component for large-scale movement trajectory generation and anomaly detection which has key safety applications in the aftermath of a disaster or other external shock. However, the current state-of-the-art (SOTA) are based on supervised deep learning - which leads to challenges when the distribution of trajectories changes due to such a shock. We provide a neuro-symbolic rule-based framework to conduct error correction and detection of these models to integrate into our movement trajectory platform. We provide a suite of experiments on several recent SOTA models where we show highly accurate error detection, the ability to improve accuracy with a changing test distribution, and accuracy improvement for the base use case in addition to a suite of theoretical properties that informed algorithm development. Specifically, we show an F1 sco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#26415;&#35745;&#31639;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#8220;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#8221;&#26426;&#21046;&#36827;&#34892;&#27867;&#21270;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#26415;&#35745;&#31639;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#8220;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#8221;&#26426;&#21046;&#36827;&#34892;&#27867;&#21270;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.01154v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#23545;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28044;&#29616;&#35745;&#31639;&#21644;&#38382;&#39064;&#27714;&#35299;&#33021;&#21147;&#30340;&#26356;&#22909;&#29702;&#35299;&#23545;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#24212;&#29992;&#33539;&#22260;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#35757;&#32451;&#20013;&#29992;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#65292;&#26159;&#22914;&#20309;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#65292;&#36825;&#31181;&#35745;&#31639;&#33021;&#22815;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26500;&#25104;&#20102;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#38656;&#35201;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#37327;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986; discontinuities&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#20851;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#30340;&#25968;&#25454;&#36816;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#65292;&#35753;&#23427;&#23398;&#20250;&#20102;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;&#35813;&#27169;&#22411;&#25193;&#23637;&#30340;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25903;&#25345;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20316;&#26102;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#65292;&#20854;&#20013;&#35745;&#31639;&#22312;&#20540;&#31354;&#38388;&#20013;&#21457;&#29983;&#65292;&#19968;&#26086;&#36755;&#20837;&#34987;&#32534;&#30721;&#65292;&#35821;&#35328;&#27169;&#22411;&#23601;&#21487;&#20197;&#23545;&#31639;&#26415;&#35745;&#31639;&#36827;&#34892;&#39044;&#27979;&#21644;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.01154v4 Announce Type: replace  Abstract: A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARN&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#32467;&#26500;&#24863;&#30693;&#31354;&#38388;&#27880;&#24847;&#23618;&#21644;GRU&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#32858;&#21512;&#25968;&#25454;&#37325;&#24314;&#21040;&#39640;&#20998;&#36776;&#29575;&#30340;&#19981;&#35268;&#21017;&#20998;&#21306;&#12290;</title><link>https://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
SRAN: &#32467;&#26500;&#24863;&#30693;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#31354;&#38388;&#26102;&#38388; disaggregation
&lt;/p&gt;
&lt;p&gt;
SARN: Structurally-Aware Recurrent Network for Spatio-Temporal Disaggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARN&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#32467;&#26500;&#24863;&#30693;&#31354;&#38388;&#27880;&#24847;&#23618;&#21644;GRU&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#32858;&#21512;&#25968;&#25454;&#37325;&#24314;&#21040;&#39640;&#20998;&#36776;&#29575;&#30340;&#19981;&#35268;&#21017;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2306.07292v4 &#21457;&#24067;&#20844;&#21578;&#31867;&#22411;: replace-cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.07292v4 Announce Type: replace-cross  Abstract: Open data is frequently released spatially aggregated, usually to comply with privacy policies. But coarse, heterogeneous aggregations complicate learning and integration for downstream AI/ML systems. In this work, we consider models to disaggregate spatio-temporal data from a low-resolution, irregular partition (e.g., census tract) to a high-resolution, irregular partition (e.g., city block). We propose an overarching model named the Structurally-Aware Recurrent Network (SARN), which integrates structurally-aware spatial attention (SASA) layers into the Gated Recurrent Unit (GRU) model. The spatial attention layers capture spatial interactions among regions, while the gated recurrent module captures the temporal dependencies. Each SASA layer calculates both global and structural attention -- global attention facilitates comprehensive interactions between different geographic levels, while structural attention leverages the con
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Multi-Chain Reasoning&#8221;&#65288;MCR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20803;&#25512;&#29702;&#22810;&#20010;&#24605;&#24819;&#38142;&#26469;&#25913;&#36827;&#22810;&#36339;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#21319;&#26368;&#32456;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#22810;&#38142;&#24605;&#32500;&#20803;&#25512;&#29702;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Multi-Chain Reasoning&#8221;&#65288;MCR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20803;&#25512;&#29702;&#22810;&#20010;&#24605;&#24819;&#38142;&#26469;&#25913;&#36827;&#22810;&#36339;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#21319;&#26368;&#32456;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2304.13007v4 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#29616;&#20195;&#30340;&#22810;&#36339;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#25512;&#29702;&#27493;&#39588;&#65292;&#31216;&#20026;&#8220;&#24605;&#24819;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#36798;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#65292;&#22810;&#20010;&#38142;&#34987;&#25277;&#26679;&#65292;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#26159;&#26412;&#36523;&#25918;&#24323;&#20102;&#23545;&#20013;&#38388;&#27493;&#39588;&#30340;&#32771;&#34385;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#27809;&#26377;&#32771;&#34385;&#19981;&#21516;&#38142;&#20043;&#38388;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20063;&#27809;&#26377;&#20026;&#39044;&#27979;&#30340;&#31572;&#26696;&#25552;&#20379;&#32479;&#19968;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#38142;&#25512;&#29702;&#8221;&#65288;MCR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20803;&#25512;&#29702;&#22810;&#24605;&#24819;&#38142;&#65292;&#32780;&#19981;&#26159;&#23545;&#23427;&#20204;&#30340;&#31572;&#26696;&#36827;&#34892;&#32858;&#21512;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#22312;&#23427;&#20204;&#20043;&#38388;&#28151;&#21512;&#20449;&#24687;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#24182;&#39044;&#27979;&#31572;&#26696;&#26102;&#36873;&#25321;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#24378;&#22522;&#32447;&#36890;&#24120;&#22312;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#32447;&#32034;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#21518;&#32493;&#32447;&#32034;&#30340;&#35201;&#27714;&#36234;&#26469;&#36234;&#20005;&#26684;&#8212;&#8212;&#19982;&#35813;&#30452;&#35273;&#30456;&#37197;&#21512;&#12290;Developer Testing for Mobile Apps: A Case Study on EValuating Security Features
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.13007v4 Announce Type: replace-cross  Abstract: Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35780;&#20272; rubric&#20013;&#33719;&#21462;&#23398;&#20064;&#32773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#31616;&#21270;&#21442;&#25968;&#33719;&#21462;&#65292;&#20197;&#33258;&#21160;&#21270;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#25216;&#33021;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2209.05467</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#32593;&#32476;&#24314;&#27169;&#35780;&#20272; rubrics&#65306;&#19968;&#31181;&#21153;&#23454;&#30340;approach
&lt;/p&gt;
&lt;p&gt;
Modelling Assessment Rubrics through Bayesian Networks: a Pragmatic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.05467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35780;&#20272; rubric&#20013;&#33719;&#21462;&#23398;&#20064;&#32773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#31616;&#21270;&#21442;&#25968;&#33719;&#21462;&#65292;&#20197;&#33258;&#21160;&#21270;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#25216;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05467v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#32773;&#25216;&#33021;&#33258;&#21160;&#35780;&#20272;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#35780;&#20272; rubric&#36890;&#24120;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#30456;&#20851;&#25216;&#33021;&#27700;&#24179;&#21644;&#36164;&#36136;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#26576;&#20123;&#23618;&#27425;&#65288;&#37096;&#20998;&#65289;&#39034;&#24207;&#23450;&#20041;&#35780;&#20272; rubric&#20013;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#23398;&#20064;&#32773;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#36923;&#36753;&#38376;&#65288;&#24120;&#31216;&#20026;&#8220;&#28151;&#28102;&#38376;&#8221;&#65289;&#26469;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20197;&#20415;&#36890;&#36807;&#19987;&#23478;&#31616;&#21270;&#21442;&#25968;&#30340;&#33719;&#21462;&#65292;&#24182;&#22312;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#22914;&#20309;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#23545;&#35745;&#31639;&#26426;&#24605;&#32500;&#25216;&#33021;&#30340;&#27979;&#35797;&#20219;&#21153;&#30340;&#35780;&#20272;&#30340;&#12290;&#20174;&#35780;&#20272; rubric&#20013;&#31616;&#21270;&#27169;&#22411;&#30340;&#33719;&#21462;&#26041;&#24335;&#65292;&#20026;&#24555;&#36895;&#33258;&#21160;&#21270;&#22810;&#31181;&#20219;&#21153;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05467v3 Announce Type: replace-cross  Abstract: Automatic assessment of learner competencies is a fundamental task in intelligent tutoring systems. An assessment rubric typically and effectively describes relevant competencies and competence levels. This paper presents an approach to deriving a learner model directly from an assessment rubric defining some (partial) ordering of competence levels. The model is based on Bayesian networks and exploits logical gates with uncertainty (often referred to as noisy gates) to reduce the number of parameters of the model, so to simplify their elicitation by experts and allow real-time inference in intelligent tutoring systems. We illustrate how the approach can be applied to automatize the human assessment of an activity developed for testing computational thinking skills. The simple elicitation of the model starting from the assessment rubric opens up the possibility of quickly automating the assessment of several tasks, making them m
&lt;/p&gt;</description></item></channel></rss>