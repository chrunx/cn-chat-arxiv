<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#25552;&#31034;&#38598;&#24182;&#35843;&#25972;&#25552;&#31034;&#35843;&#29992;&#65292;&#20197;&#20248;&#21270;&#25552;&#31034;&#20351;&#29992;&#25928;&#29575;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#26631;&#20934;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01423</link><description>&lt;p&gt;
&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#65306;&#22312;LLM&#33258;&#21160;&#25552;&#31034;&#20013;&#20855;&#26377;&#33258;&#36866;&#24212;&#22686;&#38271;&#30340;&#29983;&#21629;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#25552;&#31034;&#38598;&#24182;&#35843;&#25972;&#25552;&#31034;&#35843;&#29992;&#65292;&#20197;&#20248;&#21270;&#25552;&#31034;&#20351;&#29992;&#25928;&#29575;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#26631;&#20934;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01423v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#25191;&#34892;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#20219;&#21153;&#65292;&#20854;&#20013;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#26174;&#33879;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#26412;&#36523;&#23384;&#22312;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#26377;&#20004;&#31181;&#65306;&#31532;&#19968;&#31181;&#65292;&#20363;&#22914;&#38142;&#24335;&#24605;&#24819;&#65288;CoT&#65289;&#65292;&#28041;&#21450;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#65292;&#22240;&#27492;&#31216;&#20026;&#19987;&#23478;&#35774;&#35745;&#25552;&#31034;&#65288;EDPs&#65289;&#12290;&#19968;&#26086;&#36825;&#20123;&#25552;&#31034;&#30830;&#31435;&#65292;&#23427;&#20204;&#23601;&#26159;&#19981;&#21487;&#25913;&#21464;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25928;&#26524;&#19978;&#38480;&#30001;&#20154;&#31867;&#35774;&#35745;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#20915;&#23450;&#12290;&#24403;&#23558;&#36825;&#20123;&#38745;&#24577;EDPs&#24212;&#29992;&#20110;LLMs&#26102;&#65292;&#23545;&#20110;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#30340;&#31616;&#21333;&#21644;&#22797;&#26434;&#38382;&#39064;&#65292;&#37117;&#20250;&#37319;&#21462;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#25991;&#26412;&#27169;&#24335;&#23545;&#31616;&#21333;&#38382;&#39064;&#30340;&#20351;&#29992;&#25928;&#29575;&#20302;&#19979;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#28041;&#21450;&#30001;LLM&#33258;&#20027;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#27492;&#31867;&#25552;&#31034;&#34987;&#31216;&#20026;&#33258;&#21161;&#24335;&#29983;&#25104;&#25552;&#31034;&#65288;AGPs&#65289;&#12290;&#20256;&#32479;&#30340;AGPs&#36890;&#36807;&#33258;&#36866;&#24212;&#24615;&#32500;&#25345;LLMs&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#28789;&#27963;&#24615;&#20173;&#28982;&#21463;&#21040;&#23616;&#38480;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#26631;&#20934;&#27169;&#24335;&#20559;&#24046;&#30340;&#32858;&#31867;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;AGPs&#19981;&#21306;&#20998;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#23548;&#33268;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#25552;&#31034;&#38271;&#24230;&#27604;&#23454;&#38469;&#38656;&#27714;&#35201;&#38271;&#65292;&#32780;&#22312;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#21457;&#25381;&#20316;&#29992;&#36739;&#24369;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36882;&#24402;&#25552;&#31034;&#25628;&#32034;&#65288;RRS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#35843;&#29992;&#26469;&#36866;&#24212;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#25552;&#31034;&#38598;&#12290;&#36882;&#24402;&#25628;&#32034;&#19968;&#26041;&#38754;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#21442;&#25968;&#65292;&#25552;&#20379;&#23545;LLM&#23616;&#37096;&#33021;&#21147;&#22686;&#24378;&#30340;&#21160;&#24577;&#25552;&#31034;&#26426;&#21046;&#12290;&#36825;&#31181;&#26426;&#21046;&#30340;&#20316;&#29992;&#26159;&#20248;&#21270;&#25552;&#31034;&#20351;&#29992;&#25928;&#29575;&#65292;&#24182;&#22686;&#21152;LLM&#22312;&#38750;&#26631;&#20934;&#38382;&#39064;&#19978;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RRS&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#35777;&#26126;&#20854;&#22312;&#31616;&#21270;&#22797;&#26434;&#30340;&#38382;&#39064;&#22788;&#29702;&#21644;&#24378;&#21270;&#23545;&#38750;&#26631;&#20934;&#38382;&#39064;&#30340;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#19987;&#23478;&#35774;&#35745;&#25552;&#31034;&#21644;&#33258;&#21161;&#24335;&#29983;&#25104;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01423v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit remarkable proficiency in addressing a diverse array of tasks within the Natural Language Processing (NLP) domain, with various prompt design strategies significantly augmenting their capabilities. However, these prompts, while beneficial, each possess inherent limitations. The primary prompt design methodologies are twofold: The first, exemplified by the Chain of Thought (CoT), involves manually crafting prompts specific to individual datasets, hence termed Expert-Designed Prompts (EDPs). Once these prompts are established, they are unalterable, and their effectiveness is capped by the expertise of the human designers. When applied to LLMs, the static nature of EDPs results in a uniform approach to both simple and complex problems within the same dataset, leading to the inefficient use of tokens for straightforward issues. The second method involves prompts autonomously generated by the LLM, known 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#20559;&#22909;&#23545;&#40784;&#31574;&#30053;&#19979;&#20173;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#29702;&#35299;&#21644;&#36991;&#20813;&#36825;&#31181;&#8220;&#36867;&#36920;&#8221;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01420</link><description>&lt;p&gt;
&#12298;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#65306;&#20174;&#32479;&#35745;&#35270;&#35282;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36867;&#36920;&#12299;
&lt;/p&gt;
&lt;p&gt;
Mission Impossible: A Statistical Perspective on Jailbreaking LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#20559;&#22909;&#23545;&#40784;&#31574;&#30053;&#19979;&#20173;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#29702;&#35299;&#21644;&#36991;&#20813;&#36825;&#31181;&#8220;&#36867;&#36920;&#8221;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01420v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#21313;&#23383;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26377;&#38480;&#30340;&#36136;&#25511;&#26465;&#20214;&#19979;&#25509;&#21463;&#20102;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#24847;&#26009;&#20043;&#22806;&#30340;&#25110;&#29978;&#33267;&#26159;&#26377;&#23475;&#30340;&#34892;&#20026;&#65292;&#20363;&#22914;&#27844;&#38706;&#20449;&#24687;&#12289;&#25955;&#25773;&#34394;&#20551;&#26032;&#38395;&#25110;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#12290;&#25269;&#24481;&#25514;&#26045;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#20559;&#22909;&#23545;&#40784;&#65292;&#21253;&#25324;&#20351;&#29992;&#31934;&#24515;&#32534;&#20889;&#30340;&#25991;&#26412;&#23454;&#20363;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#23454;&#20363;&#20307;&#29616;&#20102;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#21363;&#20351;&#36825;&#26679;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#20559;&#22909;&#23545;&#40784;&#30340;LLMs&#21487;&#33021;&#20250;&#34987;&#24341;&#35825;&#20174;&#20107;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36867;&#36920;&#29616;&#35937;&#36890;&#24120;&#26159;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#30340;&#25552;&#31034;&#25991;&#26412;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#31181;&#25552;&#31034;&#25991;&#26412;&#20855;&#26377;&#23545;LLM&#30340;&#24694;&#24847;&#30340;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20174;&#32479;&#35745;&#23398;&#35270;&#35282;&#20026;&#20559;&#22909;&#23545;&#40784;&#21644;&#36867;&#36920;&#29616;&#35937;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#12290;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#39318;&#20808;&#35777;&#26126;&#20102;&#22914;&#26524;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26377;&#23475;&#34892;&#20026;&#65292;&#39044;&#35757;&#32451;&#30340;LLMs&#23558;&#27169;&#20223;&#36825;&#20123;&#34892;&#20026;&#12290;&#22522;&#20110;&#21516;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#40784;&#34892;&#20026;&#30340;&#32479;&#35745;&#27010;&#24565;&#65292;&#24182;&#23545;&#36867;&#36920;&#34892;&#20026;&#36827;&#34892;&#20102;&#19979;&#30028;&#20272;&#35745;&#12290;&#22312;&#38754;&#23545;&#23545;&#40784;&#31574;&#30053;&#30340;&#28508;&#22312;&#24369;&#28857;&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#32479;&#35745;&#26041;&#27861;&#24050;&#32463;&#21487;&#20197;&#24456;&#22909;&#22320;&#29702;&#35299;&#24182;&#19988;&#23581;&#35797;&#36991;&#20813;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36825;&#20123;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01420v1 Announce Type: cross  Abstract: Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jai
&lt;/p&gt;</description></item><item><title>"&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32570;&#20047;&#33258;&#36866;&#24212;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#27807;&#36890;&#25928;&#29575;&#25552;&#39640;&#30340;&#20851;&#38190;&#12290;"</title><link>https://arxiv.org/abs/2408.01417</link><description>&lt;p&gt;
"&#23569;&#35828;&#35805;&#65292;&#22810;&#20114;&#21160;&#65306;&#22312;&#22810;&#27169;&#24577;LLM&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#23545;&#35805;&#36866;&#24212;&#24615;"
&lt;/p&gt;
&lt;p&gt;
Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01417
&lt;/p&gt;
&lt;p&gt;
"&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32570;&#20047;&#33258;&#36866;&#24212;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#27807;&#36890;&#25928;&#29575;&#25552;&#39640;&#30340;&#20851;&#38190;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#33258;&#21457;&#22320;&#20351;&#29992;&#36234;&#26469;&#36234;&#39640;&#25928;&#30340;&#35328;&#35821;&#65292;&#36890;&#36807;&#35843;&#25972;&#21644;&#24418;&#25104;&#21363;&#20852;&#32422;&#23450;&#12290;&#36825;&#31181;&#29616;&#35937;&#24050;&#32463;&#22312;&#21442;&#32771;&#28216;&#25103;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#35821;&#35328;&#30340;&#19968;&#20123;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#36229;&#20986;&#20102;&#20256;&#36798;&#24847;&#22270;&#30340;&#33539;&#22260;&#12290;&#33267;&#20170;&#23578;&#26410;&#25506;&#35752;&#30340;&#26159;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#26159;&#21542;&#20250;&#20687;&#20154;&#31867;&#19968;&#26679;&#22312;&#20114;&#21160;&#20013;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#37319;&#29992;&#21738;&#20123;&#26426;&#21046;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;ICCA&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#20114;&#21160;&#20013;&#30340;&#23545;&#35805;&#36866;&#24212;&#24615;&#20316;&#20026;&#19968;&#31181;&#20869;&#22312;&#34892;&#20026;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#29702;&#35299;&#20182;&#20204;&#30340;&#23545;&#35805;&#32773;&#30340;&#35328;&#35821;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#25928;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20687;&#20154;&#31867;&#37027;&#26679;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#33258;&#21457;&#22320;&#20351;&#33258;&#24049;&#30340;&#35821;&#35328;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#21518;&#32773;&#33021;&#21147;&#21482;&#33021;&#22312;&#26576;&#20123;&#27169;&#22411;(&#20363;&#22914;GPT-4)&#20013;&#36890;&#36807;&#34987;&#21160;&#30340;&#25552;&#31034;&#26041;&#24335;&#28608;&#21457;&#20986;&#26469;&#12290;&#36825;&#34920;&#26126;&#20102;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#29305;&#24615;&#36824;&#27809;&#26377;&#23436;&#20840;&#34987;&#24403;&#21069;&#30340;&#27169;&#22411;&#25152;&#25484;&#25569;&#12290;"
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01417v1 Announce Type: cross  Abstract: Humans spontaneously use increasingly efficient language as interactions progress, by adapting and forming ad-hoc conventions. This phenomenon has been studied extensively using reference games, showing properties of human language that go beyond relaying intents. It remains unexplored whether multimodal large language models (MLLMs) similarly increase communication efficiency during interactions, and what mechanisms they may adopt for this purpose. We introduce ICCA, an automated framework to evaluate such conversational adaptation as an in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and observe that while they may understand the increasingly efficient language of their interlocutor, they do not spontaneously make their own language more efficient over time. This latter ability can only be elicited in some models (e.g., GPT-4) with heavy-handed prompting. This shows that this property of linguistic interacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#35270;&#35282;&#65292;&#20998;&#31867;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#35843;&#35299;&#32773;&#31867;&#22411;&#12289;&#25628;&#32034;&#26041;&#27861;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#26356;&#22909;&#29702;&#35299;&#20854;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#26102;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01416</link><description>&lt;p&gt;
&#23547;&#25214;&#21512;&#36866;&#30340;&#35843;&#35299;&#32773;&#65306;&#22240;&#26524;&#35299;&#37322;&#24615;&#30740;&#31350;&#30340;&#21382;&#21490;&#12289;&#32508;&#36848;&#21644;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#35270;&#35282;&#65292;&#20998;&#31867;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#35843;&#35299;&#32773;&#31867;&#22411;&#12289;&#25628;&#32034;&#26041;&#27861;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#26356;&#22909;&#29702;&#35299;&#20854;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#26102;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01416v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#35299;&#37322;&#24615;&#20026;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20197;&#21450;&#20026;&#20160;&#20040;&#22312;&#29305;&#23450;&#26041;&#24335;&#19979;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#32479;&#19968;&#24615;&#65306;&#22823;&#22810;&#25968;&#30740;&#31350;&#37319;&#29992;&#19987;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#20849;&#20139;&#29702;&#35770;&#22522;&#30784;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#34913;&#37327;&#36827;&#23637;&#24182;&#19982;&#19981;&#21516;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26426;&#21046;&#29702;&#35299;&#32463;&#24120;&#34987;&#35752;&#35770;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#30340;&#22522;&#30784;&#22240;&#26524;&#21333;&#20301;&#36890;&#24120;&#27809;&#26377;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#35266;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35299;&#37322;&#24615;&#30740;&#31350;&#30340;&#21382;&#21490;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#36825;&#20123;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;&#30340;&#22240;&#26524;&#21333;&#20301;&#65288;&#35843;&#35299;&#32773;&#65289;&#30340;&#31867;&#22411;&#20197;&#21450;&#29992;&#20110;&#25628;&#32034;&#35843;&#35299;&#32773;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27599;&#31181;&#35843;&#35299;&#32773;&#30340;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#35828;&#26126;&#22312;&#26681;&#25454;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#20102;&#35299;&#21644;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20309;&#26102;&#26368;&#36866;&#24212;&#29992;&#29305;&#23450;&#31867;&#22411;&#30340;&#35843;&#35299;&#32773;&#21644;&#25628;&#32034;&#26041;&#27861;&#21462;&#20915;&#20110;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#12289;&#24037;&#20316;&#26041;&#24335;&#21450;&#20854;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01416v1 Announce Type: cross  Abstract: Interpretability provides a toolset for understanding how and why neural networks behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this paper, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate depending on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#26465;&#20214;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;LoRA&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#20102;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01415</link><description>&lt;p&gt;
&#26465;&#20214;LoRA&#21442;&#25968;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Conditional LoRA Parameter Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#26465;&#20214;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;LoRA&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#20102;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COND P-DIFF&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20102;&#23545;&#29305;&#23450;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#39640;&#24615;&#33021;LoRA&#65288;&#20302;&#31209;&#36866;&#24212;&#65289;&#21442;&#25968;&#36827;&#34892;&#25511;&#21046;&#21487;&#29983;&#25104;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#21462;&#21442;&#25968;&#30340;&#25928;&#29575;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#26465;&#20214;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#20855;&#26377;&#39640;&#34920;&#29616;&#21147;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26080;&#35770;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#36824;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23454;&#39564;&#32467;&#26524;&#22343;&#19968;&#33268;&#34920;&#26126;COND P-DIFF&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#39640;&#25928;&#29575;&#21442;&#25968;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01415v1 Announce Type: new  Abstract: Generative models have achieved remarkable success in image, video, and text domains. Inspired by this, researchers have explored utilizing generative models to generate neural network parameters. However, these efforts have been limited by the parameter size and the practicality of generating high-performance parameters. In this paper, we propose COND P-DIFF, a novel approach that demonstrates the feasibility of controllable high-performance parameter generation, particularly for LoRA (Low-Rank Adaptation) weights, during the fine-tuning process. Specifically, we employ an autoencoder to extract efficient latent representations for parameters. We then train a conditional latent diffusion model to synthesize high-performing model parameters from random noise based on specific task conditions. Experimental results in both computer vision and natural language processing domains consistently demonstrate that COND P-DIFF can generate high-pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;PC&#178;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20266;&#20998;&#31867;&#21644;&#20266;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#26469;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01349</link><description>&lt;p&gt;
PC&#178;&#65306;&#22522;&#20110;&#20266;&#20998;&#31867;&#30340;&#20266;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;PC&#178;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20266;&#20998;&#31867;&#21644;&#20266;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#26469;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#30340;&#26080;&#32541;&#38598;&#25104;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22122;&#22768;&#23545;&#24212;&#23398;&#20064;(NCL)&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#22122;&#22768;&#36890;&#24120;&#28304;&#20110;&#25968;&#25454;&#23545;&#30340;&#38169;&#37197;&#65292;&#36825;&#26159;&#19982;&#20256;&#32479;&#26377;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#30456;&#27604;&#30340;&#19968;&#20010;&#26174;&#33879;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20266;&#20998;&#31867;&#30340;&#20266;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;(PC&#178;)&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;PC&#178;&#25552;&#20379;&#20102;&#19968;&#20010;&#19977;&#37325;&#31574;&#30053;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#8220;&#20266;&#20998;&#31867;&#8221;&#20219;&#21153;&#65292;&#23558;&#25551;&#36848;&#35299;&#37322;&#20026;&#20998;&#31867;&#26631;&#31614;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#26426;&#21046;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#22270;&#20687;-&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;PC&#178;&#30340;&#20266;&#20998;&#31867;&#33021;&#21147;&#65292;&#29983;&#25104;&#20266;&#25551;&#36848;&#65292;&#20026;&#27599;&#20010;&#38169;&#37197;&#23545;&#25552;&#20379;&#26356;&#20855;&#26377;&#20449;&#24687;&#21644;&#30452;&#35266;&#30340;&#30417;&#30563;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20266;&#25551;&#36848;&#30340;&#25391;&#33633;&#26469;&#36827;&#19968;&#27493;&#22686;&#21152;&#25439;&#22833;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#21319;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21516;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01349v1 Announce Type: cross  Abstract: In the realm of cross-modal retrieval, seamlessly integrating diverse modalities within multimedia remains a formidable challenge, especially given the complexities introduced by noisy correspondence learning (NCL). Such noise often stems from mismatched data pairs, which is a significant obstacle distinct from traditional noisy labels. This paper introduces Pseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address this challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an auxiliary "pseudo-classification" task that interprets captions as categorical labels, steering the model to learn image-text semantic similarity through a non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques, capitalizing on PC$^2$'s pseudo-classification capability, we generate pseudo-captions to provide more informative and tangible supervision for each mismatched pair. Thirdly, the oscillation of pse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#23427;&#33021;&#22815;&#23558;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#26377;&#25928;&#34701;&#21512;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01343</link><description>&lt;p&gt;
StitchFusion: &#19968;&#31181;&#34701;&#21512;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#23427;&#33021;&#22815;&#23558;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#26377;&#25928;&#34701;&#21512;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01343v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#21253;&#21547;&#19987;&#38376;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#36825;&#20123;&#27169;&#22359;&#29305;&#22320;&#20026;&#29305;&#23450;&#30340;&#27169;&#24577;&#35774;&#35745;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#36755;&#20837;&#30340;&#28789;&#27963;&#24615;&#21644;&#22686;&#21152;&#20102;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#19988;&#26377;&#25928;&#30340;&#22823;&#27169;&#34701;&#21512;&#26694;&#26550;StitchFusion&#65292;&#35813;&#26694;&#26550;&#30452;&#25509;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#29305;&#24449;&#34701;&#21512;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#34701;&#21512;&#65292;&#36890;&#36807;&#20849;&#20139;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20449;&#24687;&#27969;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#30340;&#21452;&#21521;&#20256;&#36755;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#35270;&#35273;&#27169;&#24577;&#30340;&#36755;&#20837;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#34701;&#21512;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#24577;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;&#36866;&#37197;&#22120;&#27169;&#22359;&#65288;MultiAdapter&#65289;&#65292;&#20197;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#21033;&#29992;MultiAdapter&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#26377;&#25928;&#34701;&#21512;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01343v1 Announce Type: new  Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#23478;&#31995;&#32479;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#20132;&#20114;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01342</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Leveraging Knowledge Graph Embedding for Effective Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#23478;&#31995;&#32479;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#20132;&#20114;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01342v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26368;&#36817;&#65292;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#65292;&#23427;&#32467;&#21512;&#20102;&#23545;&#35805;&#31995;&#32479;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#27604;&#65292;CRS&#36890;&#36807;&#20114;&#21160;&#65288;&#21363;&#23545;&#35805;&#65289;&#26356;&#22909;&#22320;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CRS&#30740;&#31350;&#24573;&#35270;&#20102;&#26377;&#25928;&#22788;&#29702;&#23646;&#24615;&#12289;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25552;&#20986;&#19981;&#24403;&#30340;&#38382;&#39064;&#21644;&#25512;&#33616;&#19981;&#24403;&#30340;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;&#31216;&#20026;KG-CRS&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#29992;&#25143;-&#39033;&#30446;&#22270;&#21644;&#39033;&#30446;-&#23646;&#24615;&#22270;&#25972;&#21512;&#21040;&#19968;&#20010;&#21160;&#24577;&#22270;&#20013;&#65292;&#21363;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#21160;&#24577;&#21464;&#21270;&#65292;&#36890;&#36807;&#31227;&#38500;&#36127;&#38754;&#39033;&#30446;&#25110;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#37051;&#25509;&#33410;&#28857;&#20043;&#38388;&#30340;&#20256;&#25773;&#65292;&#23398;&#20064;&#20102;&#29992;&#25143;&#12289;&#39033;&#30446;&#21644;&#23646;&#24615;&#30340;&#20449;&#24687;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#23478;&#31995;&#32479;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#29983;&#25104;&#20505;&#36873;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#35821;&#35328;&#30340;&#34920;&#31034;&#26469;&#35299;&#37322;&#29992;&#25143;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#20132;&#20114;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#21160;&#24577;&#22270;&#23884;&#20837;&#26041;&#27861;&#22312;&#23545;&#35805;&#25512;&#33616;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#25512;&#33616;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;KG-CRS&#22312;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#20919;&#21551;&#21160;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#23545;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#37117;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01342v1 Announce Type: cross  Abstract: Conversational recommender system (CRS), which combines the techniques of dialogue system and recommender system, has obtained increasing interest recently. In contrast to traditional recommender system, it learns the user preference better through interactions (i.e. conversations), and then further boosts the recommendation performance. However, existing studies on CRS ignore to address the relationship among attributes, users, and items effectively, which might lead to inappropriate questions and inaccurate recommendations. In this view, we propose a knowledge graph based conversational recommender system (referred as KG-CRS). Specifically, we first integrate the user-item graph and item-attribute graph into a dynamic graph, i.e., dynamically changing during the dialogue process by removing negative items or attributes. We then learn informative embedding of users, items, and attributes by also considering propagation through neighbo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01334</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#38271;&#26399;&#20219;&#21153;&#29702;&#35299;&#30340;&#39592;&#24178;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Backbone for Long-Horizon Robot Task Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#32763;&#35793;&#25688;&#35201;: &#31471;&#21040;&#31471;&#26426;&#22120;&#20154;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#20219;&#21153;&#39046;&#22495;&#65292;&#24120;&#24120;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#32467;&#26524;&#21644;&#19981;&#33391;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Therblig&#30340;&#26694;&#26550;&#65292;&#21363;TBBF (Therblig-based Backbone Framework)&#65292;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;&#21644;&#36716;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#23558;&#39640;&#32423;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#35299;&#20026;&#22522;&#26412;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;&#20351;&#29992;therbligs&#65288;&#22522;&#26412;&#21160;&#20316;&#20803;&#32032;&#65289;&#20316;&#20026;&#25903;&#25745;&#65292;&#24182;&#19982;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#29702;&#35299;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#12290;&#22312;&#31163;&#32447;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Meta-RGate SynerFusion (MGSF)&#32593;&#32476;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#21508;&#31181;&#20219;&#21153;&#30340;therbligs&#12290;&#22312;&#32447;&#27979;&#35797;&#38454;&#27573;&#65292;&#22312;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#28436;&#31034;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;MGSF&#32593;&#32476;&#25552;&#21462;&#39640;&#38454;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;ActionREG&#65288;&#21160;&#20316;&#27880;&#20876;&#65289;&#23558;&#20854;&#32534;&#30721;&#25104;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;Meta-Learner&#65292;&#23427;&#21487;&#20197;&#20174;&#21333;&#20010;&#20219;&#21153;&#30340;&#34920;&#29616;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#27867;&#21270;&#21040;&#19981;&#21516;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#20013;&#36827;&#34892;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#25552;&#21319;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01334v1 Announce Type: new  Abstract: End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Additionally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;en_tldr: This paper provides a comprehensive review of the performance and challenges of multimodal large language models across various tasks, suggesting future research directions.</title><link>https://arxiv.org/abs/2408.01319</link><description>&lt;p&gt;
&#20840;&#38754;&#22238;&#39038;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;en_tldr: This paper provides a comprehensive review of the performance and challenges of multimodal large language models across various tasks, suggesting future research directions.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25968;&#25454;&#29190;&#28856;&#21644;&#31185;&#25216;&#24555;&#36895;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22788;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21069;&#27839;&#12290;&#36825;&#31181;&#27169;&#22411;&#35774;&#35745;&#29992;&#20110;&#26080;&#32541;&#25972;&#21512;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#24207;&#21015;&#65292;&#36828;&#36229;&#21333;&#19968;&#27169;&#24335;&#31995;&#32479;&#30340;&#22797;&#26434;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;MLLM&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#26803;&#29702;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#35782;&#21035;&#21644;&#38899;&#39057;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19981;&#21516;MLLM&#22312;&#29305;&#23450;&#20219;&#21153;&#20043;&#38388;&#30340;&#37325;&#28857;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;MLLM&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#36825;&#20123;&#35752;&#35770;&#65292;&#26412;&#25991;&#24076;&#26395;&#33021;&#22815;&#20026;MLLM&#30340;&#26410;&#26469;&#21457;&#23637;&#21644;&#24212;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01319v1 Announce Type: new  Abstract: In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#35780;&#20272;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#31639;&#27861;&#24605;&#32500;&#33021;&#21147;&#30340;&#34394;&#25311;CAT&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24179;&#21488;&#38477;&#20302;&#20102;&#20154;&#21147;&#25104;&#26412;&#24182;&#19988;&#25552;&#20379;&#20102;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2408.01263</link><description>&lt;p&gt;
&#34394;&#25311;CAT&#65306;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#31639;&#27861;&#24605;&#32500;&#35780;&#20272;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
The virtual CAT: A tool for algorithmic thinking assessment in Swiss compulsory education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#35780;&#20272;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#31639;&#27861;&#24605;&#32500;&#33021;&#21147;&#30340;&#34394;&#25311;CAT&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24179;&#21488;&#38477;&#20302;&#20102;&#20154;&#21147;&#25104;&#26412;&#24182;&#19988;&#25552;&#20379;&#20102;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01263v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#65306;&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#25484;&#25569;&#31639;&#27861;&#24605;&#32500;&#65288;AT&#65289;&#25216;&#33021;&#19981;&#20165;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#30456;&#20851;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20010;&#20154;&#33021;&#22815;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#21487;&#31649;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#34892;&#21160;&#24207;&#21015;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#25945;&#32946;&#29615;&#22659;&#20013;&#31639;&#27861;&#24605;&#32500;&#35780;&#20272;&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#22788;&#29702;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#25311;Cross Array Task&#65288;CAT&#65289;&#30340;&#25968;&#23383;&#35780;&#20272;&#24037;&#20855;&#65292;&#23427;&#26159;&#23545;&#20256;&#32479;&#26080;&#25554;&#30005;&#35780;&#20272;&#27963;&#21160;&#30340;&#25913;&#36827;&#35774;&#35745;&#65292;&#26088;&#22312;&#35780;&#20272;&#29790;&#22763;&#20041;&#21153;&#25945;&#32946;&#20013;&#30340;&#31639;&#27861;&#33021;&#21147;&#12290;&#36825;&#27454;&#24037;&#20855;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#21644;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#20154;&#21147;&#21442;&#19982;&#24182;&#32531;&#35299;&#20102;&#28508;&#22312;&#30340;&#25968;&#25454;&#25910;&#38598;&#38169;&#35823;&#12290;&#35813;&#24179;&#21488;&#25317;&#26377;&#22522;&#20110;&#25163;&#21183;&#30340;&#32534;&#31243;&#30028;&#38754;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#31215;&#26408;&#32534;&#31243;&#25509;&#21475;&#65292;&#30830;&#20445;&#20102;&#23545;&#19981;&#21516;&#23398;&#20064;&#32773;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25903;&#25345;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#34394;&#25311;CAT&#24179;&#21488;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#35797;&#28857;&#35780;&#20272;&#22312;&#29790;&#22763;&#25490;&#21517;&#21069;100&#30340;&#39640;&#20013;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#24179;&#21488;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#23398;&#29983;&#30340;&#31639;&#27861;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01263v1 Announce Type: cross  Abstract: In today's digital era, holding algorithmic thinking (AT) skills is crucial, not only in computer science-related fields. These abilities enable individuals to break down complex problems into more manageable steps and create a sequence of actions to solve them. To address the increasing demand for AT assessments in educational settings and the limitations of current methods, this paper introduces the virtual Cross Array Task (CAT), a digital adaptation of an unplugged assessment activity designed to evaluate algorithmic skills in Swiss compulsory education. This tool offers scalable and automated assessment, reducing human involvement and mitigating potential data collection errors. The platform features gesture-based and visual block-based programming interfaces, ensuring its usability for diverse learners, further supported by multilingual capabilities. To evaluate the virtual CAT platform, we conducted a pilot evaluation in Switzer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20195;&#29702;&#20154;&#19981;&#23436;&#20840;&#20102;&#35299;&#25152;&#38754;&#20020;&#30340;MDP&#30340;&#27010;&#29575;&#20998;&#24067;&#26102;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#20248;&#21270;&#25512;&#29702;&#36807;&#31243;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#20154;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.01253</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#30340;&#20803;&#25512;&#29702;&#65306;&#22522;&#20110;BAMDP&#26694;&#26550;&#30340;&#20803;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Metareasoning in uncertain environments: a meta-BAMDP framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20195;&#29702;&#20154;&#19981;&#23436;&#20840;&#20102;&#35299;&#25152;&#38754;&#20020;&#30340;MDP&#30340;&#27010;&#29575;&#20998;&#24067;&#26102;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#20248;&#21270;&#25512;&#29702;&#36807;&#31243;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#20154;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01253v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#20915;&#31574;&#21046;&#23450;&#22330;&#26223;&#20013;&#65292;&#21487;&#20197;&#35748;&#20026;&#8220;&#25512;&#29702;&#8221;&#26159;&#19968;&#31181;&#31639;&#27861;$P$&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;$a^* \in \mathcal{A}$&#65292;&#26088;&#22312;&#20248;&#21270;&#19968;&#20123;&#32467;&#26524;&#65292;&#22914;&#26368;&#22823;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25191;&#34892;$P$&#26412;&#36523;&#21487;&#33021;&#28041;&#21450;&#21040;&#19968;&#20123;&#25104;&#26412;&#65288;&#26102;&#38388;&#12289;&#33021;&#37327;&#12289;&#26377;&#38480;&#30340;&#33021;&#21147;&#31561;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#32771;&#34385;&#19982;&#25191;&#34892;&#36873;&#25321;&#22312;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#30452;&#25509;&#33719;&#24471;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#36825;&#26679;&#30340;&#25104;&#26412;&#38656;&#35201;&#22312;&#20934;&#30830;&#24314;&#27169;&#20154;&#31867;&#34892;&#20026;&#20197;&#21450;&#20248;&#21270;&#20154;&#24037;A&#35745;&#21010;&#26102;&#34987;&#32771;&#34385;&#36827;&#21435;&#65292;&#22240;&#20026;&#25152;&#26377;&#29289;&#29702;&#31995;&#32479;&#37117;&#38754;&#20020;&#30528;&#36164;&#28304;&#38480;&#21046;&#12290;&#25214;&#21040;&#27491;&#30830;&#30340;$P$&#26412;&#36523;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;$P$&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#34987;&#31216;&#20316;&#8220;&#20803;&#25512;&#29702;&#8221;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#20803;&#25512;&#29702;&#27169;&#22411;&#20551;&#35774;&#20195;&#29702;&#30693;&#36947;&#24213;&#23618;MDP&#30340;&#36716;&#31227;&#21644;&#22870;&#21169;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20195;&#29702;&#20154;&#23545;&#25152;&#38754;&#20020;&#30340;MDP&#30340;&#19968;&#38454;&#27010;&#29575;&#20998;&#24067;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#20248;&#21270;&#25512;&#29702;&#36807;&#31243;&#30340;&#36873;&#25321;&#12290;&#35813;&#26694;&#26550;&#20197;&#36125;&#21494;&#26031;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(BAMDP)&#20026;&#22522;&#30784;&#65292;&#20294;&#21435;&#25481;&#20102;&#20195;&#29702;&#20154;&#23545;MDP&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#21069;&#25552;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;\textit{meta-BAMDP}&#65288;&#20803;BAMDP&#65289;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#30340;&#20248;&#21270;&#20026;&#20195;&#29702;&#20154;&#25552;&#20379;&#20102;&#25191;&#34892;&#31574;&#30053;&#65292;&#35813;&#25191;&#34892;&#31574;&#30053;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#20102;&#35299;&#29615;&#22659;&#27010;&#29575;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01253v1 Announce Type: new  Abstract: In decision-making scenarios, \textit{reasoning} can be viewed as an algorithm $P$ that makes a choice of an action $a^* \in \mathcal{A}$, aiming to optimize some outcome such as maximizing the value function of a Markov decision process (MDP). However, executing $P$ itself may bear some costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making the choice in the underlying decision problem. Such costs need to be taken into account in order to accurately model human behavior, as well as optimizing AI planning, as all physical systems are bound to face resource constraints. Finding the right $P$ can itself be framed as an optimization problem over the space of reasoning processes $P$, generally referred to as \textit{metareasoning}. Conventionally, human metareasoning models assume that the agent knows the transition and reward distributions of the underlying MDP. This paper gener
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#21464;&#36164;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;IRS&#21644;UAV&#36741;&#21161;&#30340;MEC&#31995;&#32479;&#20013;&#30340;&#33021;&#32791;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#20195;&#29702;&#21644;&#28176;&#36827;&#26102;&#38388;&#35843;&#24230;&#22120;&#35299;&#20915;&#20102;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01248</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#21464;&#36164;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;IRS&#21644;UAV&#36741;&#21161;&#30340;MEC&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep progressive reinforcement learning-based flexible resource scheduling framework for IRS and UAV-assisted MEC system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01248
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#21464;&#36164;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;IRS&#21644;UAV&#36741;&#21161;&#30340;MEC&#31995;&#32479;&#20013;&#30340;&#33021;&#32791;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#20195;&#29702;&#21644;&#28176;&#36827;&#26102;&#38388;&#35843;&#24230;&#22120;&#35299;&#20915;&#20102;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01248v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;&#65306;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#65288;IRS&#65289;&#21644;&#26080;&#20154;&#39550;&#39542;&#33322;&#31354;&#22120;&#65288;UAV&#65289;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#31995;&#32479;&#22312;&#20020;&#26102;&#21644;&#32039;&#24613;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;UAV&#20301;&#32622;&#12289;IRS&#30456;&#20301;&#20559;&#31227;&#12289;&#20219;&#21153;&#21368;&#36733;&#21644;&#36164;&#28304;&#30340;&#20998;&#37197;&#26469;&#26368;&#23567;&#21270;MEC&#31995;&#32479;&#20013;&#30340;&#33021;&#32791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Flexible REsource Scheduling (FRES)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#28176;&#36827;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#25216;&#26415;&#21253;&#25324;&#20197;&#19979;&#21019;&#26032;&#28857;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#20219;&#21153;&#20195;&#29702;&#20197;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#12290;&#22810;&#20219;&#21153;&#20195;&#29702;&#26377;&#20004;&#20010;&#36755;&#20986;&#22836;&#65292;&#20998;&#21035;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#31867;&#22836;&#34987;&#29992;&#20110;&#24102;&#26377;&#25972;&#25968;&#21464;&#37327;&#30340;&#21368;&#36733;&#20915;&#31574;&#65292;&#32780;&#21478;&#19968;&#31867;&#22836;&#21017;&#34987;&#29992;&#20110;&#24102;&#26377;&#36830;&#32493;&#21464;&#37327;&#30340;&#36164;&#28304;&#20998;&#37197;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28176;&#36827;&#30340;&#26102;&#38388;&#35843;&#24230;&#22120;&#20197;&#20248;&#21270;&#20219;&#21153;&#23398;&#20064;&#36807;&#31243;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#36880;&#27493;&#25366;&#25496;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#39118;&#30005;&#32806;&#21512;&#20316;&#29992;&#30340;&#20869;&#22312;&#36923;&#36753;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#22522;&#20110;&#39118;&#30005;&#30340;MEC&#31995;&#32479;&#20248;&#21270;&#35843;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01248v1 Announce Type: cross  Abstract: The intelligent reflection surface (IRS) and unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) system is widely used in temporary and emergency scenarios. Our goal is to minimize the energy consumption of the MEC system by jointly optimizing UAV locations, IRS phase shift, task offloading, and resource allocation with a variable number of UAVs. To this end, we propose a Flexible REsource Scheduling (FRES) framework by employing a novel deep progressive reinforcement learning which includes the following innovations: Firstly, a novel multi-task agent is presented to deal with the mixed integer nonlinear programming (MINLP) problem. The multi-task agent has two output heads designed for different tasks, in which a classified head is employed to make offloading decisions with integer variables while a fitting head is applied to solve resource allocation with continuous variables. Secondly, a progressive scheduler is intro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#23454;&#20102; GPT-4 &#22312;&#33258;&#21160;&#21270;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;&#26041;&#38754;&#36229;&#36234;&#20102; GPT-3.5-Turbo&#65292;&#20026;&#31934;&#30830;&#21307;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.01214</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36890;&#37327;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-Throughput Phenotyping of Clinical Text Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#23454;&#20102; GPT-4 &#22312;&#33258;&#21160;&#21270;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;&#26041;&#38754;&#36229;&#36234;&#20102; GPT-3.5-Turbo&#65292;&#20026;&#31934;&#30830;&#21307;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01214v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#39640;&#36890;&#37327;&#34920;&#22411;&#33258;&#21160;&#21270;&#33258;&#21160;&#23558;&#24739;&#32773;&#30151;&#29366;&#26144;&#23556;&#21040;&#26631;&#20934;&#21270;&#27010;&#24565; ontology&#65292;&#36825;&#23545;&#20110;&#31934;&#30830;&#21307;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#22312;&#32447;&#23391;&#24503;&#23572;&#36951;&#20256;&#65288;OMIM&#65289;&#25968;&#25454;&#24211;&#20013;&#20020;&#24202;&#24635;&#32467;&#34920;&#22411;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#23427;&#20204;&#20016;&#23500;&#30340;&#34920;&#22411;&#25968;&#25454;&#65292;&#36825;&#20123;&#24635;&#32467;&#21487;&#20197;&#20316;&#20026;&#21307;&#29983;&#35760;&#24405;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102; GPT-4 &#21644; GPT-3.5-Turbo &#20043;&#38388;&#30340;&#24615;&#33021;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#26631;&#20934;&#21270;&#30151;&#29366;&#26041;&#38754;&#65292;GPT-4&#36229;&#36807;&#20102; GPT-3.5-Turbo&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20869;&#37096;&#35780;&#20998;&#32773;&#30340;&#20849;&#35782;&#30456;&#24403;&#12290;&#23613;&#31649;&#22312;&#30151;&#29366;&#26631;&#20934;&#21270;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294; GPT-4 &#30340;&#24191;&#27867;&#39044;&#35757;&#32451;&#22312;&#36328;&#22810;&#20010;&#34920;&#22411;&#20219;&#21153;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#26222;&#36941;&#36866;&#29992;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35745;&#23558;&#25104;&#20026;&#33258;&#21160;&#21270;&#20020;&#24202;&#25991;&#26412;&#34920;&#22411;&#20998;&#26512;&#30340;&#20027;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01214v1 Announce Type: cross  Abstract: High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20102;&#21517;&#20026;Deep W-Learning&#65288;DWN&#65289;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#20852;Web&#26381;&#21153;&#22120;&#31034;&#20363;&#65292;&#20197;&#22312;&#36816;&#34892;&#26102;&#25214;&#21040;&#26368;&#20339;&#24615;&#33021;&#20248;&#21270;&#37197;&#32622;&#65292;&#24182;&#19982;&#20004;&#31181;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#23454;&#29616;&#65288;&#949;-&#36138;&#23146;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65289;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2408.01188</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Deep Reinforcement Learning for Optimisation in Autonomous Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20102;&#21517;&#20026;Deep W-Learning&#65288;DWN&#65289;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#20852;Web&#26381;&#21153;&#22120;&#31034;&#20363;&#65292;&#20197;&#22312;&#36816;&#34892;&#26102;&#25214;&#21040;&#26368;&#20339;&#24615;&#33021;&#20248;&#21270;&#37197;&#32622;&#65292;&#24182;&#19982;&#20004;&#31181;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#23454;&#29616;&#65288;&#949;-&#36138;&#23146;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65289;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01188v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22312;&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#33021;&#22312;&#36816;&#34892;&#26102;&#23398;&#20064;&#32780;&#19981;&#38656;&#35201;&#29615;&#22659;&#27169;&#22411;&#25110;&#39044;&#23450;&#20041;&#21160;&#20316;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;AS&#20013;RL&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21482;&#33021;&#20248;&#21270;&#19968;&#20010;&#30446;&#26631;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#31995;&#32479;&#65288;&#22914;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#20013;&#65292;&#38656;&#35201;&#23558;&#22810;&#20010;&#30446;&#26631;&#22312;&#21333;&#20010;&#30446;&#26631;&#20989;&#25968;&#20013;&#20197;&#39044;&#20808;&#23450;&#20041;&#30340;&#26435;&#37325;&#32467;&#21512;&#12290;&#23384;&#22312;&#22810;&#31181;MORL&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#25968;&#21482;&#22312;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#24212;&#29992;&#65292;&#32780;&#19981;&#26159;&#22312;&#30495;&#23454;&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21517;&#20026;Deep W-Learning&#65288;DWN&#65289;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#22312;&#35832;&#22914;&#33258;&#36866;&#24212;&#26381;&#21153;&#22120;&#36825;&#26679;&#30340;&#26032;&#20852;Web&#26381;&#21153;&#22120;&#31034;&#20363;&#20013;&#23545;&#20854;&#36827;&#34892;&#24212;&#29992;&#65292;&#20197;&#25214;&#21040;&#22312;&#36816;&#34892;&#26102;&#24615;&#33021;&#20248;&#21270;&#26041;&#38754;&#30340;&#29702;&#24819;&#37197;&#32622;&#12290;&#25105;&#20204;&#23558;DWN&#19982;&#20004;&#31181;&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#949;-&#36138;&#23146;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;DW
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01188v1 Announce Type: new  Abstract: Reinforcement Learning (RL) is used extensively in Autonomous Systems (AS) as it enables learning at runtime without the need for a model of the environment or predefined actions. However, most applications of RL in AS, such as those based on Q-learning, can only optimize one objective, making it necessary in multi-objective systems to combine multiple objectives in a single objective function with predefined weights. A number of Multi-Objective Reinforcement Learning (MORL) techniques exist but they have mostly been applied in RL benchmarks rather than real-world AS systems. In this work, we use a MORL technique called Deep W-Learning (DWN) and apply it to the Emergent Web Servers exemplar, a self-adaptive server, to find the optimal configuration for runtime performance optimization. We compare DWN to two single-objective optimization implementations: {\epsilon}-greedy algorithm and Deep Q-Networks. Our initial evaluation shows that DW
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25972;&#21512;&#20102;&#22810;&#31181;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#65288;Particle Swarm Optimization, Ant Colony Optimization&#31561;&#65289;&#20110;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#36864;&#28779;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#19979;&#33719;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01187</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#31574;&#30053;&#20248;&#21270; variational &#37327;&#23376;&#30005;&#36335;&#30340;&#24378;&#21270;&#23398;&#20064; Metaheuristic &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Variational Quantum Circuits Using Metaheuristic Strategies in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25972;&#21512;&#20102;&#22810;&#31181;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#65288;Particle Swarm Optimization, Ant Colony Optimization&#31561;&#65289;&#20110;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#36864;&#28779;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#19979;&#33719;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01187v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30456;&#36739;&#20110;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#31616;&#27905;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#38469;&#30340;&#22909;&#22788;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;QRL&#38754;&#20020;&#35832;&#22914;&#35299;&#20915;&#26041;&#26696;&#26223;&#35266;&#24179;&#22374;&#31561;&#25361;&#25112;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#25928;&#29575;&#20302;&#19979;&#65292;&#36825;&#38656;&#35201;&#20351;&#29992;&#26080;&#26799;&#24230;&#31639;&#27861;&#12290;&#26412;&#24037;&#20316;&#25506;&#35752;&#23558;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#12289;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#12289;&#31105;&#24524;&#25628;&#32034;&#65288;TS&#65289;&#12289;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#21644;&#21644;&#35856;&#25628;&#32034;&#65288;HS&#65289;&#65292;&#19982;QRL&#38598;&#25104;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#21442;&#25968;&#20248;&#21270;&#26041;&#38754;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;$5\times5$ MiniGrid&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#25152;&#26377;&#31639;&#27861;&#22343;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#32467;&#26524;&#65292;&#20854;&#20013;&#27169;&#25311;&#36864;&#28779;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#34920;&#29616;&#26368;&#20339;&#12290;&#22312;Cart Pole&#29615;&#22659;&#20013;&#65292;&#27169;&#25311;&#36864;&#28779;&#21644;&#36951;&#20256;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#21040;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#20013;&#65292;&#20197;&#39564;&#35777;&#20854;&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01187v1 Announce Type: cross  Abstract: Quantum Reinforcement Learning (QRL) offers potential advantages over classical Reinforcement Learning, such as compact state space representation and faster convergence in certain scenarios. However, practical benefits require further validation. QRL faces challenges like flat solution landscapes, where traditional gradient-based methods are inefficient, necessitating the use of gradient-free algorithms. This work explores the integration of metaheuristic algorithms -- Particle Swarm Optimization, Ant Colony Optimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony Search -- into QRL. These algorithms provide flexibility and efficiency in parameter optimization. Evaluations in $5\times5$ MiniGrid Reinforcement Learning environments show that, all algorithms yield near-optimal results, with Simulated Annealing and Particle Swarm Optimization performing best. In the Cart Pole environment, Simulated Annealing, Geneti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#23427;&#20204;&#23384;&#22312;&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#21644;&#22312;&#30830;&#20445;&#20449;&#24687;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2408.01168</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35823;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#28431;&#27934;&#12289;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Misinforming LLMs: vulnerabilities, challenges and opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#23427;&#20204;&#23384;&#22312;&#30340;&#35823;&#23548;&#24615;&#38382;&#39064;&#21644;&#22312;&#30830;&#20445;&#20449;&#24687;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;arXiv:2408.01168v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#20173;&#24120;&#24120;&#34987;&#35823;&#35299;&#12290;&#23613;&#31649;&#34920;&#29616;&#20986;&#36830;&#36143;&#30340;&#22238;&#31572;&#21644;&#26126;&#26174;&#30340;&#25512;&#29702;&#34892;&#20026;&#65292;LLMs&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#35789;&#27719;&#23884;&#20837;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;LLMs&#20381;&#36182;&#20110;&#35789;&#27719;&#23884;&#20837;&#21521;&#37327;&#24207;&#21015;&#27169;&#24335;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#30340;&#20869;&#32622;&#26426;&#21046;&#23384;&#22312;&#26681;&#26412;&#30340;&#19981;&#20449;&#20219;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#32467;&#21512;&#22522;&#20110;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19982;&#20107;&#23454;&#22522;&#30784;&#21644;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#30340;&#30740;&#31350;&#21487;&#33021;&#20250;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#20449;&#36182;&#30340;LLM&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22522;&#20110;&#25552;&#20379;&#30340;&#20107;&#23454;&#29983;&#25104;&#35821;&#21477;&#24182;&#35299;&#37322;&#20854;&#33258;&#25105;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01168v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant advances in natural language processing, but their underlying mechanisms are often misunderstood. Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely on statistical patterns in word embeddings rather than true cognitive processes. This leads to vulnerabilities such as "hallucination" and misinformation. The paper argues that current LLM architectures are inherently untrustworthy due to their reliance on correlations of sequential patterns of word embedding vectors. However, ongoing research into combining generative transformer-based models with fact bases and logic programming languages may lead to the development of trustworthy LLMs capable of generating statements based on given truth and explaining their self-reasoning process.
&lt;/p&gt;</description></item><item><title>TCR-GPT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;T&#32454;&#32990;&#21463;&#20307;&#24207;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20154;&#28304;&#21270;TCRs&#30340;&#24207;&#21015;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01156</link><description>&lt;p&gt;
TCR-GPT: &#32467;&#21512;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;T&#32454;&#32990;&#21463;&#20307;&#24211;
&lt;/p&gt;
&lt;p&gt;
TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for T-Cell Receptor Repertoires Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01156
&lt;/p&gt;
&lt;p&gt;
TCR-GPT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;T&#32454;&#32990;&#21463;&#20307;&#24207;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20154;&#28304;&#21270;TCRs&#30340;&#24207;&#21015;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01156v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;T&#32454;&#32990;&#21463;&#20307;&#65288;TCRs&#65289;&#22312;&#20813;&#30123;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#35782;&#21035;&#24182;&#32467;&#21512;&#30001;&#24863;&#26579;&#25110;&#30284;&#24615;&#32454;&#32990;&#21576;&#29616;&#30340;&#29305;&#23450;&#25239;&#21407;&#12290;&#20102;&#35299;TCR&#30340;&#24207;&#21015;&#27169;&#24335;&#23545;&#20110;&#24320;&#21457;&#38024;&#23545;&#20813;&#30123;&#27835;&#30103;&#30340;&#31574;&#30053;&#21644;&#35774;&#35745;&#26377;&#25928;&#30340;&#30123;&#33495;&#33267;&#20851;&#37325;&#35201;&#12290;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#33258;&#22238;&#24402;&#36716;&#25442;&#22120;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;TCR&#24211;&#30340;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;TCR&#24207;&#21015;&#65292;&#36825;&#20123;&#24207;&#21015;&#32487;&#25215;&#20102;&#24211;&#20013;&#28508;&#22312;&#30340;&#24207;&#21015;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCR-GPT&#65292;&#19968;&#20010;&#22522;&#20110;&#20165;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#32467;&#26500;&#30340;&#32463;&#27982;&#27169;&#22411;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#22797;&#21046;TCR&#24211;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#12290;TCR-GPT&#22312;&#36890;&#36807;&#30382;&#23572;&#26862;&#30456;&#20851;&#31995;&#25968;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#25512;&#26029;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#31934;&#30830;&#24230;&#36798;&#21040;&#20102;0.953&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#24050;&#32463;&#35843;&#25972;&#20102;TCR&#24207;&#21015;&#30340;&#20998;&#24067;&#65292;&#20197;&#20445;&#35777;&#22312;&#20154;&#28304;&#21270;TCRs&#65288;Hu-TCRs&#65289;&#30340;&#24207;&#21015;&#29983;&#25104;&#20013;&#25512;&#24191;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;TCR-GPT&#33021;&#22815;&#38024;&#23545;&#29305;&#23450;&#30340;&#24207;&#21015;&#31354;&#38388;&#35774;&#35745;&#20986;&#26356;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;TCR&#24207;&#21015;&#21644;&#23454;&#38469;Hu-TCRs&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#22810;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;TCR-GPT&#27169;&#22411;&#22312;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;TCR&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01156v1 Announce Type: cross  Abstract: T-cell receptors (TCRs) play a crucial role in the immune system by recognizing and binding to specific antigens presented by infected or cancerous cells. Understanding the sequence patterns of TCRs is essential for developing targeted immune therapies and designing effective vaccines. Language models, such as auto-regressive transformers, offer a powerful solution to this problem by learning the probability distributions of TCR repertoires, enabling the generation of new TCR sequences that inherit the underlying patterns of the repertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only transformer architecture, designed to uncover and replicate sequence patterns in TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring sequence probability distributions measured by Pearson correlation coefficient. Furthermore, by leveraging Reinforcement Learning(RL), we adapted the distribution of TCR sequences t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#22312;&#20840;&#29699;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#38543;&#39057;&#29575;&#30340;&#25351;&#25968;&#19979;&#38477;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20302;&#39057;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#27491;&#38754;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#39640;&#39057;&#29575;&#20449;&#21495;&#30340;&#36129;&#29486;&#19982;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#36127;&#30456;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2408.01139</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#29702;&#35889;&#37325;&#35201;&#24615;&#20998;&#35299;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#30340;&#20840;&#23616;&#25200;&#21160;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01139
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#22312;&#20840;&#29699;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#38543;&#39057;&#29575;&#30340;&#25351;&#25968;&#19979;&#38477;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20302;&#39057;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#27491;&#38754;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#39640;&#39057;&#29575;&#20449;&#21495;&#30340;&#36129;&#29486;&#19982;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#36127;&#30456;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01139v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25200;&#21160;&#40065;&#26834;&#24615;&#35780;&#20272;&#20102;&#27169;&#22411;&#23545;&#21508;&#31181;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#27745;&#26579;&#21644; adversarial&#25915;&#20987;&#12290;&#29702;&#35299;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#26426;&#21046;&#23545;&#20110;&#20840;&#23616;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20840;&#23616;&#26426;&#21046;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#30340;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#21551;&#21457;&#12290;&#39318;&#20808;&#65292;&#20197;&#21069;&#30340;&#20840;&#29699;&#35299;&#37322;&#24615;&#24037;&#20316;&#19982;&#40065;&#26834;&#24615;&#22522;&#20934;&#65288;&#20363;&#22914;&#24179;&#22343;&#27745;&#26579;&#38169;&#35823;mCE&#65289;&#21516;&#26102;&#36827;&#34892;&#65292;&#24182;&#19981;&#26159;&#20026;&#20102;&#30452;&#25509;&#35299;&#37322;&#22270;&#20687;&#27169;&#22411;&#20013;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21463;&#25200;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#38543;&#39057;&#29575;&#25351;&#25968;&#19979;&#38477;&#12290;&#36825;&#31181;&#24130;&#24459;&#31867;&#20284;&#30340;&#19979;&#38477;&#34920;&#26126;&#65306;&#20302;&#39057;&#20449;&#21495;&#36890;&#24120;&#27604;&#39640;&#39057;&#20449;&#21495;&#26356;&#40065;&#26834;&#8212;&#8212;&#28982;&#32780;&#65292;&#39640;&#20998;&#31867;&#31934;&#24230;&#24182;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#27934;&#23519;&#21040;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;-mCE&#21644;&#39640;&#39057;&#20449;&#21495;&#30340;&#36129;&#29486;&#26377;&#36127;&#30456;&#20851;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#39640;&#39057;&#20449;&#21495;&#36739;&#23567;&#30340;&#22270;&#20687;&#21306;&#22495;&#20013;&#65292;&#21363;&#20351;&#23384;&#22312;&#39640;&#22122;&#22768;&#27700;&#24179;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20063;&#24456;&#39640;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#39640;&#39057;&#29575;&#20449;&#21495;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#20013;&#30340;&#36127;&#38754;&#20316;&#29992;&#65292;&#24182;&#20026;&#27169;&#22411;&#32467;&#26500;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#23545;&#20110;&#36731;&#24230;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#20063;&#20542;&#21521;&#20110;&#22312;&#20302;&#39057;&#20449;&#21495;&#26356;&#22823;&#30340;&#31354;&#38388;&#21306;&#22495;&#20013;&#20445;&#25345;&#26356;&#39640;&#30340;SNR&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#23545;&#22270;&#20687;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20840;&#29699;&#35299;&#37322;&#24615;&#29702;&#35299;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21518;&#32493;&#30340;&#27169;&#22411;&#35774;&#35745;&#12289;&#29702;&#35299;&#21644;&#20248;&#21270;&#24037;&#20316;&#12290;&#19979;&#26041;&#26159;&#35813;&#35770;&#25991;&#30340;&#33521;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#65292;&#35831;&#27880;&#24847;&#65292;&#23545;&#20110;&#20197;&#19979;&#30340;tldr&#21644;en_tldr&#37096;&#20998;&#65292;&#25105;&#20250;&#24635;&#32467;&#20986;&#19968;&#20010;&#20013;&#25991;&#21644;&#33521;&#25991;&#29256;&#30340;&#27010;&#35201;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01139v1 Announce Type: cross  Abstract: Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals -- yet high classification accuracy can not be ac
&lt;/p&gt;</description></item><item><title>Mamba&#26550;&#26500;&#20197;&#32463;&#20856;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#28789;&#24863;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#36817;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#19982;Transformer&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#26377;&#26395;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01129</link><description>&lt;p&gt;
Mamba&#26550;&#26500;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01129
&lt;/p&gt;
&lt;p&gt;
Mamba&#26550;&#26500;&#20197;&#32463;&#20856;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#28789;&#24863;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#36817;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#19982;Transformer&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#26377;&#26395;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#38376;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24341;&#21457;&#20102;&#19968;&#22330;&#26174;&#33879;&#30340;&#38761;&#21629;&#12290;&#20316;&#20026;&#26368;&#20856;&#22411;&#30340;&#26550;&#26500;&#65292;Transformer&#24050;&#32463;&#36171;&#33021;&#20102;&#22823;&#37327;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#21253;&#21547;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30707;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#23601;&#65292;&#20294;Transformer&#20173;&#28982;&#38754;&#20020;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#23548;&#33268;&#30340;&#32791;&#26102;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;Mamba&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#20511;&#37492;&#20102;&#32463;&#20856;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#21457;&#23637;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#26367;&#20195;&#26041;&#26696;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#22312;&#20445;&#25345;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#36817;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;Transformer&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#28608;&#21169;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#31215;&#26497;&#25506;&#32034;Mamba&#22312;&#21508;&#31181;&#39046;&#22495;&#23454;&#29616;&#21331;&#36234;&#34920;&#29616;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01129v1 Announce Type: cross  Abstract: Deep learning, as a vital technique, has sparked a notable revolution in artificial intelligence. As the most representative architecture, Transformers have empowered numerous advanced models, especially the large language models that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models, has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domain
&lt;/p&gt;</description></item><item><title>BioRAG&#26159;&#19968;&#31181;&#37319;&#29992;RAG-LLM&#25216;&#26415;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20179;&#24211;&#32500;&#25252;&#21644;&#20449;&#24687;&#26816;&#32034;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01107</link><description>&lt;p&gt;
BioRAG: &#22522;&#20110;RAG-LLM&#26694;&#26550;&#30340;&#29983;&#29289;&#23398;&#38382;&#39064;&#25512;&#29702;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BioRAG: A RAG-LLM Framework for Biological Question Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01107
&lt;/p&gt;
&lt;p&gt;
BioRAG&#26159;&#19968;&#31181;&#37319;&#29992;RAG-LLM&#25216;&#26415;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20179;&#24211;&#32500;&#25252;&#21644;&#20449;&#24687;&#26816;&#32034;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01107v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#30001;&#20110;&#21457;&#29616;&#30340;&#36895;&#24230;&#21152;&#24555;&#12289;&#27934;&#23519;&#21147;&#30340;&#28436;&#21464;&#20197;&#21450;&#30693;&#35782;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#32500;&#25252;&#19968;&#20010;&#20840;&#38754;&#30340;&#36164;&#26009;&#20179;&#24211;&#21644;&#20934;&#30830;&#30340;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01107v1 Announce Type: cross  Abstract: The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BioRAG deconstructs the question and employs an it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;CoLoRA&#65292;&#36890;&#36807;&#38543;&#26426;&#22833;&#30495;&#39044;&#35757;&#32451;&#65288;PROD&#65289;&#21644;&#36129;&#29486;&#24615;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#38024;&#23545;&#22810;&#20010;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#65292;&#22823;&#24133;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#31934;&#24230;&#19982;&#20869;&#23384;&#38656;&#27714;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01099</link><description>&lt;p&gt;
&#22522;&#20110;&#36129;&#29486;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;CoLoRA&#65292;&#36890;&#36807;&#38543;&#26426;&#22833;&#30495;&#39044;&#35757;&#32451;&#65288;PROD&#65289;&#21644;&#36129;&#29486;&#24615;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#38024;&#23545;&#22810;&#20010;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#65292;&#22823;&#24133;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#31934;&#24230;&#19982;&#20869;&#23384;&#38656;&#27714;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01099v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#39640;&#23618;&#27425;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#20511;&#21161;&#25513;&#30721;&#24314;&#27169;&#21644;&#25552;&#31034;&#35843;&#20248;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39640;&#25928;&#21442;&#25968;&#35843;&#20248;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#23618;&#27425;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65288;&#22914;&#20943;&#36731;AI&#36793;&#32536;&#35774;&#22791;&#19978;&#26032;&#20219;&#21153;&#26102;&#30340;&#20869;&#23384;&#33192;&#32960;&#38382;&#39064;&#65289;&#20013;&#30340;&#37325;&#35201;&#24615;&#19982;&#25910;&#30410;&#22791;&#21463;&#20851;&#27880;&#65292;&#39640;&#25928;&#30340;&#23567;&#25209;&#37327;&#21442;&#25968;&#35843;&#20248;&#31574;&#30053;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26377;&#25928;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#31216;&#20026;&#36129;&#29486;&#24615;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;CoLoRA&#65289;&#65292;&#29992;&#20110;&#22810;&#20010;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#65292;&#20197;&#21450;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#38543;&#26426;&#39034;&#24207;&#22833;&#30495;&#65288;PROD&#65289;&#12290;&#19982;&#20043;&#21069;&#25152;&#26377;&#32593;&#32476;&#21442;&#25968;&#35843;&#20248;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;CoLoRA&#36890;&#36807;&#21033;&#29992;LoRA&#65288;&#20302;&#31209;&#36866;&#24212;&#24615;&#65289;&#38024;&#23545;&#27599;&#20010;&#26032;&#30340;&#35270;&#35273;&#20219;&#21153;&#26469;&#26377;&#25928;&#22320;&#35843;&#20248;&#23569;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#36129;&#29486;&#27979;&#37327;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#30340;&#24863;&#30693;&#21464;&#21270;&#26435;&#37325;&#26469;&#25351;&#23548;&#32593;&#32476;&#21442;&#25968;&#30340;&#36873;&#23450;&#21644;&#20248;&#21270;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;PROD&#21644;CoLoRA&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20016;&#23500;&#20102;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#24182;&#20026;AI&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#26032;&#20219;&#21153;&#38598;&#25104;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01099v1 Announce Type: cross  Abstract: Recently, pre-trained model and efficient parameter tuning have achieved remarkable success in natural language processing and high-level computer vision with the aid of masked modeling and prompt tuning. In low-level computer vision, however, there have been limited investigations on pre-trained models and even efficient fine-tuning strategy has not yet been explored despite its importance and benefit in various real-world tasks such as alleviating memory inflation issue when integrating new tasks on AI edge devices. Here, we propose a novel efficient parameter tuning approach dubbed contribution-based low-rank adaptation (CoLoRA) for multiple image restorations along with effective pre-training method with random order degradations (PROD). Unlike prior arts that tune all network parameters, our CoLoRA effectively fine-tunes small amount of parameters by leveraging LoRA (low-rank adaptation) for each new vision task with our contribut
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#35201;&#28857;</title><link>https://arxiv.org/abs/2408.01091</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01091
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01091v1 Announce Type: new  Abstract: Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. The dataset and code are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#35758;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;AI&#35780;&#20215;&#23610;&#24230;&#65292;&#19987;&#38376;&#20026;&#33521;&#35821;&#23398;&#26415;&#29992;&#36884;&#35774;&#35745;&#65292;&#20197;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#26356;&#26377;&#25928;&#22320;&#24212;&#29992;&#65292;&#30830;&#20445;&#35780;&#20272;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01075</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;AI&#35780;&#20272;&#23610;&#24230;&#65306;&#20026;&#33521;&#35821;&#23398;&#26415;&#29992;&#36884;&#30340;AI&#35780;&#20272;&#25913;&#36827;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The EAP-AIAS: Adapting the AI Assessment Scale for English for Academic Purposes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#35758;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;AI&#35780;&#20215;&#23610;&#24230;&#65292;&#19987;&#38376;&#20026;&#33521;&#35821;&#23398;&#26415;&#29992;&#36884;&#35774;&#35745;&#65292;&#20197;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#26356;&#26377;&#25928;&#22320;&#24212;&#29992;&#65292;&#30830;&#20445;&#35780;&#20272;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01075v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01075v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) presents both opportunities and challenges for English for Academic Purposes (EAP) instruction. This paper proposes an adaptation of the AI Assessment Scale (AIAS) specifically tailored for EAP contexts, termed the EAP-AIAS.   This framework aims to provide a structured approach for integrating GenAI tools into EAP assessment practices while maintaining academic integrity and supporting language development. The EAP-AIAS consists of five levels, ranging from "No AI" to "Full AI", each delineating appropriate GenAI usage in EAP tasks. We discuss the rationale behind this adaptation, considering the unique needs of language learners and the dual focus of EAP on language proficiency and academic acculturation.   This paper explores potential applications of the EAP-AIAS across various EAP assessment types, including writing tasks, presentations, and research projects. By 
&lt;/p&gt;</description></item><item><title>&#33258;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24110;&#21161;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#33258;&#36523;&#22797;&#21046;&#25110;&#21382;&#21490;&#29256;&#26412;&#30340;&#23545;&#24328;&#20013;&#23398;&#20064;&#65292;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2408.01072</link><description>&lt;p&gt;
&#33258;&#23545;&#24328;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-play Methods in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01072
&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24110;&#21161;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#33258;&#36523;&#22797;&#21046;&#25110;&#21382;&#21490;&#29256;&#26412;&#30340;&#23545;&#24328;&#20013;&#23398;&#20064;&#65292;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#33258;&#23545;&#24328;&#65288;Self-play&#65289;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#26032;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#23545;&#24328;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21253;&#25324;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#22522;&#26412;&#30340;&#21338;&#24328;&#35770;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#23545;&#24328;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#33258;&#23545;&#24328;&#31639;&#27861;&#24402;&#31867;&#21040;&#36825;&#20010;&#26694;&#26550;&#20043;&#19979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#33258;&#23545;&#24328;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24357;&#21512;&#20102;&#31639;&#27861;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#24328;&#38754;&#20020;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#20026;&#29702;&#35299;&#33258;&#23545;&#24328;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22810;&#23618;&#38754;&#29305;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01072v1 Announce Type: new  Abstract: Self-play, characterized by agents' interactions with copies or past versions of itself, has recently gained prominence in reinforcement learning. This paper first clarifies the preliminaries of self-play, including the multi-agent reinforcement learning framework and basic game theory concepts. Then it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different scenarios. Finally, the survey highlights open challenges and future research directions in self-play. This paper is an essential guide map for understanding the multifaceted landscape of self-play in RL.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;KAN&#26550;&#26500;&#30340;GNNs&#65292;GNN-MolKAN&#21644;GNN-MolKAN+&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#29305;&#24615;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01018</link><description>&lt;p&gt;
GNN-MolKAN: &#32467;&#21512;KAN&#25552;&#21319;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;GNNs
&lt;/p&gt;
&lt;p&gt;
GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular Representation Learning with GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;KAN&#26550;&#26500;&#30340;GNNs&#65292;GNN-MolKAN&#21644;GNN-MolKAN+&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#29305;&#24615;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01018v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;&#35774;&#35745;&#20013;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#26631;&#27880;&#19981;&#36275;&#21644;&#26550;&#26500;&#35774;&#35745;&#19981;&#20339;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20363;&#22914;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#36807;&#24230;&#21387;&#32553;&#32780;&#23548;&#33268;&#20998;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#32454;&#33410;&#20002;&#22833;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20998;&#23376;&#34920;&#24449;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#31867;&#65292;&#21363;GNN-MolKAN&#21450;&#20854;&#22686;&#24378;&#21464;&#31181;GNN-MolKAN+&#65292;&#23427;&#20204;&#23558;&#20154;&#24037;&#26234;&#33021;+&#31185;&#23398;&#39046;&#22495;&#30340;Kolmogorov-Arnold Networks&#65288;KAN&#65289;&#26550;&#26500;&#34701;&#20837;GNNs&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#24555;&#36895;KAN&#65288;AdFastKAN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#32423;KAN&#65292;&#25552;&#20379;&#20102;&#22686;&#21152;&#30340;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#26631;&#20934;GNN&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;1) &#21331;&#36234;&#24615;&#33021;&#65306;GNN-MolKAN&#21644;GNN-MolKAN+&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;2) &#32467;&#26500;&#25484;&#25569;&#65306;KAN&#32467;&#21512;&#20102;GNN&#30340;&#32467;&#26500;&#21160;&#24577;&#24863;&#30693;&#33021;&#21147;&#65307;3) &#39640;&#25928;&#23398;&#20064;&#65306;AdFastKAN&#23454;&#29616;&#20102;&#24555;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#31283;&#23450;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;GNN-MolKAN&#21644;GNN-MolKAN+&#22312;&#22810;&#26679;&#21270;&#20998;&#23376;&#29305;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20102;KAN&#26694;&#26550;&#22312;GNNs&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01018v1 Announce Type: cross  Abstract: Effective molecular representation learning is crucial for molecular property prediction and drug design. However, existing approaches struggle with limitations in insufficient annotations and suboptimal architecture design. For instance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the loss of important structural details in molecules, thus impairing molecular representations. In this work, we propose a new class of GNNs, GNN-MolKAN and its augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold Networks (KAN) architecture from AI + Science into GNNs to address these challenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an advanced KAN that offers increased stability and speed, further enhancing the performance of standard GNNs. Notably, our approach holds three key benefits: 1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior prediction ability, robust generalizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#38598;&#22312;&#36866;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01016</link><description>&lt;p&gt;
&#36947;&#36335;&#20132;&#36890;&#22270;&#25968;&#25454;&#65306;&#22522;&#20934;&#27979;&#35797;&#19982;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#38598;&#22312;&#36866;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;: &#36947;&#36335;&#20132;&#36890;&#22581;&#22622;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#33021;&#20351;&#24471;&#20132;&#36890;&#31649;&#29702;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#25552;&#39640;&#37066;&#21306;&#20307;&#39564;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#24182;&#25972;&#20307;&#25552;&#39640;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#34429;&#28982;&#26377;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#23588;&#20854;&#26159;&#22312;&#37117;&#24066;&#22320;&#21306;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#24182;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#65292;&#22240;&#20026;&#25968;&#25454;&#37327;&#19981;&#22815;&#65288;&#21363;&#20256;&#24863;&#22120;&#21644;&#36947;&#36335;&#36830;&#25509;&#25968;&#37327;&#19981;&#36275;&#65289;&#20197;&#21450;&#19968;&#20123;&#22806;&#37096;&#22240;&#32032;&#65292;&#22914;&#30446;&#26631;&#22320;&#21306;&#30340;&#29305;&#24615;&#24046;&#24322;&#65292;&#22914;&#22478;&#24066;&#12289;&#39640;&#36895;&#20844;&#36335;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#20301;&#32622;&#31561;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#20016;&#23500;&#20855;&#26377;&#26032;&#22320;&#29702;&#29305;&#24449;&#30340;&#25991;&#29486;&#12290;IBB&#20132;&#36890;&#22270;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22312;2451&#20010;&#19981;&#21516;&#22320;&#28857;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01016v1 Announce Type: cross  Abstract: Road traffic congestion prediction is a crucial component of intelligent transportation systems, since it enables proactive traffic management, enhances suburban experience, reduces environmental impact, and improves overall safety and efficiency. Although there are several public datasets, especially for metropolitan areas, these datasets may not be applicable to practical scenarios due to insufficiency in the scale of data (i.e. number of sensors and road links) and several external factors like different characteristics of the target area such as urban, highways and the data collection location. To address this, this paper introduces a novel IBB Traffic graph dataset as an alternative benchmark dataset to mitigate these limitations and enrich the literature with new geographical characteristics. IBB Traffic graph dataset covers the sensor data collected at 2451 distinct locations. Moreover, we propose a novel Road Traffic Prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TT-LoRA&#30340;&#20302;&#31209;&#24352;&#37327;&#26463;&#36817;&#20284;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26356;&#24555;&#30340;LLMs&#24494;&#35843;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.01008</link><description>&lt;p&gt;
&#24352;&#37327;&#26463;&#20302;&#31209;&#36817;&#20284; (TT-LoRA): &#21152;&#36895; LLMs &#20197;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TT-LoRA&#30340;&#20302;&#31209;&#24352;&#37327;&#26463;&#36817;&#20284;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26356;&#24555;&#30340;LLMs&#24494;&#35843;&#21152;&#36895;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01008v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;: &#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#38382;&#31572;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;LLMs&#26085;&#30410;&#22797;&#26434;&#30340;&#38656;&#27714;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#65292;&#38459;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22914;Low-Rank Approximation&#65288;LoRA&#65289;&#21644;Adapters&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#21487;&#21387;&#32553;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LoRA&#22312;&#29616;&#20195;&#22823;&#22411;LLM&#20013;&#36234;&#26469;&#36234;&#22686;&#22810;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19978;&#38590;&#20197;&#26377;&#25928;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Low-Rank Economic Tensor-Train Adaptation&#65288;LoRETTA&#65289;&#21033;&#29992;&#24352;&#37327;&#26463;&#20998;&#35299;&#65292;&#20294;&#23427;&#23578;&#26410;&#23454;&#29616;&#23545;&#38750;&#24120;&#22823;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#24517;&#35201;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01008v1 Announce Type: cross  Abstract: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks, such as question-answering, sentiment analysis, text summarization, and machine translation. However, the ever-growing complexity of LLMs demands immense computational resources, hindering the broader research and application of these models. To address this, various parameter-efficient fine-tuning strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been developed. Despite their potential, these methods often face limitations in compressibility. Specifically, LoRA struggles to scale effectively with the increasing number of trainable parameters in modern large scale LLMs. Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which utilizes tensor train decomposition, has not yet achieved the level of compression necessary for fine-tuning very large scale mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ARCHCODE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#32452;&#32455;&#24182;&#25512;&#26029;&#36719;&#20214;&#38656;&#27714;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.00994</link><description>&lt;p&gt;
ArchCode&#65306;&#23558;&#36719;&#20214;&#38656;&#27714;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;
&lt;/p&gt;
&lt;p&gt;
ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ARCHCODE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#32452;&#32455;&#24182;&#25512;&#26029;&#36719;&#20214;&#38656;&#27714;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00994v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26412;&#25991;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#25193;&#23637;&#21040;&#33021;&#22815;&#33258;&#21160;&#22788;&#29702;&#20174;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#20013;&#32473;&#20986;&#30340;&#20840;&#38754;&#36719;&#20214;&#38656;&#27714;&#12290;&#36825;&#20123;&#38656;&#27714;&#21253;&#25324;&#21151;&#33021;&#24615;&#65288;&#21363;&#23545;&#36755;&#20837;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#65289;&#21644;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;/&#31354;&#38388;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#12289;&#21487;&#32500;&#25252;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#25551;&#36848;&#35201;&#20040;&#21487;&#33021;&#20887;&#38271;&#22320;&#34920;&#36798;&#35201;&#27714;&#65292;&#35201;&#20040;&#29978;&#33267;&#21487;&#33021;&#30465;&#30053;&#19968;&#20123;&#35201;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;ARCHCODE&#65292;&#19968;&#20010;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#8220;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#8221;&#30340;&#21407;&#29702;&#26469;&#32452;&#32455;&#20174;&#25551;&#36848;&#20013;&#35266;&#23519;&#21040;&#30340;&#38656;&#27714;&#65292;&#24182;&#20174;&#36825;&#20123;&#25551;&#36848;&#20013;&#25512;&#26029;&#20986;&#26410;&#34920;&#36798;&#30340;&#38656;&#27714;&#12290;ARCHCODE&#20174;&#32473;&#20986;&#30340;&#25551;&#36848;&#20013;&#29983;&#25104;&#38656;&#27714;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#20135;&#29983;&#20195;&#30721;&#29255;&#27573;&#21644;&#27979;&#35797;&#29992;&#20363;&#12290;&#27599;&#20010;&#27979;&#35797;&#29992;&#20363;&#37117;&#38024;&#23545;&#19968;&#20010;&#35201;&#27714;&#65292;&#20801;&#35768;&#26681;&#25454;&#20195;&#30721;&#29255;&#27573;&#25191;&#34892;&#32467;&#26524;&#19982;&#35201;&#27714;&#30340;&#31526;&#21512;&#24615;&#23545;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#25490;&#21517;&#12290;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;ARCHCODE&#22312;&#22788;&#29702;&#22797;&#26434;&#36719;&#20214;&#38656;&#27714;&#21644;&#29983;&#25104;&#39640;&#36136;&#37327;&#20195;&#30721;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#38598;&#20013;&#20110;&#23454;&#29616;ARCHCODE&#22312;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#24037;&#31243;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00994v1 Announce Type: cross  Abstract: This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;multi-agent&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#21152;&#31995;&#32479;&#25269;&#24481;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.00989</link><description>&lt;p&gt;
&#20855;&#26377;&#24694;&#24847;&#20195;&#29702;&#30340;multi-agent&#31995;&#32479;&#30340;&#24377;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Resilience of Multi-Agent Systems with Malicious Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;multi-agent&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#21152;&#31995;&#32479;&#25269;&#24481;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00989v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;&#65306;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#20381;&#38752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#19987;&#23478;&#20195;&#29702;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#34987;&#21333;&#29420;&#37096;&#32626;&#26102;&#65292;&#23384;&#22312;&#19968;&#20010;&#39118;&#38505;&#65292;&#21363;&#24694;&#24847;&#29992;&#25143;&#21487;&#33021;&#20250;&#24341;&#20837;&#24694;&#24847;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#29983;&#25104;&#30340;&#32467;&#26524;&#26159;&#38169;&#35823;&#30340;&#25110;&#19981;&#30456;&#20851;&#30340;&#65292;&#20197;&#33267;&#20110;&#20854;&#20182;&#38750;&#19987;&#38376;&#20195;&#29702;&#38590;&#20197;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#20195;&#29702;&#31995;&#32479;&#32467;&#26500;&#65288;&#20363;&#22914;A-&gt;B-&gt;C&#65292;A&lt;-&gt;B&lt;-&gt;C&#65289;&#19979;&#65292;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;&#65292;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24377;&#24615;&#26159;&#22810;&#23569;&#65311;&#65288;2&#65289;&#25105;&#20204;&#22914;&#20309;&#33021;&#22686;&#21152;&#31995;&#32479;&#25269;&#24481;&#24694;&#24847;&#20195;&#29702;&#30340;&#33021;&#21147;&#65311;&#20026;&#20102;&#27169;&#25311;&#24694;&#24847;&#20195;&#29702;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;AutoTransform&#21644;AutoInject&#65292;&#23558;&#20219;&#20309;&#20195;&#29702;&#36716;&#25442;&#25104;&#24694;&#24847;&#20195;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21151;&#33021;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#30693;&#35782;&#25277;&#21462;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#31995;&#32479;&#22312;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;multi-agent&#31995;&#32479;&#22312;&#38754;&#23545;&#24694;&#24847;&#20195;&#29702;&#26102;&#34920;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#33030;&#24369;&#24615;&#65292;&#20294;&#36890;&#36807;&#36866;&#24403;&#30340;&#32467;&#26500;&#35774;&#35745;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#21046;&#23450;&#65292;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#26159;&#21487;&#20197;&#24471;&#21040;&#22686;&#24378;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#25581;&#31034;&#20102;multi-agent&#31995;&#32479;&#22312;&#38754;&#23545;&#24694;&#24847;&#25915;&#20987;&#26102;&#30340;&#25239;&#24615;&#65292;&#20063;&#20026;&#26500;&#24314;&#26356;&#23433;&#20840;&#12289;&#26356;&#20581;&#22766;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00989v1 Announce Type: new  Abstract: Multi-agent systems, powered by large language models, have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, when agents are deployed separately, there is a risk that malicious users may introduce malicious agents who generate incorrect or irrelevant results that are too stealthy to be identified by other non-specialized agents. Therefore, this paper investigates two essential questions: (1) What is the resilience of various multi-agent system structures (e.g., A$\rightarrow$B$\rightarrow$C, A$\leftrightarrow$B$\leftrightarrow$C) under malicious agents, on different downstream tasks? (2) How can we increase system resilience to defend against malicious agents? To simulate malicious agents, we devise two methods, AutoTransform and AutoInject, to transform any agent into a malicious one while preserving its functional integrity. We run comprehensive experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;ESG-AI&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#25237;&#36164;&#32773;&#35780;&#20272;&#21644;&#31649;&#29702;AI&#25237;&#36164;&#65292;&#21516;&#26102;&#25512;&#21160;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00965</link><description>&lt;p&gt;
&#29615;&#22659;&#12289;&#31038;&#20250;&#12289;&#27835;&#29702;(ESG)&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#65306;&#20840;&#38754;&#30340;&#36131;&#20219;AI&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Integrating ESG and AI: A Comprehensive Responsible AI Assessment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;ESG-AI&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#25237;&#36164;&#32773;&#35780;&#20272;&#21644;&#31649;&#29702;AI&#25237;&#36164;&#65292;&#21516;&#26102;&#25512;&#21160;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00965v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#21508;&#20010;&#34892;&#19994;&#37096;&#38376;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#23558;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#32771;&#34385;&#22240;&#32032;&#19982;AI&#25237;&#36164;&#30456;&#32467;&#21512;&#65292;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#25216;&#26415;&#19978;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#20174;&#25237;&#36164;&#32773;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#25972;&#21512;&#19981;&#20165;&#33021;&#22815;&#20943;&#36731;&#39118;&#38505;&#65292;&#36824;&#33021;&#22815;&#36890;&#36807;&#19982;&#26356;&#24191;&#27867;&#30340;&#20840;&#29699;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#25552;&#21319;&#38271;&#26399;&#30340;&#20215;&#20540;&#21019;&#36896;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#25506;&#35752;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;ESG-AI&#26694;&#26550;&#65292;&#36825;&#20010;&#26694;&#26550;&#26159;&#22522;&#20110;&#19982;28&#23478;&#20844;&#21496;&#21512;&#20316;&#32463;&#39564;&#20013;&#30340;&#27934;&#23519;&#32780;&#24320;&#21457;&#30340;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#12290;&#36890;&#36807;&#19982;&#19994;&#30028;&#23454;&#36341;&#32773;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#12290;ESG-AI&#26694;&#26550;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#24110;&#21161;&#20351;&#29992;&#32773;&#65292;&#22914;&#25237;&#36164;&#32773;&#65292;&#21435;&#35780;&#20272;&#21644;&#31649;&#29702;AI&#25237;&#36164;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#25512;&#21160;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#30446;&#26631;&#21644;&#30408;&#21033;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#35780;&#20272;&#24037;&#20855;&#65292;&#20197;&#30830;&#20445;&#35813;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00965v1 Announce Type: new  Abstract: Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors asse
&lt;/p&gt;</description></item><item><title>PERSOMA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#33021;&#39640;&#25928;&#22788;&#29702;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#12290;</title><link>https://arxiv.org/abs/2408.00960</link><description>&lt;p&gt;
PERSOMA&#20010;&#24615;&#21270;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#22312;&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00960
&lt;/p&gt;
&lt;p&gt;
PERSOMA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#33021;&#39640;&#25928;&#22788;&#29702;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PERSOMA&#30340;&#20010;&#24615;&#21270;&#36719;&#25552;&#31034;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#29992;&#25143;&#30340;&#24191;&#27867;&#20132;&#20114;&#21382;&#21490;&#36827;&#34892;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#29702;&#35299;&#12290;&#19982;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;PERSOMA&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#25429;&#33719;&#29992;&#25143;&#21382;&#21490;&#30340;&#26041;&#24335;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#20132;&#20114;&#30340;&#25805;&#20316;&#20943;&#23569;&#21644;&#21387;&#32553;&#31639;&#27861;&#65292;&#23558;&#29992;&#25143;&#20132;&#20114;&#20449;&#24687;&#36716;&#21270;&#20026;&#29305;&#24449;&#20016;&#23500;&#30340;&#36719;&#25552;&#31034;&#23884;&#20837;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#36890;&#36807;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#39564;&#35777;&#20102;PERSOMA&#26550;&#26500;&#65292;&#21253;&#25324;&#21028;&#26029;&#21508;&#31867;&#21442;&#25968;&#21387;&#32553;&#21644;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#22914;&#20302;&#31209;&#27880;&#24847;&#21147;&#65288;LoRA&#65289;&#31561;&#26041;&#27861;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#27979;&#35797;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;PERSOMA&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#22823;&#37327;&#30340;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#23884;&#20837;&#21644;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00960v1 Announce Type: cross  Abstract: Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v1 Announce Type: cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;WarpSci&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22312;GPU&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38656;&#35201;&#22797;&#26434;&#29615;&#22659;&#27169;&#22411;&#25968;&#25454;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2408.00930</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;GPU&#19978;&#23454;&#29616;&#39640;&#25968;&#25454;&#21534;&#21520;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#39046;&#22495;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain Agnostic Framework for Data-Driven Scientific Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00930
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;WarpSci&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22312;GPU&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38656;&#35201;&#22797;&#26434;&#29615;&#22659;&#27169;&#22411;&#25968;&#25454;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#25105;&#20204;&#20171;&#32461;WarpSci&#65292;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#20811;&#26381;&#22312;&#20855;&#26377;&#22823;&#37327;&#39640;&#32500;&#35266;&#27979;&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#31995;&#32479;&#29942;&#39048;&#30340;&#39046;&#22495;&#26080;&#20851;&#26694;&#26550;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204; framework eliminates the need for data transfer between the CPU and GPU&#65292;&#20351;&#21333;&#20010;&#25110;&#22810;&#20010;GPU&#19978;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#25968;&#21315;&#20010;&#27169;&#25311;&#12290;&#36825;&#23545;&#20110;&#30740;&#31350;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#30740;&#31350;&#29305;&#21035;&#26377;&#21033;&#65292;&#22240;&#20026;&#22312;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#27169;&#22411;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00930v1 Announce Type: cross  Abstract: We introduce WarpSci, a domain agnostic framework designed to overcome crucial system bottlenecks encountered in the application of reinforcement learning to intricate environments with vast datasets featuring high-dimensional observation or action spaces. Notably, our framework eliminates the need for data transfer between the CPU and GPU, enabling the concurrent execution of thousands of simulations on a single or multiple GPUs. This high data throughput architecture proves particularly advantageous for data-driven scientific research, where intricate environment models are commonly essential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#39033;&#26032;&#30340;&#25968;&#25454;&#27844;&#38706;&#25216;&#26415;&#65292;&#21363;&#22312;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#26102;&#65292;&#36890;&#36807;&#27880;&#20837;&#24694;&#24847;&#25351;&#20196;&#24182;&#20351;&#29992;GCG&#21518;&#32512;&#25552;&#39640;&#25968;&#25454;&#27844;&#38706;&#30340;&#25104;&#21151;&#29575;&#65292;&#23588;&#20854;&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#23545;&#19994;&#21153;&#36896;&#25104;&#32422;450&#19975;&#32654;&#20803;&#30340;&#25439;&#22833;&#12290;&#34429;&#28982;&#23384;&#22312;&#39118;&#38505;&#65292;&#20294;&#36890;&#36807;&#31649;&#29702;&#21644;&#23433;&#20840;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#38477;&#20302;&#27492;&#31867;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.00925</link><description>&lt;p&gt;
&#30333;&#30382;&#20070;&#65306;&#21033;&#29992;GCG&#21518;&#32512;&#30340;&#25968;&#25454;&#27844;&#38706;&#31616;&#35201;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#39033;&#26032;&#30340;&#25968;&#25454;&#27844;&#38706;&#25216;&#26415;&#65292;&#21363;&#22312;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#26102;&#65292;&#36890;&#36807;&#27880;&#20837;&#24694;&#24847;&#25351;&#20196;&#24182;&#20351;&#29992;GCG&#21518;&#32512;&#25552;&#39640;&#25968;&#25454;&#27844;&#38706;&#30340;&#25104;&#21151;&#29575;&#65292;&#23588;&#20854;&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#23545;&#19994;&#21153;&#36896;&#25104;&#32422;450&#19975;&#32654;&#20803;&#30340;&#25439;&#22833;&#12290;&#34429;&#28982;&#23384;&#22312;&#39118;&#38505;&#65292;&#20294;&#36890;&#36807;&#31649;&#29702;&#21644;&#23433;&#20840;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#38477;&#20302;&#27492;&#31867;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00925v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20132;&#21449;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65288;XPIA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#25968;&#25454;&#27844;&#38706;&#30340;&#25216;&#26415;&#65292;&#36817;&#26469;&#22312;&#35813;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;XPIA&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#33021;&#22815;&#22312;&#31532;&#19977;&#26041;&#25968;&#25454;&#20013;&#27880;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#24456;&#21487;&#33021;&#20250;&#34987;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36741;&#21161;&#29992;&#25143;&#26102;&#28040;&#36153;&#65292;&#29992;&#25143;&#25104;&#20026;&#21463;&#23475;&#32773;&#12290;XPIA&#36890;&#24120;&#34987;&#29992;&#20316;&#25968;&#25454;&#27844;&#38706;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20272;&#35745;&#19968;&#27425;&#24179;&#22343;&#25968;&#25454;&#27844;&#38706;&#30340;&#25104;&#26412;&#23545;&#20225;&#19994;&#26469;&#35828;&#25509;&#36817;450&#19975;&#32654;&#20803;&#65292;&#36825;&#19968;&#25104;&#26412;&#21253;&#25324;&#20102;&#22914;&#20225;&#19994;&#20973;&#35777;&#34987;&#31713;&#25913;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#27844;&#38706;&#12290;&#38543;&#30528;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#65292;&#22914;GCG&#21518;&#32512;&#25915;&#20987;&#30340;&#20852;&#36215;&#65292;&#20351;&#29992;GCG&#21518;&#32512;&#30340;XPIA&#21457;&#29983;&#30340;&#27010;&#29575;&#20196;&#20154;&#25285;&#24551;&#12290;&#22312;&#25105;&#20316;&#20026;Microsoft&#20154;&#24037;&#26234;&#33021;&#32418;&#38431;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#30340;XPIA&#22330;&#26223;&#20013;&#20351;&#29992;GCG&#21518;&#32512;&#37197;&#21512;&#27880;&#20837;&#28431;&#27934;&#30340;&#21487;&#34892;&#24615;&#25915;&#20987;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23384;&#22312;GCG&#21518;&#32512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#25968;&#25454;&#27844;&#38706;&#30340;&#27010;&#29575;&#20960;&#20046;&#22686;&#21152;&#20102;20%&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#27844;&#38706;&#30340;&#21487;&#33021;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#23454;&#38469;&#30340;&#25104;&#21151;&#29575;&#65292;&#22240;&#20026;&#38500;&#20102;&#25216;&#26415;&#22240;&#32032;&#22806;&#65292;&#36824;&#26377;&#31649;&#29702;&#21644;&#23433;&#20840;&#31574;&#30053;&#31561;&#38750;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20026;&#25968;&#25454;&#20445;&#25252;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#21644;&#24605;&#32771;&#26041;&#21521;&#65292;&#21516;&#26102;&#20063;&#20026;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#25552;&#20379;&#21442;&#32771;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00925v1 Announce Type: cross  Abstract: The cross-prompt injection attack (XPIA) is an effective technique that can be used for data exfiltration, and that has seen increasing use. In this attack, the attacker injects a malicious instruction into third party data which an LLM is likely to consume when assisting a user, who is the victim. XPIA is often used as a means for data exfiltration, and the estimated cost of the average data breach for a business is nearly $4.5 million, which includes breaches such as compromised enterprise credentials. With the rise of gradient-based attacks such as the GCG suffix attack, the odds of an XPIA occurring which uses a GCG suffix are worryingly high. As part of my work in Microsoft's AI Red Team, I demonstrated a viable attack model using a GCG suffix paired with an injection in a simulated XPIA scenario. The results indicate that the presence of a GCG suffix can increase the odds of successful data exfiltration by nearly 20%, with some c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20107;&#20214;&#26816;&#27979;&#20013;&#20351;&#29992;GPT-4&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#20026;GPT-4&#25552;&#20379;&#30340;&#8220;&#35768;&#21487;&#35777;&#21644;&#26426;&#20250;&#8221;&#65288;L&amp;O&#65289;&#65292;&#23454;&#29616;&#20102;0.759 AUC&#30340;&#32622;&#20449;&#24230;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2408.00914</link><description>&lt;p&gt;
&#25480;&#20104; GPT-4 &#35768;&#21487;&#35777;&#21644;&#26426;&#20250;&#65306;&#22686;&#24378;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Granting GPT-4 License and Opportunity: Enhancing Accuracy and Confidence Estimation for Few-Shot Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20107;&#20214;&#26816;&#27979;&#20013;&#20351;&#29992;GPT-4&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#20026;GPT-4&#25552;&#20379;&#30340;&#8220;&#35768;&#21487;&#35777;&#21644;&#26426;&#20250;&#8221;&#65288;L&amp;O&#65289;&#65292;&#23454;&#29616;&#20102;0.759 AUC&#30340;&#32622;&#20449;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00914v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#32763;&#35793;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914; GPT-4&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#20013;&#26174;&#31034;&#20986;&#36275;&#22815;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#8220;&#38134;&#33394;&#8221;&#25968;&#25454;&#21644;&#26032;&#30693;&#35782;&#24211;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#21644;&#23457;&#26597;&#65292;&#36825;&#26679;&#30340;&#24037;&#20316;&#27969;&#31243;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#12290;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26159;&#27492;&#31867;&#27169;&#22411;&#22914; GPT-4&#30340;&#24050;&#25253;&#21578;&#24369;&#28857;&#65292;&#32780;&#34917;&#20607;&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102; GPT-4&#22312;&#22522;&#20110; BETTER &#30693;&#35782;&#24211;&#30340;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26377;&#25928;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#20851;&#38190;&#21019;&#26032;&#26159;&#23558;&#25552;&#20379;&#32473; GPT-4&#30340;&#25552;&#31034;&#21644;&#20219;&#21153;&#25193;&#23637;&#20026;&#25552;&#20379;&#19968;&#20010;&#20801;&#35768;&#29468;&#27979;&#32780;&#19981;&#30830;&#23450;&#30340;&#35768;&#21487;&#35777;&#21644;&#37327;&#21270;&#24182;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#20250;&#65288;L&amp;O&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#39069;&#22806;&#35774;&#22791;&#21363;&#21487;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#21487;&#29992;&#30340;&#32622;&#20449;&#24230;&#25351;&#26631;&#65288;0.759 AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00914v1 Announce Type: new  Abstract: Large Language Models (LLMs) such as GPT-4 have shown enough promise in the few-shot learning context to suggest use in the generation of "silver" data and refinement of new ontologies through iterative application and review. Such workflows become more effective with reliable confidence estimation. Unfortunately, confidence estimation is a documented weakness of models such as GPT-4, and established methods to compensate require significant additional complexity and computation. The present effort explores methods for effective confidence estimation with GPT-4 with few-shot learning for event detection in the BETTER ontology as a vehicle. The key innovation is expanding the prompt and task presented to GPT-4 to provide License to speculate when unsure and Opportunity to quantify and explain its uncertainty (L&amp;O). This approach improves accuracy and provides usable confidence measures (0.759 AUC) with no additional machinery.
&lt;/p&gt;</description></item><item><title>AnoT&#36890;&#36807;&#23558;&#26102;&#24207;&#30693;&#35782;&#22270;&#36716;&#25442;&#25104;&#35268;&#21017;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36866;&#24212;&#30693;&#35782;&#26356;&#26032;&#30340;&#27169;&#24335;&#21464;&#21270;&#21644;&#35821;&#20041;&#28418;&#31227;&#12290;</title><link>https://arxiv.org/abs/2408.00872</link><description>&lt;p&gt;
&#26102;&#24207;&#30693;&#35782;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Online Detection of Anomalies in Temporal Knowledge Graphs with Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00872
&lt;/p&gt;
&lt;p&gt;
AnoT&#36890;&#36807;&#23558;&#26102;&#24207;&#30693;&#35782;&#22270;&#36716;&#25442;&#25104;&#35268;&#21017;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36866;&#24212;&#30693;&#35782;&#26356;&#26032;&#30340;&#27169;&#24335;&#21464;&#21270;&#21644;&#35821;&#20041;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#30693;&#35782;&#22270;&#65288;TKGs&#65289;&#23545;&#20110;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#28436;&#21464;&#20851;&#31995;&#38750;&#24120;&#23453;&#36149;&#65292;&#20294;&#24448;&#24448;&#20805;&#26021;&#30528;&#22122;&#22768;&#65292;&#22240;&#27492;&#38656;&#35201;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22312;&#25429;&#25417;TKGs&#20013;&#33410;&#28857;&#21644;&#36793;&#31867;&#30340;&#20016;&#23500;&#35821;&#20041;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#32780;TKG&#23884;&#20837;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#21066;&#24369;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36866;&#24212;&#22240;&#30693;&#35782;&#26356;&#26032;&#32780;&#23548;&#33268;&#30340;&#27169;&#24335;&#21464;&#21270;&#21644;&#35821;&#20041;&#28418;&#31227;&#26041;&#38754;&#20063;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AnoT&#65292;&#19968;&#31181;&#38024;&#23545;TKG&#30340;&#39640;&#25928;&#21487;&#35299;&#37322;&#24615;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;AnoT&#39318;&#20808;&#23558;TKG&#24635;&#32467;&#20026;&#19968;&#31181;&#26032;&#30340;&#35268;&#21017;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;TKGs&#20013;&#28789;&#27963;&#22320;&#25512;&#26029;&#20986;&#22797;&#26434;&#30340;&#27169;&#24335;&#12290;&#24403;&#26032;&#30340;&#30693;&#35782;&#20986;&#29616;&#26102;&#65292;AnoT&#23558;&#23427;&#26144;&#23556;&#21040;&#35268;&#21017;&#22270;&#20013;&#30340;&#19968;&#20010;&#33410;&#28857;&#19978;&#65292;&#24182;&#36882;&#24402;&#22320;&#36941;&#21382;&#35268;&#21017;&#22270;&#26469;&#25512;&#23548;&#20986;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00872v1 Announce Type: new  Abstract: Temporal knowledge graphs (TKGs) are valuable resources for capturing evolving relationships among entities, yet they are often plagued by noise, necessitating robust anomaly detection mechanisms. Existing dynamic graph anomaly detection approaches struggle to capture the rich semantics introduced by node and edge categories within TKGs, while TKG embedding methods lack interpretability, undermining the credibility of anomaly detection. Moreover, these methods falter in adapting to pattern changes and semantic drifts resulting from knowledge updates. To tackle these challenges, we introduce AnoT, an efficient TKG summarization method tailored for interpretable online anomaly detection in TKGs. AnoT begins by summarizing a TKG into a novel rule graph, enabling flexible inference of complex patterns in TKGs. When new knowledge emerges, AnoT maps it onto a node in the rule graph and traverses the rule graph recursively to derive the anomaly
&lt;/p&gt;</description></item><item><title>UniMoT&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#23376;-&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;tokenizer&#65292;&#23427;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#23558;&#20998;&#23376;&#36716;&#25442;&#25104;&#24207;&#21015;&#30340;&#20998;&#23376;&#20196;&#29260;&#65292;&#23454;&#29616;&#20998;&#23376;&#19982;&#25991;&#26412;&#30340;&#39640;&#25928;&#38598;&#25104;&#12290;</title><link>https://arxiv.org/abs/2408.00863</link><description>&lt;p&gt;
UniMoT&#65306;&#20855;&#26377;&#31163;&#25955;&#20196;&#29260;&#34920;&#31034;&#30340;&#32479;&#19968;&#20998;&#23376;-&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00863
&lt;/p&gt;
&lt;p&gt;
UniMoT&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#23376;-&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;tokenizer&#65292;&#23427;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#23558;&#20998;&#23376;&#36716;&#25442;&#25104;&#24207;&#21015;&#30340;&#20998;&#23376;&#20196;&#29260;&#65292;&#23454;&#29616;&#20998;&#23376;&#19982;&#25991;&#26412;&#30340;&#39640;&#25928;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00863v1 Announce Type: cross  &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#25512;&#21160;&#30740;&#31350;&#31038;&#21306;&#25193;&#23637;&#20854;&#33021;&#21147;&#21040;&#20998;&#23376;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#23376;LLMs&#20351;&#29992;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#19981;&#24179;&#31561;&#22320;&#23545;&#24453;&#20998;&#23376;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#20998;&#23376;&#27169;&#24577;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniMoT&#65292;&#19968;&#20010;&#32479;&#19968;&#20998;&#23376;-&#25991;&#26412;LLM&#65292;&#37319;&#29992;&#22522;&#20110;tokenizer&#30340;&#26550;&#26500;&#65292;&#23558;&#20998;&#23376;&#20196;&#29260;&#25193;&#23637;&#21040;LLM&#30340;&#35789;&#27719;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#65288;Vector Quantization&#65289;&#30340;tokenizer&#65292;&#20854;&#37319;&#29992;Q-Former&#26469;&#24357;&#21512;&#20998;&#23376;&#19982;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;tokenizer&#23558;&#20998;&#23376;&#36716;&#25442;&#20026;&#20855;&#26377;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#30340;&#20998;&#23376;&#20196;&#29260;&#24207;&#21015;&#65292;&#23553;&#35013;&#20102;&#20998;&#23376;&#21644;&#25991;&#26412;&#30340;&#39640;&#23618;&#20449;&#24687;&#12290;&#35013;&#22791;&#20102;&#36825;&#20010;tokenizer&#65292;UniMoT&#21487;&#20197;&#23558;&#20998;&#23376;&#21644;&#25991;&#26412;&#27169;&#24577;&#32479;&#19968;&#21040;&#20849;&#20139;&#20196;&#29260;&#34920;&#31034;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#30495;&#27491;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00863v1 Announce Type: cross  Abstract: The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive
&lt;/p&gt;</description></item><item><title>&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#19982;&#31070;&#32463;&#28210;&#26579;&#32467;&#21512;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D&#36229;&#22768;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00860</link><description>&lt;p&gt;
UlRe-NeRF: &#20351;&#29992;&#31070;&#32463;&#28210;&#26579;&#30340;3D&#36229;&#22768;&#25104;&#20687;&#65292;&#36890;&#36807;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#19982;&#31070;&#32463;&#28210;&#26579;&#32467;&#21512;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D&#36229;&#22768;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25552;&#20132;  &#25688;&#35201;&#65306;&#19977;&#32500;&#36229;&#22768;&#25104;&#20687;&#22312;&#21307;&#30103;&#35786;&#26029;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#19977;&#32500;&#36229;&#22768;&#25104;&#20687;&#26041;&#27861;&#23384;&#22312;&#22266;&#23450;&#20998;&#36776;&#29575;&#12289;&#23384;&#20648;&#25928;&#29575;&#20302;&#12289;&#19978;&#19979;&#25991;&#36830;&#25509;&#19981;&#36275;&#31561;&#38382;&#39064;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#24322;&#24120;&#21644;&#21453;&#23556;&#29305;&#24615;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;NeRF&#65288;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#30340;&#25216;&#26415;&#22312;&#35270;&#35282;&#21512;&#25104;&#21644;&#19977;&#32500;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#65292;&#20294;&#22312;&#39640;&#28165;&#26224;&#24230;&#36229;&#22768;&#25104;&#20687;&#26041;&#38754;&#20173;&#26377;&#30740;&#31350;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;UlRe-NeRF&#65292;&#23427;&#23558;&#38544;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26174;&#24335;&#30340;&#36229;&#22768;&#20307;&#32472;&#21046;&#21151;&#33021;&#38598;&#25104;&#21040;&#19968;&#20010;&#36229;&#22768;&#31070;&#32463;&#28210;&#26579;&#26550;&#26500;&#20013;&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#20102;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#21644;&#35856;&#27874;&#32534;&#30721;&#65292;&#20351;&#29992;&#26041;&#21521;&#24615;MLP&#27169;&#22359;&#29983;&#25104;&#35270;&#35282;&#20381;&#36182;&#30340;&#39640;&#39057;&#21453;&#23556;&#24378;&#24230;&#20272;&#35745;&#65292;&#20197;&#21450;&#19968;&#20010;&#31354;&#38388;MLP&#27169;&#22359;&#26469;&#20272;&#35745;&#25972;&#20010;&#31354;&#38388;&#20869;&#30340;&#21453;&#23556;&#24378;&#24230;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#25216;&#26415;&#65292;UlRe-NeRF&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#29289;&#29702;&#30340;3D ultrasound&#28210;&#26579;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#23384;&#20648;&#21644;&#26356;&#20248;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#21453;&#23556;&#21644;&#36879;&#35270;&#38382;&#39064;&#26102;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UlRe-NeRF&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#36229;&#22768;&#25104;&#20687;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v1 Announce Type: new  Abstract: Three-dimensional ultrasound imaging is a critical technology widely used in medical diagnostics. However, traditional 3D ultrasound imaging methods have limitations such as fixed resolution, low storage efficiency, and insufficient contextual connectivity, leading to poor performance in handling complex artifacts and reflection characteristics. Recently, techniques based on NeRF (Neural Radiance Fields) have made significant progress in view synthesis and 3D reconstruction, but there remains a research gap in high-quality ultrasound imaging. To address these issues, we propose a new model, UlRe-NeRF, which combines implicit neural networks and explicit ultrasound volume rendering into an ultrasound neural rendering architecture. This model incorporates reflection direction parameterization and harmonic encoding, using a directional MLP module to generate view-dependent high-frequency reflection intensity estimates, and a spatial MLP mod
&lt;/p&gt;</description></item><item><title>Y &#31038;&#20132;&#26159;&#19968;&#20010;&#20351;&#29992;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21452;&#32990;&#32974;&#65292;&#23427;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#22312;&#32447;&#24179;&#21488;&#30340;&#21160;&#24577;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2408.00818</link><description>&lt;p&gt;
Y &#31038;&#20132;&#65306;&#22522;&#20110; LLMs &#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21452;&#32990;&#32974;
&lt;/p&gt;
&lt;p&gt;
Y Social: an LLM-powered Social Media Digital Twin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00818
&lt;/p&gt;
&lt;p&gt;
Y &#31038;&#20132;&#26159;&#19968;&#20010;&#20351;&#29992;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21452;&#32990;&#32974;&#65292;&#23427;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#22312;&#32447;&#24179;&#21488;&#30340;&#21160;&#24577;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#21452;&#32990;&#32974; Y&#65292;&#23427;&#26088;&#22312;&#27169;&#25311;&#19968;&#20010;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#12290;&#25968;&#23383;&#21452;&#32990;&#32974;&#26159;&#29289;&#29702;&#31995;&#32479;&#30340;&#34394;&#25311;&#22797;&#21046;&#21697;&#65292;&#29992;&#20110;&#36827;&#34892;&#39640;&#32423;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20687; Y &#36825;&#26679;&#30340;&#25968;&#23383;&#21452;&#32990;&#32974;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#27169;&#25311;&#21644;&#29702;&#35299;&#22797;&#26434;&#22312;&#32447;&#20114;&#21160;&#12290;Y &#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21487;&#20197;&#22797;&#29616;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#20934;&#30830;&#22320;&#27169;&#25311;&#29992;&#25143;&#20114;&#21160;&#12289;&#20869;&#23481;&#20256;&#25773;&#21644;&#32593;&#32476;&#21160;&#24577;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#26041;&#38754;&#65292;Y &#25552;&#20379;&#26377;&#20851;&#29992;&#25143;&#21442;&#19982;&#24230;&#12289;&#20449;&#24687;&#20256;&#25773;&#21644;&#24179;&#21488;&#25919;&#31574;&#24433;&#21709;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38598;&#25104; LLMs&#65292;Y &#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#30340;&#25991;&#26412;&#20869;&#23481;&#21644;&#39044;&#27979;&#29992;&#25143;&#21453;&#24212;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00818v1 Announce Type: new  Abstract: In this paper we introduce Y, a new-generation digital twin designed to replicate an online social media platform. Digital twins are virtual replicas of physical systems that allow for advanced analyses and experimentation. In the case of social media, a digital twin such as Y provides a powerful tool for researchers to simulate and understand complex online interactions. {\tt Y} leverages state-of-the-art Large Language Models (LLMs) to replicate sophisticated agent behaviors, enabling accurate simulations of user interactions, content dissemination, and network dynamics. By integrating these aspects, Y offers valuable insights into user engagement, information spread, and the impact of platform policies. Moreover, the integration of LLMs allows Y to generate nuanced textual content and predict user responses, facilitating the study of emergent phenomena in online environments.   To better characterize the proposed digital twin, in this
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#30899;&#25490;&#25918;&#65292;&#29305;&#21035;&#26159;&#22312;&#21160;&#24577;&#20132;&#36890;&#26465;&#20214;&#19979;&#24615;&#33021;&#26356;&#20339;&#65292;&#26174;&#31034;&#20102;&#20854;&#24191;&#38420;&#30340;&#23454;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2408.00814</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36866;&#24212;&#24615;&#20132;&#36890;&#20449;&#21495;&#23433;&#20840;&#19982;&#25928;&#29575;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal safety and efficiency improvement by multi objective deep reinforcement learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#30899;&#25490;&#25918;&#65292;&#29305;&#21035;&#26159;&#22312;&#21160;&#24577;&#20132;&#36890;&#26465;&#20214;&#19979;&#24615;&#33021;&#26356;&#20339;&#65292;&#26174;&#31034;&#20102;&#20854;&#24191;&#38420;&#30340;&#23454;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;ATSC&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#26696;&#26088;&#22312;&#25552;&#39640;&#36335;&#21475;&#25511;&#21046;&#31574;&#30053;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#33073;&#30899;&#21270;&#19977;&#20010;&#30446;&#26631;&#12290;&#20256;&#32479;&#30340;ATSC&#26041;&#27861;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#20132;&#36890;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#36866;&#24212;&#23454;&#26102;&#21160;&#24577;&#20132;&#36890;&#26465;&#20214;&#26102;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dueling Double Deep Q Network&#65288;D3QN&#65289;&#26694;&#26550;&#30340;DRL-based ATSC&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#20013;&#22269;&#30340;&#38271;&#27801;&#30340;&#19968;&#20010;&#27169;&#25311;&#36335;&#21475;&#30340;&#34920;&#29616;&#34987;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#20256;&#32479;&#30340;ATSC&#21644;&#21333;&#32431;&#20248;&#21270;&#25928;&#29575;&#30340;ATSC&#31639;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;ATSC&#31639;&#27861;&#36890;&#36807;&#23454;&#29616;&#20132;&#36890;&#20107;&#25925;&#20943;&#23569;&#20102;16%&#21644;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#20943;&#23569;&#20102;4%&#65292;&#31361;&#20986;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#36824;&#33021;&#22312;&#21464;&#21270;&#22810;&#31471;&#30340;&#20132;&#36890;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00814v1 Announce Type: cross  Abstract: This research introduces an innovative method for adaptive traffic signal control (ATSC) through the utilization of multi-objective deep reinforcement learning (DRL) techniques. The proposed approach aims to enhance control strategies at intersections while simultaneously addressing safety, efficiency, and decarbonization objectives. Traditional ATSC methods typically prioritize traffic efficiency and often struggle to adapt to real-time dynamic traffic conditions. To address these challenges, the study suggests a DRL-based ATSC algorithm that incorporates the Dueling Double Deep Q Network (D3QN) framework. The performance of this algorithm is assessed using a simulated intersection in Changsha, China. Notably, the proposed ATSC algorithm surpasses both traditional ATSC and ATSC algorithms focused solely on efficiency optimization by achieving over a 16% reduction in traffic conflicts and a 4% decrease in carbon emissions. Regarding tr
&lt;/p&gt;</description></item><item><title>ChipExpert&#26159;&#19987;&#38376;&#20026;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#39640;&#38376;&#27099;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.00804</link><description>&lt;p&gt;
ChipExpert&#65306;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#19987;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00804
&lt;/p&gt;
&lt;p&gt;
ChipExpert&#26159;&#19987;&#38376;&#20026;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#39640;&#38376;&#27099;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00804v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#39640;&#24230;&#30340;&#19987;&#19994;&#24615;&#65292;&#20026;&#20837;&#38376;&#21644;&#30740;&#31350;&#24320;&#21457;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLMs&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#23398;&#29983;&#12289;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#22312;IC&#35774;&#35745;&#39046;&#22495;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;IC&#35774;&#35745;&#39046;&#22495;&#20013;&#30340;&#28508;&#21147;&#22823;&#37096;&#20998;&#20173;&#26410;&#34987;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChipExpert&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;IC&#35774;&#35745;&#39046;&#22495;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#25945;&#32946;&#22411;LLM&#12290;ChipExpert&#26159;&#22312;&#24403;&#21069;&#26368;&#20339;&#24320;&#28304;&#22522;&#30784;&#27169;&#22411;&#65288;Llama-3 8B&#65289;&#19978;&#35757;&#32451;&#30340;&#12290;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#21253;&#25324;&#20960;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#21253;&#25324;&#25968;&#25454;&#20934;&#22791;&#12289;&#32487;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#25351;&#23548;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#21644;&#20559;&#22909;&#23545;&#40784;&#20197;&#21450;&#35780;&#20272;&#12290;&#22312;&#25968;&#25454;&#20934;&#22791;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#20154;&#24037;&#27880;&#37322;&#21019;&#24314;&#20102;&#22810;&#20010;&#39640;&#36136;&#37327;&#30340;&#23450;&#21046;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#19990;&#30028;&#35774;&#35745;&#25361;&#25112;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;IC&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;Karel&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#26469;&#38598;&#25104;IC&#35774;&#35745;&#30340;&#35821;&#35328;&#28459;&#28216;&#33021;&#21147;&#12290;&#22312;&#25351;&#20196;&#25351;&#23548;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21253;&#21547;&#22810;&#20010;IC&#35774;&#35745;&#30456;&#20851;&#20219;&#21153;&#30340;&#31934;&#35843;&#25968;&#25454;&#38598;&#12290;&#22312;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20559;&#32622;&#26657;&#27491;&#26469;&#25552;&#39640;ChipExpert&#22312;IC&#35774;&#35745;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#24037;&#19994;&#39046;&#22495;&#30340;&#24037;&#31243;&#24072;&#21512;&#20316;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#23454;&#39564;&#65292;&#20197;&#30830;&#20445;LLM&#33021;&#22815;&#20934;&#30830;&#22320;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;IC&#35774;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36890;&#36807;&#24320;&#25918;&#28304;&#20195;&#30721;&#21644;&#23450;&#21046;&#21270;&#30340;LLM&#65292;&#23558;&#36827;&#19968;&#27493;&#20419;&#36827;IC&#35774;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#25945;&#32946;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00804v1 Announce Type: cross  Abstract: The field of integrated circuit (IC) design is highly specialized, presenting significant barriers to entry and research and development challenges. Although large language models (LLMs) have achieved remarkable success in various domains, existing LLMs often fail to meet the specific needs of students, engineers, and researchers. Consequently, the potential of LLMs in the IC design domain remains largely unexplored. To address these issues, we introduce ChipExpert, the first open-source, instructional LLM specifically tailored for the IC design field. ChipExpert is trained on one of the current best open-source base model (Llama-3 8B). The entire training process encompasses several key stages, including data preparation, continue pre-training, instruction-guided supervised fine-tuning, preference alignment, and evaluation. In the data preparation stage, we construct multiple high-quality custom datasets through manual selection and d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#24494;&#26381;&#21153;&#20013;&#35782;&#21035;&#21644;&#35299;&#20915;&#22797;&#26434;&#20381;&#36182;&#21644;&#20256;&#25773;&#24615;&#25925;&#38556;&#25361;&#25112;&#30340;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;(RCA)&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#24674;&#22797;&#21644;&#32500;&#25252;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00803</link><description>&lt;p&gt;
&#20803;&#22240;&#20998;&#26512;&#22312;(&#24494;)&#26381;&#21153;&#20013;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Root Cause Analysis in (Micro) Services: Methodologies, Challenges, and Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#24494;&#26381;&#21153;&#20013;&#35782;&#21035;&#21644;&#35299;&#20915;&#22797;&#26434;&#20381;&#36182;&#21644;&#20256;&#25773;&#24615;&#25925;&#38556;&#25361;&#25112;&#30340;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;(RCA)&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#24674;&#22797;&#21644;&#32500;&#25252;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00803v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#24494;&#26381;&#21153;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#21644;&#20256;&#25773;&#24615;&#25925;&#38556;&#29305;&#24615;&#65292;&#36825;&#20123;&#26381;&#21153;&#20197;&#20854;&#23494;&#38598;&#30340;&#32593;&#32476;&#20114;&#32852;&#29305;&#24615;&#20026;&#26631;&#24535;&#65292;&#22312;&#30830;&#23450;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36805;&#36895;&#35782;&#21035;&#21644;&#35299;&#20915;&#30772;&#22351;&#24615;&#30340;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#24555;&#36895;&#24674;&#22797;&#21644;&#32500;&#25252;&#31995;&#32479;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#30151;&#29366;&#25968;&#25454;&#35786;&#26029;&#25925;&#38556;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;&#24494;&#26381;&#21153;&#20013;&#20803;&#22240;&#20998;&#26512;(RCA)&#25216;&#26415;&#30340;&#20840;&#38754;&#12289;&#32467;&#26500;&#21270;&#22238;&#39038;&#65292;&#25506;&#32034;&#21253;&#25324;&#24230;&#37327;&#12289;&#36319;&#36394;&#12289;&#26085;&#24535;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#22312;&#20869;&#30340;&#26041;&#27861;&#35770;&#12290;&#23427;&#28145;&#20837;&#25506;&#35752;&#20102;&#24494;&#26381;&#21153;&#26550;&#26500;&#20013;&#30340;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#20301;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#21160;&#21270;&#36827;&#27493;&#30340;&#26368;&#21069;&#27839;&#65292;&#23427;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00803v1 Announce Type: cross  Abstract: The complex dependencies and propagative faults inherent in microservices, characterized by a dense network of interconnected services, pose significant challenges in identifying the underlying causes of issues. Prompt identification and resolution of disruptive problems are crucial to ensure rapid recovery and maintain system stability. Numerous methodologies have emerged to address this challenge, primarily focusing on diagnosing failures through symptomatic data. This survey aims to provide a comprehensive, structured review of root cause analysis (RCA) techniques within microservices, exploring methodologies that include metrics, traces, logs, and multi-model data. It delves deeper into the methodologies, challenges, and future trends within microservices architectures. Positioned at the forefront of AI and automation advancements, it offers guidance for future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#24335;&#26412;&#20307;&#20132;&#20114;&#27010;&#24565;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#25552;&#38382;&#20934;&#30830;&#36716;&#21270;&#20026;SPARQL&#26597;&#35810;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#34394;&#20551;&#20449;&#24687;&#30340;&#20135;&#29983;&#12290;</title><link>https://arxiv.org/abs/2408.00800</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#24335;&#26412;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#24335;&#26412;&#20307;&#20132;&#20114;&#27010;&#24565;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#25552;&#38382;&#20934;&#30830;&#36716;&#21270;&#20026;SPARQL&#26597;&#35810;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#34394;&#20551;&#20449;&#24687;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#36129;&#29486;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#65292;&#23427;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#30028;&#38754;&#26469;&#22686;&#24378;&#26412;&#20307;&#35770;&#30340;SPARQL&#26597;&#35810;&#29983;&#25104;&#65292;&#20174;&#32780;&#20026;&#27491;&#24335;&#30693;&#35782;&#25552;&#20379;&#30452;&#35266;&#30340;&#35775;&#38382;&#36884;&#24452;&#12290;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25552;&#38382;&#65292;&#31995;&#32479;&#23558;&#36825;&#20123;&#25552;&#38382;&#36716;&#25442;&#25104;&#20934;&#30830;&#30340;SPARQL&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20005;&#26684;&#26597;&#35810;&#26412;&#20307;&#35770;&#30340;&#20869;&#23481;&#65292;&#38450;&#27490;&#30001;LLM&#24102;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#34394;&#26500;&#20869;&#23481;&#12290;&#20026;&#20102;&#25552;&#39640;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#65292;&#23558;&#26469;&#33258;&#39046;&#22495;&#29305;&#23450;&#26631;&#20934;&#30340;&#39069;&#22806;&#25991;&#26412;&#20449;&#24687;&#25972;&#21512;&#21040;&#26412;&#20307;&#35770;&#20013;&#65292;&#20197;&#20415;&#23545;&#23427;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#31934;&#30830;&#25551;&#36848;&#12290;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;SPARQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;LLMs&#26597;&#35810;&#26412;&#20307;&#35770;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00800v1 Announce Type: cross  Abstract: The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21517;&#20026;Golden-Retriever&#30340;&#31995;&#32479;&#34987;&#35774;&#35745;&#29992;&#26469;&#22686;&#24378;&#24037;&#19994;&#30693;&#35782;&#24211;&#30340;&#26816;&#32034;&#25928;&#29575;&#65292;&#36890;&#36807;&#25512;&#25970;&#24335;&#30340;&#26597;&#35810;&#22686;&#24378;&#21644;&#26126;&#30830;&#30340;&#19978;&#19979;&#25991;&#35299;&#37322;&#65292;&#25552;&#21319;&#20102;RAG&#26694;&#26550;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00798</link><description>&lt;p&gt;
Golden-Retriever: &#39640;&#20445;&#30495;&#33021;&#21160;&#24615;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22312;&#24037;&#19994;&#30693;&#35782;&#24211;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00798
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21517;&#20026;Golden-Retriever&#30340;&#31995;&#32479;&#34987;&#35774;&#35745;&#29992;&#26469;&#22686;&#24378;&#24037;&#19994;&#30693;&#35782;&#24211;&#30340;&#26816;&#32034;&#25928;&#29575;&#65292;&#36890;&#36807;&#25512;&#25970;&#24335;&#30340;&#26597;&#35810;&#22686;&#24378;&#21644;&#26126;&#30830;&#30340;&#19978;&#19979;&#25991;&#35299;&#37322;&#65292;&#25552;&#21319;&#20102;RAG&#26694;&#26550;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00798v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26412;&#25991;&#20171;&#32461;&#20102;Golden-Retriever&#65292;&#26088;&#22312;&#39640;&#25928;&#22320;&#23548;&#33322;&#24222;&#22823;&#30340;&#24037;&#19994;&#30693;&#35782;&#24211;&#65292;&#20811;&#26381;&#20102;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;(RAG)&#26694;&#26550;&#22312;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#21644;&#19978;&#19979;&#25991;&#35299;&#37322;&#26041;&#38754;&#30340;&#20256;&#32479;&#25361;&#25112;&#12290;Golden-Retriever&#22312;&#25991;&#26723;&#26816;&#32034;&#21069;&#23454;&#26045;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24605;&#30340;&#26597;&#35810;&#22686;&#24378;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#28041;&#21450;&#35782;&#21035;&#36755;&#20837;&#38382;&#39064;&#20013;&#30340;&#26415;&#35821;&#12289;&#22522;&#20110;&#19978;&#19979;&#25991;&#28548;&#28165;&#20854;&#24847;&#20041;&#65292;&#24182;&#22312;&#22686;&#24378;&#26597;&#35810;&#20043;&#21069;&#21015;&#20986;&#25152;&#26377;&#26415;&#35821;&#21644;&#32553;&#20889;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#28041;&#21450;&#20174;&#36755;&#20837;&#38382;&#39064;&#20013;&#25552;&#21462;&#21644;&#21015;&#20986;&#25152;&#26377;&#30340;&#26415;&#35821;&#21644;&#32553;&#20889;&#12289;&#30830;&#23450;&#19982;&#20043;&#30456;&#31526;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20174;&#39044;&#23450;&#20041;&#30340;&#26415;&#35821;&#20856;&#20013;&#20026;&#27599;&#39033;&#23547;&#27714;&#25193;&#23637;&#30340;&#23450;&#20041;&#21644;&#25551;&#36848;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;&#22686;&#24378;&#30830;&#20445;&#20102;RAG&#26694;&#26550;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#21644;&#35299;&#37322;&#36855;&#24785;&#24773;&#22659;&#65292;&#33021;&#22815;&#26816;&#32034;&#21040;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#19977;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#26679;&#26412;&#38598;&#19978;&#30340;&#25628;&#32034;&#20934;&#30830;&#24615;&#24471;&#20998;&#19978;&#33719;&#24471;&#20102;&#26126;&#26174;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#20891;&#20107;&#21644;&#24037;&#19994;&#35821;&#35328;&#24212;&#29992;&#26041;&#38754;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#30417;&#25511;&#25968;&#25454;&#38598;&#20013;&#65292;&#26597;&#35810;&#24369;&#30340;&#39046;&#22495;&#24863;&#34987;&#30452;&#25509;&#36716;&#21270;&#20026;&#22686;qq&#24378;&#38382;&#39064;&#23545;&#21407;&#22987;&#26597;&#35810;&#30340;&#33391;&#22909;&#21709;&#24212;&#12290;&#25972;&#20307;&#19978;&#65292;Golden-Retriever&#22312;&#25968;&#37327;&#36739;&#22810;&#30340;&#26679;&#26412;&#38598;&#19978;&#20063;&#33719;&#24471;&#20102;&#26368;&#39640;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#20877;&#27425;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24037;&#19994;&#30693;&#35782;&#26816;&#32034;&#21644;&#38382;&#31572;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23427;&#22312;&#25345;&#32493;&#32435;&#20837;&#26032;&#21457;&#29616;&#30340;&#39046;&#22495;&#26415;&#35821;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00798v1 Announce Type: cross  Abstract: This paper introduces Golden-Retriever, designed to efficiently navigate vast industrial knowledge bases, overcoming challenges in traditional LLM fine-tuning and RAG frameworks with domain-specific jargon and context interpretation. Golden-Retriever incorporates a reflection-based question augmentation step before document retrieval, which involves identifying jargon, clarifying its meaning based on context, and augmenting the question accordingly. Specifically, our method extracts and lists all jargon and abbreviations in the input question, determines the context against a pre-defined list, and queries a jargon dictionary for extended definitions and descriptions. This comprehensive augmentation ensures the RAG framework retrieves the most relevant documents by providing clear context and resolving ambiguities, significantly improving retrieval accuracy. Evaluations using three open-source LLMs on a domain-specific question-answer d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21327;&#20316;&#20849;&#36827;&#21270;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;SNN&#21098;&#26525;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2408.00794</link><description>&lt;p&gt;
CCSRP: &#36890;&#36807;&#21327;&#20316;&#20849;&#36827;&#21270;&#27861;&#23454;&#29616;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#31283;&#20581;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
CCSRP: Robust Pruning of Spiking Neural Networks through Cooperative Coevolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#20849;&#36827;&#21270;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;SNN&#21098;&#26525;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00794v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#21508;&#31181;&#21160;&#24577;&#35270;&#35273;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#28508;&#21147;&#65292;&#20294;&#37027;&#20123;&#36866;&#21512;&#23454;&#38469;&#37096;&#32626;&#30340;&#36890;&#24120;&#32570;&#20047;&#22312;&#36164;&#28304;&#21463;&#38480;&#21644;&#20851;&#38190;&#23433;&#20840;&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#32039;&#20945;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#32593;&#32476;&#21098;&#26525;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#31561;&#31574;&#30053;&#26469;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#32039;&#20945;&#24615;&#25110;&#40065;&#26834;&#24615;&#65292;&#32780;&#23545;&#20110;SNNs&#30340;&#31867;&#20284;&#26041;&#27861;&#30740;&#31350;&#29978;&#23569;&#12290;&#31283;&#20581;&#20462;&#21098;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21450;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#31283;&#20581;&#20462;&#21098;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#21644;&#21453;&#22797;&#35797;&#39564;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#21098;&#26525;&#26631;&#20934;&#25110;&#36741;&#21161;&#27169;&#22359;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#20204;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#26469;&#33258;&#21160;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21098;&#26525;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#36229;&#20986;&#20102;&#21098;&#26525;&#26631;&#20934;&#25110;&#36741;&#21161;&#27169;&#22359;&#30340;&#36873;&#25321;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00794v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs) have shown promise in various dynamic visual tasks, yet those ready for practical deployment often lack the compactness and robustness essential in resource-limited and safety-critical settings. Prior research has predominantly concentrated on enhancing the compactness or robustness of artificial neural networks through strategies like network pruning and adversarial training, with little exploration into similar methodologies for SNNs. Robust pruning of SNNs aims to reduce computational overhead while preserving both accuracy and robustness. Current robust pruning approaches generally necessitate expert knowledge and iterative experimentation to establish suitable pruning criteria or auxiliary modules, thus constraining their broader application. Concurrently, evolutionary algorithms (EAs) have been employed to automate the pruning of artificial neural networks, delivering remarkable outcomes yet overloo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36776;&#35782;&#21644;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#26102;&#30340;&#20449;&#20208;&#39134;&#36291;&#65292;&#30452;&#25509;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26500;&#24314;&#20869;&#22312;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2408.00786</link><description>&lt;p&gt;
&#26159;&#21542;&#20449;&#20219;&#65306;&#26426;&#22120;&#23398;&#20064;&#30340;&#20449;&#20208;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
Whether to trust: the ML leap of faith
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36776;&#35782;&#21644;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#26102;&#30340;&#20449;&#20208;&#39134;&#36291;&#65292;&#30452;&#25509;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26500;&#24314;&#20869;&#22312;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00786v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#37319;&#32435;&#26469;&#35828;&#65292;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20449;&#20219;&#36890;&#24120;&#34987;&#29702;&#35299;&#20026;&#19968;&#31181;&#24577;&#24230;&#65292;&#20294;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#27979;&#37327;&#23427;&#65292;&#20063;&#19981;&#33021;&#23545;&#20854;&#36827;&#34892;&#31649;&#29702;&#12290;&#25105;&#20204;&#23558;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20449;&#20219;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21450;&#20854;&#32452;&#25104;&#37096;&#20214;&#30340;&#20449;&#20219;&#28151;&#20026;&#19968;&#35848;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29992;&#25143;&#22312;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#26102;&#25152;&#20570;&#30340;&#20449;&#20208;&#39134;&#36291;&#24448;&#24448;&#19981;&#34987;&#29702;&#35299;&#12290;&#24403;&#21069;&#26500;&#24314;&#20449;&#20219;&#30340;&#21162;&#21147;&#35299;&#37322;&#20102;ML&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#38750;ML&#19987;&#23478;&#26469;&#35828;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#65292;&#22240;&#20026;&#36825;&#20010;&#36807;&#31243;&#24456;&#22797;&#26434;&#65292;&#32780;&#19988;&#35299;&#37322;&#19982;&#20182;&#20204;&#33258;&#24049;&#26410;&#34920;&#36798;&#30340;&#24605;&#32500;&#27169;&#22411;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;ML&#20013;&#26500;&#24314;&#20869;&#22312;&#20449;&#20219;&#65292;&#36890;&#36807;&#36776;&#35782;&#21644;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;ML&#26102;&#30340;&#20449;&#20208;&#39134;&#36291;&#65288;Leap of Faith&#65292;LoF&#65289;&#12290;&#25105;&#20204;&#30340;LoF&#30697;&#38453;&#35782;&#21035;&#20986;ML&#27169;&#22411;&#19982;&#29992;&#25143;&#33258;&#36523;&#24605;&#32500;&#27169;&#22411;&#30340;&#21305;&#37197;&#24773;&#20917;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#21407;&#22987;&#25968;&#25454;&#21644;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#36755;&#20837;&#21040;ML&#27169;&#22411;&#20013;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35268;&#21017;&#20026;&#22522;&#30784;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20005;&#26684;&#20294;&#23454;&#38469;&#22320;&#35782;&#21035;&#36825;&#31181;&#21305;&#37197;&#12290;&#36825;&#31181;&#21305;&#37197;&#26159;&#22522;&#20110;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#20869;&#37096;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#23457;&#26597;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#30830;&#23450;&#30340;&#35268;&#21017;&#20316;&#20026;&#26657;&#20934;&#28857;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#21453;&#39304;&#19982;&#36825;&#20123;&#26657;&#20934;&#28857;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#29992;&#25143;&#23545;ML&#27169;&#22411;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23450;&#20041;&#30340;&#65292;&#20449;&#20219;&#24847;&#21619;&#30528;&#22312;&#27809;&#26377;&#20805;&#20998;&#20102;&#35299;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24895;&#24847;&#25509;&#21463;&#27169;&#22411;&#36755;&#20986;&#20316;&#20026;&#20915;&#31574;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00786v1 Announce Type: cross  Abstract: Human trust is critical for trustworthy AI adoption. Trust is commonly understood as an attitude, but we cannot accurately measure this, nor manage it. We conflate trust in the overall system, ML, and ML's component parts; so most users do not understand the leap of faith they take when they trust ML. Current efforts to build trust explain ML's process, which can be hard for non-ML experts to comprehend because it is complex, and explanations are unrelated to their own (unarticulated) mental models. We propose an innovative way of directly building intrinsic trust in ML, by discerning and measuring the Leap of Faith (LoF) taken when a user trusts ML. Our LoF matrix identifies where an ML model aligns to a user's own mental model. This match is rigorously yet practically identified by feeding the user's data and objective function both into an ML model and an expert-validated rules-based AI model, a verified point of reference that can 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#32447;&#32034;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#24773;&#22659;&#30693;&#35782;&#26469;&#25552;&#39640;&#22312;&#31038;&#20132;&#24773;&#22659;&#20013;&#20998;&#26512;&#38754;&#37096;&#34920;&#24773;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22234;&#24466;&#22256;&#22659;&#36825;&#19968;&#31038;&#20250;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2408.00780</link><description>&lt;p&gt;
&#28145;&#24230;&#20998;&#26512;&#36890;&#36807;&#30693;&#35782;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
In-Depth Analysis of Emotion Recognition through Knowledge-Based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#32447;&#32034;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#24773;&#22659;&#30693;&#35782;&#26469;&#25552;&#39640;&#22312;&#31038;&#20132;&#24773;&#22659;&#20013;&#20998;&#26512;&#38754;&#37096;&#34920;&#24773;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22234;&#24466;&#22256;&#22659;&#36825;&#19968;&#31038;&#20250;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00780v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#31038;&#20132;&#24773;&#22659;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#24037;&#20316;&#65292;&#23427;&#38656;&#35201;&#25972;&#21512;&#38754;&#37096;&#34920;&#24773;&#21644;&#24773;&#22659;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#33258;&#21160;&#24773;&#32490;&#35782;&#21035;&#30340;&#20256;&#32479;&#26041;&#27861;&#19987;&#27880;&#20110;&#33073;&#24773;&#22659;&#30340;&#20449;&#21495;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#24773;&#22659;&#22312;&#22609;&#36896;&#24773;&#32490;&#24863;&#30693;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#23545;&#26032;&#20852;&#30340;&#31038;&#20250;&#24773;&#22659;&#19979;&#30340;&#24773;&#32490;&#35782;&#21035;&#36825;&#19968;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#30340;&#24773;&#32490;&#24863;&#30693;&#29702;&#35770;&#26469;&#25351;&#23548;&#33258;&#21160;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#32447;&#32034;&#25972;&#21512;&#65288;BCI&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#38754;&#37096;&#34920;&#24773;&#30340;&#33073;&#24773;&#22659;&#20449;&#24687;&#21644;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#24773;&#22659;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#22234;&#24466;&#22256;&#22659;&#36825;&#19968;&#31038;&#20250;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#21508;&#31181;&#20027;&#35266;&#24773;&#24863;&#35780;&#20272;&#27979;&#35797;&#20013;&#23545;BCI&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00780v1 Announce Type: cross  Abstract: Emotion recognition in social situations is a complex task that requires integrating information from both facial expressions and the situational context. While traditional approaches to automatic emotion recognition have focused on decontextualized signals, recent research emphasizes the importance of context in shaping emotion perceptions. This paper contributes to the emerging field of context-based emotion recognition by leveraging psychological theories of human emotion perception to inform the design of automated methods. We propose an approach that combines emotion recognition methods with Bayesian Cue Integration (BCI) to integrate emotion inferences from decontextualized facial expressions and contextual knowledge inferred via Large-language Models. We test this approach in the context of interpreting facial expressions during a social task, the prisoner's dilemma. Our results provide clear support for BCI across a range of au
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;Frontend Diffusion&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#30340;&#33609;&#22270;&#21644;&#31471;&#21040;&#31471;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#25277;&#35937;&#24847;&#22270;&#21040;&#35814;&#32454;&#32593;&#39029;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2408.00778</link><description>&lt;p&gt;
&#21069;&#31471;&#25193;&#25955;&#65306;&#36890;&#36807;&#25277;&#35937;&#21040;&#35814;&#32454;&#20219;&#21153;&#36807;&#28193;&#25506;&#32034;&#24847;&#22270;&#39537;&#21160;&#30340;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Frontend Diffusion: Exploring Intent-Based User Interfaces through Abstract-to-Detailed Task Transitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;Frontend Diffusion&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#30340;&#33609;&#22270;&#21644;&#31471;&#21040;&#31471;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#25277;&#35937;&#24847;&#22270;&#21040;&#35814;&#32454;&#32593;&#39029;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00778v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00778v1 Announce Type: cross  Abstract: The emergence of Generative AI is catalyzing a paradigm shift in user interfaces from command-based to intent-based outcome specification. In this paper, we explore abstract-to-detailed task transitions in the context of frontend code generation as a step towards intent-based user interfaces, aiming to bridge the gap between abstract user intentions and concrete implementations. We introduce Frontend Diffusion, an end-to-end LLM-powered tool that generates high-quality websites from user sketches. The system employs a three-stage task transition process: sketching, writing, and coding. We demonstrate the potential of task transitions to reduce human intervention and communication costs in complex tasks. Our work also opens avenues for exploring similar approaches in other domains, potentially extending to more complex, interdependent tasks such as video production.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20809;&#27969;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#20132;&#36890;&#20107;&#20214;&#30340;&#23454;&#26102;&#12289;&#39640;&#25928;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#20026;&#39550;&#39542;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#65292;&#35782;&#21035;&#21069;&#26041;&#36947;&#36335;&#28508;&#22312;&#30340;&#23041;&#32961;&#25110;&#31361;&#21457;&#20107;&#20214;&#65292;&#25552;&#39640;&#39550;&#39542;&#24773;&#20917;&#24863;&#30693;&#65292;&#24182;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00768</link><description>&lt;p&gt;
&#20351;&#29992;&#31354;&#38388; filling curves &#23545;&#27604;&#20809;&#27969;&#21644;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20809;&#27969;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#20132;&#36890;&#20107;&#20214;&#30340;&#23454;&#26102;&#12289;&#39640;&#25928;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#20026;&#39550;&#39542;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#65292;&#35782;&#21035;&#21069;&#26041;&#36947;&#36335;&#28508;&#22312;&#30340;&#23041;&#32961;&#25110;&#31361;&#21457;&#20107;&#20214;&#65292;&#25552;&#39640;&#39550;&#39542;&#24773;&#20917;&#24863;&#30693;&#65292;&#24182;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38754;&#20020;&#30528;&#22312;&#35270;&#39057;&#12289;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#31561;&#20132;&#36890;&#25968;&#25454;&#20013;&#35782;&#21035;&#20107;&#20214;&#21644;&#25910;&#38598;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#35780;&#20215;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#32467;&#26500;&#30340;&#12289;&#22810;&#27169;&#24577;&#30340;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#65292;&#19988;&#32570;&#20047;&#20803;&#25968;&#25454;&#25110;&#27880;&#37322;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20809;&#27969;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#39057;&#25968;&#25454;&#65288;&#26469;&#33258;&#36710;&#36742;&#21069;&#21521;&#25668;&#20687;&#22836;&#65289;&#20013;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#20132;&#36890;&#20107;&#20214;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36710;&#36742;&#21608;&#22260;&#20809;&#27969;&#22330;&#30340;&#24178;&#25200;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#20107;&#20214;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#39550;&#39542;&#21592;&#30340;&#35270;&#32447;&#65292;&#20197;&#39044;&#27979;&#28508;&#22312;&#20107;&#20214;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#36755;&#36865;&#21040;&#31354;&#38388;&#22635;&#28385;&#26354;&#32447;&#19978;&#65292;&#20197;&#38477;&#20302;&#32500;&#24230;&#24182;&#23454;&#29616;&#35745;&#31639;&#19978;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#26631;&#20934;&#30340;&#23454;&#39564;&#65292;&#38024;&#23545;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#26816;&#27979;&#20107;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#39564;&#35777;&#20102;&#26412;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#35745;&#31639;&#25928;&#29575;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#31639;&#27861;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24635;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#31354;&#38388;&#35299;&#26500;&#30340;&#26102;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#27010;&#24565;&#25152;&#23637;&#31034;&#30340;&#31639;&#27861;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#20809;&#27969;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36798;&#21040;&#33391;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#26102;&#65292;&#26356;&#24555;&#22320;&#23436;&#25104;&#35745;&#31639;&#23494;&#38598;&#22411;&#25805;&#20316;&#12290;Our approach is designed to serve as an early warning or an auxiliary system that can provide real-time feedback to drivers or autonomous vehicles by identifying potential hazards or sudden events in the road ahead, improving situational awareness and potentially enhancing safety. In summary, this paper presents a novel framework for computationally efficient traffic event detection, which relies on optical flow, deep learning, and space-filling curves, offering a promising solution for the autonomous driving industry to achieve real-time event detection with minimal computational resources.
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00768v1 Announce Type: new  Abstract: Gathering data and identifying events in various traffic situations remains an essential challenge for the systematic evaluation of a perception system's performance. Analyzing large-scale, typically unstructured, multi-modal, time series data obtained from video, radar, and LiDAR is computationally demanding, particularly when meta-information or annotations are missing. We compare Optical Flow (OF) and Deep Learning (DL) to feed computationally efficient event detection via space-filling curves on video data from a forward-facing, in-vehicle camera. Our first approach leverages unexpected disturbances in the OF field from vehicle surroundings; the second approach is a DL model trained on human visual attention to predict a driver's gaze to spot potential event locations. We feed these results to a space-filling curve to reduce dimensionality and achieve computationally efficient event retrieval. We systematically evaluate our concept b
&lt;/p&gt;</description></item><item><title>SentenceVAE&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#25913;&#20026;&#30001;&#21477;&#23376;&#36880;&#20010;&#22788;&#29702;&#30340;&#31574;&#30053;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00655</link><description>&lt;p&gt;
SentenceVAE&#65306;&#36890;&#36807;&#19979;&#19968;&#20010;&#21477;&#23376;&#39044;&#27979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26356;&#24555;&#12289;&#26356;&#38271;&#21644;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00655
&lt;/p&gt;
&lt;p&gt;
SentenceVAE&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#25913;&#20026;&#30001;&#21477;&#23376;&#36880;&#20010;&#22788;&#29702;&#30340;&#31574;&#30053;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00655v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;  &#25688;&#35201;&#65306;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20027;&#35201;&#20381;&#38752;&#19979;&#19968;&#20010;token&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#19979;&#19968;&#20010;&#21477;&#23376;&#39044;&#27979;&#65292;&#26088;&#22312;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21477;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SentenceVAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#21477;&#24335;&#32534;&#30721;&#22120;&#21644;&#21477;&#24335;&#35299;&#30721;&#22120;&#32452;&#25104;&#30340;tiny&#27169;&#22411;&#12290;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#21477;&#23376;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20195;&#24065;&#65292;&#32780;&#35299;&#30721;&#22120;&#37325;&#24314;&#36825;&#20010;&#21387;&#32553;&#30340;&#25968;&#25454;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#21407;&#22987;&#30340;&#21477;&#23376;&#24418;&#24335;&#12290;&#36890;&#36807;&#23558;SentenceVAE&#38598;&#25104;&#21040;LLM&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#23618;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21477;&#23376;&#32423;&#21035;&#30340;LLM&#65288;Sentence-level LLMs, SLLMs&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#25353;&#21477;&#23376;&#22788;&#29702;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;SentenceVAE&#36824;&#36890;&#36807;&#23558;&#25991;&#26412;&#20998;&#21106;&#25104;&#21477;&#23376;&#65292;&#20445;&#25345;&#20102;&#21407;&#22987;&#35821;&#20041;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#65292;&#22240;&#27492;&#25552;&#39640;&#20102;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00655v2 Announce Type: replace  Abstract: Contemporary large language models (LLMs) primarily rely on next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a Sentence Encoder and a Sentence Decoder. The encoder effectively condenses the information within a sentence into a singular token, while the decoder reconstructs this compressed data back into its original sentential form. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference approach, markedly accelerating inference speeds. SentenceVAE also maintains the integrity of the original semantic content by segmenting the text into sentences, thereby improving a
&lt;/p&gt;</description></item><item><title>V2INet &#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#35282;&#24230;&#20449;&#24687;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#21333;&#19968;&#35270;&#35282;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#26657;&#27491;&#21518;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.00374</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#34701;&#21512;&#30340; conformal &#36712;&#36857;&#39044;&#27979;&#22312;&#21512;&#20316;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00374
&lt;/p&gt;
&lt;p&gt;
V2INet &#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#35282;&#24230;&#20449;&#24687;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#21333;&#19968;&#35270;&#35282;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#26657;&#27491;&#21518;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00374v2 Announce Type: replace-cross &#25688;&#35201;: &#30446;&#21069;&#20851;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#38543;&#30528;&#36830;&#25509;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22914;&#36710;&#23545;&#36710;&#65288;V2V&#65289;&#21644;&#36710;&#23545;&#22522;&#30784;&#35774;&#26045;&#65288;V2I&#65289;&#36890;&#20449;&#65292;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#25910;&#38598;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21464;&#24471;&#21487;&#29992;&#12290;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#38598;&#25104;&#26377;&#28508;&#21147;&#20811;&#26381;&#20165;&#20174;&#21333;&#19968;&#35270;&#35282;&#25910;&#38598;&#25968;&#25454;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#65292;&#22914;&#36974;&#25377;&#21644;&#26377;&#38480;&#35270;&#37326;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; V2INet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#21333;&#19968;&#35270;&#22270;&#27169;&#22411;&#26469;&#24314;&#27169;&#22810;&#35270;&#22270;&#25968;&#25454;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#24471;&#21040;&#20102;&#26657;&#27491;&#65292;&#20197;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00374v2 Announce Type: replace-cross  Abstract: Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated 
&lt;/p&gt;</description></item><item><title>Gemma 2&#26159;Gemma&#31995;&#21015;&#20013;&#26032;&#22411;&#36731;&#37327;&#32423;&#24320;&#25918;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#26550;&#26500;&#21644;&#24212;&#29992;&#33976;&#39311;&#30693;&#35782;&#22521;&#35757;&#65292;&#22312;20&#20159;&#21644;90&#20159;&#21442;&#25968;&#35268;&#27169;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2408.00118</link><description>&lt;p&gt;
Gemma 2: &#25552;&#39640;&#23454;&#29992;&#35268;&#27169;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Gemma 2: Improving Open Language Models at a Practical Size
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00118
&lt;/p&gt;
&lt;p&gt;
Gemma 2&#26159;Gemma&#31995;&#21015;&#20013;&#26032;&#22411;&#36731;&#37327;&#32423;&#24320;&#25918;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#26550;&#26500;&#21644;&#24212;&#29992;&#33976;&#39311;&#30693;&#35782;&#22521;&#35757;&#65292;&#22312;20&#20159;&#21644;90&#20159;&#21442;&#25968;&#35268;&#27169;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00118v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449;&#25688;&#35201;&#65306;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemma 2&#65292;Gemma&#23478;&#26063;&#30340;&#19968;&#20010;&#26032;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#35268;&#27169;&#20174;20&#20159;&#21040;270&#20159;&#19981;&#31561;&#12290;&#22312;&#26412;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#21521;Transformer&#26550;&#26500;&#24212;&#29992;&#20102;&#20960;&#39033;&#24050;&#30693;&#30340;&#25216;&#26415;&#25913;&#36827;&#65292;&#22914;&#24067;&#38647;&#29305;&#21513;&#31561;&#20154;&#65288;2020a&#65289;&#25552;&#20986;&#30340;&#26412;&#22320;-&#20840;&#23616;&#27880;&#24847;&#21147;&#20132;&#21449;&#21644;&#22467;&#36763;&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#30340;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#33976;&#39311;&#30693;&#35782;&#65288;Hinton et al.&#65292;2015&#65289;&#32780;&#19981;&#26159;&#25509;&#19979;&#26469;&#39044;&#27979;&#30340;&#26041;&#24335;&#35757;&#32451;&#20102;2&#20159;&#21644;90&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#27169;&#22411;&#22312;&#23427;&#20204;&#30340;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#20026;&#27604;&#23427;&#20204;&#22823;2-3&#20493;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#31454;&#20105;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#21457;&#24067;&#32473;&#20102;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00118v2 Announce Type: replace-cross  Abstract: In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#33258;&#35780;&#20272;&#31995;&#32479;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#31185;&#23398;&#23478;&#20204;&#23547;&#27714;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.19631</link><description>&lt;p&gt;
"&#19968;&#20010;&#22909;&#30340;&#26426;&#22120;&#20154;&#24635;&#26159;&#30693;&#36947;&#20854;&#23616;&#38480;&#24615;": &#36890;&#36807;&#22240;&#23376;&#21270;&#30340;&#33258;&#25105;&#33258;&#20449;&#37327;&#24230;&#35780;&#20272;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
"A Good Bot Always Knows Its Limitations": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19631
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#33258;&#35780;&#20272;&#31995;&#32479;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#31185;&#23398;&#23478;&#20204;&#23547;&#27714;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19631v2 &#23459;&#24067;&#31867;&#22411;: &#26367;&#25442;&#36328;&#26639;&#25688;&#35201;: &#26234;&#33021;&#26426;&#22120;&#22914;&#20309;&#35780;&#20272;&#20854;&#22312;&#23436;&#25104;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65311;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#31639;&#27861;&#24615; reasoning &#30340;&#33258;&#20027;&#31995;&#32479;&#26469;&#35828;&#26159;&#28966;&#28857;&#12290;&#22312;&#36825;&#37324;&#65292;&#25552;&#20986;&#26234;&#33021;&#26426;&#22120;&#30340;&#33258;&#25105;&#33258;&#20449;&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#23545;&#19990;&#30028;&#30340;&#29366;&#24577;&#12289;&#33258;&#24049;&#30340;&#30693;&#35782;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#30340;&#33258;&#25105;&#35780;&#20272;&#65292;&#26159;&#19968;&#31181;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#26377;&#29992;&#33021;&#21147;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#31216;&#20026;&#22240;&#23376;&#21270;&#30340;&#33258;&#25105;&#33258;&#20449;&#65288;Factorized Machine Self-confidence, FaMSeC&#65289;&#65292;&#20026;&#31639;&#27861;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#12289;&#24037;&#31243;&#23548;&#21521;&#30340;&#25551;&#36848;&#65292;&#21253;&#25324;&#65306;&#32467;&#26524;&#35780;&#20272;&#12289;&#27714;&#35299;&#22120;&#36136;&#37327;&#12289;&#27169;&#22411;&#36136;&#37327;&#12289;&#23545;&#40784;&#36136;&#37327;&#21644;&#36807;&#21435;&#30340;&#32463;&#39564;&#12290;&#22312;FaMSeC&#20013;&#65292;&#33258;&#25105;&#33258;&#20449;&#25351;&#26631;&#26159;&#20174;&#23618;&#27425;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#19981;&#20165;&#33021;&#22815;&#21578;&#30693;&#31995;&#32479;&#20869;&#37096;&#30340;&#30693;&#35782;&#21644;&#33021;&#21147;&#65292;&#20063;&#33021;&#22312;&#36328;&#31995;&#32479;&#20043;&#38388;&#36827;&#34892;&#33258;&#25105;&#27807;&#36890;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#30340;&#27807;&#36890;&#21644;&#25945;&#32946;&#65292;&#24182;&#23545;&#20110;&#35774;&#35745;&#21644;&#26500;&#24314;&#26356;&#21152;&#21487;&#20449;&#21644;&#21487;&#38752;&#30340;&#33258;&#20027;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19631v2 Announce Type: replace-cross  Abstract: How can intelligent machines assess their competencies in completing tasks? This question has come into focus for autonomous systems that algorithmically reason and make decisions under uncertainty. It is argued here that machine self-confidence - a form of meta-reasoning based on self-assessments of an agent's knowledge about the state of the world and itself, as well as its ability to reason about and execute tasks - leads to many eminently computable and useful competency indicators for such agents. This paper presents a culmination of work on this concept in the form of a computational framework called Factorized Machine Self-confidence (FaMSeC), which provides a holistic engineering-focused description of factors driving an algorithmic decision-making process, including: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self confidence indicators are derived from hierarch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;LLMs&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22840;&#22823;&#12290;&#34429;&#28982;LLMs&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#23545;&#35821;&#35328;&#30340;&#29702;&#35299;&#33021;&#21147;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#27979;&#35797;&#12290;Abstract&#20013;&#30340;&#25805;&#20316;&#34920;&#26126;&#65292;&#35780;&#20272;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#38656;&#35201;&#36776;&#35782;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#24182;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2407.19630</link><description>&lt;p&gt;
LLMs' Understanding of Natural Language Revealed
&lt;/p&gt;
&lt;p&gt;
LLMs' Understanding of Natural Language Revealed
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;LLMs&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22840;&#22823;&#12290;&#34429;&#28982;LLMs&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#23545;&#35821;&#35328;&#30340;&#29702;&#35299;&#33021;&#21147;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#27979;&#35797;&#12290;Abstract&#20013;&#30340;&#25805;&#20316;&#34920;&#26126;&#65292;&#35780;&#20272;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#38656;&#35201;&#36776;&#35782;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#24182;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19630v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#19968;&#39033;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#39537;&#21160;&#20498;&#32622;&#24037;&#31243;&#23454;&#39564;&#25104;&#26524;&#65292;&#26088;&#22312;&#20174;&#24213;&#37096;&#21521;&#19978;&#23545;&#35821;&#35328;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#39537;&#21160;&#21453;&#21521;&#24037;&#31243;&#12290;&#23613;&#31649;LLMs&#22312;&#35768;&#22810;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#25928;&#29992;&#65292;&#20294;&#22823;&#37327;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#25191;&#34892;&#38656;&#35201;&#23545;&#31526;&#21495;&#21464;&#37327;&#36827;&#34892;&#37327;&#21270;&#21644;&#25805;&#20316;&#30340;&#20219;&#21153;&#26041;&#38754;&#26159;&#26080;&#33021;&#20026;&#21147;&#30340;&#65292;&#20363;&#22914;&#35745;&#21010;&#21644;&#38382;&#39064;&#35299;&#20915;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#26723;&#20013;&#23558;&#37325;&#28857;&#27979;&#35797;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#36825;&#26159;&#23427;&#20204;&#30340;&#19987;&#38271;&#12290;&#27491;&#22914;&#25105;&#20204;&#23558;&#35201;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#24050;&#32463;&#34987;&#24191;&#27867;&#22320;&#22840;&#22823;&#20102;&#12290;&#34429;&#28982;LLMs&#24050;&#32463;&#34987;&#35777;&#26126;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#36830;&#36143;&#35821;&#35328;&#65288;&#22240;&#20026;&#36825;&#23601;&#26159;&#23427;&#20204;&#30340;&#35774;&#35745;&#26041;&#24335;&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#23578;&#26410;&#34987;&#27491;&#30830;&#27979;&#35797;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#36890;&#36807;&#25191;&#34892;&#19968;&#20010;&#25805;&#20316;&#26469;&#27979;&#35797;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#36825;&#20010;&#25805;&#20316;&#38656;&#35201;&#33021;&#22815;&#36776;&#35782;&#21644;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#21516;&#26102;&#20063;&#33021;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19630v2 Announce Type: replace  Abstract: Large language models (LLMs) are the result of a massive experiment in bottom-up, data-driven reverse engineering of language at scale. Despite their utility in a number of downstream NLP tasks, ample research has shown that LLMs are incapable of performing reasoning in tasks that require quantification over and the manipulation of symbolic variables (e.g., planning and problem solving); see for example [25][26]. In this document, however, we will focus on testing LLMs for their language understanding capabilities, their supposed forte. As we will show here, the language understanding capabilities of LLMs have been widely exaggerated. While LLMs have proven to generate human-like coherent language (since that's how they were designed), their language understanding capabilities have not been properly tested. In particular, we believe that the language understanding capabilities of LLMs should be tested by performing an operation that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#30431;AI&#27861;&#26696;&#21644;NIST&#26694;&#26550;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.17374</link><description>&lt;p&gt;
&#19982;AI&#20174;&#19994;&#32773;&#21644;AI&#21512;&#35268;&#19987;&#23478;&#20849;&#21516;&#35774;&#35745;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#30431;AI&#27861;&#26696;&#21644;NIST&#26694;&#26550;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#30417;&#31649;&#26085;&#30410;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#20844;&#21496;&#32780;&#35328;&#65292;&#36827;&#34892;&#24433;&#21709;&#35780;&#20272;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#25253;&#21578;&#25991;&#26723;&#20854;&#21512;&#35268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25253;&#21578;&#24448;&#24448;&#32570;&#20047;&#23545;&#27861;&#35268;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#19988;&#36890;&#24120;&#21482;&#20851;&#27880;AI&#31995;&#32479;&#30456;&#20851;&#30340;&#38544;&#31169;&#26041;&#38754;&#65292;&#32780;&#24573;&#30053;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#31995;&#32479;&#22320;&#35774;&#35745;&#24182;&#35780;&#20272;&#36825;&#20123;&#25253;&#21578;&#65292;&#21516;&#26102;&#32771;&#34385;AI&#20174;&#19994;&#32773;&#21644;AI&#21512;&#35268;&#19987;&#23478;&#30340;&#24847;&#35265;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#19982;14&#20301;AI&#20174;&#19994;&#32773;&#21644;6&#20301;AI&#21512;&#35268;&#19987;&#23478;&#32463;&#36807;&#21453;&#22797;&#30340;&#21327;&#21516;&#35774;&#35745;&#21644;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#30431;AI&#27861;&#26696;&#12289;NIST&#30340;AI&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#21644;ISO 42001 AI&#31649;&#29702;&#31995;&#32479;&#30340;AI&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#19968;&#23478;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#30340;&#22522;&#20110;AI&#30340;&#20250;&#35758;&#20276;&#20387;&#29983;&#20135;&#19968;&#20010;&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#65292;&#35780;&#20272;&#20102;&#35813;&#27169;&#26495;&#30340;&#25928;&#26524;&#12290;&#22312;&#35813;&#20844;&#21496;&#30340;8&#20301;AI&#20174;&#19994;&#32773;&#21644;&#20855;&#26377;&#30456;&#21516;&#32972;&#26223;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#25253;&#21578;&#30340;&#32467;&#26500;&#12289;&#20869;&#23481;&#21644;&#23454;&#29992;&#24615;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#29992;&#25143;&#23545;&#25253;&#21578;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#32473;&#20104;&#20102;&#31215;&#26497;&#35780;&#20215;&#65292;&#24182;&#19988;&#35748;&#20026;&#25253;&#21578;&#30340;&#23454;&#29992;&#24615;&#36739;&#39640;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#23558;&#26469;&#30340;&#24433;&#21709;&#35780;&#20272;&#25253;&#21578;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21453;&#39304;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;AI&#25191;&#27861;&#32773;&#21644;&#21512;&#35268;&#19987;&#23478;&#30340;&#21512;&#20316;&#23545;&#20110;&#30830;&#20445;&#25253;&#21578;&#30340;&#23436;&#25972;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17374v2 Announce Type: replace-cross  Abstract: In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#8221;&#65288;CCVA-FL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#35299;&#20915;&#21307;&#30103;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861; CCVA-FL&#65306;&#24212;&#29992;&#20110;&#22522;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#24615;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#8221;&#65288;CCVA-FL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#35299;&#20915;&#21307;&#30103;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#36328;&#23458;&#25143;&#31471;&#21464;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v3 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;LoRA&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#26356;&#26032;&#20302;&#31209;&#30697;&#38453;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#25552;&#21319;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2407.11046</link><description>&lt;p&gt;
LoRA &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;&#24615;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on LoRA of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11046
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;LoRA&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#26356;&#26032;&#20302;&#31209;&#30697;&#38453;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#25552;&#21319;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11046v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#20132;&#21449; &#25688;&#35201;: &#20302;&#31209;&#36866;&#24212;&#24615;(LoRA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36890;&#36807;&#26356;&#26032;&#23494;&#38598;&#22411;&#31070;&#32463;&#32593;&#32476;&#23618;&#19982;&#21487;&#25554;&#25300;&#30340;&#20302;&#31209;&#30697;&#38453;&#26469;&#36827;&#34892;&#30340;&#26368;&#26377;&#25928;&#30340;&#21442;&#25968;&#31934;&#31616;&#24494;&#35843;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#36328;&#20219;&#21153;&#27867;&#21270;&#20197;&#21450;&#22312;&#30830;&#20445;&#38544;&#31169;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;LoRA&#26368;&#36817;&#21463;&#21040;&#20102;&#22823;&#37327;&#30340;&#20851;&#27880;&#65292;&#19982;&#23427;&#30456;&#20851;&#30340;&#24037;&#20316;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#23545;LoRA&#30340;&#24403;&#21069;&#36827;&#23637;&#36827;&#34892;&#20840;&#38754;&#27010;&#36848;&#21464;&#24471;&#23588;&#20026;&#24517;&#35201;&#12290;&#26412;&#25991;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#23545;LoRA&#30340;&#21457;&#23637;&#36827;&#34892;&#20998;&#31867;&#21644;&#32508;&#36848;&#65306;(1) &#19979;&#28216;&#36866;&#24212;&#25913;&#36827;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#25552;&#39640;&#20102;LoRA&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;(2) &#36328;&#20219;&#21153;&#27867;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#28151;&#21512;&#20102;&#22810;&#20010;LoRA&#25554;&#20214;&#20197;&#23454;&#29616;&#36328;&#20219;&#21153;&#27867;&#21270;&#65307;(3) &#25928;&#29575;&#25913;&#36827;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;LoRA&#30340;&#35745;&#31639;&#25928;&#29575;&#65307;(4) &#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;LoRA&#65307;(5) &#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#31687;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;LoRA&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11046v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation~(LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this
&lt;/p&gt;</description></item><item><title>D-Rax &#26159;&#19968;&#20010;&#26032;&#30340;&#22495;&#29305;&#23450;&#25918;&#23556;&#23398;&#21161;&#25163;&#65292;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#27169;&#22411;&#39044;&#27979;&#65292;&#26356;&#31934;&#30830;&#22320;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#22788;&#29702;&#33016;&#37096;X&#20809;&#29255;&#12290;</title><link>https://arxiv.org/abs/2407.02604</link><description>&lt;p&gt;
D-Rax: &#22495;&#29305;&#23450;&#25918;&#23556;&#23398;&#21161;&#25163;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#27169;&#22411;&#39044;&#27979;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.02604
&lt;/p&gt;
&lt;p&gt;
D-Rax &#26159;&#19968;&#20010;&#26032;&#30340;&#22495;&#29305;&#23450;&#25918;&#23556;&#23398;&#21161;&#25163;&#65292;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#27169;&#22411;&#39044;&#27979;&#65292;&#26356;&#31934;&#30830;&#22320;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#22788;&#29702;&#33016;&#37096;X&#20809;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02604v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20174;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#21040;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#31243;&#24230;&#65292;&#36825;&#22312;&#21307;&#23398;&#22270;&#20687;&#21644;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;VLMs&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#39046;&#22495;&#25552;&#20379;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#24110;&#21161;&#35299;&#20915;&#21307;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#35813;&#39046;&#22495;&#26412;&#36523;&#23384;&#22312;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#30340;&#39044;&#27979;&#21644;&#24314;&#35758;&#31561;&#65292;VLMs&#22312;&#20020;&#24202;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#21463;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#33016;&#37096;X&#20809;&#29255;&#65288;CXR&#65289;&#20026;&#37325;&#28857;&#30340;&#22495;&#29305;&#23450;&#12289;&#20250;&#35805;&#24335;&#25918;&#23556;&#23398;&#21161;&#25163;&#8212;&#8212;D-Rax&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#19987;&#23478;&#30340;&#35265;&#35299;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#30340;&#20808;&#36827;&#24615;&#65292;D-Rax&#33021;&#22815;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35299;&#35835;CXR&#22270;&#20687;&#65292;&#20174;&#32780;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#25552;&#20379;&#19968;&#26465;&#20840;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#24182;&#26377;&#21161;&#20110;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#31934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#30446;&#21069;&#25105;&#20204;&#27491;&#22312;&#36880;&#27493;&#23436;&#21892;D-Rax&#30340;&#21151;&#33021;&#65292;&#24182;&#35745;&#21010;&#36827;&#34892;&#26356;&#22810;&#20020;&#24202;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20854;&#23545;&#21307;&#30103;&#23454;&#36341;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02604v2 Announce Type: replace  Abstract: Large vision language models (VLMs) have progressed incredibly from research to applicability for general-purpose use cases. LLaVA-Med, a pioneering large language and vision assistant for biomedicine, can perform multi-modal biomedical image and data analysis to provide a natural language interface for radiologists. While it is highly generalizable and works with multi-modal data, it is currently limited by well-known challenges that exist in the large language model space. Hallucinations and imprecision in responses can lead to misdiagnosis which currently hinder the clinical adaptability of VLMs. To create precise, user-friendly models in healthcare, we propose D-Rax -- a domain-specific, conversational, radiologic assistance tool that can be used to gain insights about a particular radiologic image. In this study, we enhance the conversational analysis of chest X-ray (CXR) images to support radiological reporting, offering compre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25972;&#21512;&#25193;&#25955;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39118;&#38505;&#31649;&#29702;&#21644;&#21327;&#21516;&#21160;&#20316;&#24314;&#27169;&#25552;&#39640;&#20102;&#22810;&#20010;&#20195;&#29702;&#34892;&#21160;&#30340;&#23433;&#20840;&#24615;&#12290;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23601;&#22320;&#12289;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#36712;&#36857;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DSRL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#33021;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2407.00741</link><description>&lt;p&gt;
&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#31163;&#32447;&#22810; agent &#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Offline Multi-agent Reinforcement Learning with Safety Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25972;&#21512;&#25193;&#25955;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39118;&#38505;&#31649;&#29702;&#21644;&#21327;&#21516;&#21160;&#20316;&#24314;&#27169;&#25552;&#39640;&#20102;&#22810;&#20010;&#20195;&#29702;&#34892;&#21160;&#30340;&#23433;&#20840;&#24615;&#12290;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23601;&#22320;&#12289;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#36712;&#36857;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DSRL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#33021;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00741v5 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#22810; agent &#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#65292;&#20854;&#24212;&#29992;&#24050;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#23433;&#20840;&#39118;&#38505;&#30340;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20391;&#37325;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040; MARL &#33539;&#24335;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#39118;&#38505;&#32531;&#35299;&#21644;&#21327;&#21516;&#21160;&#20316;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#34892;&#21160;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23601;&#22320;&#12289;&#20998;&#24067;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#26550;&#26500;&#65292;&#24182;&#22686;&#21152;&#20102;&#39044;&#27979;&#36712;&#36857;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#31639;&#27861;&#20197;&#30830;&#20445;&#25805;&#20316;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23545;DSRL&#22522;&#20934;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#20005;&#26684;&#36981;&#23432;&#20005;&#26684;&#30340;&#23433;&#20840;&#38480;&#21046;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#21644;&#34892;&#20026;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#38598;&#20307;&#24615;&#33021;&#65292;&#36824;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00741v5 Announce Type: replace  Abstract: In recent advancements in Multi-agent Reinforcement Learning (MARL), its application has extended to various safety-critical scenarios. However, most methods focus on online learning, which presents substantial risks when deployed in real-world settings. Addressing this challenge, we introduce an innovative framework integrating diffusion models within the MARL paradigm. This approach notably enhances the safety of actions taken by multiple agents through risk mitigation while modeling coordinated action. Our framework is grounded in the Centralized Training with Decentralized Execution (CTDE) architecture, augmented by a Diffusion Model for prediction trajectory generation. Additionally, we incorporate a specialized algorithm to further ensure operational safety. We evaluate our model against baselines on the DSRL benchmark. Experiment results demonstrate that our model not only adheres to stringent safety constraints but also achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;HPC&#38598;&#32676;&#19978;&#36816;&#34892;&#23454;&#26102;AI&#24212;&#29992;&#30340;&#25176;&#31649;&#21644;&#20113;VM&#26381;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#36991;&#20813;&#20102;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#32463;&#29992;&#25143;&#21516;&#24847;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2407.00110</link><description>&lt;p&gt;
Chat AI: &#19968;&#31181;&#26080;&#32541;&#25903;&#25345;Slurm&#30340;HPC&#26381;&#21153;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;HPC&#38598;&#32676;&#19978;&#36816;&#34892;&#23454;&#26102;AI&#24212;&#29992;&#30340;&#25176;&#31649;&#21644;&#20113;VM&#26381;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#36991;&#20813;&#20102;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#32463;&#29992;&#25143;&#21516;&#24847;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00110v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#25991;&#25688;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#20419;&#20351;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#12289;&#23433;&#20840;&#12289;&#31169;&#23494;&#30340;&#25176;&#31649;&#22522;&#30784;&#35774;&#26045;&#65292;&#36825;&#19981;&#20165;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#36816;&#34892;&#24320;&#28304;&#25110;&#33258;&#23450;&#20041;&#30340;&#31934;&#35843;LLM&#65292;&#32780;&#19988;&#30830;&#20445;&#29992;&#25143;&#30340;&#25968;&#25454;&#19981;&#20250;&#22312;&#27809;&#26377;&#29992;&#25143;&#21516;&#24847;&#30340;&#24773;&#20917;&#19979;&#34987;&#23384;&#20648;&#12290;&#34429;&#28982;&#37197;&#22791;&#20102;&#20808;&#36827;GPU&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#31995;&#32479;&#38750;&#24120;&#36866;&#21512;&#35757;&#32451;LLM&#65292;&#20294;&#23427;&#20204;&#30340;&#25209;&#27425;&#35843;&#24230;&#33539;&#24335;&#24182;&#19981;&#26159;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;AI&#24212;&#29992;&#30340;&#25176;&#31649;&#32780;&#35774;&#35745;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20113;&#31995;&#32479;&#38750;&#24120;&#36866;&#21512;&#25552;&#20379;Web&#26381;&#21153;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#35775;&#38382;HPC&#38598;&#32676;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#26114;&#36149;&#19988;&#31232;&#32570;&#30340;&#39640;&#31471;GPU&#65292;&#36825;&#20123;GPU&#23545;&#20110;&#33719;&#24471;&#26368;&#20339;&#25512;&#29702;&#24615;&#33021;&#26159;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#24182;&#22312;&#20113;VM&#19978;&#36816;&#34892;&#20102;&#19968;&#20010;Web&#26381;&#21153;&#65292;&#35813;&#26381;&#21153;&#20855;&#26377;&#23433;&#20840;&#35775;&#38382;&#22522;&#20110;HPC&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#21518;&#31471;&#30340;&#33021;&#21147;&#65292;&#35813;&#21518;&#31471;&#36816;&#34892;&#30528;&#22810;&#31181;LLM&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26550;&#26500;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#25968;&#25454;&#35775;&#38382;&#65292;&#29992;&#25143;&#25968;&#25454;&#19981;&#20250;&#34987;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00110v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of HPC clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of LLM models on HPC syste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#23545;&#39640;&#12289;&#20302;&#22826;&#38451;&#33021;&#27963;&#21160;&#27700;&#24179;&#36827;&#34892;&#24179;&#34913;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;SET&#25968;&#25454;&#38598;&#19978;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#30340;&#20934;&#30830;&#39044;&#25253;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#23792;&#26399;&#38388;&#12290;</title><link>https://arxiv.org/abs/2406.15847</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#21464;&#37327;&#21464;&#25442;&#22120;&#22686;&#24378;&#22826;&#38451;&#33021;&#39550;&#39542;&#21592;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Enhancing Solar Driver Forecasting with Multivariate Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.15847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#23545;&#39640;&#12289;&#20302;&#22826;&#38451;&#33021;&#27963;&#21160;&#27700;&#24179;&#36827;&#34892;&#24179;&#34913;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;SET&#25968;&#25454;&#38598;&#19978;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#30340;&#20934;&#30830;&#39044;&#25253;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#23792;&#26399;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.15847v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;PatchTST&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;F10.7&#12289;S10.7&#12289;M10.7&#21644;Y10.7&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#30340;&#39044;&#25253;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#22826;&#38451;&#33021;&#27963;&#21160;&#30340;&#20302;&#21644;&#39640;&#27700;&#24179;&#26377;&#24179;&#31561;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26681;&#25454;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#21382;&#21490;&#20998;&#24067;&#19982;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#21152;&#26435;&#26679;&#26412;&#12290;&#22826;&#38451;&#33021;&#39537;&#21160;&#22120;&#39044;&#25253;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;18&#22825;&#30340;&#22238;&#30475;&#31383;&#21475;&#65292;&#24182;&#33021;&#39044;&#27979;&#26410;&#26469;6&#22825;&#30340;&#20540;&#12290;&#24403;&#26412;&#27169;&#22411;&#22312;&#19982;Space Environment Technologies&#65288;SET&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#27169;&#22411;&#25552;&#20379;&#30340;&#39044;&#27979;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#26631;&#20934;&#22343;&#26041;&#35823;&#24046;&#37117;&#26356;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#22826;&#38451;&#33021;&#27963;&#21160;&#39640;&#23792;&#26399;&#65292;&#39044;&#27979;&#31934;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20174;Github&#30340;https://github.com/ARCLab-MIT/sw-driver-forecaster&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.15847v2 Announce Type: replace-cross  Abstract: In this work, we develop a comprehensive framework for F10.7, S10.7, M10.7, and Y10.7 solar driver forecasting with a time series Transformer (PatchTST). To ensure an equal representation of high and low levels of solar activity, we construct a custom loss function to weight samples based on the distance between the solar driver's historical distribution and the training set. The solar driver forecasting framework includes an 18-day lookback window and forecasts 6 days into the future. When benchmarked against the Space Environment Technologies (SET) dataset, our model consistently produces forecasts with a lower standard mean error in nearly all cases, with improved prediction accuracy during periods of high solar activity. All the code is available on Github https://github.com/ARCLab-MIT/sw-driver-forecaster.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#36825;&#26159;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#34394;&#25311;&#21161;&#25163;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#36825;&#20123;&#35760;&#24405;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;</title><link>https://arxiv.org/abs/2406.07867</link><description>&lt;p&gt;
&#29616;&#23454;&#23545;&#35805;&#65306;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.07867
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#36825;&#26159;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#34394;&#25311;&#21161;&#25163;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#36825;&#20123;&#35760;&#24405;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22788;&#29702;&#29992;&#25143;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#38899;&#39057;-&#35270;&#35273;&#35328;&#35821;&#20316;&#20026;&#22238;&#24212;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#21019;&#24314;&#19981;&#20381;&#36182;&#20013;&#38388;&#25991;&#26412;&#30340;&#34394;&#25311;&#21161;&#25163;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;MultiDialog&#65292;&#36825;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#65288;&#21363;&#22768;&#38899;&#21644;&#35270;&#35273;&#65289;&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21547;&#32422;340&#23567;&#26102;&#30340;9,000&#22810;&#20010;&#23545;&#35805;&#30340;&#24179;&#34892;&#38899;&#39057;-&#35270;&#35273;&#35760;&#24405;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;TopicalChat&#24405;&#21046;&#30340;&#12290;MultiDialog&#21253;&#21547;&#20102;&#23545;&#35805;&#20249;&#20276;&#26681;&#25454;&#32473;&#23450;&#33050;&#26412;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#24182;&#36827;&#34892;&#24773;&#24863;&#26631;&#27880;&#30340;&#38899;&#39057;-&#35270;&#39057;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#26399;&#24453;&#36825;&#20123;&#35760;&#24405;&#23558;&#20026;&#22810;&#27169;&#24577;&#21512;&#25104;&#30740;&#31350;&#24320;&#36767;&#26032;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#38754;&#23545;&#38754;&#23545;&#35805;&#30340;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#32467;&#21512;&#20102;&#32463;&#36807;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#38899;&#39057;-&#35270;&#35273;&#21475;&#35821;&#23545;&#35805;&#39046;&#22495;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;-&#25991;&#26412;&#32852;&#21512;&#39044;&#35757;&#32451;&#32435;&#20837;&#27169;&#22411;&#20043;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.07867v2 Announce Type: replace  Abstract: In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20351;&#29305;&#23450;&#22320;&#21306;&#30340;&#22270;&#20687;&#34920;&#29616;&#19982;&#29616;&#23454;&#19990;&#30028;&#30456;&#31526;&#12290;</title><link>https://arxiv.org/abs/2406.04551</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#25913;&#36827;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;Vendi&#20998;&#25968;&#25351;&#24341;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20351;&#29305;&#23450;&#22320;&#21306;&#30340;&#22270;&#20687;&#34920;&#29616;&#19982;&#29616;&#23454;&#19990;&#30028;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04551v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04551v2 Announce Type: replace  Abstract: With the growing popularity of text-to-image generative models, there has been increasing focus on understanding their risks and biases. Recent work has found that state-of-the-art models struggle to depict everyday objects with the true diversity of the real world and have notable gaps between geographic regions. In this work, we aim to increase the diversity of generated images of common objects such that per-region variations are representative of the real world. We introduce an inference time intervention, contextualized Vendi Score Guidance (c-VSG), that guides the backwards steps of latent diffusion models to increase the diversity of a sample as compared to a "memory bank" of previously generated images while constraining the amount of variation within that of an exemplar set of real-world contextualizing images. We evaluate c-VSG with two geographically representative datasets and find that it substantially increases the dive
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#20985;&#25928;&#29992;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#30830;&#20445;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2405.19024</link><description>&lt;p&gt;
&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#36870;&#28216;&#25103;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.19024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#20985;&#25928;&#29992;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#30830;&#20445;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19024v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;&#65306;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20985;&#25928;&#29992;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;CURL&#65289;&#26159;&#26631;&#20934;RL&#30446;&#26631;&#30340;&#19968;&#33324;&#21270;&#65292;&#23427;&#20351;&#29992;&#20102;&#29366;&#24577;&#30340;&#21344;&#29992;&#24230;&#37327;&#20985;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#32447;&#24615;&#20989;&#25968;&#12290;CURL&#22240;&#20854;&#33021;&#22815;&#20195;&#34920;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#23454;&#20363;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;RL&#20363;&#22914;&#27169;&#20223;&#23398;&#20064;&#12289;&#32431;&#25506;&#32034;&#12289;&#21463;&#38480;MDP&#12289;&#31163;&#32447;RL&#12289;&#20154;&#36896;&#34892;&#20026;&#35268;&#33539;&#21270;RL&#31561;&#32780;&#26368;&#36817;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#23427;&#19987;&#27880;&#20110;&#24674;&#22797;&#19968;&#20010;&#26410;&#30693;&#30340;&#34892;&#20026;&#22870;&#21169;&#20989;&#25968;&#65292;&#23427;&#33021;&#22815;&#20026;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20851;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20854;&#20013;&#38382;&#39064;&#34987;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#21487;&#34892;&#30340;&#22870;&#21169;&#20989;&#25968;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CURL&#38382;&#39064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#23578;&#26410;&#34987;&#32771;&#34385;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#26631;&#20934;&#30340;IRL&#30740;&#31350;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#36870;&#20985;&#25928;&#29992;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#20985;&#25928;&#29992;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#30830;&#20445;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19024v3 Announce Type: replace-cross  Abstract: We consider inverse reinforcement learning problems with concave utilities. Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function. CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others. Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent. There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions. However, inverse RL for CURL problems has not been considered previously. In this paper we show that most of the standard IRL
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32465;&#23450;&#22270;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#26088;&#22312;&#20026;&#22810;&#29289;&#29702;&#21644;&#22797;&#26434;&#22810;&#22495;&#29616;&#35937;&#20219;&#21153;&#25552;&#20379;&#22810;&#20449;&#24687;&#36755;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2405.13586</link><description>&lt;p&gt;
&#22810;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#32465;&#22270;&#27861;&#29992;&#20110;&#22810;&#21464;&#24322;&#24615;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Bond Graphs for multi-physics informed Neural Networks for multi-variate time series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.13586
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32465;&#23450;&#22270;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#26088;&#22312;&#20026;&#22810;&#29289;&#29702;&#21644;&#22797;&#26434;&#22810;&#22495;&#29616;&#35937;&#20219;&#21153;&#25552;&#20379;&#22810;&#20449;&#24687;&#36755;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13586v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#24341;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13586v2 Announce Type: replace-cross  Abstract: In the trend of hybrid Artificial Intelligence techniques, Physical-Informed Machine Learning has seen a growing interest. It operates mainly by imposing data, learning, or architecture bias with simulation data, Partial Differential Equations, or equivariance and invariance properties. While it has shown great success on tasks involving one physical domain, such as fluid dynamics, existing methods are not adapted to tasks with complex multi-physical and multi-domain phenomena. In addition, it is mainly formulated as an end-to-end learning scheme. To address these challenges, we propose to leverage Bond Graphs, a multi-physics modeling approach, together with Message Passing Graph Neural Networks. We propose a Neural Bond graph Encoder (NBgE) producing multi-physics-informed representations that can be fed into any task-specific model. It provides a unified way to integrate both data and architecture biases in deep learning. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#26032;&#38395;&#25991;&#31456;&#30340;&#23884;&#20837;&#29983;&#25104;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#21644;&#20027;&#39064;&#65292;&#24182;&#23545;&#23427;&#20204;&#19982;&#29305;&#23450;&#20107;&#20214;&#30340;&#21382;&#21490;&#32852;&#31995;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#23569;&#23186;&#20307;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2405.13071</link><description>&lt;p&gt;
&#26032;&#30340;&#22522;&#20110;&#20107;&#20214;&#23884;&#20837;&#26032;&#38395;&#25991;&#31456;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Method for News Article Event-Based Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.13071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#26032;&#38395;&#25991;&#31456;&#30340;&#23884;&#20837;&#29983;&#25104;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#21644;&#20027;&#39064;&#65292;&#24182;&#23545;&#23427;&#20204;&#19982;&#29305;&#23450;&#20107;&#20214;&#30340;&#21382;&#21490;&#32852;&#31995;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#23569;&#23186;&#20307;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13071v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#26032;&#38395;&#25991;&#31456;&#30340;&#23884;&#20837;&#26159;&#22810;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20363;&#22914;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#12289;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#21644;&#21046;&#20316;&#26032;&#38395;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26032;&#38395;&#23884;&#20837;&#26041;&#27861;&#24182;&#26410;&#20248;&#21270;&#20197;&#25429;&#25417;&#26032;&#38395;&#20107;&#20214;&#30340;&#38544;&#24615;&#19978;&#19979;&#25991;&#12290;&#22823;&#22810;&#25968;&#23884;&#20837;&#26041;&#27861;&#20381;&#36182;&#20110;&#20840;&#25991;&#26412;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#26102;&#38388;&#30456;&#20851;&#30340;&#23884;&#20837;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#21644;&#20027;&#39064;&#20197;&#21450;&#23427;&#20204;&#19982;&#29305;&#23450;&#20107;&#20214;&#30340;&#21382;&#21490;&#32852;&#31995;&#26469;&#20248;&#21270;&#26032;&#38395;&#23884;&#20837;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#24314;&#35758;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#32473;&#23450;&#30340;&#26032;&#38395;&#25991;&#31456;&#20013;&#22788;&#29702;&#21644;&#25552;&#21462;&#20107;&#20214;&#12289;&#23454;&#20307;&#21644;&#20027;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24403;&#21069;&#21644;&#21382;&#21490;&#25968;&#25454;&#19978;&#35757;&#32451;&#20998;&#26102;&#27573;&#30340;GloVe&#27169;&#22411;&#26469;&#20026;&#20027;&#39064;&#21644;&#23454;&#20307;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#26102;&#38388;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25340;&#25509;&#29983;&#25104;&#30340;&#26032;&#26032;&#38395;&#23884;&#20837;&#65306;&#24179;&#28369;&#20498;&#25968;&#39057;&#29575;&#65292;&#20197;&#21450;&#20027;&#39064;&#21644;&#23454;&#20307;&#30340;&#22522;&#20110;&#26102;&#38388;&#30340;&#20851;&#38190;&#35789;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#26032;&#38395;&#20027;&#39064;&#35782;&#21035;&#21644;&#30456;&#20284;&#26032;&#38395;&#25991;&#31456;&#25628;&#32034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#23186;&#20307;&#29615;&#22659;&#19979;&#20943;&#23569;&#23186;&#20307;&#20559;&#35265;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.13071v2 Announce Type: replace-cross  Abstract: Embedding news articles is a crucial tool for multiple fields, such as media bias detection, identifying fake news, and making news recommendations. However, existing news embedding methods are not optimized to capture the latent context of news events. Most embedding methods rely on full-text information and neglect time-relevant embedding generation. In this paper, we propose a novel lightweight method that optimizes news embedding generation by focusing on entities and themes mentioned in articles and their historical connections to specific events. We suggest a method composed of three stages. First, we process and extract events, entities, and themes from the given news articles. Second, we generate periodic time embeddings for themes and entities by training time-separated GloVe models on current and historical data. Lastly, we concatenate the news embeddings generated by two distinct approaches: Smooth Inverse Frequency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25910;&#38598;&#29992;&#25143;&#23545;&#26410;&#32463;&#21382;&#30005;&#24433;&#30340;&#20449;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#32467;&#21512;&#29992;&#25143;&#35780;&#20998;&#12289;&#20449;&#24565;&#21644;&#25512;&#33616;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2405.11053</link><description>&lt;p&gt;
&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#21069;&#30340;&#29992;&#25143;&#20449;&#24565;&#25968;&#25454;&#38598;&#65306;&#25910;&#38598;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#21069;&#36873;&#25321;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25910;&#38598;&#29992;&#25143;&#23545;&#26410;&#32463;&#21382;&#30005;&#24433;&#30340;&#20449;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#32467;&#21512;&#29992;&#25143;&#35780;&#20998;&#12289;&#20449;&#24565;&#21644;&#25512;&#33616;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11053v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11053v3 Announce Type: replace-cross  Abstract: An increasingly important aspect of designing recommender systems involves considering how recommendations will influence consumer choices. This paper addresses this issue by introducing a method for collecting user beliefs about un-experienced items - a critical predictor of choice behavior. We implemented this method on the MovieLens platform, resulting in a rich dataset that combines user ratings, beliefs, and observed recommendations. We document challenges to such data collection, including selection bias in response and limited coverage of the product space. This unique resource empowers researchers to delve deeper into user behavior and analyze user choices absent recommendations, measure the effectiveness of recommendations, and prototype algorithms that leverage user belief data, ultimately leading to more impactful recommender systems. The dataset can be found at https://grouplens.org/datasets/movielens/ml_belief_2024
&lt;/p&gt;</description></item><item><title>FloorSet&#26159;&#19968;&#20010;&#21253;&#21547;&#24102;&#26377;&#30495;&#23454;&#19990;&#30028;SoC&#35774;&#35745;&#32422;&#26463;&#30340;VLSI&#24067;&#23616;&#35268;&#21010;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#19968;&#20010;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2405.05480</link><description>&lt;p&gt;
FloorSet --- &#19968;&#20010;&#24102;&#26377;&#30495;&#23454;&#19990;&#30028;SoC&#35774;&#35745;&#32422;&#26463;&#30340;VLSI&#24067;&#23616;&#35268;&#21010;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of Real-World SoCs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.05480
&lt;/p&gt;
&lt;p&gt;
FloorSet&#26159;&#19968;&#20010;&#21253;&#21547;&#24102;&#26377;&#30495;&#23454;&#19990;&#30028;SoC&#35774;&#35745;&#32422;&#26463;&#30340;VLSI&#24067;&#23616;&#35268;&#21010;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#19968;&#20010;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.05480v4 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#23545;&#20110;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoCs&#65289;&#21450;&#20854;&#23376;&#31995;&#32479;&#65292;&#24067;&#23616;&#35268;&#21010;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21644;&#38750;&#21516;&#23567;&#21487;&#30340;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#27493;&#39588;&#12290;&#23427;&#20195;&#34920;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#24102;&#26377;120&#20010;&#20998;&#21306;&#30340;&#22823;&#22411;SoC&#20135;&#29983;&#20102;&#22823;&#32422;10E250&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#38543;&#30528;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#20986;&#29616;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#23545;&#20110;&#19968;&#20010;&#33021;&#22815;&#28085;&#30422;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#24182;&#19988;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#32422;&#26463;&#21644;&#30446;&#26631;&#30340;&#29616;&#20195;&#22522;&#20934;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;FloorSet &#8212;&#8212;&#20004;&#20010;&#20840;&#38754;&#30340;&#21512;&#25104;&#22266;&#23450;&#36718;&#24275;&#24067;&#23616;&#35774;&#35745;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#21453;&#26144;&#20102;&#30495;&#23454;SoC&#30340;&#20998;&#24067;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;100&#19975;&#35757;&#32451;&#26679;&#26412;&#21644;100&#20010;&#27979;&#35797;&#26679;&#26412;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#19968;&#31181;&#21512;&#25104;&#24067;&#23616;&#12290;FloorSet-Prime&#21253;&#21547;&#20102;&#20840;&#36793;&#25509;&#30697;&#24418;&#20998;&#21306;&#30340;&#24067;&#23616;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#36208;&#32447;&#38271;&#24230;&#12290;&#19968;&#20010;&#31616;&#21270;&#29256;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21435;&#38500;&#31354;&#38386;&#21306;&#22495;&#21644;&#31616;&#21270;&#26012;&#21521;&#36830;&#25509;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;FloorSet-Duo&#21017;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;SoC&#20013;&#24120;&#35265;&#22797;&#26434;&#24230;&#30340;&#27169;&#25311;&#65292;&#21253;&#25324;&#19981;&#23545;&#31216;&#20998;&#21306;&#12289;&#23396;&#31435;&#20998;&#21306;&#21644;&#19981;&#35268;&#21017;&#24418;&#29366;&#12290;FloorSet&#30340;&#21457;&#24067;&#26631;&#24535;&#30528;&#24067;&#23616;&#35268;&#21010;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#24320;&#22987;&#65292;&#20026;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#26159;&#29702;&#35770;&#30740;&#31350;&#21644;&#24037;&#19994;&#23454;&#38469;&#30340;&#21452;&#37325;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.05480v4 Announce Type: replace-cross  Abstract: Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial and non-trivial step of the physical design flow. It represents a difficult combinatorial optimization problem. A typical large scale SoC with 120 partitions generates a search-space of nearly 10E250. As novel machine learning (ML) approaches emerge to tackle such problems, there is a growing need for a modern benchmark that comprises a large training dataset and performance metrics that better reflect real-world constraints and objectives compared to existing benchmarks. To address this need, we present FloorSet -- two comprehensive datasets of synthetic fixed-outline floorplan layouts that reflect the distribution of real SoCs. Each dataset has 1M training samples and 100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime comprises fully-abutted rectilinear partitions and near-optimal wire-length. A simplified dataset that reflec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26089;&#26399;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;&#30340;&#36807;&#31243;&#65292;&#30740;&#31350;&#20154;&#21592;&#20943;&#23569;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#26377;&#25928;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2405.03389</link><description>&lt;p&gt;
&#19981;&#35201;&#28010;&#36153;&#24744;&#30340;&#23453;&#36149;&#26102;&#38388;&#65306;&#26089;&#26399;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Don't Waste Your Time: Early Stopping Cross-Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.03389
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26089;&#26399;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;&#30340;&#36807;&#31243;&#65292;&#30740;&#31350;&#20154;&#21592;&#20943;&#23569;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#26377;&#25928;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2405.03389v2 Announce Type: &#26367;&#25442;&#26679;&#26412;&#25688;&#35201;&#65306;&#22312;&#34920;&#25968;&#25454;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#37319;&#29992;&#20132;&#21449;&#39564;&#35777;&#65307;&#30830;&#20445;&#27979;&#37327;&#30340;&#24615;&#33021;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#38543;&#21518;&#30340;&#38598;&#21512;&#23398;&#20064;&#19981;&#20250;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#19982;&#30041;&#20986;&#39564;&#35777;&#30456;&#27604;&#65292;&#20351;&#29992;k-&#25240;&#20132;&#21449;&#39564;&#35777;&#26174;&#33879;&#22686;&#21152;&#20102;&#39564;&#35777;&#21333;&#20010;&#37197;&#32622;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#34429;&#28982;&#30830;&#20445;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#27492;&#22686;&#24378;&#20102;&#24615;&#33021;&#65292;&#20294;&#39069;&#22806;&#30340;&#25104;&#26412;&#24448;&#24448;&#36229;&#36807;&#20102;&#22312;&#26102;&#38388;&#39044;&#31639;&#20869;&#36827;&#34892;&#26377;&#25928;&#27169;&#22411;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#24102;&#26377;&#20132;&#21449;&#39564;&#35777;&#30340;&#27169;&#22411;&#36873;&#25321;&#26356;&#21152;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#20013;&#23545;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#26089;&#26399;&#20572;&#39039;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#20004;&#31867;&#31639;&#27861;&#65288;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#38543;&#26426;&#26862;&#26519;&#65289;&#21644;36&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;&#38543;&#26426;&#25628;&#32034;&#30340;&#26089;&#26399;&#20572;&#39039;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;3-&#12289;5-&#21644;10-&#25240;&#20132;&#21449;&#39564;&#35777;&#26102;&#65292;&#25240;&#21472;&#25968;&#37327;&#23545;&#26089;&#26399;&#20572;&#39039;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24615;&#33021;&#25351;&#26631;&#30340;&#25935;&#24863;&#24615;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#20572;&#27490;&#20132;&#21449;&#39564;&#35777;&#26368;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#20013;&#37319;&#29992;&#26089;&#26399;&#20572;&#39039;&#30340;&#20132;&#21449;&#39564;&#35777;&#26159;&#21487;&#34892;&#30340;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.03389v2 Announce Type: replace-cross  Abstract: State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit. However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration. While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget. We aim to make model selection with cross-validation more effective. Therefore, we study early stopping the process of cross-validation during model selection. We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets. We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds. In addition, we inve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#19978;&#30340;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#26377;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#24635;&#20307;&#34920;&#29616;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#27169;&#22411;&#22312;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#22343;&#34913;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.17513</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20107;&#20214;&#25512;&#29702;&#30340;&#32508;&#21512;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation on Event Reasoning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#19978;&#30340;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#26377;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#24635;&#20307;&#34920;&#29616;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#27169;&#22411;&#22312;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#22343;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17513v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20107;&#20214;&#25512;&#29702;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#22522;&#30784;&#33021;&#21147;&#12290;&#23427;&#38656;&#35201;&#20107;&#20214;&#27169;&#24335;&#30693;&#35782;&#36827;&#34892;&#20840;&#23616;&#25512;&#29702;&#65292;&#24182;&#19988;&#38656;&#35201;&#22788;&#29702;&#21508;&#31181;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#12290;LLM&#22312;&#19981;&#21516;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#19978;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#20173;&#28982;&#26410;&#30693;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;LLM&#30340;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;EV2&#29992;&#20110;&#35780;&#20272;&#20107;&#20214;&#25512;&#29702;&#12290;EV2&#21253;&#21547;&#20004;&#20010;&#23618;&#27425;&#30340;&#35780;&#20215;&#65292;&#21363;&#27169;&#24335;&#21644;&#23454;&#20363;&#65292;&#24182;&#19988;&#22312;&#20851;&#31995;&#21644;&#25512;&#29702;&#33539;&#24335;&#26041;&#38754;&#26159;&#20840;&#38754;&#30340;&#12290;&#25105;&#20204;&#22312;EV2&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#23436;&#25104;&#20107;&#20214;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#26410;&#36798;&#21040;&#28385;&#24847;&#12290;&#25105;&#20204;&#36824;&#27880;&#24847;&#21040;LLM&#22312;&#20107;&#20214;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#19981;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;LLM&#20855;&#26377;&#20107;&#20214;&#27169;&#24335;&#30693;&#35782;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#20107;&#20214;&#25512;&#29702;&#26041;&#38754;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.17513v2 Announce Type: replace-cross  Abstract: Event reasoning is a fundamental ability that underlies many applications. It requires event schema knowledge to perform global reasoning and needs to deal with the diversity of the inter-event relations and the reasoning paradigms. How well LLMs accomplish event reasoning on various relations and reasoning paradigms remains unknown. To mitigate this disparity, we comprehensively evaluate the abilities of event reasoning of LLMs. We introduce a novel benchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of evaluation of schema and instance and is comprehensive in relations and reasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs have abilities to accomplish event reasoning but their performances are far from satisfactory. We also notice the imbalance of event reasoning abilities in LLMs. Besides, LLMs have event schema knowledge, however, they're not aligned with humans on how to
&lt;/p&gt;</description></item><item><title>DASA&#31639;&#27861;&#26159;&#38024;&#23545;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#38382;&#39064;&#35774;&#35745;&#30340;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#36890;&#20449;&#30340;&#24310;&#36831;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.17247</link><description>&lt;p&gt;
DASA: &#33258;&#36866;&#24212;&#24310;&#36831;&#30340;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17247
&lt;/p&gt;
&lt;p&gt;
DASA&#31639;&#27861;&#26159;&#38024;&#23545;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#38382;&#39064;&#35774;&#35745;&#30340;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#36890;&#20449;&#30340;&#24310;&#36831;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v3 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#25105;&#20204;&#32771;&#34385;&#36825;&#26679;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;N&#20010;&#20195;&#29702;&#35797;&#22270;&#36890;&#36807;&#24182;&#34892;&#34892;&#21160;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#26469;&#21152;&#36895;&#19968;&#20010;&#20849;&#21516;&#30340;&#38543;&#26426;&#36924;&#36817;(SA)&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#21521;&#19978;&#28216;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#30340;&#26102;&#38388;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19981;&#21464;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#20943;&#36731;&#24310;&#36831;&#21644;&#33853;&#21518;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\texttt{DASA}&#65292;&#23427;&#26159;&#22522;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20195;&#29702;&#38543;&#26426;&#36924;&#36817;&#12290;&#25105;&#20204;&#20026;\texttt{DASA}&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#20998;&#26512;&#65292;&#20551;&#35774;&#20195;&#29702;&#30340;&#38543;&#26426;&#35266;&#27979;&#36807;&#31243;&#26159;&#29420;&#31435;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#26174;&#33879;&#22320;&#25512;&#36827;&#20102;&#29616;&#26377;&#30340;&#32467;&#26524;&#65292;\texttt{DASA}&#26159;&#31532;&#19968;&#20010;&#19982;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#21462;&#20915;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30456;&#20851;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;Markovian&#25277;&#26679;&#19979;&#30340;N&#20493;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21508;&#31181;SA&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#24403;&#22788;&#29702;&#24310;&#36831;&#21644;&#24322;&#27493;&#29615;&#22659;&#26102;&#65292;&#20854;&#31639;&#27861;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v3 Announce Type: replace-cross  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tau_{mix}$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#32423;&#27979;&#35797;&#26041;&#27861;&#65292;&#20026;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#30340;&#26377;&#25928;&#27979;&#35797;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15857</link><description>&lt;p&gt;
&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#31995;&#32479;&#32423;&#33258;&#21160;&#27979;&#35797;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated System-level Testing of Unmanned Aerial Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#32423;&#27979;&#35797;&#26041;&#27861;&#65292;&#20026;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#30340;&#26377;&#25928;&#27979;&#35797;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#25688;&#35201;&#65306;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#65288;UAS&#65289;&#20381;&#36182;&#20110;&#21508;&#31181;&#33322;&#31354;&#30005;&#23376;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#23545;&#20110;&#23433;&#20840;&#24615;&#21644;&#20219;&#21153;&#25191;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#38469;&#23433;&#20840;&#26631;&#20934;&#30340;&#37325;&#22823;&#35201;&#27714;&#26159;&#23545;&#33322;&#31354;&#30005;&#23376;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#20005;&#26684;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#12290;&#24403;&#21069;&#24037;&#19994;&#20570;&#27861;&#26159;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#22330;&#26223;&#65292;&#20351;&#29992;&#27169;&#25311;&#22120;&#25163;&#21160;/&#33258;&#21160;&#25191;&#34892;&#36825;&#20123;&#22330;&#26223;&#65292;&#24182;&#25163;&#21160;&#35780;&#20272;&#32467;&#26524;&#12290;&#27979;&#35797;&#22330;&#26223;&#36890;&#24120;&#21253;&#25324;&#35774;&#32622;&#29305;&#23450;&#39134;&#34892;&#25110;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#21463;&#27979;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#27492;&#30446;&#30340;&#30340;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#20063;&#35201;&#27714;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26080;&#20154;&#33322;&#31354;&#31995;&#32479;&#31995;&#32479;&#32423;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;AITester&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#26469;&#33258;&#21160;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#35780;&#20272;&#21508;&#31181;&#27979;&#35797;&#22330;&#26223;&#12290;&#27979;&#35797;&#22330;&#26223;&#36890;&#36807;AI&#25216;&#26415;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#65292;&#24182;&#27169;&#25311;&#21644;&#35780;&#20272;&#31995;&#32479;&#20013;&#19981;&#21516;&#32452;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27979;&#35797;&#30340;&#35206;&#30422;&#29575;&#21644;&#21487;&#38752;&#24230;&#12290;&#27492;&#22806;&#65292;AITester&#36824;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#20998;&#26512;&#21644;&#20248;&#21270;&#27979;&#35797;&#26696;&#20363;&#65292;&#20174;&#32780;&#22312;&#30701;&#26102;&#38388;&#20869;&#21457;&#29616;&#24182;&#38548;&#31163;&#31995;&#32479;&#28508;&#22312;&#30340;&#38169;&#35823;&#21644;&#38382;&#39064;&#12290;AITester&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#25104;&#26412;&#24182;&#25552;&#39640;UAS&#31995;&#32479;&#27979;&#35797;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v2 Announce Type: replace-cross  Abstract: Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems. The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes. The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings. The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation. In this paper, we propose a novel approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#65292;&#30830;&#20445;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;&#65292;&#20197;&#36798;&#21040;&#39640;&#25928;&#30340;3D&#32593;&#26684;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;</title><link>https://arxiv.org/abs/2403.15559</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#20445;&#23558;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#24212;&#29992;&#21040;3D&#32593;&#26684;&#30340;&#32441;&#29702;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#65292;&#30830;&#20445;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;&#65292;&#20197;&#36798;&#21040;&#39640;&#25928;&#30340;3D&#32593;&#26684;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15559v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#21270;&#26102;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#30830;&#20445;&#22810;&#35270;&#35282;&#30340;&#19968;&#33268;&#24615;&#12290;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#32858;&#21512;&#22810;&#35270;&#35282;&#36755;&#20837;&#65292;&#20854;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#22312;&#32858;&#21512;&#27493;&#39588;&#20013;&#30001;&#20110;&#24179;&#22343;&#25805;&#20316;&#24341;&#36215;&#30340;&#27169;&#31946;&#24615;&#65292;&#25110;&#32773;&#23616;&#37096;&#29305;&#24449;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20998;&#20026;&#22235;&#20010;&#38454;&#27573;&#26469;&#23454;&#29616;&#22810;&#35270;&#35282;&#30340;&#19968;&#33268;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#20174;&#39044;&#23450;&#20041;&#30340;&#22810;&#35270;&#35282;&#38598;&#29983;&#25104;&#19968;&#20010;2D&#32441;&#29702;&#30340;&#36807;&#23436;&#22791;&#38598;&#21512;&#12290;&#31532;&#20108;&#38454;&#27573;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#30340;&#35270;&#22270;&#65292;&#36825;&#20123;&#35270;&#22270;&#22312;&#35206;&#30422;&#19979;&#30340;3D&#27169;&#22411;&#20013;&#26159;&#30456;&#20114;&#19968;&#33268;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35299;&#20915;&#21322;&#27491;&#23450;&#35268;&#21010;&#26469;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#31532;&#19977;&#38454;&#27573;&#25191;&#34892;&#38750;&#21018;&#24615;&#23545;&#40784;&#65292;&#20197;&#22312;&#37325;&#21472;&#21306;&#22495;&#23545;&#36873;&#23450;&#30340;&#35270;&#22270;&#36827;&#34892;&#23545;&#40784;&#12290;&#31532;&#22235;&#38454;&#27573;&#35299;&#20915;&#19968;&#20010;MRF&#38382;&#39064;&#65292;&#23427;&#29992;&#20110;&#19968;&#33268;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#37325;&#24314;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#37319;&#20998;&#36873;&#37319;&#26679;&#26469;&#32454;&#21270;&#35270;&#35282;&#36873;&#25321;&#12290;&#25972;&#20307;&#26694;&#26550;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#65292;&#30452;&#21040;&#36798;&#21040;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;3D&#32593;&#26684;&#22810;&#35270;&#35282;&#32441;&#29702;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15559v2 Announce Type: replace-cross  Abstract: A fundamental problem in the texturing of 3D meshes using pre-trained text-to-image models is to ensure multi-view consistency. State-of-the-art approaches typically use diffusion models to aggregate multi-view inputs, where common issues are the blurriness caused by the averaging operation in the aggregation step or inconsistencies in local features. This paper introduces an optimization framework that proceeds in four stages to achieve multi-view consistency. Specifically, the first stage generates an over-complete set of 2D textures from a predefined set of viewpoints using an MV-consistent diffusion process. The second stage selects a subset of views that are mutually consistent while covering the underlying 3D model. We show how to achieve this goal by solving semi-definite programs. The third stage performs non-rigid alignment to align the selected views across overlapping regions. The fourth stage solves an MRF problem t
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#21644;&#20248;&#21183;&#65292;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#24494;&#35843;&#65292;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#33719;&#24471;&#25509;&#36817;&#20174;&#38646;&#24320;&#22987;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25552;&#20379;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.02504</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#21644;&#20248;&#21183;&#65292;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#24494;&#35843;&#65292;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#33719;&#24471;&#25509;&#36817;&#20174;&#38646;&#24320;&#22987;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25552;&#20379;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v3 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;&#65306;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20805;&#24403;&#34920;&#36798;&#24605;&#24819;&#21644;&#24773;&#24863;&#30340;&#20027;&#35201;&#28192;&#36947;&#65292;&#25991;&#26412;&#20998;&#26512;&#24050;&#25104;&#20026;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#20351;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20986;&#23453;&#36149;&#35265;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#25903;&#25345;&#35832;&#22914;&#35780;&#20272;&#20010;&#24615;&#29305;&#24449;&#12289;&#30417;&#27979;&#24515;&#29702;&#20581;&#24247;&#21644;&#20154;&#38469;&#27807;&#36890;&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;&#31561;&#21162;&#21147;&#12290;&#22312;&#25991;&#26412;&#20998;&#26512;&#20013;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20250;&#37319;&#29992;&#25163;&#21160;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#32791;&#26102;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#20808;&#26500;&#24314;&#30340;&#35789;&#20856;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#24773;&#20917;&#65292;&#25110;&#32773;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20195;&#34920;&#20102;&#25991;&#26412;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#21464;&#38761;&#26041;&#27861;&#12290;&#36825;&#31181;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32780;&#19982;&#20247;&#19981;&#21516;&#65292;&#26174;&#31034;&#20986;&#22312;&#36127;&#36131;&#22810;&#31181;&#19981;&#21516;&#20219;&#21153;&#26102;&#30340;&#39640;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#24494;&#35843;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#33719;&#24471;&#20960;&#20046;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26102;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#27599;&#20010;&#26032;&#20219;&#21153;&#37117;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v3 Announce Type: replace-cross  Abstract: Given that natural language serves as the primary conduit for expressing thoughts and emotions, text analysis has become a key technique in psychological research. It enables the extraction of valuable insights from natural language, facilitating endeavors like personality traits assessment, mental health monitoring, and sentiment analysis in interpersonal communications. In text analysis, existing studies often resort to either human coding, which is time-consuming, using pre-built dictionaries, which often fails to cover all possible scenarios, or training models from scratch, which requires large amounts of labeled data. In this tutorial, we introduce the pretrain-finetune paradigm. The pretrain-finetune paradigm represents a transformative approach in text analysis and natural language processing. This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in f
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#21019;&#26032;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Routoo&#8221;&#30340;&#24314;&#31569;&#65292;&#26088;&#22312;&#26681;&#25454;&#24615;&#33021;&#12289;&#25104;&#26412;&#21644;&#25928;&#29575;&#26469;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#25552;&#31034;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#26377;&#25928;&#30340;&#36335;&#30001;&#23398;&#20064;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Routoo: Learning to Route to Large Language Models Effectively
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#21019;&#26032;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Routoo&#8221;&#30340;&#24314;&#31569;&#65292;&#26088;&#22312;&#26681;&#25454;&#24615;&#33021;&#12289;&#25104;&#26412;&#21644;&#25928;&#29575;&#26469;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#25552;&#31034;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;: arXiv:2401.13979v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;&#36328;&#23398;&#31185;&#25688;&#35201;: &#24320;&#21457;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25104;&#26412;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#65292;&#25928;&#29575;&#20063;&#36234;&#26469;&#36234;&#20302;&#12290;&#27492;&#22806;&#65292;&#38381;&#28304;&#21644;&#22823;&#22411;&#30340;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#21709;&#24212;&#36136;&#37327;&#65292;&#20294;&#20195;&#20215;&#26356;&#39640;&#65292;&#19981;&#22914;&#23567;&#35268;&#27169;&#27169;&#22411;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Routoo&#8221;&#30340;&#24314;&#31569;&#35774;&#35745;&#65292;&#26088;&#22312;&#26681;&#25454;&#24615;&#33021;&#12289;&#25104;&#26412;&#21644;&#25928;&#29575;&#26469;&#20248;&#21270;&#29305;&#23450;&#25552;&#31034;&#30340;LLM&#36873;&#25321;&#12290;Routoo&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#19968;&#20010;&#24615;&#33021;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25104;&#26412;&#33258;&#36866;&#24212;&#35299;&#30721;&#22120;&#12290;&#24615;&#33021;&#39044;&#27979;&#22120;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;LLM&#65292;&#23427;&#19981;&#38656;&#35201;&#25191;&#34892;&#21644;&#35780;&#20272;&#23601;&#33021;&#20272;&#35745;&#21508;&#31181;&#24213;&#23618;LLM&#30340;&#24615;&#33021;&#12290;&#25104;&#26412;&#33258;&#36866;&#24212;&#35299;&#30721;&#22120;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#21644;&#20854;&#20182;&#32422;&#26463;&#65288;&#22914;&#25104;&#26412;&#21644;&#24310;&#36831;&#65289;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;MMLU&#22522;&#20934;&#23545;57&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Routoo&#22312;&#21305;&#37197;MMLU&#22522;&#20934;&#28857;&#26041;&#38754;&#19982;&#24320;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.13979v2 Announce Type: replace-cross  Abstract: Developing foundational large language models (LLMs) is becoming increasingly costly and inefficient. Also, closed-source and larger open-source models generally offer better response quality but come with higher inference costs than smaller models. In this paper, we introduce Routoo, an architecture designed to optimize the selection of LLMs for specific prompts based on performance, cost, and efficiency. Routoo consists of two key components: a performance predictor and a cost-aware decoding. The performance predictor is a lightweight LLM that estimates the performance of various underlying LLMs without needing to execute and evaluate them. The cost-aware decoding then selects the most suitable model based on these predictions and other constraints like cost and latency. We evaluated Routoo using the MMLU benchmark across 57 domains employing open-source models. Our results show that Routoo matches the performance of the Mixt
&lt;/p&gt;</description></item><item><title>D-TSN&#21033;&#29992;&#21487;&#24494;&#26641;&#25628;&#32034;&#20248;&#21270;&#31574;&#30053;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#34892;&#21160;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2401.11660</link><description>&lt;p&gt;
&#21487;&#24494;&#26641;&#25628;&#32034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differentiable Tree Search Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11660
&lt;/p&gt;
&lt;p&gt;
D-TSN&#21033;&#29992;&#21487;&#24494;&#26641;&#25628;&#32034;&#20248;&#21270;&#31574;&#30053;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#34892;&#21160;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11660v2 &#21457;&#24067;&#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#24341;&#29992;&#25688;&#35201;&#65306;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30340;&#31574;&#30053;&#20989;&#25968;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#20174;&#26377;&#38480;&#30340;&#36164;&#26009;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#22312;&#32447;&#25628;&#32034;&#20013;&#30830;&#23450;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#19981;&#20934;&#30830;&#65292;&#38169;&#35823;&#20250;&#32047;&#31215;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#34429;&#28982;&#22914;TreeQN&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#24341;&#20837;&#31639;&#27861;&#30340;&#20559;&#35265;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#20559;&#35265;&#30340;&#24378;&#24230;&#24448;&#24448;&#19981;&#22815;&#24378;&#65292;&#19981;&#36275;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#24494;&#26641;&#25628;&#32034;&#32593;&#32476;&#65288;D-TSN&#65289;&#30340;&#20840;&#26032;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#36890;&#36807;&#22312;&#26368;&#20339;&#31532;&#19968;&#22312;&#32447;&#25628;&#32034;&#31639;&#27861;&#30340;&#32467;&#26500;&#20013;&#23884;&#20837;&#31639;&#27861;&#32467;&#26500;&#26469;&#26174;&#33879;&#21152;&#24378;&#20559;&#35265;&#12290;D-TSN&#20351;&#29992;&#39044;&#27979;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#30340;&#21487;&#24494;&#26641;&#25628;&#32034;&#65292;&#36825;&#20801;&#35768;&#23545;&#25628;&#32034;&#36807;&#31243;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#22797;&#26434;&#30340;&#36319;&#36394;&#20219;&#21153;&#21644;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#31034;&#20102;D-TSN&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#32570;&#20047;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#21547;&#22797;&#26434;&#24863;&#30693;&#21644;&#35268;&#21010;&#29305;&#24615;&#30340;&#38543;&#26426;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#21160;&#20316;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11660v2 Announce Type: replace-cross  Abstract: In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance. An alternative approach involves learning a world model from the limited data and determining actions through online search. However, the performance is adversely affected by compounding errors arising from inaccuracies in the learned world model. While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic inductive biases into the neural network architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks. In this work, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm. D-TSN employs a learned world model to conduct a fully d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26696;&#20363;&#25512;&#29702;&#26469;&#25552;&#39640;&#22522;&#20110;OCL&#30340;MC/DC&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.03469</link><description>&lt;p&gt;
&#20351;&#29992;OCL&#21644;&#25628;&#32034;&#26041;&#27861;&#30340;&#39640;&#25928;MC/DC&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Test Data Generation for MC/DC with OCL and Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26696;&#20363;&#25512;&#29702;&#26469;&#25552;&#39640;&#22522;&#20110;OCL&#30340;MC/DC&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03469v3 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#33322;&#31354;&#30005;&#23376;&#36719;&#20214;&#31995;&#32479;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#38656;&#35201;&#31526;&#21512;&#22269;&#38469;&#23433;&#20840;&#26631;&#20934;&#22914;DO-178C&#30340;&#35201;&#27714;&#12290;&#33322;&#31354;&#30005;&#23376;&#24037;&#19994;&#30340;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#26159;&#25353;&#29031;&#23433;&#20840;&#26631;&#20934;&#25512;&#33616;&#30340;&#20934;&#21017;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#12290;DO-178C&#25512;&#33616;&#30340;&#20934;&#21017;&#20043;&#19968;&#26159;&#25913;&#36827;&#30340;&#26465;&#20214;/&#20915;&#31574;&#35206;&#30422;&#29575;(MC/DC)&#20934;&#21017;&#12290;&#24403;&#21069;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20351;&#29992;&#29992;&#20363;&#32422;&#26463;&#35821;&#35328;(OCL)&#32534;&#20889;&#30340;&#32422;&#26463;&#65292;&#24182;&#24212;&#29992;&#25628;&#32034;&#25216;&#26415;&#26469;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#19981;&#25903;&#25345;MC/DC&#20934;&#21017;&#65292;&#35201;&#20040;&#22312;&#20351;&#29992;&#22823;&#22411;&#33322;&#31354;&#30005;&#23376;&#31995;&#32479;&#26102;&#36935;&#21040;&#24615;&#33021;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#20013;&#33258;&#21160;&#29983;&#25104;MC/DC&#27979;&#35797;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;(CBR)&#21644;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;MC/DC&#23450;&#21046;OCL&#30340;&#32553;&#20943;&#33539;&#22260;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03469v3 Announce Type: replace-cross  Abstract: System-level testing of avionics software systems requires compliance with different international safety standards such as DO-178C. An important consideration of the avionics industry is automated test data generation according to the criteria suggested by safety standards. One of the recommended criteria by DO-178C is the modified condition/decision coverage (MC/DC) criterion. The current model-based test data generation approaches use constraints written in Object Constraint Language (OCL), and apply search techniques to generate test data. These approaches either do not support MC/DC criterion or suffer from performance issues while generating test data for large-scale avionics systems. In this paper, we propose an effective way to automate MC/DC test data generation during model-based testing. We develop a strategy that utilizes case-based reasoning (CBR) and range reduction heuristics designed to solve MC/DC-tailored OCL 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#21345;&#26041;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#21644;&#24191;&#20041;&#27779;&#23572;&#24503;-&#27779;&#23572;&#22827;&#29926;&#20857;&#20301;&#31227;&#26816;&#39564;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#24335;&#31526;&#21512;&#39044;&#26399;&#12290;</title><link>https://arxiv.org/abs/2312.10695</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#31574;&#30053;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Strategy Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10695
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#21345;&#26041;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#21644;&#24191;&#20041;&#27779;&#23572;&#24503;-&#27779;&#23572;&#22827;&#29926;&#20857;&#20301;&#31227;&#26816;&#39564;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#24335;&#31526;&#21512;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#65292;&#29992;&#20110;&#30830;&#23450;&#22312;&#37325;&#22797;&#30340;&#25112;&#30053;&#24418;&#24335;&#28216;&#25103;&#20013;&#65292;&#26681;&#25454;&#20195;&#29702;&#20154;&#34892;&#21160;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#36825;&#28041;&#21450;&#21040;&#20004;&#37096;&#20998;&#20869;&#23481;&#65306;&#30830;&#23450;&#20195;&#29702;&#21830;&#30340;&#32431;&#31574;&#30053;&#39057;&#29575;&#26159;&#21542;&#36275;&#22815;&#25509;&#36817;&#30446;&#26631;&#39057;&#29575;&#65292;&#20197;&#21450;&#30830;&#23450;&#22312;&#19981;&#21516;&#28216;&#25103;&#22238;&#21512;&#20013;&#36873;&#25321;&#30340;&#32431;&#31574;&#30053;&#26159;&#21542;&#29420;&#31435;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27979;&#35797;&#36890;&#36807;&#24212;&#29992;&#21345;&#26041;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26469;&#30830;&#23450;&#31532;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#24212;&#29992;&#24191;&#20041;&#30340;&#27779;&#23572;&#24503;-&#27779;&#23572;&#22827;&#29926;&#20857;&#20301;&#31227;&#26816;&#39564;&#26469;&#30830;&#23450;&#31532;&#20108;&#37096;&#20998;&#12290;&#20004;&#32773;&#30340;&#32467;&#26524;&#36890;&#36807;Bonferroni&#26657;&#27491;&#30456;&#32467;&#21512;&#65292;&#20026;&#32473;&#23450;&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;$\alpha$&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#35813;&#27979;&#35797;&#24212;&#29992;&#20110;&#20844;&#24320;&#30340;&#20154;&#31867;&#30707;&#22836;-&#21098;&#20992;-&#24067;&#28216;&#25103;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#21253;&#25324;500&#21517;&#20154;&#31867;&#29609;&#23478;&#30340;50&#27425;&#22238;&#21512;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#20551;&#35774;&#27979;&#35797;&#65292;&#35813;&#20551;&#35774;&#26159;&#21442;&#19982;&#32773;&#36981;&#24490;&#32473;&#23450;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10695v4 Announce Type: replace-cross  Abstract: We present a nonparametric statistical test for determining whether an agent is following a given mixed strategy in a repeated strategic-form game given samples of the agent's play. This involves two components: determining whether the agent's frequencies of pure strategies are sufficiently close to the target frequencies, and determining whether the pure strategies selected are independent between different game iterations. Our integrated test involves applying a chi-squared goodness of fit test for the first component and a generalized Wald-Wolfowitz runs test for the second component. The results from both tests are combined using Bonferroni correction to produce a complete test for a given significance level $\alpha.$ We applied the test to publicly available data of human rock-paper-scissors play. The data consists of 50 iterations of play for 500 human players. We test with a null hypothesis that the players are following
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#26469;&#25552;&#39640;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#30340;&#36710;&#27969;&#37327;&#21644;&#36895;&#24230;&#26465;&#20214;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#36890;&#36807;&#29575;&#24182;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.09436</link><description>&lt;p&gt;
&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#22312;&#31895;&#31890;&#24230;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Temporal Transfer Learning for Traffic Optimization with Coarse-grained Advisory Autonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#26469;&#25552;&#39640;&#21672;&#35810;&#33258;&#27835;&#19979;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#30340;&#36710;&#27969;&#37327;&#21644;&#36895;&#24230;&#26465;&#20214;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#36890;&#36807;&#29575;&#24182;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09436v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09436v2 Announce Type: replace  Abstract: The recent development of connected and automated vehicle (CAV) technologies has spurred investigations to optimize dense urban traffic to maximize vehicle speed and throughput. This paper explores advisory autonomy, in which real-time driving advisories are issued to the human drivers, thus achieving near-term performance of automated vehicles. Due to the complexity of traffic systems, recent studies of coordinating CAVs have resorted to leveraging deep reinforcement learning (RL). Coarse-grained advisory is formalized as zero-order holds, and we consider a range of hold duration from 0.1 to 40 seconds. However, despite the similarity of the higher frequency tasks on CAVs, a direct application of deep RL fails to be generalized to advisory autonomy tasks. To overcome this, we utilize zero-shot transfer, training policies on a set of source tasks--specific traffic scenarios with designated hold durations--and then evaluating the effi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Unreal Engine&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.04421</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#19968;&#20010;&#38024;&#23545;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39550;&#39542;&#24773;&#26223;
&lt;/p&gt;
&lt;p&gt;
SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.04421
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Unreal Engine&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2309.04421v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#22312;&#27773;&#36710;&#39046;&#22495;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25163;&#21183;&#21160;&#20316;&#25968;&#25454;&#24211;&#21487;&#33021;&#20250;&#24456;&#33392;&#38590;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#30001;&#34394;&#25311;3D&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25163;&#21183;&#25968;&#25454;&#30340;&#26500;&#24819;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#29983;&#25104;&#29616;&#23454;&#30340;&#25163;&#21183;&#21160;&#20316;&#65292;&#25552;&#20379;&#33258;&#23450;&#20041;&#36873;&#39033;&#24182;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#30340;&#21361;&#38505;&#12290;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#22810;&#31181;&#21464;&#20307;&#65292;&#21253;&#25324;&#25163;&#21183;&#36895;&#24230;&#12289;&#34920;&#29616;&#21147;&#21644;&#25163;&#24418;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19981;&#21516;&#20301;&#32622;&#30340;&#25668;&#20687;&#22836;&#21644;&#31867;&#22411;&#30340;&#25668;&#20687;&#22836;&#65292;&#22914;RGB&#12289;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#33719;&#21462;&#36825;&#20123;&#25668;&#20687;&#22836;&#30340;&#39069;&#22806;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#8220;SynthoGestures&#65288;https://github.com/amrgomaaelhady/SynthoGestures&#65289;&#8221;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#31934;&#24230;&#65292;&#24182;&#21487;&#20197;&#20195;&#26367;&#25110;&#34917;&#20805;&#30495;&#23454;&#25163;&#21183;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33410;&#30465;&#26102;&#38388;&#65292;&#25105;&#20204;&#20026;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.04421v2 Announce Type: replace  Abstract: Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures (https://github.com/amrgomaaelhady/SynthoGestures), improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#26415;&#35745;&#31639;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#8220;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#8221;&#26426;&#21046;&#36827;&#34892;&#27867;&#21270;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#26415;&#35745;&#31639;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#8220;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#8221;&#26426;&#21046;&#36827;&#34892;&#27867;&#21270;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.01154v4 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#23545;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28044;&#29616;&#35745;&#31639;&#21644;&#38382;&#39064;&#27714;&#35299;&#33021;&#21147;&#30340;&#26356;&#22909;&#29702;&#35299;&#23545;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#24212;&#29992;&#33539;&#22260;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#35757;&#32451;&#20013;&#29992;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#65292;&#26159;&#22914;&#20309;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#65292;&#36825;&#31181;&#35745;&#31639;&#33021;&#22815;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26500;&#25104;&#20102;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#38656;&#35201;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#37327;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986; discontinuities&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#20851;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#30340;&#25968;&#25454;&#36816;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#65292;&#35753;&#23427;&#23398;&#20250;&#20102;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;&#35813;&#27169;&#22411;&#25193;&#23637;&#30340;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25903;&#25345;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20316;&#26102;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#65292;&#20854;&#20013;&#35745;&#31639;&#22312;&#20540;&#31354;&#38388;&#20013;&#21457;&#29983;&#65292;&#19968;&#26086;&#36755;&#20837;&#34987;&#32534;&#30721;&#65292;&#35821;&#35328;&#27169;&#22411;&#23601;&#21487;&#20197;&#23545;&#31639;&#26415;&#35745;&#31639;&#36827;&#34892;&#39044;&#27979;&#21644;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.01154v4 Announce Type: replace  Abstract: A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARN&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#32467;&#26500;&#24863;&#30693;&#31354;&#38388;&#27880;&#24847;&#23618;&#21644;GRU&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#32858;&#21512;&#25968;&#25454;&#37325;&#24314;&#21040;&#39640;&#20998;&#36776;&#29575;&#30340;&#19981;&#35268;&#21017;&#20998;&#21306;&#12290;</title><link>https://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
SRAN: &#32467;&#26500;&#24863;&#30693;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#31354;&#38388;&#26102;&#38388; disaggregation
&lt;/p&gt;
&lt;p&gt;
SARN: Structurally-Aware Recurrent Network for Spatio-Temporal Disaggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARN&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#32467;&#26500;&#24863;&#30693;&#31354;&#38388;&#27880;&#24847;&#23618;&#21644;GRU&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#32858;&#21512;&#25968;&#25454;&#37325;&#24314;&#21040;&#39640;&#20998;&#36776;&#29575;&#30340;&#19981;&#35268;&#21017;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2306.07292v4 &#21457;&#24067;&#20844;&#21578;&#31867;&#22411;: replace-cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.07292v4 Announce Type: replace-cross  Abstract: Open data is frequently released spatially aggregated, usually to comply with privacy policies. But coarse, heterogeneous aggregations complicate learning and integration for downstream AI/ML systems. In this work, we consider models to disaggregate spatio-temporal data from a low-resolution, irregular partition (e.g., census tract) to a high-resolution, irregular partition (e.g., city block). We propose an overarching model named the Structurally-Aware Recurrent Network (SARN), which integrates structurally-aware spatial attention (SASA) layers into the Gated Recurrent Unit (GRU) model. The spatial attention layers capture spatial interactions among regions, while the gated recurrent module captures the temporal dependencies. Each SASA layer calculates both global and structural attention -- global attention facilitates comprehensive interactions between different geographic levels, while structural attention leverages the con
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Multi-Chain Reasoning&#8221;&#65288;MCR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20803;&#25512;&#29702;&#22810;&#20010;&#24605;&#24819;&#38142;&#26469;&#25913;&#36827;&#22810;&#36339;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#21319;&#26368;&#32456;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#22810;&#38142;&#24605;&#32500;&#20803;&#25512;&#29702;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Multi-Chain Reasoning&#8221;&#65288;MCR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20803;&#25512;&#29702;&#22810;&#20010;&#24605;&#24819;&#38142;&#26469;&#25913;&#36827;&#22810;&#36339;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#21319;&#26368;&#32456;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2304.13007v4 &#26032;&#38395;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;  &#32763;&#35793;&#25688;&#35201;&#65306;&#29616;&#20195;&#30340;&#22810;&#36339;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#25512;&#29702;&#27493;&#39588;&#65292;&#31216;&#20026;&#8220;&#24605;&#24819;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#36798;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#65292;&#22810;&#20010;&#38142;&#34987;&#25277;&#26679;&#65292;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#26159;&#26412;&#36523;&#25918;&#24323;&#20102;&#23545;&#20013;&#38388;&#27493;&#39588;&#30340;&#32771;&#34385;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#27809;&#26377;&#32771;&#34385;&#19981;&#21516;&#38142;&#20043;&#38388;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20063;&#27809;&#26377;&#20026;&#39044;&#27979;&#30340;&#31572;&#26696;&#25552;&#20379;&#32479;&#19968;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#38142;&#25512;&#29702;&#8221;&#65288;MCR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20803;&#25512;&#29702;&#22810;&#24605;&#24819;&#38142;&#65292;&#32780;&#19981;&#26159;&#23545;&#23427;&#20204;&#30340;&#31572;&#26696;&#36827;&#34892;&#32858;&#21512;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#22312;&#23427;&#20204;&#20043;&#38388;&#28151;&#21512;&#20449;&#24687;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#24182;&#39044;&#27979;&#31572;&#26696;&#26102;&#36873;&#25321;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#24378;&#22522;&#32447;&#36890;&#24120;&#22312;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#32447;&#32034;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#21518;&#32493;&#32447;&#32034;&#30340;&#35201;&#27714;&#36234;&#26469;&#36234;&#20005;&#26684;&#8212;&#8212;&#19982;&#35813;&#30452;&#35273;&#30456;&#37197;&#21512;&#12290;Developer Testing for Mobile Apps: A Case Study on EValuating Security Features
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.13007v4 Announce Type: replace-cross  Abstract: Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35780;&#20272; rubric&#20013;&#33719;&#21462;&#23398;&#20064;&#32773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#31616;&#21270;&#21442;&#25968;&#33719;&#21462;&#65292;&#20197;&#33258;&#21160;&#21270;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#25216;&#33021;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2209.05467</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#32593;&#32476;&#24314;&#27169;&#35780;&#20272; rubrics&#65306;&#19968;&#31181;&#21153;&#23454;&#30340;approach
&lt;/p&gt;
&lt;p&gt;
Modelling Assessment Rubrics through Bayesian Networks: a Pragmatic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.05467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35780;&#20272; rubric&#20013;&#33719;&#21462;&#23398;&#20064;&#32773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#31616;&#21270;&#21442;&#25968;&#33719;&#21462;&#65292;&#20197;&#33258;&#21160;&#21270;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#25216;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05467v3 &#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449; &#25688;&#35201;&#65306;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#32773;&#25216;&#33021;&#33258;&#21160;&#35780;&#20272;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#35780;&#20272; rubric&#36890;&#24120;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#30456;&#20851;&#25216;&#33021;&#27700;&#24179;&#21644;&#36164;&#36136;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#26576;&#20123;&#23618;&#27425;&#65288;&#37096;&#20998;&#65289;&#39034;&#24207;&#23450;&#20041;&#35780;&#20272; rubric&#20013;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#23398;&#20064;&#32773;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#36923;&#36753;&#38376;&#65288;&#24120;&#31216;&#20026;&#8220;&#28151;&#28102;&#38376;&#8221;&#65289;&#26469;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20197;&#20415;&#36890;&#36807;&#19987;&#23478;&#31616;&#21270;&#21442;&#25968;&#30340;&#33719;&#21462;&#65292;&#24182;&#22312;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#22914;&#20309;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#23545;&#35745;&#31639;&#26426;&#24605;&#32500;&#25216;&#33021;&#30340;&#27979;&#35797;&#20219;&#21153;&#30340;&#35780;&#20272;&#30340;&#12290;&#20174;&#35780;&#20272; rubric&#20013;&#31616;&#21270;&#27169;&#22411;&#30340;&#33719;&#21462;&#26041;&#24335;&#65292;&#20026;&#24555;&#36895;&#33258;&#21160;&#21270;&#22810;&#31181;&#20219;&#21153;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05467v3 Announce Type: replace-cross  Abstract: Automatic assessment of learner competencies is a fundamental task in intelligent tutoring systems. An assessment rubric typically and effectively describes relevant competencies and competence levels. This paper presents an approach to deriving a learner model directly from an assessment rubric defining some (partial) ordering of competence levels. The model is based on Bayesian networks and exploits logical gates with uncertainty (often referred to as noisy gates) to reduce the number of parameters of the model, so to simplify their elicitation by experts and allow real-time inference in intelligent tutoring systems. We illustrate how the approach can be applied to automatize the human assessment of an activity developed for testing computational thinking skills. The simple elicitation of the model starting from the assessment rubric opens up the possibility of quickly automating the assessment of several tasks, making them m
&lt;/p&gt;</description></item></channel></rss>