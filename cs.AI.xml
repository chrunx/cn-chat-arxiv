<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.AI.xml</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#25913;&#36827;&#27169;&#22411;&#35780;&#20215;&#32773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#26410;&#26631;&#27880;&#30340;&#25351;&#20196;&#36827;&#34892;&#36845;&#20195;&#24335;&#30340;&#33258;&#25105;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#23545;&#27604;&#24615;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#26469;&#29983;&#25104;&#25512;&#29702;&#36712;&#36857;&#21644;&#26368;&#32456;&#21028;&#26029;&#32467;&#26524;&#12290;&#22312;&#26080;&#20219;&#20309;&#20559;&#22909;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36825;&#19968;&#36807;&#31243;&#25913;&#36827;&#30340;&#8220;&#33258;&#25105;&#23398;&#20064;&#35780;&#20272;&#32773;&#8221;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#19968;&#20010;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Llama3-70B-Instruct&#65289;&#22312;RewardBench&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;75.4&#25552;&#21319;&#33267;88.3&#65288;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#26102;&#21487;&#36798;88.7&#65289;&#65292;&#36825;&#36229;&#36807;&#20102;&#24120;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20215;&#32773;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02666</link><description>&lt;p&gt;
Self-Taught Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#25913;&#36827;&#27169;&#22411;&#35780;&#20215;&#32773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#26410;&#26631;&#27880;&#30340;&#25351;&#20196;&#36827;&#34892;&#36845;&#20195;&#24335;&#30340;&#33258;&#25105;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#23545;&#27604;&#24615;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#26469;&#29983;&#25104;&#25512;&#29702;&#36712;&#36857;&#21644;&#26368;&#32456;&#21028;&#26029;&#32467;&#26524;&#12290;&#22312;&#26080;&#20219;&#20309;&#20559;&#22909;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36825;&#19968;&#36807;&#31243;&#25913;&#36827;&#30340;&#8220;&#33258;&#25105;&#23398;&#20064;&#35780;&#20272;&#32773;&#8221;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#19968;&#20010;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Llama3-70B-Instruct&#65289;&#22312;RewardBench&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;75.4&#25552;&#21319;&#33267;88.3&#65288;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#26102;&#21487;&#36798;88.7&#65289;&#65292;&#36825;&#36229;&#36807;&#20102;&#24120;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20215;&#32773;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02666v1 Announce Type: cross  Abstract: Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM jud
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20165;&#38656;&#27169;&#22411;API&#25509;&#21475;&#35775;&#38382;&#26469;&#25171;&#30772;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;&#38544;&#34255;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2408.02651</link><description>&lt;p&gt;
Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20165;&#38656;&#27169;&#22411;API&#25509;&#21475;&#35775;&#38382;&#26469;&#25171;&#30772;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;&#38544;&#34255;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02651v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs. Yet, the potential for generating harmful content through these models seems to persist. This paper explores the concept of jailbreaking LLMs-reversing their alignment through adversarial triggers. Previous methods, such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts, have had limited success on black-box models due to their requirements for model access and for producing a low variety of manually crafted prompts, making them susceptible to being blocked. This paper introduces a novel approach using reinforcement learning to optimize adversarial triggers, requiring only inference API 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;SEAS&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#20248;&#21270;&#21021;&#22987;&#21270;&#12289;&#25915;&#20987;&#21644; adversarial &#35757;&#32451;&#65292;&#20351;&#24471; model &#33021;&#26356;&#26377;&#25928;&#22320;&#25269;&#24481;&#36890;&#36807;&#33258;&#36523;&#25968;&#25454;&#29983;&#25104;&#30340;&#25915;&#20987;&#65292;&#25552;&#39640; LLM &#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02632</link><description>&lt;p&gt;
SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;SEAS&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#20248;&#21270;&#21021;&#22987;&#21270;&#12289;&#25915;&#20987;&#21644; adversarial &#35757;&#32451;&#65292;&#20351;&#24471; model &#33021;&#26356;&#26377;&#25928;&#22320;&#25269;&#24481;&#36890;&#36807;&#33258;&#36523;&#25968;&#25454;&#29983;&#25104;&#30340;&#25915;&#20987;&#65292;&#25552;&#39640; LLM &#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02632v1 Announce Type: cross  Abstract: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving }\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#21548;&#32780;&#35828;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LSLM&#65289;&#8221;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19982;&#20154;&#31867;&#23454;&#26102;&#20132;&#20114;&#30340;&#36807;&#31243;&#20013;&#26082;&#33021;&#35828;&#35805;&#21448;&#33021;&#21548;&#12290;&#36825;&#31181;&#27169;&#22411;&#35774;&#35745;&#37319;&#29992;&#20102;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#37197;&#22791;&#20102;&#29420;&#31435;&#30340;&#35821;&#38899;&#29983;&#25104;&#21644;&#35299;&#30721;&#22120;&#21644;&#23454;&#26102;&#38899;&#39057;&#25910;&#21548;&#26426;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#30495;&#27491;&#30340;&#23454;&#26102;&#20132;&#20114;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#30340;&#33258;&#28982;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02622</link><description>&lt;p&gt;
Language Model Can Listen While Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#21548;&#32780;&#35828;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LSLM&#65289;&#8221;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19982;&#20154;&#31867;&#23454;&#26102;&#20132;&#20114;&#30340;&#36807;&#31243;&#20013;&#26082;&#33021;&#35828;&#35805;&#21448;&#33021;&#21548;&#12290;&#36825;&#31181;&#27169;&#22411;&#35774;&#35745;&#37319;&#29992;&#20102;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#37197;&#22791;&#20102;&#29420;&#31435;&#30340;&#35821;&#38899;&#29983;&#25104;&#21644;&#35299;&#30721;&#22120;&#21644;&#23454;&#26102;&#38899;&#39057;&#25910;&#21548;&#26426;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#30495;&#27491;&#30340;&#23454;&#26102;&#20132;&#20114;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#30340;&#33258;&#28982;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02622v1 Announce Type: cross  Abstract: Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Backward-HXP&#30340;&#26032;&#21382;&#21490;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#23545;&#21382;&#21490;&#34892;&#20026;&#36827;&#34892;&#36817;&#20284;&#36817;&#20284;&#65292;&#23601;&#33021;&#22815;&#20026;&#38271;&#26399;&#21382;&#21490;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2408.02606</link><description>&lt;p&gt;
Backward explanations via redefinition of predicates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Backward-HXP&#30340;&#26032;&#21382;&#21490;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#23545;&#21382;&#21490;&#34892;&#20026;&#36827;&#34892;&#36817;&#20284;&#36817;&#20284;&#65292;&#23601;&#33021;&#22815;&#20026;&#38271;&#26399;&#21382;&#21490;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02606v1 Announce Type: new  Abstract: History eXplanation based on Predicates (HXP), studies the behavior of a Reinforcement Learning (RL) agent in a sequence of agent's interactions with the environment (a history), through the prism of an arbitrary predicate. To this end, an action importance score is computed for each action in the history. The explanation consists in displaying the most important actions to the user. As the calculation of an action's importance is #W[1]-hard, it is necessary for long histories to approximate the scores, at the expense of their quality. We therefore propose a new HXP method, called Backward-HXP, to provide explanations for these histories without having to approximate scores. Experiments show the ability of B-HXP to summarise long histories.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Progressively Selective Label Enhancement for Language Model Alignment&#8221;&#65288;PSLE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#36880;&#27493;&#31934;&#36873;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30456;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#20135;&#29983;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#26356;&#26032;&#30340;&#25351;&#23548;&#21407;&#21017;&#25913;&#36827;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#23454;&#29616;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#23494;&#20999;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2408.02599</link><description>&lt;p&gt;
Progressively Selective Label Enhancement for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Progressively Selective Label Enhancement for Language Model Alignment&#8221;&#65288;PSLE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#36880;&#27493;&#31934;&#36873;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30456;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#20135;&#29983;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#26356;&#26032;&#30340;&#25351;&#23548;&#21407;&#21017;&#25913;&#36827;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#23454;&#29616;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#23494;&#20999;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02599v1 Announce Type: cross  Abstract: Large Language Models have demonstrated impressive capabilities in various language tasks but may produce content that misaligns with human expectations, raising ethical and legal concerns. Therefore, it is important to explore the limitations and implement restrictions on the models to ensure safety and compliance, with Reinforcement Learning from Human Feedback (RLHF) being the primary method. Due to challenges in stability and scalability with the RLHF stages, researchers are exploring alternative methods to achieve effects comparable to those of RLHF. However, these methods often depend on large high-quality datasets and inefficiently utilize generated data. To deal with this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment, a framework that fully utilizes all generated data by guiding the model with principles to align outputs with human expectations. Using a dynamically update
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36890;&#36807;&#22788;&#29702;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#25551;&#36848;&#24615;&#22270;&#20687;&#25551;&#36848;&#30340;&#36755;&#20837;&#19977;&#20803;&#32452;&#65292;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#12290;&#36890;&#36807;&#25429;&#25417;&#25991;&#26412;&#19982;&#35270;&#35273;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#35773;&#21050;&#12290;</title><link>https://arxiv.org/abs/2408.02595</link><description>&lt;p&gt;
Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02595
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36890;&#36807;&#22788;&#29702;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#25551;&#36848;&#24615;&#22270;&#20687;&#25551;&#36848;&#30340;&#36755;&#20837;&#19977;&#20803;&#32452;&#65292;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#22810;&#27169;&#24577;&#35773;&#21050;&#12290;&#36890;&#36807;&#25429;&#25417;&#25991;&#26412;&#19982;&#35270;&#35273;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02595v1 Announce Type: new  Abstract: Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primar
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#21151;&#33021;&#30340;&#29305;&#23450;&#31867;&#22411;&#39046;&#22495;&#30340;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#65288;&#22914;Llama2&#65292;Mistral&#65292;Gemma&#21644;Aya&#65289;&#26469;&#25552;&#39640;&#20854;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#39640;&#30340;&#36136;&#37327;&#25688;&#35201;&#36755;&#20986;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#20204;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#29992;&#25143;&#30340;&#26816;&#32034;&#36807;&#31243;&#25552;&#20379;&#20102;&#26356;&#21152;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2408.02584</link><description>&lt;p&gt;
Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#21151;&#33021;&#30340;&#29305;&#23450;&#31867;&#22411;&#39046;&#22495;&#30340;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#65288;&#22914;Llama2&#65292;Mistral&#65292;Gemma&#21644;Aya&#65289;&#26469;&#25552;&#39640;&#20854;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#39640;&#30340;&#36136;&#37327;&#25688;&#35201;&#36755;&#20986;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#20204;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#29992;&#25143;&#30340;&#26816;&#32034;&#36807;&#31243;&#25552;&#20379;&#20102;&#26356;&#21152;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02584v1 Announce Type: cross  Abstract: The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-relat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#36890;&#36807;&#32858;&#31867;&#21644;&#25366;&#25496;&#35821;&#38899;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;&#23545;&#20110;&#19981;&#21516;&#30340;&#35821;&#38899;&#21475;&#38899;&#65292;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#35821;&#38899;&#21475;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#21253;&#25324;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#20998;&#24067;&#24615;&#20581;&#22766;&#20248;&#21270;&#20197;&#21450;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25216;&#26415;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21363;&#20351;&#38754;&#23545;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#21319;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02582</link><description>&lt;p&gt;
Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#36890;&#36807;&#32858;&#31867;&#21644;&#25366;&#25496;&#35821;&#38899;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;&#23545;&#20110;&#19981;&#21516;&#30340;&#35821;&#38899;&#21475;&#38899;&#65292;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#35821;&#38899;&#21475;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#21253;&#25324;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#20998;&#24067;&#24615;&#20581;&#22766;&#20248;&#21270;&#20197;&#21450;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25216;&#26415;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21363;&#20351;&#38754;&#23545;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#21319;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02582v1 Announce Type: cross  Abstract: Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical speech patterns. As a result, those systems often poorly perform on atypical accented speech. In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech. For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech. Fine-tuning ASR on the mined Indian accent speech using the proposed superv
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#34701;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#25104;&#21151;&#25581;&#31034;&#20102;&#21477;&#23376;&#12289;&#22270;&#20687;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#65292;&#20026;&#31038;&#20132;&#23186;&#20307;&#20013;&#24773;&#24863;&#30340;&#34920;&#36798;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2408.02571</link><description>&lt;p&gt;
Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#34701;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#25104;&#21151;&#25581;&#31034;&#20102;&#21477;&#23376;&#12289;&#22270;&#20687;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#65292;&#20026;&#31038;&#20132;&#23186;&#20307;&#20013;&#24773;&#24863;&#30340;&#34920;&#36798;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02571v1 Announce Type: new  Abstract: The emoticons are symbolic representations that generally accompany the textual content to visually enhance or summarize the true intention of a written message. Although widely utilized in the realm of social media, the core semantics of these emoticons have not been extensively explored based on multiple modalities. Incorporating textual and visual information within a single message develops an advanced way of conveying information. Hence, this research aims to analyze the relationship among sentences, visuals, and emoticons. For an orderly exposition, this paper initially provides a detailed examination of the various techniques for extracting multimodal features, emphasizing the pros and cons of each method. Through conducting a comprehensive examination of several multimodal algorithms, with specific emphasis on the fusion approaches, we have proposed a novel contrastive learning based multimodal architecture. The proposed model em
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#37325;&#28857;&#25506;&#35752;&#20102;&#22522;&#20110;&#29702;&#35770;&#24515;&#26234;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#25239;&#24615;&#22260;&#26827;&#26827;&#31867;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#31574;&#30053;&#35268;&#21010;&#25216;&#26415;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02559</link><description>&lt;p&gt;
Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#37325;&#28857;&#25506;&#35752;&#20102;&#22522;&#20110;&#29702;&#35770;&#24515;&#26234;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#25239;&#24615;&#22260;&#26827;&#26827;&#31867;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#31574;&#30053;&#35268;&#21010;&#25216;&#26415;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02559v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that alth
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25512;&#20986;&#20102;MeshAnything V2&#65292;&#36825;&#26159;&#19968;&#39033;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#30456;&#37051;&#32593;&#26684;tokenization&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;MeshAnything V2&#22312;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#36825;&#31181;&#25913;&#36827;&#24402;&#21151;&#20110;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;tokenization&#26041;&#27861;&#8212;&#8212;Adjacent Mesh Tokenization&#65288;AMT&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#21487;&#33021;&#30340;&#24773;&#20917;&#21482;&#20351;&#29992;&#21333;&#20010;&#39030;&#28857;&#32780;&#19981;&#26159;&#19977;&#20010;&#39030;&#28857;&#26469;&#34920;&#31034;&#27599;&#20010;&#38754;&#65292;&#20174;&#32780;&#24179;&#22343;&#24773;&#20917;&#19979;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#20102;&#19968;&#21322;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;token&#24207;&#21015;&#26356;&#21152;&#32039;&#20945;&#21644;&#32467;&#26500;&#21270;&#65292;&#20174;&#32780;&#23545;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#20135;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;AMT&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02555</link><description>&lt;p&gt;
MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25512;&#20986;&#20102;MeshAnything V2&#65292;&#36825;&#26159;&#19968;&#39033;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#30456;&#37051;&#32593;&#26684;tokenization&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;MeshAnything V2&#22312;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#36825;&#31181;&#25913;&#36827;&#24402;&#21151;&#20110;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;tokenization&#26041;&#27861;&#8212;&#8212;Adjacent Mesh Tokenization&#65288;AMT&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#21487;&#33021;&#30340;&#24773;&#20917;&#21482;&#20351;&#29992;&#21333;&#20010;&#39030;&#28857;&#32780;&#19981;&#26159;&#19977;&#20010;&#39030;&#28857;&#26469;&#34920;&#31034;&#27599;&#20010;&#38754;&#65292;&#20174;&#32780;&#24179;&#22343;&#24773;&#20917;&#19979;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#20102;&#19968;&#21322;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;token&#24207;&#21015;&#26356;&#21152;&#32039;&#20945;&#21644;&#32467;&#26500;&#21270;&#65292;&#20174;&#32780;&#23545;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#20135;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;AMT&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#32593;&#26684;&#29983;&#25104;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02555v1 Announce Type: new  Abstract: We introduce MeshAnything V2, an autoregressive transformer that generates Artist-Created Meshes (AM) aligned to given shapes. It can be integrated with various 3D asset production pipelines to achieve high-quality, highly controllable AM generation. MeshAnything V2 surpasses previous methods in both efficiency and performance using models of the same size. These improvements are due to our newly proposed mesh tokenization method: Adjacent Mesh Tokenization (AMT). Different from previous methods that represent each face with three vertices, AMT uses a single vertex whenever possible. Compared to previous methods, AMT requires about half the token sequence length to represent the same mesh in average. Furthermore, the token sequences from AMT are more compact and well-structured, fundamentally benefiting AM generation. Our extensive experiments show that AMT significantly improves the efficiency and performance of AM generation. Project P
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;&#35299;&#30721;&#32908;&#32905;&#21151;&#33021;&#24615;&#32593;&#32476;&#26469;&#25552;&#39640;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32908;&#32905;&#21516;&#27493;&#32780;&#38750;&#21333;&#19968;&#32908;&#32905;&#27963;&#21160;&#26469;&#32534;&#30721;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#20154;&#31867;-&#26426;&#22120;&#30028;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#24863;&#30693;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.02547</link><description>&lt;p&gt;
The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;&#35299;&#30721;&#32908;&#32905;&#21151;&#33021;&#24615;&#32593;&#32476;&#26469;&#25552;&#39640;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32908;&#32905;&#21516;&#27493;&#32780;&#38750;&#21333;&#19968;&#32908;&#32905;&#27963;&#21160;&#26469;&#32534;&#30721;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#20154;&#31867;-&#26426;&#22120;&#30028;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#24863;&#30693;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02547v1 Announce Type: new  Abstract: Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots. Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems. The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions. This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation. We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle con
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAG Foundry&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#25968;&#25454;&#21019;&#24314;&#12289;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#35780;&#20272;&#27969;&#31243;&#65292;&#31616;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;RAG&#30340;&#23454;&#39564;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;RAG&#26041;&#27861;&#30340;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#23454;&#39564;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#37096;&#25110;&#19987;&#26377;&#30693;&#35782;&#28304;&#65292;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#21019;&#24314;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;RAG&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2408.02545</link><description>&lt;p&gt;
RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAG Foundry&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#25968;&#25454;&#21019;&#24314;&#12289;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#35780;&#20272;&#27969;&#31243;&#65292;&#31616;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;RAG&#30340;&#23454;&#39564;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;RAG&#26041;&#27861;&#30340;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#23454;&#39564;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#37096;&#25110;&#19987;&#26377;&#30693;&#35782;&#28304;&#65292;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#21019;&#24314;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;RAG&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02545v1 Announce Type: cross  Abstract: Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llam
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counterfactual Shapley Values&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36879;&#26126;&#24230;&#65292;&#36890;&#36807;&#32467;&#21512;&#21453;&#20107;&#23454;&#20998;&#26512;&#19982;Shapley&#20540;&#26469;&#37327;&#21270;&#21644;&#27604;&#36739;&#19981;&#21516;&#29366;&#24577;&#32500;&#24230;&#23545;&#19981;&#21516;&#21160;&#20316;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29305;&#24449;&#20540;&#20989;&#25968;&#65292;&#22914;&#8220;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#21644;&#8220;&#24179;&#22343;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#26102;&#30340;&#38590;&#39064;&#65292;&#24182;&#35745;&#31639;&#20102;Shapley&#20540;&#20197;&#35780;&#20272;&#26368;&#20248;&#21160;&#20316;&#21644;&#38750;&#26368;&#20248;&#21160;&#20316;&#20043;&#38388;&#30340;&#36129;&#29486;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;GridWorld&#12289;FrozenLake&#21644;Taxi&#31561;&#22810;&#20010;RL&#39046;&#22495;&#20869;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;CSV&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#22797;&#26434;RL&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#24230;&#65292;&#36824;&#37327;&#21270;&#20102;&#19981;&#21516;&#20915;&#31574;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2408.02529</link><description>&lt;p&gt;
Counterfactual Shapley Values for Explaining Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counterfactual Shapley Values&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36879;&#26126;&#24230;&#65292;&#36890;&#36807;&#32467;&#21512;&#21453;&#20107;&#23454;&#20998;&#26512;&#19982;Shapley&#20540;&#26469;&#37327;&#21270;&#21644;&#27604;&#36739;&#19981;&#21516;&#29366;&#24577;&#32500;&#24230;&#23545;&#19981;&#21516;&#21160;&#20316;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29305;&#24449;&#20540;&#20989;&#25968;&#65292;&#22914;&#8220;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#21644;&#8220;&#24179;&#22343;&#21453;&#20107;&#23454;&#24046;&#24322;&#29305;&#24449;&#20540;&#8221;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#26102;&#30340;&#38590;&#39064;&#65292;&#24182;&#35745;&#31639;&#20102;Shapley&#20540;&#20197;&#35780;&#20272;&#26368;&#20248;&#21160;&#20316;&#21644;&#38750;&#26368;&#20248;&#21160;&#20316;&#20043;&#38388;&#30340;&#36129;&#29486;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;GridWorld&#12289;FrozenLake&#21644;Taxi&#31561;&#22810;&#20010;RL&#39046;&#22495;&#20869;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;CSV&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#22797;&#26434;RL&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#24230;&#65292;&#36824;&#37327;&#21270;&#20102;&#19981;&#21516;&#20915;&#31574;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02529v1 Announce Type: new  Abstract: This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the ``Counterfactual Difference Characteristic Value" and the ``Average Counterfactual Difference Characteristic Value." These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;PredicTaps&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#21333;&#20987;&#25110;&#21452;&#20987;&#30340;&#31867;&#22411;&#65292;&#23558;&#35302;&#25720;&#35774;&#22791;&#21333;&#20987;&#26816;&#27979;&#26102;&#30340;&#24310;&#36831;&#20174;150-500&#27627;&#31186;&#38477;&#20302;&#33267;12&#27627;&#31186;&#65288;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#21644;17.6&#27627;&#31186;&#65288;&#26234;&#33021;&#25163;&#26426;&#65289;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2408.02525</link><description>&lt;p&gt;
Single-tap Latency Reduction with Single- or Double- tap Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;PredicTaps&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#21333;&#20987;&#25110;&#21452;&#20987;&#30340;&#31867;&#22411;&#65292;&#23558;&#35302;&#25720;&#35774;&#22791;&#21333;&#20987;&#26816;&#27979;&#26102;&#30340;&#24310;&#36831;&#20174;150-500&#27627;&#31186;&#38477;&#20302;&#33267;12&#27627;&#31186;&#65288;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#21644;17.6&#27627;&#31186;&#65288;&#26234;&#33021;&#25163;&#26426;&#65289;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02525v1 Announce Type: cross  Abstract: Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops (touchpad), and single and double taps are the most basic and common operations on them. The detection of single or double taps causes the single-tap latency problem, which creates a bottleneck in terms of the sensitivity of touch inputs. To reduce the single-tap latency, we propose a novel machine-learning-based tap prediction method called PredicTaps. Our method predicts whether a detected tap is a single tap or the first contact of a double tap without having to wait for the hundreds of milliseconds conventionally required. We present three evaluations and one user evaluation that demonstrate its broad applicability and usability for various tap situations on two form factors (touchpad and smartphone). The results showed PredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops and to 17.6 ms on smartphones without reducing usability.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26102;&#26159;&#21542;&#25552;&#20379;&#27491;&#30830;&#35768;&#21487;&#35777;&#20449;&#24687;&#30340;&#26631;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#21463;&#29256;&#26435;&#20445;&#25252;&#20195;&#30721;&#26102;&#30340;&#35768;&#21487;&#35777;&#21512;&#35268;&#33021;&#21147;&#65292;&#24182;&#39044;&#38450;&#19982;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#20135;&#26435;&#32416;&#32439;&#12290;</title><link>https://arxiv.org/abs/2408.02487</link><description>&lt;p&gt;
A First Look at License Compliance Capability of LLMs in Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26102;&#26159;&#21542;&#25552;&#20379;&#27491;&#30830;&#35768;&#21487;&#35777;&#20449;&#24687;&#30340;&#26631;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#21463;&#29256;&#26435;&#20445;&#25252;&#20195;&#30721;&#26102;&#30340;&#35768;&#21487;&#35777;&#21512;&#35268;&#33021;&#21147;&#65292;&#24182;&#39044;&#38450;&#19982;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#20135;&#26435;&#32416;&#32439;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02487v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. To establish this benchmark, we conduct an empirical study to identify a reasonable standard for "striking similarity" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code. Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20027;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#26159;&#31995;&#32479;&#24615;&#22320;&#22238;&#39038;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;LLMs&#36716;&#21270;&#20026;&#21487;&#20197;&#25191;&#34892;&#20915;&#31574;&#21644;&#34892;&#21160;&#30340;LLM-based Agents&#30340;&#21487;&#33021;&#24615;&#19982;&#25361;&#25112;&#12290;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#24037;&#20316;&#65292;&#26412;&#25991;&#39044;&#27979;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24314;&#31435;&#32479;&#19968;&#26631;&#20934;&#21644;&#22522;&#20934;&#20197;&#35780;&#20272;LLM&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;LLM-based Agent&#26041;&#38754;&#30340;&#28508;&#22312;&#19981;&#36275;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2408.02479</link><description>&lt;p&gt;
From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02479
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20027;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#26159;&#31995;&#32479;&#24615;&#22320;&#22238;&#39038;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;LLMs&#36716;&#21270;&#20026;&#21487;&#20197;&#25191;&#34892;&#20915;&#31574;&#21644;&#34892;&#21160;&#30340;LLM-based Agents&#30340;&#21487;&#33021;&#24615;&#19982;&#25361;&#25112;&#12290;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#24037;&#20316;&#65292;&#26412;&#25991;&#39044;&#27979;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24314;&#31435;&#32479;&#19968;&#26631;&#20934;&#21644;&#22522;&#20934;&#20197;&#35780;&#20272;LLM&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;LLM-based Agent&#26041;&#38754;&#30340;&#28508;&#22312;&#19981;&#36275;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02479v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current p
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25506;&#35752;&#20102;AI&#22312;&#33258;&#21160;&#21270;&#22788;&#29702;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;CMR&#65289;&#20998;&#21106;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#25581;&#31034;&#20102;&#31181;&#26063;&#20559;&#35265;&#20135;&#29983;&#30340;&#21407;&#22240;&#65292;&#26088;&#22312;&#23547;&#25214;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02462</link><description>&lt;p&gt;
An investigation into the causes of race bias in AI-based cine CMR segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25506;&#35752;&#20102;AI&#22312;&#33258;&#21160;&#21270;&#22788;&#29702;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;CMR&#65289;&#20998;&#21106;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#25581;&#31034;&#20102;&#31181;&#26063;&#20559;&#35265;&#20135;&#29983;&#30340;&#21407;&#22240;&#65292;&#26088;&#22312;&#23547;&#25214;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02462v1 Announce Type: cross  Abstract: Artificial intelligence (AI) methods are being used increasingly for the automated segmentation of cine cardiac magnetic resonance (CMR) imaging. However, these methods have been shown to be subject to race bias, i.e. they exhibit different levels of performance for different races depending on the (im)balance of the data used to train the AI model. In this paper we investigate the source of this bias, seeking to understand its root cause(s) so that it can be effectively mitigated. We perform a series of classification and segmentation experiments on short-axis cine CMR images acquired from Black and White subjects from the UK Biobank and apply AI interpretability methods to understand the results. In the classification experiments, we found that race can be predicted with high accuracy from the images alone, but less accurately from ground truth segmentations, suggesting that the distributional shift between races, which is often the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;GAT&#65288;Graph Attention Network&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24322;&#26500;&#30693;&#35782;&#22270;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#25972;&#21512;&#20004;&#20010;&#21333;&#29420;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#39044;&#27979;&#32570;&#22833;&#23454;&#20307;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;GAT&#26041;&#27861;&#22312;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#22270;&#26102;&#36935;&#21040;&#30340;&#19981;&#24179;&#34913;&#26679;&#26412;&#25968;&#37327;&#38382;&#39064;&#20197;&#21450;&#39044;&#27979;&#19982;&#24050;&#26377;&#20851;&#31995;&#30456;&#20851;&#30340;&#23614;&#65288;&#22836;&#65289;&#23454;&#20307;&#26102;&#30340;&#20302;&#24615;&#33021;&#38382;&#39064;&#12290;

{{endoftext}}</title><link>https://arxiv.org/abs/2408.02456</link><description>&lt;p&gt;
Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;GAT&#65288;Graph Attention Network&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24322;&#26500;&#30693;&#35782;&#22270;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#25972;&#21512;&#20004;&#20010;&#21333;&#29420;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#39044;&#27979;&#32570;&#22833;&#23454;&#20307;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;GAT&#26041;&#27861;&#22312;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#22270;&#26102;&#36935;&#21040;&#30340;&#19981;&#24179;&#34913;&#26679;&#26412;&#25968;&#37327;&#38382;&#39064;&#20197;&#21450;&#39044;&#27979;&#19982;&#24050;&#26377;&#20851;&#31995;&#30456;&#20851;&#30340;&#23614;&#65288;&#22836;&#65289;&#23454;&#20307;&#26102;&#30340;&#20302;&#24615;&#33021;&#38382;&#39064;&#12290;

{{endoftext}}
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02456v1 Announce Type: cross  Abstract: Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of the KGs, they are becoming inaccuracy and incomplete. This problem can be solved by the knowledge graph completion methods, of which graph attention network (GAT)-based methods stand out since their superior performance. However, existing GAT-based knowledge graph completion methods often suffer from overfitting issues when dealing with heterogeneous knowledge graphs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;LIBRA&#65292;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#20420;&#35821;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;21&#20010;&#36866;&#24212;&#24615;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#25991;&#26412;&#29702;&#35299;&#21644;&#19981;&#21516;&#38271;&#24230;&#19978;&#19979;&#25991;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02439</link><description>&lt;p&gt;
Long Input Benchmark for Russian Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;LIBRA&#65292;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#20420;&#35821;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;21&#20010;&#36866;&#24212;&#24615;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#25991;&#26412;&#29702;&#35299;&#21644;&#19981;&#21516;&#38271;&#24230;&#19978;&#19979;&#25991;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02439v1 Announce Type: cross  Abstract: Recent advancements in Natural Language Processing (NLP) have fostered the development of Large Language Models (LLMs) that can solve an immense variety of tasks. One of the key aspects of their application is their ability to work with long text documents and to process long sequences of tokens. This has created a demand for proper evaluation of long-context understanding. To address this need for the Russian language, we propose LIBRA (Long Input Benchmark for Russian Analysis), which comprises 21 adapted datasets to study the LLM's abilities to understand long texts thoroughly. The tests are divided into four complexity groups and allow the evaluation of models across various context lengths ranging from 4k up to 128k tokens. We provide the open-source datasets, codebase, and public leaderboard for LIBRA to guide forthcoming research.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PENDRAM&#30340;&#36890;&#29992;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#21152;&#36895;&#65292;&#23545;&#20110;&#37027;&#20123;&#23545;&#24615;&#33021;&#21644;&#33021;&#25928;&#20855;&#26377;&#26497;&#39640;&#35201;&#27714;&#30340;&#39046;&#22495;&#23588;&#20026;&#36866;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.02412</link><description>&lt;p&gt;
PENDRAM: Enabling High-Performance and Energy-Efficient Processing of Deep Neural Networks through a Generalized DRAM Data Mapping Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02412
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PENDRAM&#30340;&#36890;&#29992;DRAM&#25968;&#25454;&#26144;&#23556;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#21152;&#36895;&#65292;&#23545;&#20110;&#37027;&#20123;&#23545;&#24615;&#33021;&#21644;&#33021;&#25928;&#20855;&#26377;&#26497;&#39640;&#35201;&#27714;&#30340;&#39046;&#22495;&#23588;&#20026;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02412v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs), a prominent type of Deep Neural Networks (DNNs), have emerged as a state-of-the-art solution for solving machine learning tasks. To improve the performance and energy efficiency of CNN inference, the employment of specialized hardware accelerators is prevalent. However, CNN accelerators still face performance- and energy-efficiency challenges due to high off-chip memory (DRAM) access latency and energy, which are especially crucial for latency- and energy-constrained embedded applications. Moreover, different DRAM architectures have different profiles of access latency and energy, thus making it challenging to optimize them for high performance and energy-efficient CNN accelerators. To address this, we present PENDRAM, a novel design space exploration methodology that enables high-performance and energy-efficient CNN acceleration through a generalized DRAM data mapping policy. Specifically, it expl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCGF&#30340;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992; denoising diffusion &#27169;&#22411;&#32467;&#21512;&#20102;&#22270;&#20687;&#26080;&#25439;&#24674;&#22797;&#21644;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#33021;&#22815;&#26377;&#25928;&#36866;&#37197;&#21644;&#35782;&#21035;&#26410;&#30693;&#30340;&#26497;&#31471;&#22825;&#27668;&#29616;&#35937;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;&#30340;&#24674;&#22797;&#27169;&#22359;&#26469;&#28165;&#38500;&#24433;&#21709;&#23450;&#20301;&#30340;&#22825;&#27668;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992; EVA-02 &#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02408</link><description>&lt;p&gt;
Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCGF&#30340;&#22810;&#22825;&#27668;&#36328;&#35270;&#35282;&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992; denoising diffusion &#27169;&#22411;&#32467;&#21512;&#20102;&#22270;&#20687;&#26080;&#25439;&#24674;&#22797;&#21644;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#33021;&#22815;&#26377;&#25928;&#36866;&#37197;&#21644;&#35782;&#21035;&#26410;&#30693;&#30340;&#26497;&#31471;&#22825;&#27668;&#29616;&#35937;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;&#30340;&#24674;&#22797;&#27169;&#22359;&#26469;&#28165;&#38500;&#24433;&#21709;&#23450;&#20301;&#30340;&#22825;&#27668;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992; EVA-02 &#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#22320;&#29702;&#23450;&#20301;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02408v1 Announce Type: new  Abstract: Cross-view geo-localization in GNSS-denied environments aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery. Recent research shows that learning discriminative image representations under specific weather conditions can significantly enhance performance. However, the frequent occurrence of unseen extreme weather conditions hinders progress. This paper introduces MCGF, a Multi-weather Cross-view Geo-localization Framework designed to dynamically adapt to unseen weather conditions. MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. For image restoration, MCGF incorporates a shared encoder and a lightweight restoration module to help the backbone eliminate weather-specific information. For geo-localization, MCGF uses EVA-02 as a backbone for feature extraction, with cross-entropy loss for
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#23454;&#38469;shellcodes&#30340;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#24694;&#24847;&#36719;&#20214;&#20195;&#30721;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#24341;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19981;&#21516;&#32780;&#26377;&#25152;&#24046;&#24322;&#65292;&#34920;&#26126;&#20102;&#36866;&#24403;&#19978;&#19979;&#25991;&#23545;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#22686;&#21152;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26410;&#24102;&#26469;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#19968;&#20010;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36824;&#33021;&#35782;&#21035;&#24182;&#25490;&#38500;&#19981;&#24517;&#35201;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#26356;&#31934;&#30830;&#22320;&#29983;&#25104;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2408.02402</link><description>&lt;p&gt;
Enhancing AI-based Generation of Software Exploits with Contextual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#23454;&#38469;shellcodes&#30340;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#24694;&#24847;&#36719;&#20214;&#20195;&#30721;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#24341;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19981;&#21516;&#32780;&#26377;&#25152;&#24046;&#24322;&#65292;&#34920;&#26126;&#20102;&#36866;&#24403;&#19978;&#19979;&#25991;&#23545;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#22686;&#21152;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26410;&#24102;&#26469;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#19968;&#20010;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36824;&#33021;&#35782;&#21035;&#24182;&#25490;&#38500;&#19981;&#24517;&#35201;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#26356;&#31934;&#30830;&#22320;&#29983;&#25104;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02402v1 Announce Type: cross  Abstract: This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extended Perfect Information Monte Carlo&#65288;EPIMC&#65289;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#23436;&#32654;&#20449;&#24687;&#33945;&#29305;&#21345;&#27931;&#65288;PIMC&#65289;&#30340;&#31574;&#30053;&#34701;&#21512;&#32531;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#36831;&#23436;&#32654;&#20449;&#24687;&#35299;&#20915;&#26041;&#26696;&#20197;&#20943;&#23569;&#31574;&#30053;&#34701;&#21512;&#30340;&#38382;&#39064;&#12290;EPIMC&#31639;&#27861;&#36890;&#36807;&#25512;&#36831;&#20351;&#29992;&#23436;&#32654;&#20449;&#24687;&#35780;&#20272;&#22120;&#26469;&#20248;&#21270;PIMC&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#19982;&#27492;&#30456;&#20851;&#30340;&#26032;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#20351;&#24471;&#22312;&#35832;&#22914;&#26725;&#29260;&#21644;&#26045;&#22612;&#29305;&#31561;&#19981;&#23436;&#32654;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#34892;&#21160;&#20272;&#35745;&#26356;&#26377;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.02380</link><description>&lt;p&gt;
Perfect Information Monte Carlo with Postponing Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extended Perfect Information Monte Carlo&#65288;EPIMC&#65289;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#23436;&#32654;&#20449;&#24687;&#33945;&#29305;&#21345;&#27931;&#65288;PIMC&#65289;&#30340;&#31574;&#30053;&#34701;&#21512;&#32531;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#36831;&#23436;&#32654;&#20449;&#24687;&#35299;&#20915;&#26041;&#26696;&#20197;&#20943;&#23569;&#31574;&#30053;&#34701;&#21512;&#30340;&#38382;&#39064;&#12290;EPIMC&#31639;&#27861;&#36890;&#36807;&#25512;&#36831;&#20351;&#29992;&#23436;&#32654;&#20449;&#24687;&#35780;&#20272;&#22120;&#26469;&#20248;&#21270;PIMC&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#19982;&#27492;&#30456;&#20851;&#30340;&#26032;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#20351;&#24471;&#22312;&#35832;&#22914;&#26725;&#29260;&#21644;&#26045;&#22612;&#29305;&#31561;&#19981;&#23436;&#32654;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#34892;&#21160;&#20272;&#35745;&#26356;&#26377;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02380v1 Announce Type: new  Abstract: Imperfect information games, such as Bridge and Skat, present challenges due to state-space explosion and hidden information, posing formidable obstacles for search algorithms. Determinization-based algorithms offer a resolution by sampling hidden information and solving the game in a perfect information setting, facilitating rapid and effective action estimation. However, transitioning to perfect information introduces challenges, notably one called strategy fusion.This research introduces `Extended Perfect Information Monte Carlo' (EPIMC), an online algorithm inspired by the state-of-the-art determinization-based approach Perfect Information Monte Carlo (PIMC). EPIMC enhances the capabilities of PIMC by postponing the perfect information resolution, reducing alleviating issues related to strategy fusion. However, the decision to postpone the leaf evaluator introduces novel considerations, such as the interplay between prior levels of r
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35752;&#35770;&#20102;&#22312;&#27431;&#30431;AI&#27861;&#26696;&#31561;&#27861;&#35268;&#21363;&#23558;&#20986;&#21488;&#30340;&#32972;&#26223;&#19979;&#65292;&#24320;&#21457;&#21644;&#35748;&#35777;&#23433;&#20840;AI&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#36890;&#36807;15&#20301;&#26469;&#33258;XAI&#21644;&#35748;&#35777;&#39046;&#22495;&#30340;&#19987;&#23478;&#30340;&#23450;&#24615;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#26041;&#27861;&#22312;&#23433;&#20840;AI&#24320;&#21457;&#21644;&#35748;&#35777;&#20013;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;&#21644;&#22833;&#36133;&#65292;&#20294;&#32771;&#34385;&#21040;&#35748;&#35777;&#38656;&#35201;&#20851;&#20110;&#25216;&#26415;&#31995;&#32479;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#20449;&#24687;&#65292;&#20854;&#24433;&#21709;&#21487;&#33021;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2408.02379</link><description>&lt;p&gt;
The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35752;&#35770;&#20102;&#22312;&#27431;&#30431;AI&#27861;&#26696;&#31561;&#27861;&#35268;&#21363;&#23558;&#20986;&#21488;&#30340;&#32972;&#26223;&#19979;&#65292;&#24320;&#21457;&#21644;&#35748;&#35777;&#23433;&#20840;AI&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#36890;&#36807;15&#20301;&#26469;&#33258;XAI&#21644;&#35748;&#35777;&#39046;&#22495;&#30340;&#19987;&#23478;&#30340;&#23450;&#24615;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#26041;&#27861;&#22312;&#23433;&#20840;AI&#24320;&#21457;&#21644;&#35748;&#35777;&#20013;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;&#21644;&#22833;&#36133;&#65292;&#20294;&#32771;&#34385;&#21040;&#35748;&#35777;&#38656;&#35201;&#20851;&#20110;&#25216;&#26415;&#31995;&#32479;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#20449;&#24687;&#65292;&#20854;&#24433;&#21709;&#21487;&#33021;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02379v1 Announce Type: cross  Abstract: Developing and certifying safe - or so-called trustworthy - AI has become an increasingly salient issue, especially in light of upcoming regulation such as the EU AI Act. In this context, the black-box nature of machine learning models limits the use of conventional avenues of approach towards certifying complex technical systems. As a potential solution, methods to give insights into this black-box - devised in the field of eXplainable AI (XAI) - could be used. In this study, the potential and shortcomings of such methods for the purpose of safe AI development and certification are discussed in 15 qualitative interviews with experts out of the areas of (X)AI and certification. We find that XAI methods can be a helpful asset for safe AI development, as they can show biases and failures of ML-models, but since certification relies on comprehensive and correct information about technical systems, their impact is expected to be limited.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DCC Sidekick&#30340;Web&#29256;&#23545;&#35805;&#24335;AI&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#23637;&#31034;&#12289;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#28040;&#24687;&#20197;&#21450;&#22534;&#26632;&#24103;&#35835;&#25968;&#65292;&#25552;&#39640;&#20102;LLM&#25903;&#25345;&#30340;C/C++&#32534;&#35793;&#22120;&#29983;&#25104;&#32534;&#31243;&#38169;&#35823;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#26469;&#33258;&#19968;&#20010;&#22823;&#22411;&#30340;&#28595;&#22823;&#21033;&#20122;&#21021;&#32423;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#30340;&#20351;&#29992;&#25968;&#25454;&#65292;&#21457;&#29616;DCC Sidekick&#24037;&#20855;&#22312;&#19971;&#21608;&#20869;&#35760;&#24405;&#20102;11,222&#20010;&#20250;&#35805;&#65292;&#28041;&#21450;17,982&#20010;&#38169;&#35823;&#35299;&#37322;&#12290;&#25968;&#25454;&#34920;&#26126;&#65292;&#36229;&#36807;50%&#30340;&#20114;&#21160;&#21457;&#29983;&#22312;&#38750;&#33829;&#19994;&#26102;&#38388;&#65292;&#20984;&#26174;&#20102;&#35813;&#24037;&#20855;&#20316;&#20026;&#38543;&#26102;&#21487;&#29992;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#36741;&#21161;&#30340;&#35843;&#35797;&#24037;&#20855;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#22823;&#35268;&#27169;&#21021;&#32423;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#23454;&#26045;&#27934;&#23519;&#21644;&#24314;&#35758;&#65292;&#20026;&#23547;&#27714;&#25972;&#21512;AI&#24037;&#20855;&#30340;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#25945;&#23398;&#26041;&#38754;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2408.02378</link><description>&lt;p&gt;
Scaling CS1 Support with Compiler-Integrated Conversational AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DCC Sidekick&#30340;Web&#29256;&#23545;&#35805;&#24335;AI&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#23637;&#31034;&#12289;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#28040;&#24687;&#20197;&#21450;&#22534;&#26632;&#24103;&#35835;&#25968;&#65292;&#25552;&#39640;&#20102;LLM&#25903;&#25345;&#30340;C/C++&#32534;&#35793;&#22120;&#29983;&#25104;&#32534;&#31243;&#38169;&#35823;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#26469;&#33258;&#19968;&#20010;&#22823;&#22411;&#30340;&#28595;&#22823;&#21033;&#20122;&#21021;&#32423;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#30340;&#20351;&#29992;&#25968;&#25454;&#65292;&#21457;&#29616;DCC Sidekick&#24037;&#20855;&#22312;&#19971;&#21608;&#20869;&#35760;&#24405;&#20102;11,222&#20010;&#20250;&#35805;&#65292;&#28041;&#21450;17,982&#20010;&#38169;&#35823;&#35299;&#37322;&#12290;&#25968;&#25454;&#34920;&#26126;&#65292;&#36229;&#36807;50%&#30340;&#20114;&#21160;&#21457;&#29983;&#22312;&#38750;&#33829;&#19994;&#26102;&#38388;&#65292;&#20984;&#26174;&#20102;&#35813;&#24037;&#20855;&#20316;&#20026;&#38543;&#26102;&#21487;&#29992;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#36741;&#21161;&#30340;&#35843;&#35797;&#24037;&#20855;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#22823;&#35268;&#27169;&#21021;&#32423;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#23454;&#26045;&#27934;&#23519;&#21644;&#24314;&#35758;&#65292;&#20026;&#23547;&#27714;&#25972;&#21512;AI&#24037;&#20855;&#30340;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#25945;&#23398;&#26041;&#38754;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02378v1 Announce Type: cross  Abstract: This paper introduces DCC Sidekick, a web-based conversational AI tool that enhances an existing LLM-powered C/C++ compiler by generating educational programming error explanations. The tool seamlessly combines code display, compile- and run-time error messages, and stack frame read-outs alongside an AI interface, leveraging compiler error context for improved explanations. We analyse usage data from a large Australian CS1 course, where 959 students engaged in 11,222 DCC Sidekick sessions, resulting in 17,982 error explanations over seven weeks. Notably, over 50% of interactions occurred outside business hours, underscoring the tool's value as an always-available resource. Our findings reveal strong adoption of AI-assisted debugging tools, demonstrating their scalability in supporting extensive CS1 courses. We provide implementation insights and recommendations for educators seeking to incorporate AI tools with appropriate pedagogical 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25216;&#26415;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#20855;&#20307;&#39046;&#22495;&#20013;&#24212;&#29992;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#20851;&#31995;&#30340;&#39640;&#25928;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2408.02377</link><description>&lt;p&gt;
A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25216;&#26415;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#20855;&#20307;&#39046;&#22495;&#20013;&#24212;&#29992;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#20851;&#31995;&#30340;&#39640;&#25928;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02377v1 Announce Type: cross  Abstract: Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text. While these relations are fully applicable across scientific areas, existing models are trained on few domain-specific datasets such as SciERC and do not perform well on new target domains. In this paper, we experiment with leveraging in-context learning capabilities of Large Language Models to perform schema-constrained data annotation, collecting in-domain training instances for a Transformer-based relation extraction model deployed on titles and abstracts of research papers in the Architecture, Construction, Engineering and Operations (AECO) domain. By assessing the performance gain with respect to a baseline Deep Learning architecture trained on off-domain data, w
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;"&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;"&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#25935;&#24863;&#30340;&#21161;&#25163;&#65292;&#20197;&#25351;&#23548;&#20854;&#20449;&#24687;&#20849;&#20139;&#34892;&#20026;&#36981;&#24490;&#38544;&#31169;&#20445;&#25252;&#21407;&#21017;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02373</link><description>&lt;p&gt;
Operationalizing Contextual Integrity in Privacy-Conscious Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;"&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;"&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#25935;&#24863;&#30340;&#21161;&#25163;&#65292;&#20197;&#25351;&#23548;&#20854;&#20449;&#24687;&#20849;&#20139;&#34892;&#20026;&#36981;&#24490;&#38544;&#31169;&#20445;&#25252;&#21407;&#21017;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02373v1 Announce Type: new  Abstract: Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks on behalf of users. While the helpfulness of such assistants can increase dramatically with access to user information including emails and documents, this raises privacy concerns about assistants sharing inappropriate information with third parties without user supervision. To steer information-sharing assistants to behave in accordance with privacy expectations, we propose to operationalize $\textit{contextual integrity}$ (CI), a framework that equates privacy with the appropriate flow of information in a given context. In particular, we design and evaluate a number of strategies to steer assistants' information-sharing actions to be CI compliant. Our evaluation is based on a novel form filling benchmark composed of synthetic data and human annotations, and it reveals that prompting frontier LLMs to perform CI-based reasoning yields stron
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#30340;&#38142;&#24335;&#24605;&#32771;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#23545;&#35805;&#26412;&#20307;&#20851;&#31995;&#25552;&#21462;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26412;&#20307;&#26500;&#24314;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#26368;&#36817;&#21457;&#23637;&#29992;&#20110;&#25512;&#29702;&#38382;&#39064;&#30340;&#38142;&#24335;&#24605;&#32771;&#35299;&#30721;&#26426;&#21046;&#25193;&#23637;&#21040;&#29983;&#25104;&#20851;&#31995;&#25552;&#21462;&#19978;&#65292;&#25991;&#31456;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#27880;&#26412;&#20307;&#25968;&#25454;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20851;&#31995;&#25552;&#21462;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#20154;&#24037;&#26412;&#20307;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2408.02361</link><description>&lt;p&gt;
Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#30340;&#38142;&#24335;&#24605;&#32771;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#23545;&#35805;&#26412;&#20307;&#20851;&#31995;&#25552;&#21462;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26412;&#20307;&#26500;&#24314;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#26368;&#36817;&#21457;&#23637;&#29992;&#20110;&#25512;&#29702;&#38382;&#39064;&#30340;&#38142;&#24335;&#24605;&#32771;&#35299;&#30721;&#26426;&#21046;&#25193;&#23637;&#21040;&#29983;&#25104;&#20851;&#31995;&#25552;&#21462;&#19978;&#65292;&#25991;&#31456;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#27880;&#26412;&#20307;&#25968;&#25454;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20851;&#31995;&#25552;&#21462;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#20154;&#24037;&#26412;&#20307;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02361v1 Announce Type: cross  Abstract: State-of-the-art task-oriented dialogue systems typically rely on task-specific ontologies for fulfilling user queries. The majority of task-oriented dialogue data, such as customer service recordings, comes without ontology and annotation. Such ontologies are normally built manually, limiting the application of specialised systems. Dialogue ontology construction is an approach for automating that process and typically consists of two steps: term extraction and relation extraction. In this work, we focus on relation extraction in a transfer learning set-up. To improve the generalisation, we propose an extension to the decoding mechanism of large language models. We adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning problems, to generative relation extraction. Here, we generate multiple branches in the decoding space and select the relations based on a confidence threshold. By constraining the decoding to ontology t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#8220;&#19968;&#33268;&#25512;&#29702;&#24726;&#35770;&#8221;(CRP)&#65292;&#25351;&#20986;&#22312;&#22788;&#29702;&#20197;&#19981;&#21516;&#26041;&#24335;&#25551;&#36848;&#20294;&#31561;&#25928;&#30340;&#20219;&#21153;&#26102;&#65292;&#22914;&#8220;&#21578;&#35785;&#25105;&#29616;&#22312;&#20960;&#28857;&#38047;&#65311;&#8221;&#21644;&#8220;&#29616;&#22312;&#20960;&#28857;&#20102;&#65311;&#8221;&#65292;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#35201;&#32032;&#26159;&#33021;&#22815;&#19968;&#33268;&#22320;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;CRP&#34920;&#26126;&#65292;&#22312;&#25512;&#29702;&#19978;&#30340;&#19968;&#33268;&#24615;&#24847;&#21619;&#30528;&#26080;&#27861;&#36991;&#20813;&#29359;&#38169;&#65292;&#22240;&#20026;&#19968;&#20010;&#21147;&#27714;&#27169;&#20223;&#20154;&#31867;&#26234;&#21147;&#30340;AI&#65292;&#21363;&#20351;&#22312;&#23581;&#35797;&#32473;&#20986;&#31572;&#26696;&#26102;&#20063;&#24456;&#21487;&#33021;&#22240;&#25512;&#29702;&#19968;&#33268;&#32780;&#20135;&#29983;&#38169;&#35823;&#65288; hallucinations&#65292;&#21363;&#30475;&#20284;&#21512;&#29702;&#20294;&#23454;&#38469;&#19978;&#38169;&#35823;&#30340;&#31572;&#26696;&#65289;&#12290;&#35813;&#24726;&#35770;&#36824;&#25351;&#20986;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#38750;&#19968;&#33268;&#25512;&#29702;&#30340;AI&#65288;&#20854;&#27700;&#24179;&#20302;&#20110;&#20154;&#31867;&#65289;&#20063;&#21487;&#33021;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26816;&#27979;&#36825;&#20123;&#38169;&#35823;&#31572;&#26696;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02357</link><description>&lt;p&gt;
On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#8220;&#19968;&#33268;&#25512;&#29702;&#24726;&#35770;&#8221;(CRP)&#65292;&#25351;&#20986;&#22312;&#22788;&#29702;&#20197;&#19981;&#21516;&#26041;&#24335;&#25551;&#36848;&#20294;&#31561;&#25928;&#30340;&#20219;&#21153;&#26102;&#65292;&#22914;&#8220;&#21578;&#35785;&#25105;&#29616;&#22312;&#20960;&#28857;&#38047;&#65311;&#8221;&#21644;&#8220;&#29616;&#22312;&#20960;&#28857;&#20102;&#65311;&#8221;&#65292;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#35201;&#32032;&#26159;&#33021;&#22815;&#19968;&#33268;&#22320;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;CRP&#34920;&#26126;&#65292;&#22312;&#25512;&#29702;&#19978;&#30340;&#19968;&#33268;&#24615;&#24847;&#21619;&#30528;&#26080;&#27861;&#36991;&#20813;&#29359;&#38169;&#65292;&#22240;&#20026;&#19968;&#20010;&#21147;&#27714;&#27169;&#20223;&#20154;&#31867;&#26234;&#21147;&#30340;AI&#65292;&#21363;&#20351;&#22312;&#23581;&#35797;&#32473;&#20986;&#31572;&#26696;&#26102;&#20063;&#24456;&#21487;&#33021;&#22240;&#25512;&#29702;&#19968;&#33268;&#32780;&#20135;&#29983;&#38169;&#35823;&#65288; hallucinations&#65292;&#21363;&#30475;&#20284;&#21512;&#29702;&#20294;&#23454;&#38469;&#19978;&#38169;&#35823;&#30340;&#31572;&#26696;&#65289;&#12290;&#35813;&#24726;&#35770;&#36824;&#25351;&#20986;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#38750;&#19968;&#33268;&#25512;&#29702;&#30340;AI&#65288;&#20854;&#27700;&#24179;&#20302;&#20110;&#20154;&#31867;&#65289;&#20063;&#21487;&#33021;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26816;&#27979;&#36825;&#20123;&#38169;&#35823;&#31572;&#26696;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02357v1 Announce Type: new  Abstract: We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the ability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP asserts that consistent reasoning implies fallibility -- in particular, human-like intelligence in AI necessarily comes with human-like fallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to mimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The paradox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will be correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33181;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#30340;&#20027;&#21160;&#24863;&#30693;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#23454;&#29616;&#25104;&#26412;&#33410;&#32422;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;</title><link>https://arxiv.org/abs/2408.02349</link><description>&lt;p&gt;
Active Sensing of Knee Osteoarthritis Progression with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33181;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#30340;&#20027;&#21160;&#24863;&#30693;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#23454;&#29616;&#25104;&#26412;&#33410;&#32422;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02349v1 Announce Type: cross  Abstract: Osteoarthritis (OA) is the most common musculoskeletal disease, which has no cure. Knee OA (KOA) is one of the highest causes of disability worldwide, and it costs billions of United States dollars to the global community. Prediction of KOA progression has been of high interest to the community for years, as it can advance treatment development through more efficient clinical trials and improve patient outcomes through more efficient healthcare utilization. Existing approaches for predicting KOA, however, are predominantly static, i.e. consider data from a single time point to predict progression many years into the future, and knee level, i.e. consider progression in a single joint only. Due to these and related reasons, these methods fail to deliver the level of predictive performance, which is sufficient to result in cost savings and better patient outcomes. Collecting extensive data from all patients on a regular basis could addres
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#26032;&#22411;&#21322;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#30693;&#35782;&#22522;&#38382;&#31572;&#65288;KBQA&#65289;&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#28436;&#31034;&#20102;&#20854;&#22312;&#27874;&#20848;&#35821;&#65288;Polish&#65289;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#22312;&#26497;&#31471;&#31616;&#21270;&#20154;&#24037;&#20316;&#19994;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.02337</link><description>&lt;p&gt;
Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#26032;&#22411;&#21322;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#30693;&#35782;&#22522;&#38382;&#31572;&#65288;KBQA&#65289;&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#28436;&#31034;&#20102;&#20854;&#22312;&#27874;&#20848;&#35821;&#65288;Polish&#65289;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#22312;&#26497;&#31471;&#31616;&#21270;&#20154;&#24037;&#20316;&#19994;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02337v1 Announce Type: new  Abstract: Advancements in AI and natural language processing have revolutionized machine-human language interactions, with question answering (QA) systems playing a pivotal role. The knowledge base question answering (KBQA) task, utilizing structured knowledge graphs (KG), allows for handling extensive knowledge-intensive questions. However, a significant gap exists in KBQA datasets, especially for low-resource languages. Many existing construction pipelines for these datasets are outdated and inefficient in human labor, and modern assisting tools like Large Language Models (LLM) are not utilized to reduce the workload. To address this, we have designed and implemented a modern, semi-automated approach for creating datasets, encompassing tasks such as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR), tailored explicitly for low-resource environments. We executed this pipeline and introduced the PUGG dataset, the first Poli
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#35774;&#32622;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#20285;&#39532;&#36798;&#35823;&#24046;&#27169;&#22411;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#26356;&#39640;&#30340;&#38454;&#30697;&#65292;&#29305;&#21035;&#26159;&#23792;&#24230;&#65292;&#26469;&#25913;&#36827;&#35823;&#24046;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25968;&#25454;&#39537;&#21160;&#22122;&#22768;&#65288;&#21363;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#20272;&#35745;&#21644;&#32531;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02295</link><description>&lt;p&gt;
Generalized Gaussian Temporal Difference Error For Uncertainty-aware Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#35774;&#32622;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#20285;&#39532;&#36798;&#35823;&#24046;&#27169;&#22411;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#26356;&#39640;&#30340;&#38454;&#30697;&#65292;&#29305;&#21035;&#26159;&#23792;&#24230;&#65292;&#26469;&#25913;&#36827;&#35823;&#24046;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25968;&#25454;&#39537;&#21160;&#22122;&#22768;&#65288;&#21363;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#20272;&#35745;&#21644;&#32531;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02295v1 Announce Type: cross  Abstract: Conventional uncertainty-aware temporal difference (TD) learning methods often rely on simplistic assumptions, typically including a zero-mean Gaussian distribution for TD errors. Such oversimplification can lead to inaccurate error representations and compromised uncertainty estimation. In this paper, we introduce a novel framework for generalized Gaussian error modeling in deep reinforcement learning, applicable to both discrete and continuous control settings. Our framework enhances the flexibility of error distribution modeling by incorporating higher-order moments, particularly kurtosis, thereby improving the estimation and mitigation of data-dependent noise, i.e., aleatoric uncertainty. We examine the influence of the shape parameter of the generalized Gaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form expression that demonstrates an inverse relationship between uncertainty and the shape parameter. Add
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;Contextual Learning&#8221;&#29616;&#35937;&#19982;&#29289;&#29702;&#23398;&#30340;&#8220;Spin Glass&#8221;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#30456;&#20114;&#20316;&#29992;&#19982;&#27169;&#22411;&#32467;&#26500;&#30340;&#37327;&#23376;&#30456;&#21464;&#65292;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21482;&#38656;&#25552;&#20379;&#25552;&#31034;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2408.02288</link><description>&lt;p&gt;
Spin glass model of in-context learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02288
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;Contextual Learning&#8221;&#29616;&#35937;&#19982;&#29289;&#29702;&#23398;&#30340;&#8220;Spin Glass&#8221;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#30456;&#20114;&#20316;&#29992;&#19982;&#27169;&#22411;&#32467;&#26500;&#30340;&#37327;&#23376;&#30456;&#21464;&#65292;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21482;&#38656;&#25552;&#20379;&#25552;&#31034;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02288v1 Announce Type: cross  Abstract: Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention, and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and most importantly why an unseen function can be predicted by providing only a prompt yet without training. Our theory reveals that for single instance learning, increasing the task diversity leads to the emergence of the in-context learning, by allowing the Boltzmann distribution to co
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214; aware &#30340;&#21518;&#32452;&#21512;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#38598;&#25104;&#23398;&#20064;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#26088;&#22312;&#22312;&#20445;&#25345;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#30828;&#20214;&#25928;&#29575;&#65292;&#36890;&#36807;&#36873;&#25321; Pareto &#21069;&#27839;&#19978;&#30340;&#38598;&#25104;&#65292;&#20197;&#26368;&#22823;&#21270;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02280</link><description>&lt;p&gt;
Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and Cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214; aware &#30340;&#21518;&#32452;&#21512;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#38598;&#25104;&#23398;&#20064;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#26088;&#22312;&#22312;&#20445;&#25345;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#30828;&#20214;&#25928;&#29575;&#65292;&#36890;&#36807;&#36873;&#25321; Pareto &#21069;&#27839;&#19978;&#30340;&#38598;&#25104;&#65292;&#20197;&#26368;&#22823;&#21270;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02280v1 Announce Type: cross  Abstract: Automated Machine Learning (AutoML) significantly simplifies the deployment of machine learning models by automating tasks from data preprocessing to model selection to ensembling. AutoML systems for tabular data often employ post hoc ensembling, where multiple models are combined to improve predictive accuracy. This typically results in longer inference times, a major limitation in practical deployments. Addressing this, we introduce a hardware-aware ensemble selection approach that integrates inference time into post hoc ensembling. By leveraging an existing framework for ensemble selection with quality diversity optimization, our method evaluates ensemble candidates for their predictive accuracy and hardware efficiency. This dual focus allows for a balanced consideration of accuracy and operational efficiency. Thus, our approach enables practitioners to choose from a Pareto front of accurate and efficient ensembles. Our evaluation u
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRFormer&#30340;&#22810;&#23610;&#24230; Transformer&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#21160;&#24577;&#30340; receptive fields&#26469;&#22686;&#24378;&#23545;&#38271;&#26102;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02279</link><description>&lt;p&gt;
DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRFormer&#30340;&#22810;&#23610;&#24230; Transformer&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#21160;&#24577;&#30340; receptive fields&#26469;&#22686;&#24378;&#23545;&#38271;&#26102;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02279v1 Announce Type: cross  Abstract: Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution feat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20960;&#20309;&#20195;&#25968;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;3D&#22330;&#26223;&#30340;&#21487;&#25511;&#32534;&#36753;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#20219;&#21153;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20309;&#20195;&#25968;&#20316;&#20026;&#27491;&#24335;&#30340;&#35821;&#35328;&#26469;&#31934;&#30830;&#24314;&#27169;&#31354;&#38388;&#21464;&#25442;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;Shot&#23398;&#20064;&#33021;&#21147;&#26469;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;CGA&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#25805;&#20316;&#32773;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;3D&#22330;&#26223;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#65292;&#26080;&#38656;&#20381;&#36182;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#19987;&#38376;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#39033;&#25216;&#26415;&#20026;3D&#22330;&#26223;&#30340;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#20307;&#39564;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2408.02275</link><description>&lt;p&gt;
Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20960;&#20309;&#20195;&#25968;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;3D&#22330;&#26223;&#30340;&#21487;&#25511;&#32534;&#36753;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#20219;&#21153;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20309;&#20195;&#25968;&#20316;&#20026;&#27491;&#24335;&#30340;&#35821;&#35328;&#26469;&#31934;&#30830;&#24314;&#27169;&#31354;&#38388;&#21464;&#25442;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;Shot&#23398;&#20064;&#33021;&#21147;&#26469;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;CGA&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#25805;&#20316;&#32773;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;3D&#22330;&#26223;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#65292;&#26080;&#38656;&#20381;&#36182;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#19987;&#38376;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#39033;&#25216;&#26415;&#20026;3D&#22330;&#26223;&#30340;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#20307;&#39564;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02275v1 Announce Type: new  Abstract: This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, shenlong ensures compatibility with existing
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;Contrastive Learning (CL) &#26041;&#27861;&#21040;&#25277;&#35937;&#27010;&#24565;&#19978;&#30340;&#26032;&#23581;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#25968;&#30340;&#27010;&#24565;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#25104;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;CL&#22312;&#39044;&#27979;&#21644;&#20272;&#35745;&#33258;&#28982;&#25968;&#65288;&#21363;&#25968;&#37327;&#65289;&#26041;&#38754;&#65292;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;CL&#22312;&#22788;&#29702;&#25277;&#35937;&#27010;&#24565;&#19978;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02247</link><description>&lt;p&gt;
Contrastive Learning and Abstract Concepts: The Case of Natural Numbers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;Contrastive Learning (CL) &#26041;&#27861;&#21040;&#25277;&#35937;&#27010;&#24565;&#19978;&#30340;&#26032;&#23581;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#25968;&#30340;&#27010;&#24565;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#25104;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;CL&#22312;&#39044;&#27979;&#21644;&#20272;&#35745;&#33258;&#28982;&#25968;&#65288;&#21363;&#25968;&#37327;&#65289;&#26041;&#38754;&#65292;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;CL&#22312;&#22788;&#29702;&#25277;&#35937;&#27010;&#24565;&#19978;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02247v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has been successfully applied to classification and other downstream tasks related to concrete concepts, such as objects contained in the ImageNet dataset. No attempts seem to have been made so far in applying this promising scheme to more abstract entities. A prominent example of these could be the concept of (discrete) Quantity. CL can be frequently interpreted as a self-supervised scheme guided by some profound and ubiquitous conservation principle (e.g. conservation of identity in object classification tasks). In this introductory work we apply a suitable conservation principle to the semi-abstract concept of natural numbers by which discrete quantities can be estimated or predicted. We experimentally show, by means of a toy problem, that contrastive learning can be trained to count at a glance with high accuracy both at human as well as at super-human ranges.. We compare this with the results of a trained
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;vision-language&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#29992;&#20110;&#25705;&#25176;&#36710;&#12289;&#20056;&#23458;&#21644;&#22836;&#30420;&#30340;&#26816;&#27979;&#12289;&#20998;&#31867;&#21450;&#20851;&#32852;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;vision-language&#27169;&#22411;OwLv2&#26469;&#25552;&#39640;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#26816;&#27979;&#21644;&#20998;&#31867;&#25705;&#25176;&#36710;&#21644;&#22836;&#30420;&#20351;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02244</link><description>&lt;p&gt;
Evaluating Vision-Language Models for Zero-Shot Detection, Classification, and Association of Motorcycles, Passengers, and Helmets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;vision-language&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#29992;&#20110;&#25705;&#25176;&#36710;&#12289;&#20056;&#23458;&#21644;&#22836;&#30420;&#30340;&#26816;&#27979;&#12289;&#20998;&#31867;&#21450;&#20851;&#32852;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;vision-language&#27169;&#22411;OwLv2&#26469;&#25552;&#39640;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#26816;&#27979;&#21644;&#20998;&#31867;&#25705;&#25176;&#36710;&#21644;&#22836;&#30420;&#20351;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02244v1 Announce Type: new  Abstract: Motorcycle accidents pose significant risks, particularly when riders and passengers do not wear helmets. This study evaluates the efficacy of an advanced vision-language foundation model, OWLv2, in detecting and classifying various helmet-wearing statuses of motorcycle occupants using video data. We extend the dataset provided by the CVPR AI City Challenge and employ a cascaded model approach for detection and classification tasks, integrating OWLv2 and CNN models. The results highlight the potential of zero-shot learning to address challenges arising from incomplete and biased training datasets, demonstrating the usage of such models in detecting motorcycles, helmet usage, and occupant positions under varied conditions. We have achieved an average precision of 0.5324 for helmet detection and provided precision-recall curves detailing the detection and classification performance. Despite limitations such as low-resolution data and poor 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#31181;&#26469;&#28304;&#30340;&#24322;&#26500;&#30693;&#35782;&#27880;&#20837;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27861;&#24459;&#25351;&#25511;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#26469;&#33258;&#27861;&#24459;&#30693;&#35782;&#24211;&#12289;&#23545;&#35805;&#22411;AI&#20197;&#21450;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#30340;&#22810;&#28304;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#26816;&#32034;&#65292;&#22686;&#24378;&#20102;&#26696;&#20363;&#25551;&#36848;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.02233</link><description>&lt;p&gt;
A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method for Legal Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#31181;&#26469;&#28304;&#30340;&#24322;&#26500;&#30693;&#35782;&#27880;&#20837;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27861;&#24459;&#25351;&#25511;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#26469;&#33258;&#27861;&#24459;&#30693;&#35782;&#24211;&#12289;&#23545;&#35805;&#22411;AI&#20197;&#21450;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#30340;&#22810;&#28304;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#26816;&#32034;&#65292;&#22686;&#24378;&#20102;&#26696;&#20363;&#25551;&#36848;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02233v1 Announce Type: cross  Abstract: Legal charge prediction, an essential task in legal AI, seeks to assign accurate charge labels to case descriptions, attracting significant recent interest. Existing methods primarily employ diverse neural network structures for modeling case descriptions directly, failing to effectively leverage multi-source external knowledge. We propose a prompt learning framework-based method that simultaneously leverages multi-source heterogeneous external knowledge from a legal knowledge base, a conversational LLM, and related legal articles. Specifically, we match knowledge snippets in case descriptions via the legal knowledge base and encapsulate them into the input through a hard prompt template. Additionally, we retrieve legal articles related to a given case description through contrastive learning, and then obtain factual elements within the case description through a conversational LLM. We fuse the embedding vectors of soft prompt tokens w
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;SpecRover&#65292;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#24847;&#22270;&#25552;&#21462;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;LLM&#21644;&#31243;&#24207;&#20998;&#26512;&#33021;&#21147;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#20462;&#22797;&#21644;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2408.02232</link><description>&lt;p&gt;
SpecRover: Code Intent Extraction via LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;SpecRover&#65292;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#24847;&#22270;&#25552;&#21462;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;LLM&#21644;&#31243;&#24207;&#20998;&#26512;&#33021;&#21147;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#20462;&#22797;&#21644;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02232v1 Announce Type: cross  Abstract: Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our app
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#26159;&#20851;&#20110;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#25968;&#25454;&#24211;&#35843;&#20248;&#12290;&#30740;&#31350;&#22242;&#38431;&#36890;&#36807;&#20180;&#32454;&#35774;&#35745;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#21644;Claude-3&#65289;&#29992;&#20316;&#20855;&#22791;&#32463;&#39564;&#30340;DBA&#36827;&#34892;&#35843;&#20248;&#20219;&#21153;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;LLM&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35843;&#20248;&#31995;&#32479;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#65306;&#21442;&#25968;&#20462;&#21098;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#21442;&#25968;&#24314;&#35758;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#27604;&#20102;LLM&#39537;&#21160;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#24211;&#31995;&#32479;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02213</link><description>&lt;p&gt;
Is Large Language Model Good at Database Knob Tuning? A Comprehensive Experimental Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#26159;&#20851;&#20110;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#25968;&#25454;&#24211;&#35843;&#20248;&#12290;&#30740;&#31350;&#22242;&#38431;&#36890;&#36807;&#20180;&#32454;&#35774;&#35745;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#21644;Claude-3&#65289;&#29992;&#20316;&#20855;&#22791;&#32463;&#39564;&#30340;DBA&#36827;&#34892;&#35843;&#20248;&#20219;&#21153;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;LLM&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35843;&#20248;&#31995;&#32479;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#65306;&#21442;&#25968;&#20462;&#21098;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;&#21442;&#25968;&#24314;&#35758;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#27604;&#20102;LLM&#39537;&#21160;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#24211;&#31995;&#32479;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02213v1 Announce Type: cross  Abstract: Knob tuning plays a crucial role in optimizing databases by adjusting knobs to enhance database performance. However, traditional tuning methods often follow a Try-Collect-Adjust approach, proving inefficient and database-specific. Moreover, these methods are often opaque, making it challenging for DBAs to grasp the underlying decision-making process.   The emergence of large language models (LLMs) like GPT-4 and Claude-3 has excelled in complex natural language tasks, yet their potential in database knob tuning remains largely unexplored. This study harnesses LLMs as experienced DBAs for knob-tuning tasks with carefully designed prompts. We identify three key subtasks in the tuning system: knob pruning, model initialization, and knob recommendation, proposing LLM-driven solutions to replace conventional methods for each subtask.   We conduct extensive experiments to compare LLM-driven approaches against traditional methods across the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARCO&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35760;&#24518;&#27169;&#22359;&#25552;&#39640;&#20102;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26102;&#30340;&#29366;&#24577;&#25506;&#32034;&#25928;&#29575;&#12290;MARCO&#33021;&#22815;&#22312;&#26500;&#36896;&#22411;&#21644;&#25913;&#36827;&#22411;NCO&#26041;&#27861;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#36890;&#36807;&#35760;&#24518;&#27169;&#22359;&#23384;&#20648;&#21644;&#26816;&#32034;&#19982;&#24403;&#21069;&#29366;&#24577;&#30456;&#20851;&#32852;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20570;&#20986;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#20915;&#31574;&#30340;&#21516;&#26102;&#36991;&#20813;&#37325;&#22797;&#25506;&#32034;&#24050;&#26377;&#30340;&#29366;&#24577;&#12290;&#36825;&#31181;&#31454;&#20105;&#24615;&#31574;&#30053;&#25913;&#21892;&#20102;&#25628;&#32034;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#20102;&#19981;&#24517;&#35201;&#30340;&#37325;&#22797;&#25506;&#32034;&#65292;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#26356;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02207</link><description>&lt;p&gt;
MARCO: A Memory-Augmented Reinforcement Framework for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARCO&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35760;&#24518;&#27169;&#22359;&#25552;&#39640;&#20102;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26102;&#30340;&#29366;&#24577;&#25506;&#32034;&#25928;&#29575;&#12290;MARCO&#33021;&#22815;&#22312;&#26500;&#36896;&#22411;&#21644;&#25913;&#36827;&#22411;NCO&#26041;&#27861;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#36890;&#36807;&#35760;&#24518;&#27169;&#22359;&#23384;&#20648;&#21644;&#26816;&#32034;&#19982;&#24403;&#21069;&#29366;&#24577;&#30456;&#20851;&#32852;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20570;&#20986;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#20915;&#31574;&#30340;&#21516;&#26102;&#36991;&#20813;&#37325;&#22797;&#25506;&#32034;&#24050;&#26377;&#30340;&#29366;&#24577;&#12290;&#36825;&#31181;&#31454;&#20105;&#24615;&#31574;&#30053;&#25913;&#21892;&#20102;&#25628;&#32034;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#20102;&#19981;&#24517;&#35201;&#30340;&#37325;&#22797;&#25506;&#32034;&#65292;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#26356;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02207v1 Announce Type: cross  Abstract: Neural Combinatorial Optimization (NCO) is an emerging domain where deep learning techniques are employed to address combinatorial optimization problems as a standalone solver. Despite their potential, existing NCO methods often suffer from inefficient search space exploration, frequently leading to local optima entrapment or redundant exploration of previously visited states. This paper introduces a versatile framework, referred to as Memory-Augmented Reinforcement for Combinatorial Optimization (MARCO), that can be used to enhance both constructive and improvement methods in NCO through an innovative memory module. MARCO stores data collected throughout the optimization trajectory and retrieves contextually relevant information at each state. This way, the search is guided by two competing criteria: making the best decision in terms of the quality of the solution and avoiding revisiting already explored solutions. This approach promo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#23545;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#20445;&#25252;&#25514;&#26045;&#30340;&#20998;&#31867;&#23398;&#65292;&#20197;&#32452;&#32455;&#21644;&#23545;&#27604;&#20445;&#25252;&#25514;&#26045;&#30340;&#29305;&#24449;&#21644;&#35774;&#35745;&#36873;&#39033;&#65292;&#26088;&#22312;&#30830;&#20445;&#20445;&#25252;&#25514;&#26045;&#30340;&#35774;&#35745;&#26082;&#23433;&#20840;&#21448;&#36127;&#36131;&#12290;</title><link>https://arxiv.org/abs/2408.02205</link><description>&lt;p&gt;
Towards AI-Safety-by-Design: A Taxonomy of Runtime Guardrails in Foundation Model based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#23545;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#20445;&#25252;&#25514;&#26045;&#30340;&#20998;&#31867;&#23398;&#65292;&#20197;&#32452;&#32455;&#21644;&#23545;&#27604;&#20445;&#25252;&#25514;&#26045;&#30340;&#29305;&#24449;&#21644;&#35774;&#35745;&#36873;&#39033;&#65292;&#26088;&#22312;&#30830;&#20445;&#20445;&#25252;&#25514;&#26045;&#30340;&#35774;&#35745;&#26082;&#23433;&#20840;&#21448;&#36127;&#36131;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02205v1 Announce Type: cross  Abstract: The rapid advancement and widespread deployment of foundation model (FM) based systems have revolutionized numerous applications across various domains. However, the fast-growing capabilities and autonomy have also raised significant concerns about responsible AI and AI safety. Recently, there have been increasing attention toward implementing guardrails to ensure the runtime behavior of FM-based systems is safe and responsible. Given the early stage of FMs and their applications (such as agents), the design of guardrails have not yet been systematically studied. It remains underexplored which software qualities should be considered when designing guardrails and how these qualities can be ensured from a software architecture perspective. Therefore, in this paper, we present a taxonomy for guardrails to classify and compare the characteristics and design options of guardrails. Our taxonomy is organized into three main categories: the mo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;SelfBC&#25216;&#26415;&#36890;&#36807;&#21160;&#24577;&#25919;&#31574;&#32422;&#26463;&#26426;&#21046;&#65292;&#32467;&#21512;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#25216;&#26415;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20445;&#23432;&#34892;&#20026;&#31574;&#30053;&#30340;&#31361;&#30772;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#38750;&#20445;&#23432;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26377;&#25928;&#38450;&#27490;&#20102;&#31574;&#30053;&#23849;&#28291;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21644;&#25913;&#36827;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02165</link><description>&lt;p&gt;
SelfBC: Self Behavior Cloning for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02165
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;SelfBC&#25216;&#26415;&#36890;&#36807;&#21160;&#24577;&#25919;&#31574;&#32422;&#26463;&#26426;&#21046;&#65292;&#32467;&#21512;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#25216;&#26415;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20445;&#23432;&#34892;&#20026;&#31574;&#30053;&#30340;&#31361;&#30772;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#38750;&#20445;&#23432;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26377;&#25928;&#38450;&#27490;&#20102;&#31574;&#30053;&#23849;&#28291;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21644;&#25913;&#36827;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02165v1 Announce Type: cross  Abstract: Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LeapRec&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#24207;&#21015;&#25512;&#33616;&#22330;&#26223;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#31867;&#21035;&#27604;&#20363;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#26469;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#29616;&#65306;&#35757;&#32451;&#38454;&#27573;&#21644;&#37325;&#25490;&#38454;&#27573;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;LeapRec&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;&#8220;Calibration-Disentangled Learning&#8221;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#25512;&#33616;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#20887;&#20313;&#24615;&#65307;&#22312;&#37325;&#25490;&#38454;&#27573;&#65292;&#21017;&#20351;&#29992;&#8220;Relevance-Prioritized Reranking&#8221;&#25216;&#26415;&#26469;&#20248;&#20808;&#32771;&#34385;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#25512;&#33616;&#32467;&#26524;&#30340;&#24179;&#34913;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#38754;&#20020;&#30340;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#26032;&#25490;&#21517;&#36807;&#31243;&#20013;&#30456;&#20851;&#24615;&#19982;&#24179;&#34913;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02156</link><description>&lt;p&gt;
Calibration-Disentangled Learning and Relevance-Prioritized Reranking for Calibrated Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LeapRec&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#24207;&#21015;&#25512;&#33616;&#22330;&#26223;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#31867;&#21035;&#27604;&#20363;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#26469;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#29616;&#65306;&#35757;&#32451;&#38454;&#27573;&#21644;&#37325;&#25490;&#38454;&#27573;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;LeapRec&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;&#8220;Calibration-Disentangled Learning&#8221;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#25512;&#33616;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#20887;&#20313;&#24615;&#65307;&#22312;&#37325;&#25490;&#38454;&#27573;&#65292;&#21017;&#20351;&#29992;&#8220;Relevance-Prioritized Reranking&#8221;&#25216;&#26415;&#26469;&#20248;&#20808;&#32771;&#34385;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#25512;&#33616;&#32467;&#26524;&#30340;&#24179;&#34913;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#38754;&#20020;&#30340;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#26032;&#25490;&#21517;&#36807;&#31243;&#20013;&#30456;&#20851;&#24615;&#19982;&#24179;&#34913;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02156v1 Announce Type: cross  Abstract: Calibrated recommendation, which aims to maintain personalized proportions of categories within recommendations, is crucial in practical scenarios since it enhances user satisfaction by reflecting diverse interests. However, achieving calibration in a sequential setting (i.e., calibrated sequential recommendation) is challenging due to the need to adapt to users' evolving preferences. Previous methods typically leverage reranking algorithms to calibrate recommendations after training a model without considering the effect of calibration and do not effectively tackle the conflict between relevance and calibration during the reranking process. In this work, we propose LeapRec (Calibration-Disentangled Learning and Relevance-Prioritized Reranking), a novel approach for the calibrated sequential recommendation that addresses these challenges. LeapRec consists of two phases, model training phase and reranking phase. In the training phase, a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;ARVO&#65288;Atlas of Reproducible Vulnerabilities for Open Source Software&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;5,000&#20010;&#20869;&#23384;&#28431;&#27934;&#30340;&#22823;&#22411;&#12289;&#33258;&#21160;&#26356;&#26032;&#30340;&#24320;&#28304;&#36719;&#20214;&#28431;&#27934;&#25968;&#25454;&#24211;&#12290;&#36825;&#20123;&#28431;&#27934;&#26469;&#33258;Google OSS-Fuzz&#21457;&#29616;&#30340;C/C++&#39033;&#30446;&#65292;&#27599;&#20010;&#28431;&#27934;&#37117;&#24102;&#26377;&#35302;&#21457;&#22120;&#36755;&#20837;&#12289;&#24320;&#21457;&#32773;&#20462;&#22797;&#26041;&#26696;&#20197;&#21450;&#21487;&#20197;&#22312;&#21407;&#29366;&#21644;&#20462;&#22797;&#29366;&#24577;&#19979;&#33258;&#21160;&#32534;&#35793;&#21644;&#36816;&#34892;&#30340;&#39033;&#30446;&#20195;&#30721;&#12290;&#36825;&#31181;&#35774;&#35745;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#36719;&#20214;&#23433;&#20840;&#30740;&#31350;&#30340;&#23454;&#29992;&#24615;&#21644;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2408.02153</link><description>&lt;p&gt;
ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;ARVO&#65288;Atlas of Reproducible Vulnerabilities for Open Source Software&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;5,000&#20010;&#20869;&#23384;&#28431;&#27934;&#30340;&#22823;&#22411;&#12289;&#33258;&#21160;&#26356;&#26032;&#30340;&#24320;&#28304;&#36719;&#20214;&#28431;&#27934;&#25968;&#25454;&#24211;&#12290;&#36825;&#20123;&#28431;&#27934;&#26469;&#33258;Google OSS-Fuzz&#21457;&#29616;&#30340;C/C++&#39033;&#30446;&#65292;&#27599;&#20010;&#28431;&#27934;&#37117;&#24102;&#26377;&#35302;&#21457;&#22120;&#36755;&#20837;&#12289;&#24320;&#21457;&#32773;&#20462;&#22797;&#26041;&#26696;&#20197;&#21450;&#21487;&#20197;&#22312;&#21407;&#29366;&#21644;&#20462;&#22797;&#29366;&#24577;&#19979;&#33258;&#21160;&#32534;&#35793;&#21644;&#36816;&#34892;&#30340;&#39033;&#30446;&#20195;&#30721;&#12290;&#36825;&#31181;&#35774;&#35745;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#36719;&#20214;&#23433;&#20840;&#30740;&#31350;&#30340;&#23454;&#29992;&#24615;&#21644;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02153v1 Announce Type: cross  Abstract: High-quality datasets of real-world vulnerabilities are enormously valuable for downstream research in software security, but existing datasets are typically small, require extensive manual effort to update, and are missing crucial features that such research needs. In this paper, we introduce ARVO: an Atlas of Reproducible Vulnerabilities in Open-source software. By sourcing vulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and implementing a reliable re-compilation system, we successfully reproduce more than 5,000 memory vulnerabilities across over 250 projects, each with a triggering input, the canonical developer-written patch for fixing the vulnerability, and the ability to automatically rebuild the project from source and run it at its vulnerable and patched revisions. Moreover, our dataset can be automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to grow over time. We provide a thorough 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#26497;&#23569;&#26679;&#26412;&#26469;&#32034;&#24341;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#26041;&#27861;&#65288;Few-Shot GR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20026;&#25972;&#20010;&#25991;&#26723;&#24211;&#29983;&#25104;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;docid&#65289;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#25972;&#20010;&#25991;&#26723;&#24211;&#30340;docid&#38134;&#34892;&#65292;&#24182;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#38480;&#21046;&#27169;&#22411;&#29983;&#25104;&#30340;docid&#24517;&#39035;&#20301;&#20110;&#35813;&#38134;&#34892;&#20013;&#65292;&#36827;&#32780;&#23558;&#29983;&#25104;&#30340;docid&#26144;&#23556;&#22238;&#23545;&#24212;&#30340;&#25991;&#26723;&#65292;&#36825;&#31181;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#19979;&#28216;&#25991;&#26723;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#38656;&#27714;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#26816;&#32034;&#20219;&#21153;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#21160;&#24577;&#25991;&#26723;&#24211;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02152</link><description>&lt;p&gt;
Generative Retrieval with Few-shot Indexing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02152
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#26497;&#23569;&#26679;&#26412;&#26469;&#32034;&#24341;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#26041;&#27861;&#65288;Few-Shot GR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20026;&#25972;&#20010;&#25991;&#26723;&#24211;&#29983;&#25104;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;docid&#65289;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#25972;&#20010;&#25991;&#26723;&#24211;&#30340;docid&#38134;&#34892;&#65292;&#24182;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#38480;&#21046;&#27169;&#22411;&#29983;&#25104;&#30340;docid&#24517;&#39035;&#20301;&#20110;&#35813;&#38134;&#34892;&#20013;&#65292;&#36827;&#32780;&#23558;&#29983;&#25104;&#30340;docid&#26144;&#23556;&#22238;&#23545;&#24212;&#30340;&#25991;&#26723;&#65292;&#36825;&#31181;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#19979;&#28216;&#25991;&#26723;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#38656;&#27714;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#26816;&#32034;&#20219;&#21153;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#21160;&#24577;&#25991;&#26723;&#24211;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02152v1 Announce Type: cross  Abstract: Existing generative retrieval (GR) approaches rely on training-based indexing, i.e., fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has three limitations: high training overhead, under-utilization of the pre-trained knowledge of large language models (LLMs), and challenges in adapting to a dynamic document corpus. To address the above issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR). It has a novel few-shot indexing process, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Few-Shot GR relies solely on prompting an LLM without req
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#23558;&#30697;&#38453;&#28216;&#25103;&#31038;&#20132;&#22256;&#22659;&#25193;&#23637;&#21040;&#22810;agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#29615;&#22659;&#20013;&#65292;&#24182;&#22312;&#20854;&#20013;&#21152;&#20837;&#20102;&#29615;&#22659;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;MARL&#31574;&#30053;&#36235;&#21521;&#20110;&#25910;&#25947;&#21040;&#39118;&#38505;&#20027;&#23548;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#36825;&#20026;&#35299;&#20915;&#19968;&#33324;&#24615;&#28216;&#25103;&#20013;&#30340;&#21512;&#20316;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.02148</link><description>&lt;p&gt;
Environment Complexity and Nash Equilibria in a Sequential Social Dilemma
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#23558;&#30697;&#38453;&#28216;&#25103;&#31038;&#20132;&#22256;&#22659;&#25193;&#23637;&#21040;&#22810;agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#29615;&#22659;&#20013;&#65292;&#24182;&#22312;&#20854;&#20013;&#21152;&#20837;&#20102;&#29615;&#22659;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;MARL&#31574;&#30053;&#36235;&#21521;&#20110;&#25910;&#25947;&#21040;&#39118;&#38505;&#20027;&#23548;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#36825;&#20026;&#35299;&#20915;&#19968;&#33324;&#24615;&#28216;&#25103;&#20013;&#30340;&#21512;&#20316;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02148v1 Announce Type: cross  Abstract: Multi-agent reinforcement learning (MARL) methods, while effective in zero-sum or positive-sum games, often yield suboptimal outcomes in general-sum games where cooperation is essential for achieving globally optimal outcomes. Matrix game social dilemmas, which abstract key aspects of general-sum interactions, such as cooperation, risk, and trust, fail to model the temporal and spatial dynamics characteristic of real-world scenarios. In response, our study extends matrix game social dilemmas into more complex, higher-dimensional MARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma to more closely match the decision-space of a one-shot matrix game while also introducing variable environment complexity. Our findings indicate that as complexity increases, MARL agents trained in these environments converge to suboptimal strategies, consistent with the risk-dominant Nash equilibria strategies found in matrix games
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#20013;&#22269;&#25991;&#21270;&#20013;&#28151;&#21512;&#24773;&#24863;&#24773;&#20917;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#34920;&#24773;&#24863;&#26102;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#25991;&#21270;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.02143</link><description>&lt;p&gt;
Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#20013;&#22269;&#25991;&#21270;&#20013;&#28151;&#21512;&#24773;&#24863;&#24773;&#20917;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#34920;&#24773;&#24863;&#26102;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#25991;&#21270;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02143v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages. There is a growing interest in academia to use these models to simulate and study human behaviors. However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture. Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data. This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. Our methodology is based on the studies of Miyamoto et al. (2010), which identified distinctive emotional indicators in Japanese and American human responses. We first administer their mixed emotion survey to five different LLMs and analyze their outputs. 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#22686;&#24378;&#30340;GAN&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#39640;&#25928;&#25552;&#21462;&#40657;&#31665;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#23545;&#35270;&#39057;&#20998;&#31867;&#30340;&#27169;&#22411;&#24179;&#22343;&#25552;&#21319;&#25928;&#26524;&#39640;&#36798;26.11%&#65292;&#24182;&#22312;&#19968;&#20123;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22823;33.36%&#30340;&#25552;&#21319;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#36827;&#40657;&#31665;&#27169;&#22411;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02140</link><description>&lt;p&gt;
VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#22686;&#24378;&#30340;GAN&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#39640;&#25928;&#25552;&#21462;&#40657;&#31665;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#23545;&#35270;&#39057;&#20998;&#31867;&#30340;&#27169;&#22411;&#24179;&#22343;&#25552;&#21319;&#25928;&#26524;&#39640;&#36798;26.11%&#65292;&#24182;&#22312;&#19968;&#20123;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22823;33.36%&#30340;&#25552;&#21319;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#36827;&#40657;&#31665;&#27169;&#22411;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02140v1 Announce Type: new  Abstract: In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Exanna&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#20195;&#29702;&#20154;&#22312;&#20316;&#20986;&#20915;&#31574;&#26102;&#33021;&#22815;&#32771;&#34385;&#20854;&#33258;&#36523;&#21644;&#20182;&#20154;&#30340;&#20215;&#20540;&#35266;&#65292;&#24182;&#22312;&#25552;&#20379;&#34892;&#21160;&#29702;&#30001;&#26102;&#21152;&#20197;&#20307;&#29616;&#12290;&#36890;&#36807;&#22810;agent&#27169;&#25311;&#23454;&#39564;&#65292;&#30740;&#31350;&#26174;&#31034;&#32771;&#34385;&#20215;&#20540;&#35266;&#24182;&#25552;&#20379;&#29702;&#30001;&#30340;&#34892;&#20026;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20914;&#31361;&#35299;&#20915;&#33021;&#21147;&#12289;&#31038;&#20132;&#20307;&#39564;&#36136;&#37327;&#12289;&#38544;&#31169;&#20445;&#25252;&#24847;&#35782;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02117</link><description>&lt;p&gt;
Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;Exanna&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#20195;&#29702;&#20154;&#22312;&#20316;&#20986;&#20915;&#31574;&#26102;&#33021;&#22815;&#32771;&#34385;&#20854;&#33258;&#36523;&#21644;&#20182;&#20154;&#30340;&#20215;&#20540;&#35266;&#65292;&#24182;&#22312;&#25552;&#20379;&#34892;&#21160;&#29702;&#30001;&#26102;&#21152;&#20197;&#20307;&#29616;&#12290;&#36890;&#36807;&#22810;agent&#27169;&#25311;&#23454;&#39564;&#65292;&#30740;&#31350;&#26174;&#31034;&#32771;&#34385;&#20215;&#20540;&#35266;&#24182;&#25552;&#20379;&#29702;&#30001;&#30340;&#34892;&#20026;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20914;&#31361;&#35299;&#20915;&#33021;&#21147;&#12289;&#31038;&#20132;&#20307;&#39564;&#36136;&#37327;&#12289;&#38544;&#31169;&#20445;&#25252;&#24847;&#35782;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02117v1 Announce Type: cross  Abstract: We propose Exanna, a framework to realize agents that incorporate values in decision making. An Exannaagent considers the values of itself and others when providing rationales for its actions and evaluating the rationales provided by others. Via multiagent simulation, we demonstrate that considering values in decision making and producing rationales, especially for norm-deviating actions, leads to (1) higher conflict resolution, (2) better social experience, (3) higher privacy, and (4) higher flexibility.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25945;&#23398;&#29615;&#22659;&#30340;&#22768;&#38899;&#35774;&#35745;&#39033;&#30446;&#65292;&#36890;&#36807;&#26234;&#33021;&#24037;&#20855;&#30340;&#20351;&#29992;&#20419;&#36827;&#20102;&#21019;&#36896;&#21147;&#25945;&#23398;&#65292;&#24182;&#20998;&#20139;&#20102;&#23398;&#29983;&#22312;&#19981;&#21516;&#24180;&#32423;&#38024;&#23545;&#19981;&#21516;&#39033;&#30446;&#26102;&#36935;&#21040;&#30340;&#21508;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#38477;&#20302;&#36827;&#20837;&#38899;&#39057;&#19990;&#30028;&#30340;&#38376;&#27099;&#65292;&#35753;&#23398;&#29983;&#33021;&#22815;&#26356;&#21152;&#36731;&#26494;&#22320;&#20351;&#29992;&#38899;&#39057;&#32534;&#36753;&#36719;&#20214;&#12290;</title><link>https://arxiv.org/abs/2408.02113</link><description>&lt;p&gt;
Dise\~no de sonido para producciones audiovisuales e historias sonoras en el aula. Hacia una docencia creativa mediante el uso de herramientas inteligentes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02113
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25945;&#23398;&#29615;&#22659;&#30340;&#22768;&#38899;&#35774;&#35745;&#39033;&#30446;&#65292;&#36890;&#36807;&#26234;&#33021;&#24037;&#20855;&#30340;&#20351;&#29992;&#20419;&#36827;&#20102;&#21019;&#36896;&#21147;&#25945;&#23398;&#65292;&#24182;&#20998;&#20139;&#20102;&#23398;&#29983;&#22312;&#19981;&#21516;&#24180;&#32423;&#38024;&#23545;&#19981;&#21516;&#39033;&#30446;&#26102;&#36935;&#21040;&#30340;&#21508;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#38477;&#20302;&#36827;&#20837;&#38899;&#39057;&#19990;&#30028;&#30340;&#38376;&#27099;&#65292;&#35753;&#23398;&#29983;&#33021;&#22815;&#26356;&#21152;&#36731;&#26494;&#22320;&#20351;&#29992;&#38899;&#39057;&#32534;&#36753;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02113v1 Announce Type: cross  Abstract: This study aims to share a teaching experience teaching sound design for audiovisual productions and compares different projects tackled by students. It is not intended to be a comparative analysis of different types of teaching but rather an analysis of different problems observed in different profiles of students of the subject who study it in different grades. The world of audio can be very interesting for a large part of the students, both those with creative and technical inclinations. Musical creation and production, synchronization with images, dubbing, etc. They are disciplines that are generally interesting but can have a very high barrier to entry due to their great technical complexity. Sometimes it can take weeks or even months for the uninitiated to begin to use audio editing programs with the necessary ease, which are not always particularly intuitive for students. Learning through the use of PBL methodologies generates, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#8220;&#31209;&#8221;&#27010;&#24565;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20542;&#21521;&#20110;&#23548;&#33268;&#20302;&#31209;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#24182;&#23545;&#33258;&#28982;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#31209;&#30340;&#27010;&#24565;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#20132;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#19982;&#24352;&#37327;&#20998;&#35299;&#20043;&#38388;&#30340;&#32852;&#31995;&#20316;&#20026;&#20027;&#35201;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#20855;&#20307;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#21644;&#20248;&#21270;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2408.02111</link><description>&lt;p&gt;
Understanding Deep Learning via Notions of Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#8220;&#31209;&#8221;&#27010;&#24565;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20542;&#21521;&#20110;&#23548;&#33268;&#20302;&#31209;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#24182;&#23545;&#33258;&#28982;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#31209;&#30340;&#27010;&#24565;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#20132;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#19982;&#24352;&#37327;&#20998;&#35299;&#20043;&#38388;&#30340;&#32852;&#31995;&#20316;&#20026;&#20027;&#35201;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#20855;&#20307;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#21644;&#20248;&#21270;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02111v1 Announce Type: cross  Abstract: Despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. This thesis puts forth notions of rank as key for developing a theory of deep learning, focusing on the fundamental aspects of generalization and expressiveness. In particular, we establish that gradient-based training can induce an implicit regularization towards low rank for several neural network architectures, and demonstrate empirically that this phenomenon may facilitate an explanation of generalization over natural data (e.g., audio, images, and text). Then, we characterize the ability of graph neural networks to model interactions via a notion of rank, which is commonly used for quantifying entanglement in quantum physics. A central tool underlying these results is a connection between neural networks and tensor factorizations. Practical implications of our theory for designing explicit regularization schemes and data p
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCBEV-KAN&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#21253;&#25324;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#65289;&#26469;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;BEV&#65288;&#40479;&#30640;&#35270;&#22270;&#65289;&#21644;Transformer&#26550;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#25968;&#25454;&#28304;&#38388;&#30340;&#25972;&#21512;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#26816;&#27979;&#31867;&#21035;&#19978;&#65292;RCBEV-KAN&#27169;&#22411;&#22312;Mean Distance AP&#12289;ND Score&#21644;Evaluation Time&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#34920;&#26126;&#20854;&#26816;&#27979;&#24615;&#33021;&#22823;&#24133;&#39046;&#20808;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02088</link><description>&lt;p&gt;
KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for autonomous driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCBEV-KAN&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#21253;&#25324;&#25668;&#20687;&#22836;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;&#27627;&#31859;&#27874;&#38647;&#36798;&#65289;&#26469;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;BEV&#65288;&#40479;&#30640;&#35270;&#22270;&#65289;&#21644;Transformer&#26550;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#25968;&#25454;&#28304;&#38388;&#30340;&#25972;&#21512;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#26816;&#27979;&#31867;&#21035;&#19978;&#65292;RCBEV-KAN&#27169;&#22411;&#22312;Mean Distance AP&#12289;ND Score&#21644;Evaluation Time&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#34920;&#26126;&#20854;&#26816;&#27979;&#24615;&#33021;&#22823;&#24133;&#39046;&#20808;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02088v1 Announce Type: new  Abstract: Accurate 3D object detection in autonomous driving is critical yet challenging due to occlusions, varying object scales, and complex urban environments. This paper introduces the RCBEV-KAN algorithm, a pioneering method designed to enhance 3D object detection by fusing multimodal sensor data from cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View (BEV)-based approach, utilizing a Transformer architecture, significantly boosts detection precision and efficiency by seamlessly integrating diverse data sources, improving spatial relationship handling, and optimizing computational processes. Experimental results show that the RCBEV-KAN model demonstrates superior performance across most detection categories, achieving higher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score (0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8% faster). These results indicate that RCBEV-KAN is 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#25968;&#25454;&#35780;&#20272;&#21644;&#36873;&#25321;&#22312;&#25351;&#20196;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20840;&#38754;&#26803;&#29702;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#20026;&#36825;&#31867;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#28165;&#26224;&#12289;&#23618;&#32423;&#31934;&#32454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25351;&#20196;&#24494;&#35843;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-GAN&#65289;&#19982;&#22534;&#21472;&#26102;&#29627;&#29827;&#32593;&#32476;&#65288;SHGN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#20998;&#21106;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#22686;&#24378;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22797;&#26434;&#25104;&#20687;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;L1&#21644;L2&#37325;&#24314;&#25439;&#22833;&#65292;&#24182;&#36741;&#20197;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20197;&#22312;&#20869;&#22312;&#34880;&#31649;&#36229;&#22768;&#65288;IVUS&#65289;&#25104;&#20687;&#20013;&#31934;&#32454;&#21270;&#20998;&#21106;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#22914;&#32452;&#32455;&#36793;&#30028;&#21644;&#34880;&#31649;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20381;&#36182;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#35813;&#31639;&#27861;&#22312;&#26631;&#20934;&#21307;&#23398;&#22270;&#20687;&#24211;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02074</link><description>&lt;p&gt;
Applying Conditional Generative Adversarial Networks for Imaging Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-GAN&#65289;&#19982;&#22534;&#21472;&#26102;&#29627;&#29827;&#32593;&#32476;&#65288;SHGN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#20998;&#21106;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#22686;&#24378;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22797;&#26434;&#25104;&#20687;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;L1&#21644;L2&#37325;&#24314;&#25439;&#22833;&#65292;&#24182;&#36741;&#20197;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20197;&#22312;&#20869;&#22312;&#34880;&#31649;&#36229;&#22768;&#65288;IVUS&#65289;&#25104;&#20687;&#20013;&#31934;&#32454;&#21270;&#20998;&#21106;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#22914;&#32452;&#32455;&#36793;&#30028;&#21644;&#34880;&#31649;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20381;&#36182;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#35813;&#31639;&#27861;&#22312;&#26631;&#20934;&#21307;&#23398;&#22270;&#20687;&#24211;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02074v1 Announce Type: cross  Abstract: This study introduces an innovative application of Conditional Generative Adversarial Networks (C-GAN) integrated with Stacked Hourglass Networks (SHGN) aimed at enhancing image segmentation, particularly in the challenging environment of medical imaging. We address the problem of overfitting, common in deep learning models applied to complex imaging datasets, by augmenting data through rotation and scaling. A hybrid loss function combining L1 and L2 reconstruction losses, enriched with adversarial training, is introduced to refine segmentation processes in intravascular ultrasound (IVUS) imaging. Our approach is unique in its capacity to accurately delineate distinct regions within medical images, such as tissue boundaries and vascular structures, without extensive reliance on domain-specific knowledge. The algorithm was evaluated using a standard medical image library, showing superior performance metrics compared to existing methods
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#37319;&#29992;&#19968;&#31181;&#32467;&#21512;CNN-Transformer&#21644;&#33014;&#22218;&#32593;&#32476;&#30340;&#26696;&#20363;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20799;&#31461;&#21457;&#32946;&#36831;&#32531;&#36827;&#34892;&#26089;&#26399;&#31579;&#26597;&#65292;&#20197;&#26399;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#26089;&#26399;&#24178;&#39044;&#23558;&#26174;&#33879;&#20943;&#23569;&#21307;&#30103;&#36164;&#28304;&#28010;&#36153;&#21644;&#31038;&#20250;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2408.02073</link><description>&lt;p&gt;
Case-based reasoning approach for diagnostic screening of children with developmental delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#37319;&#29992;&#19968;&#31181;&#32467;&#21512;CNN-Transformer&#21644;&#33014;&#22218;&#32593;&#32476;&#30340;&#26696;&#20363;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20799;&#31461;&#21457;&#32946;&#36831;&#32531;&#36827;&#34892;&#26089;&#26399;&#31579;&#26597;&#65292;&#20197;&#26399;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#26089;&#26399;&#24178;&#39044;&#23558;&#26174;&#33879;&#20943;&#23569;&#21307;&#30103;&#36164;&#28304;&#28010;&#36153;&#21644;&#31038;&#20250;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02073v1 Announce Type: new  Abstract: According to the World Health Organization, the population of children with developmental delays constitutes approximately 6% to 9% of the total population. Based on the number of newborns in Huaibei, Anhui Province, China, in 2023 (94,420), it is estimated that there are about 7,500 cases (suspected cases of developmental delays) of suspicious cases annually. Early identification and appropriate early intervention for these children can significantly reduce the wastage of medical resources and societal costs. International research indicates that the optimal period for intervention in children with developmental delays is before the age of six, with the golden treatment period being before three and a half years of age. Studies have shown that children with developmental delays who receive early intervention exhibit significant improvement in symptoms; some may even fully recover. This research adopts a hybrid model combining a CNN-Tran
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20572;&#36710;&#22330;&#20840;&#31243;&#35268;&#21010;&#31995;&#32479;&#65288;ParkingE2E&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#22270;&#20687;&#21040;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23436;&#25104;&#22797;&#26434;&#20572;&#36710;&#22330;&#26223;&#19979;&#30340;&#20219;&#21153;&#65292;&#20026;&#26234;&#33021;&#39550;&#39542;&#36710;&#36742;&#30340;&#33258;&#20027;&#27850;&#36710;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.02061</link><description>&lt;p&gt;
ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20572;&#36710;&#22330;&#20840;&#31243;&#35268;&#21010;&#31995;&#32479;&#65288;ParkingE2E&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#22270;&#20687;&#21040;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23436;&#25104;&#22797;&#26434;&#20572;&#36710;&#22330;&#26223;&#19979;&#30340;&#20219;&#21153;&#65292;&#20026;&#26234;&#33021;&#39550;&#39542;&#36710;&#36742;&#30340;&#33258;&#20027;&#27850;&#36710;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02061v1 Announce Type: cross  Abstract: Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conducted extensive experiments in real-world scenarios, and the resul
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HVTrack&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28857; clouds&#20855;&#26377;&#39640;&#26102;&#21464;&#25968;&#25454;&#20013;&#30340;3D&#21333;&#23545;&#35937;&#36319;&#36394;&#65292;&#36890;&#36807;&#19977;&#20010;&#26032;&#22411;&#32452;&#20214;&#24212;&#23545;&#39640;&#26102;&#21464;&#30340;&#25361;&#25112;&#65306;&#30456;&#23545;&#23039;&#24577;&#20851;&#27880;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#26102;&#21464;&#25968;&#25454;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02049</link><description>&lt;p&gt;
3D Single-object Tracking in Point Clouds with High Temporal Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HVTrack&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28857; clouds&#20855;&#26377;&#39640;&#26102;&#21464;&#25968;&#25454;&#20013;&#30340;3D&#21333;&#23545;&#35937;&#36319;&#36394;&#65292;&#36890;&#36807;&#19977;&#20010;&#26032;&#22411;&#32452;&#20214;&#24212;&#23545;&#39640;&#26102;&#21464;&#30340;&#25361;&#25112;&#65306;&#30456;&#23545;&#23039;&#24577;&#20851;&#27880;&#35760;&#24518;&#27169;&#22359;&#12289;&#22522;&#25193;&#29305;&#24449;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#28857;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#26102;&#21464;&#25968;&#25454;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02049v1 Announce Type: new  Abstract: The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#36793;&#32536;&#29983;&#25104;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#23454;&#29616;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#20154;&#24037;&#26234;&#33021;&#20869;&#23481;&#29983;&#25104;&#36164;&#28304;&#30340;&#20248;&#21270;&#20998;&#37197;&#65292;&#20197;&#38477;&#20302;&#26381;&#21153;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2408.02047</link><description>&lt;p&gt;
Latency-Aware Resource Allocation for Mobile Edge Generation and Computing via Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#36793;&#32536;&#29983;&#25104;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#23454;&#29616;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#20154;&#24037;&#26234;&#33021;&#20869;&#23481;&#29983;&#25104;&#36164;&#28304;&#30340;&#20248;&#21270;&#20998;&#37197;&#65292;&#20197;&#38477;&#20302;&#26381;&#21153;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02047v1 Announce Type: cross  Abstract: Recently, the integration of mobile edge computing (MEC) and generative artificial intelligence (GAI) technology has given rise to a new area called mobile edge generation and computing (MEGC), which offers mobile users heterogeneous services such as task computing and content generation. In this letter, we investigate the joint communication, computation, and the AIGC resource allocation problem in an MEGC system. A latency minimization problem is first formulated to enhance the quality of service for mobile users. Due to the strong coupling of the optimization variables, we propose a new deep reinforcement learning-based algorithm to solve it efficiently. Numerical results demonstrate that the proposed algorithm can achieve lower latency than two baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;Twitter/X&#24179;&#21488;&#19978;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#20013;&#65292;&#38024;&#23545;&#19996;&#27431;V4&#22269;&#23478;&#30340;&#35821;&#35328;&#65288;&#21363;&#25463;&#20811;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#21256;&#29273;&#21033;&#35821;&#65289;&#36827;&#34892;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26102;&#65292;&#37319;&#29992;&#24494;&#35843;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20960;&#31181;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;BERT&#12289;BERTweet&#12289;Llama2&#21644;Mistral&#65292;&#20197;&#21450;GPT-4&#20316;&#20026;&#21442;&#32771;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#20219;&#21153;&#19978;&#23567;&#35268;&#27169;&#24494;&#35843;&#27169;&#22411;&#21487;&#33021;&#20248;&#20110;&#36890;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38024;&#23545;&#29305;&#23450;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#20026;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#22312;&#29305;&#23450;&#32972;&#26223;&#19979;&#65288;&#22914;&#20914;&#31361;&#26102;&#26399;&#65289;&#30340;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2408.02044</link><description>&lt;p&gt;
Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;Twitter/X&#24179;&#21488;&#19978;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#20013;&#65292;&#38024;&#23545;&#19996;&#27431;V4&#22269;&#23478;&#30340;&#35821;&#35328;&#65288;&#21363;&#25463;&#20811;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#21256;&#29273;&#21033;&#35821;&#65289;&#36827;&#34892;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26102;&#65292;&#37319;&#29992;&#24494;&#35843;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20960;&#31181;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;BERT&#12289;BERTweet&#12289;Llama2&#21644;Mistral&#65292;&#20197;&#21450;GPT-4&#20316;&#20026;&#21442;&#32771;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#20219;&#21153;&#19978;&#23567;&#35268;&#27169;&#24494;&#35843;&#27169;&#22411;&#21487;&#33021;&#20248;&#20110;&#36890;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38024;&#23545;&#29305;&#23450;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#20026;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#22312;&#29305;&#23450;&#32972;&#26223;&#19979;&#65288;&#22914;&#20914;&#31361;&#26102;&#26399;&#65289;&#30340;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02044v1 Announce Type: cross  Abstract: The aspect-based sentiment analysis (ABSA) is a standard NLP task with numerous approaches and benchmarks, where large language models (LLM) represent the current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data in underrepresented languages. On such narrow tasks, small tuned language models can often outperform universal large ones, providing available and cheap solutions.   We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for classification of sentiment towards Russia and Ukraine in the context of the ongoing military conflict. The training/testing dataset was obtained from the academic API from Twitter/X during 2023, narrowed to the languages of the V4 countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their performance under a variety of settings including translations, sentiment targets, in-context learning and more, using GPT4 as a reference model. We document several inte
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Self-Introspective Decoding (SID)&#26041;&#27861;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#25105;&#23457;&#35270;&#65292;&#25104;&#21151;&#20943;&#36731;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#19981;&#31526;&#21512;&#30495;&#23454;&#35821;&#22659;&#30340;&#25991;&#26412;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#65289;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#25551;&#36848;&#20934;&#30830;&#24615;&#65292;&#20419;&#36827;&#20102;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;</title><link>https://arxiv.org/abs/2408.02032</link><description>&lt;p&gt;
Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Self-Introspective Decoding (SID)&#26041;&#27861;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#25105;&#23457;&#35270;&#65292;&#25104;&#21151;&#20943;&#36731;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#19981;&#31526;&#21512;&#30495;&#23454;&#35821;&#22659;&#30340;&#25991;&#26412;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#65289;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#25551;&#36848;&#20934;&#30830;&#24615;&#65292;&#20419;&#36827;&#20102;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02032v1 Announce Type: new  Abstract: While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the `hallucination' problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing auxiliary analysis networks, which inevitable incur additional costs. Another approach, known as contrastive decoding, induces hallucinations by manually disturbing the vision or instruction raw inputs and mitigates them by contrasting the outputs of the disturbed and original LVLMs. However, these approaches rely on empirical holistic input disturbances and double the inference cost. To avoid these issues, we propose a simple yet effective method named Self-Introspective Decoding (SID). Our empirical investigation reveals that pretrained LVLMs ca
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PIONEER&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#26377;&#23646;&#24615;&#22270;&#20013;&#30340;&#36335;&#24452;&#20013;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#22320;&#25366;&#25496;&#20986;&#39057;&#32321;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#38750;&#21333;&#35843;&#24615;&#36136;&#21644;&#24182;&#34892;&#35745;&#31639;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21319;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.02029</link><description>&lt;p&gt;
Mining Path Association Rules in Large Property Graphs (with Appendix)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PIONEER&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#26377;&#23646;&#24615;&#22270;&#20013;&#30340;&#36335;&#24452;&#20013;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#22320;&#25366;&#25496;&#20986;&#39057;&#32321;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#38750;&#21333;&#35843;&#24615;&#36136;&#21644;&#24182;&#34892;&#35745;&#31639;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21319;&#36335;&#24452;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02029v1 Announce Type: cross  Abstract: How can we mine frequent path regularities from a graph with edge labels and vertex attributes? The task of association rule mining successfully discovers regular patterns in item sets and substructures. Still, to our best knowledge, this concept has not yet been extended to path patterns in large property graphs. In this paper, we introduce the problem of path association rule mining (PARM). Applied to any \emph{reachability path} between two vertices within a large graph, PARM discovers regular ways in which path patterns, identified by vertex attributes and edge labels, co-occur with each other. We develop an efficient and scalable algorithm PIONEER that exploits an anti-monotonicity property to effectively prune the search space. Further, we devise approximation techniques and employ parallelization to achieve scalable path association rule mining. Our experimental study using real-world graph data verifies the significance of path
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#38142;&#32467;&#38598;&#32676;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35821;&#35328;&#29615;&#22659;&#19979;&#20154;&#33080;&#19982;&#22768;&#38899;&#30340;&#20851;&#32852;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#30340;&#20132;&#21449;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#23450;&#21046;&#21270;&#30340;&#38142;&#32467;&#38598;&#32676;&#22788;&#29702;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#38754;&#23545;&#35821;&#35328;&#38388;&#22768;&#35843;&#24046;&#24322;&#21644;&#25968;&#25454;&#20013;&#22266;&#26377;&#19982;&#22806;&#22312;&#30340;&#22810;&#26679;&#24615;&#26102;&#65292;&#22686;&#24378;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#19979;&#20154;&#33080;&#19982;&#22768;&#38899;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.02025</link><description>&lt;p&gt;
Contrastive Learning-based Chaining-Cluster for Multilingual Voice-Face Association
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#38142;&#32467;&#38598;&#32676;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35821;&#35328;&#29615;&#22659;&#19979;&#20154;&#33080;&#19982;&#22768;&#38899;&#30340;&#20851;&#32852;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#30340;&#20132;&#21449;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#23450;&#21046;&#21270;&#30340;&#38142;&#32467;&#38598;&#32676;&#22788;&#29702;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#38754;&#23545;&#35821;&#35328;&#38388;&#22768;&#35843;&#24046;&#24322;&#21644;&#25968;&#25454;&#20013;&#22266;&#26377;&#19982;&#22806;&#22312;&#30340;&#22810;&#26679;&#24615;&#26102;&#65292;&#22686;&#24378;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#19979;&#20154;&#33080;&#19982;&#22768;&#38899;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02025v1 Announce Type: cross  Abstract: The innate correlation between a person's face and voice has recently emerged as a compelling area of study, especially within the context of multilingual environments. This paper introduces our novel solution to the Face-Voice Association in Multilingual Environments (FAME) 2024 challenge, focusing on a contrastive learning-based chaining-cluster method to enhance face-voice association. This task involves the challenges of building biometric relations between auditory and visual modality cues and modelling the prosody interdependence between different languages while addressing both intrinsic and extrinsic variability present in the data. To handle these non-trivial challenges, our method employs supervised cross-contrastive (SCC) learning to establish robust associations between voices and faces in multi-language scenarios. Following this, we have specifically designed a chaining-cluster-based post-processing step to mitigate the im
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22330;&#26223;&#39537;&#21160;&#30340;&#30005;&#27744;&#30005;&#21160;&#36710;&#36742;&#28909;&#31649;&#29702;&#31995;&#32479;&#21442;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#28909;&#31649;&#29702;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#26469;&#34920;&#31034;&#23884;&#20837;&#24335;&#21442;&#25968;&#38598;&#65292;&#20174;&#32780;&#20351;&#21442;&#25968;&#35843;&#25972;&#26356;&#21152;&#39640;&#25928;&#21644;&#36866;&#24212;&#22810;&#31181;&#36710;&#36742;&#20351;&#29992;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2408.02022</link><description>&lt;p&gt;
Scenario-based Thermal Management Parametrization Through Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22330;&#26223;&#39537;&#21160;&#30340;&#30005;&#27744;&#30005;&#21160;&#36710;&#36742;&#28909;&#31649;&#29702;&#31995;&#32479;&#21442;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#28909;&#31649;&#29702;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#26469;&#34920;&#31034;&#23884;&#20837;&#24335;&#21442;&#25968;&#38598;&#65292;&#20174;&#32780;&#20351;&#21442;&#25968;&#35843;&#25972;&#26356;&#21152;&#39640;&#25928;&#21644;&#36866;&#24212;&#22810;&#31181;&#36710;&#36742;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02022v1 Announce Type: cross  Abstract: The thermal system of battery electric vehicles demands advanced control. Its thermal management needs to effectively control active components across varying operating conditions. While robust control function parametrization is required, current methodologies show significant drawbacks. They consume considerable time, human effort, and extensive real-world testing. Consequently, there is a need for innovative and intelligent solutions that are capable of autonomously parametrizing embedded controllers. Addressing this issue, our paper introduces a learning-based tuning approach. We propose a methodology that benefits from automated scenario generation for increased robustness across vehicle usage scenarios. Our deep reinforcement learning agent processes the tuning task context and incorporates an image-based interpretation of embedded parameter sets. We demonstrate its applicability to a valve controller parametrization task and ver
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#20307;&#21270;&#22810;&#26102;&#38388;&#23610;&#24230;MRI&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;auto&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#22522;&#20110;&#24180;&#40836;&#12289;&#30142;&#30149;&#29366;&#24577;&#21644;&#21333;&#20010;&#24739;&#32773;&#21382;&#21490;&#25195;&#25551;&#25968;&#25454;&#30340;&#26410;&#26469;MRI&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#30340;&#36830;&#32493;&#25104;&#20687;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#37319;&#26679;&#20197;&#29983;&#25104;&#26410;&#26469;&#35299;&#21078;&#23398;&#21464;&#21270;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;10&#24180;&#21518;&#30340;MRI&#22270;&#20687;&#12290;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#25351;&#26631;&#26174;&#31034;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.02018</link><description>&lt;p&gt;
Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#20307;&#21270;&#22810;&#26102;&#38388;&#23610;&#24230;MRI&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;auto&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#22522;&#20110;&#24180;&#40836;&#12289;&#30142;&#30149;&#29366;&#24577;&#21644;&#21333;&#20010;&#24739;&#32773;&#21382;&#21490;&#25195;&#25551;&#25968;&#25454;&#30340;&#26410;&#26469;MRI&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#30340;&#36830;&#32493;&#25104;&#20687;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#37319;&#26679;&#20197;&#29983;&#25104;&#26410;&#26469;&#35299;&#21078;&#23398;&#21464;&#21270;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;10&#24180;&#21518;&#30340;MRI&#22270;&#20687;&#12290;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#25351;&#26631;&#26174;&#31034;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02018v1 Announce Type: new  Abstract: Neurodegeneration as measured through magnetic resonance imaging (MRI) is recognized as a potential biomarker for diagnosing Alzheimer's disease (AD), but is generally considered less specific than amyloid or tau based biomarkers. Due to a large amount of variability in brain anatomy between different individuals, we hypothesize that leveraging MRI time series can help improve specificity, by treating each patient as their own baseline. Here we turn to conditional variational autoencoders to generate individualized MRI predictions given the subject's age, disease status and one previous scan. Using serial imaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a novel architecture to build a latent space distribution which can be sampled from to generate future predictions of changing anatomy. This enables us to extrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated the model on a held-out set fr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#26469;&#27010;&#25324;&#24182;&#34701;&#21512;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#21644;&#38899;&#20048;&#30340;&#24773;&#32490;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#21327;&#35758;&#19979;&#65292;&#30740;&#31350;&#32773;&#20351;&#29992;&#22810;&#31181;&#29305;&#24449;&#26469;&#25551;&#32472;&#22768;&#38899;&#32467;&#26500;&#65292;&#24182;&#22312;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#22521;&#35757;&#20102;&#24322;&#26500;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#20197;&#26174;&#33879;&#20248;&#21183;&#36229;&#36234;&#20102;&#29616;&#26377;&#38899;&#20048;&#21644;&#22768;&#38899;&#24773;&#32490;&#39044;&#27979;&#25216;&#26415;&#30340;&#35760;&#24405;&#12290;&#36890;&#36807;&#36825;&#19968;&#21019;&#26032;&#24335;&#30340;&#36328;&#22495;&#23398;&#20064;&#21644;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#20316;&#32773;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#20934;&#30830;&#30340;&#36328;&#23186;&#20307;&#24773;&#32490;&#20998;&#26512;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.02009</link><description>&lt;p&gt;
Joint Learning of Emotions in Music and Generalized Sounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#26469;&#27010;&#25324;&#24182;&#34701;&#21512;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#21644;&#38899;&#20048;&#30340;&#24773;&#32490;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#21327;&#35758;&#19979;&#65292;&#30740;&#31350;&#32773;&#20351;&#29992;&#22810;&#31181;&#29305;&#24449;&#26469;&#25551;&#32472;&#22768;&#38899;&#32467;&#26500;&#65292;&#24182;&#22312;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#22521;&#35757;&#20102;&#24322;&#26500;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#20197;&#26174;&#33879;&#20248;&#21183;&#36229;&#36234;&#20102;&#29616;&#26377;&#38899;&#20048;&#21644;&#22768;&#38899;&#24773;&#32490;&#39044;&#27979;&#25216;&#26415;&#30340;&#35760;&#24405;&#12290;&#36890;&#36807;&#36825;&#19968;&#21019;&#26032;&#24335;&#30340;&#36328;&#22495;&#23398;&#20064;&#21644;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#20316;&#32773;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#20934;&#30830;&#30340;&#36328;&#23186;&#20307;&#24773;&#32490;&#20998;&#26512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02009v1 Announce Type: cross  Abstract: In this study, we aim to determine if generalized sounds and music can share a common emotional space, improving predictions of emotion in terms of arousal and valence. We propose the use of multiple datasets as a multi-domain learning technique. Our approach involves creating a common space encompassing features that characterize both generalized sounds and music, as they can evoke emotions in a similar manner. To achieve this, we utilized two publicly available datasets, namely IADS-E and PMEmo, following a standardized experimental protocol. We employed a wide variety of features that capture diverse aspects of the audio structure including key parameters of spectrum, energy, and voicing. Subsequently, we performed joint learning on the common feature space, leveraging heterogeneous model architectures. Interestingly, this synergistic scheme outperforms the state-of-the-art in both sound and music emotion prediction. The code enabli
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20808;&#36827;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#25915;&#20987;&#21518;&#24555;&#36895;&#26377;&#25928;&#22320;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#35843;&#26597;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#23454;&#29616;&#20854;&#25552;&#20986;&#30340;&#24378;&#21270;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#35843;&#26597;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;Q&#34920;&#26684;&#21644;&#26102;&#38388;&#24046;&#23398;&#20064;&#36845;&#20195;&#22320;&#25913;&#36827;&#20854;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#21462;&#20915;&#20110;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#29615;&#22659;&#36234;&#31616;&#21333;&#65292;&#23398;&#20064;&#29575;&#36234;&#39640;&#65292;&#25910;&#25947;&#36895;&#24230;&#36234;&#24555;&#65292;&#32780;&#22797;&#26434;&#29615;&#22659;&#21017;&#38656;&#35201;&#26356;&#20302;&#30340;&#23398;&#20064;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2408.01999</link><description>&lt;p&gt;
Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20808;&#36827;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#25915;&#20987;&#21518;&#24555;&#36895;&#26377;&#25928;&#22320;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#35843;&#26597;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#23454;&#29616;&#20854;&#25552;&#20986;&#30340;&#24378;&#21270;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#35843;&#26597;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;Q&#34920;&#26684;&#21644;&#26102;&#38388;&#24046;&#23398;&#20064;&#36845;&#20195;&#22320;&#25913;&#36827;&#20854;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#21462;&#20915;&#20110;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#29615;&#22659;&#36234;&#31616;&#21333;&#65292;&#23398;&#20064;&#29575;&#36234;&#39640;&#65292;&#25910;&#25947;&#36895;&#24230;&#36234;&#24555;&#65292;&#32780;&#22797;&#26434;&#29615;&#22659;&#21017;&#38656;&#35201;&#26356;&#20302;&#30340;&#23398;&#20064;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01999v1 Announce Type: cross  Abstract: This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;MetaWearS&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20943;&#23569;&#31359;&#25140;&#35774;&#22791;&#25152;&#38656;&#21021;&#22987;&#25968;&#25454;&#37327;&#65292;&#24182;&#37319;&#29992;&#21407;&#22411;&#26356;&#26032;&#26426;&#21046;&#31616;&#21270;&#27169;&#22411;&#26356;&#26032;&#36807;&#31243;&#65292;&#20197;&#25552;&#39640;&#31359;&#25140;&#35774;&#22791;&#22312;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01988</link><description>&lt;p&gt;
MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few Shots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;MetaWearS&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20943;&#23569;&#31359;&#25140;&#35774;&#22791;&#25152;&#38656;&#21021;&#22987;&#25968;&#25454;&#37327;&#65292;&#24182;&#37319;&#29992;&#21407;&#22411;&#26356;&#26032;&#26426;&#21046;&#31616;&#21270;&#27169;&#22411;&#26356;&#26032;&#36807;&#31243;&#65292;&#20197;&#25552;&#39640;&#31359;&#25140;&#35774;&#22791;&#22312;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01988v1 Announce Type: cross  Abstract: Wearable systems provide continuous health monitoring and can lead to early detection of potential health issues. However, the lifecycle of wearable systems faces several challenges. First, effective model training for new wearable devices requires substantial labeled data from various subjects collected directly by the wearable. Second, subsequent model updates require further extensive labeled data for retraining. Finally, frequent model updating on the wearable device can decrease the battery life in long-term data monitoring. Addressing these challenges, in this paper, we propose MetaWearS, a meta-learning method to reduce the amount of initial data collection required. Moreover, our approach incorporates a prototypical updating mechanism, simplifying the update process by modifying the class prototype rather than retraining the entire model. We explore the performance of MetaWearS in two case studies, namely, the detection of epil
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeMansia&#30340;&#26550;&#26500;&#65292;&#23427;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;token&#26631;&#35760;&#25216;&#26415;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#20256;&#32479;&#30340;transformer&#26550;&#26500;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01986</link><description>&lt;p&gt;
DeMansia: Mamba Never Forgets Any Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01986
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeMansia&#30340;&#26550;&#26500;&#65292;&#23427;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;token&#26631;&#35760;&#25216;&#26415;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#20256;&#32479;&#30340;transformer&#26550;&#26500;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01986v1 Announce Type: new  Abstract: This paper examines the mathematical foundations of transformer architectures, highlighting their limitations particularly in handling long sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM), and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia integrates state space models with token labeling techniques to enhance performance in image classification tasks, efficiently addressing the computational challenges posed by traditional transformers. The architecture, benchmark, and comparisons with contemporary models demonstrate DeMansia's effectiveness. The implementation of this paper is available on GitHub at https://github.com/catalpaaa/DeMansia
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SR-CIS&#30340;&#33258;&#25105;&#21453;&#24605;&#22686;&#37327;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#35760;&#24518;&#21644;&#25512;&#29702;&#30340;&#35299;&#32806;&#27169;&#22359;&#65292;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#30340;&#24555;&#36895;&#23398;&#20064;&#21644;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#31995;&#32479;&#30001;&#19968;&#20010;&#24555;&#36895;&#25512;&#29702;&#30340;&#23567;&#22411;&#27169;&#22359;&#21644;&#19968;&#20010;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#22411;&#27169;&#22359;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;Low-Rank Adaptive&#26426;&#21046;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#30340;&#30693;&#35782;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2408.01970</link><description>&lt;p&gt;
SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SR-CIS&#30340;&#33258;&#25105;&#21453;&#24605;&#22686;&#37327;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#35760;&#24518;&#21644;&#25512;&#29702;&#30340;&#35299;&#32806;&#27169;&#22359;&#65292;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#30340;&#24555;&#36895;&#23398;&#20064;&#21644;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#31995;&#32479;&#30001;&#19968;&#20010;&#24555;&#36895;&#25512;&#29702;&#30340;&#23567;&#22411;&#27169;&#22359;&#21644;&#19968;&#20010;&#24930;&#36895;&#20915;&#31574;&#30340;&#22823;&#22411;&#27169;&#22359;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;Low-Rank Adaptive&#26426;&#21046;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#30340;&#30693;&#35782;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01970v1 Announce Type: cross  Abstract: The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the infere
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ML-EAT&#30340;&#22797;&#26434;&#23618;&#27425;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#35821;&#35328;&#25216;&#26415;&#30340;&#20559;&#24046;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#19977;&#32423;&#23618;&#27425;&#19978;&#65288;&#20174;&#23439;&#35266;&#21040;&#24494;&#35266;&#65289;&#37327;&#21270;&#24046;&#24322;&#20851;&#32852;&#25928;&#24212;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;EAT&#65288;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65289;&#26041;&#27861;&#22312;&#35299;&#37322;&#19978;&#30340;&#27169;&#31946;&#24615;&#21644;&#38590;&#24230;&#65292;&#21363;&#65306;&#19981;&#21516;&#30446;&#26631;&#27010;&#24565;&#19982;&#19981;&#21516;&#23646;&#24615;&#27010;&#24565;&#30340;&#20851;&#32852;&#24230;&#65307;&#27599;&#20010;&#30446;&#26631;&#27010;&#24565;&#22312;&#19981;&#21516;&#23646;&#24615;&#27010;&#24565;&#20013;&#24433;&#21709;&#30340;&#20010;&#20307;&#25928;&#24212;&#22823;&#23567;&#65307;&#20197;&#21450;&#27599;&#20010;&#30446;&#26631;&#27010;&#24565;&#19982;&#27599;&#20010;&#23646;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#20855;&#20307;&#20851;&#32852;&#24230;&#12290;ML-EAT&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23427;&#33021;&#22815;&#23450;&#20041;&#19968;&#20010;EAT&#27169;&#24335;&#31246;&#21017;&#65292;&#27599;&#20010;&#27169;&#24335;&#37117;&#26377;&#19982;&#20043;&#23545;&#24212;&#30340;EAT-Map&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22235;&#20998;&#27133;&#22270;&#65292;&#29992;&#20110;&#35299;&#37322;ML-EAT&#32467;&#26524;&#12290;&#36890;&#36807;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#35789;&#23884;&#20837;&#30340;GPT-2&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;ML-EAT&#26469;&#25581;&#31034;&#35821;&#35328;&#25216;&#26415;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20351;&#31038;&#20250;&#31185;&#23398;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2408.01966</link><description>&lt;p&gt;
ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01966
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ML-EAT&#30340;&#22797;&#26434;&#23618;&#27425;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#35821;&#35328;&#25216;&#26415;&#30340;&#20559;&#24046;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#19977;&#32423;&#23618;&#27425;&#19978;&#65288;&#20174;&#23439;&#35266;&#21040;&#24494;&#35266;&#65289;&#37327;&#21270;&#24046;&#24322;&#20851;&#32852;&#25928;&#24212;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;EAT&#65288;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65289;&#26041;&#27861;&#22312;&#35299;&#37322;&#19978;&#30340;&#27169;&#31946;&#24615;&#21644;&#38590;&#24230;&#65292;&#21363;&#65306;&#19981;&#21516;&#30446;&#26631;&#27010;&#24565;&#19982;&#19981;&#21516;&#23646;&#24615;&#27010;&#24565;&#30340;&#20851;&#32852;&#24230;&#65307;&#27599;&#20010;&#30446;&#26631;&#27010;&#24565;&#22312;&#19981;&#21516;&#23646;&#24615;&#27010;&#24565;&#20013;&#24433;&#21709;&#30340;&#20010;&#20307;&#25928;&#24212;&#22823;&#23567;&#65307;&#20197;&#21450;&#27599;&#20010;&#30446;&#26631;&#27010;&#24565;&#19982;&#27599;&#20010;&#23646;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#20855;&#20307;&#20851;&#32852;&#24230;&#12290;ML-EAT&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23427;&#33021;&#22815;&#23450;&#20041;&#19968;&#20010;EAT&#27169;&#24335;&#31246;&#21017;&#65292;&#27599;&#20010;&#27169;&#24335;&#37117;&#26377;&#19982;&#20043;&#23545;&#24212;&#30340;EAT-Map&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22235;&#20998;&#27133;&#22270;&#65292;&#29992;&#20110;&#35299;&#37322;ML-EAT&#32467;&#26524;&#12290;&#36890;&#36807;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#35789;&#23884;&#20837;&#30340;GPT-2&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;ML-EAT&#26469;&#25581;&#31034;&#35821;&#35328;&#25216;&#26415;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20351;&#31038;&#20250;&#31185;&#23398;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01966v1 Announce Type: cross  Abstract: This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language mod
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HeteroKRLAttack&#30340;&#24378;&#21270;&#23398;&#20064;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#24322;&#36136;&#22270;&#36827;&#34892;&#39640;&#25928;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30772;&#22351;&#12290;&#36890;&#36807;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;Top-K&#31639;&#27861;&#20197;&#20943;&#23569;&#34892;&#21160;&#31354;&#38388;&#65292;&#25991;&#31456;&#30340;&#36129;&#29486;&#22312;&#20110;&#23454;&#29616;&#20102;&#23545;&#24322;&#36136;&#22270;&#30340;&#33410;&#28857;&#30340;&#20934;&#30830;&#20998;&#31867;&#20219;&#21153;&#30340;&#26377;&#25928;&#22320;&#25915;&#20987;&#31574;&#30053;&#30340;&#21457;&#29616;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#26174;&#33879;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#38477;&#20302;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;Top-K&#31639;&#27861;&#23545;&#25915;&#20987;&#24615;&#33021;&#26377;&#30528;&#20851;&#38190;&#30340;&#25552;&#21319;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.01964</link><description>&lt;p&gt;
Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01964
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HeteroKRLAttack&#30340;&#24378;&#21270;&#23398;&#20064;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#24322;&#36136;&#22270;&#36827;&#34892;&#39640;&#25928;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30772;&#22351;&#12290;&#36890;&#36807;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;Top-K&#31639;&#27861;&#20197;&#20943;&#23569;&#34892;&#21160;&#31354;&#38388;&#65292;&#25991;&#31456;&#30340;&#36129;&#29486;&#22312;&#20110;&#23454;&#29616;&#20102;&#23545;&#24322;&#36136;&#22270;&#30340;&#33410;&#28857;&#30340;&#20934;&#30830;&#20998;&#31867;&#20219;&#21153;&#30340;&#26377;&#25928;&#22320;&#25915;&#20987;&#31574;&#30053;&#30340;&#21457;&#29616;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#26174;&#33879;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#38477;&#20302;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;Top-K&#31639;&#27861;&#23545;&#25915;&#20987;&#24615;&#33021;&#26377;&#30528;&#20851;&#38190;&#30340;&#25552;&#21319;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01964v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have attracted substantial interest due to their exceptional performance on graph-based data. However, their robustness, especially on heterogeneous graphs, remains underexplored, particularly against adversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion black-box attack method for heterogeneous graphs. By integrating reinforcement learning with a Top-K algorithm to reduce the action space, our method efficiently identifies effective attack strategies to disrupt node classification tasks. We validate the effectiveness of HeteroKRLAttack through experiments on multiple heterogeneous graph datasets, showing significant reductions in classification accuracy compared to baseline methods. An ablation study underscores the critical role of the Top-K algorithm in enhancing attack performance. Our findings highlight potential vulnerabilities in current models and provide guidance for future d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#20154;&#20026;&#26412;&#30340;&#25968;&#25454;&#31185;&#23398;&#24037;&#20316;&#20013;&#30340;&#24433;&#21709;&#65292;&#20197;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#20026;&#26696;&#20363;&#30740;&#31350;&#23545;&#35937;&#12290;&#25991;&#31456;&#29305;&#21035;&#20851;&#27880;&#20102;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#25171;&#20987;&#32593;&#32476;&#35875;&#35328;&#21644;&#20419;&#36827;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#24037;&#20316;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#32452;&#32455;&#21644;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#22914;&#20309;&#21033;&#29992;&#24320;&#25918;&#24335;&#27169;&#22411;&#26469;&#20998;&#26512;&#22823;&#37327;&#20449;&#24687;&#24182;&#19988;&#20445;&#25345;&#24037;&#20316;&#30340;&#23458;&#35266;&#24615;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#29983;&#25104;&#24615;AI&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#65292;&#20026;AI&#22312;&#31038;&#20250;&#23618;&#38754;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2408.01962</link><description>&lt;p&gt;
The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#35752;&#20102;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#20154;&#20026;&#26412;&#30340;&#25968;&#25454;&#31185;&#23398;&#24037;&#20316;&#20013;&#30340;&#24433;&#21709;&#65292;&#20197;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#20026;&#26696;&#20363;&#30740;&#31350;&#23545;&#35937;&#12290;&#25991;&#31456;&#29305;&#21035;&#20851;&#27880;&#20102;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#25171;&#20987;&#32593;&#32476;&#35875;&#35328;&#21644;&#20419;&#36827;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#24037;&#20316;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#32452;&#32455;&#21644;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#22914;&#20309;&#21033;&#29992;&#24320;&#25918;&#24335;&#27169;&#22411;&#26469;&#20998;&#26512;&#22823;&#37327;&#20449;&#24687;&#24182;&#19988;&#20445;&#25345;&#24037;&#20316;&#30340;&#23458;&#35266;&#24615;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#29983;&#25104;&#24615;AI&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#65292;&#20026;AI&#22312;&#31038;&#20250;&#23618;&#38754;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01962v1 Announce Type: cross  Abstract: Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an int
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;&#65292;&#20197;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#20851;&#32852;&#38738;&#23569;&#24180;&#19982;&#31038;&#20250;&#38382;&#39064;&#65292;&#32780;&#22823;&#37096;&#20998;Golem&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21450;&#38738;&#23569;&#24180;&#30340;&#24773;&#20917;&#19979;&#20250;&#28041;&#21450;&#31038;&#20250;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#26292;&#21147;&#12289;&#33647;&#29289;&#20351;&#29992;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01961</link><description>&lt;p&gt;
Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;&#65292;&#20197;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#20851;&#32852;&#38738;&#23569;&#24180;&#19982;&#31038;&#20250;&#38382;&#39064;&#65292;&#32780;&#22823;&#37096;&#20998;Golem&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21450;&#38738;&#23569;&#24180;&#30340;&#24773;&#20917;&#19979;&#20250;&#28041;&#21450;&#31038;&#20250;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#26292;&#21147;&#12289;&#33647;&#29289;&#20351;&#29992;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01961v1 Announce Type: cross  Abstract: Popular and news media often portray teenagers with sensationalism, as both a risk to society and at risk from society. As AI begins to absorb some of the epistemic functions of traditional media, we study how teenagers in two countries speaking two languages: 1) are depicted by AI, and 2) how they would prefer to be depicted. Specifically, we study the biases about teenagers learned by static word embeddings (SWEs) and generative language models (GLMs), comparing these with the perspectives of adolescents living in the U.S. and Nepal. We find English-language SWEs associate teenagers with societal problems, and more than 50% of the 1,000 words most associated with teenagers in the pretrained GloVe SWE reflect such problems. Given prompts about teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss societal problems, most commonly violence, but also drug use, mental illness, and sexual taboo. Nepali models, while n
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnomalySD&#30340;&#22522;&#20110;Stable Diffusion&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#22810;&#31867;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#22823;&#37327;&#27491;&#24120;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#33021;&#22815;&#23545;&#19981;&#21516;&#23545;&#35937;&#28789;&#27963;&#36866;&#29992;&#30340;&#26816;&#27979;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2408.01960</link><description>&lt;p&gt;
AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnomalySD&#30340;&#22522;&#20110;Stable Diffusion&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#22810;&#31867;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#22823;&#37327;&#27491;&#24120;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#33021;&#22815;&#23545;&#19981;&#21516;&#23545;&#35937;&#28789;&#27963;&#36866;&#29992;&#30340;&#26816;&#27979;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01960v1 Announce Type: new  Abstract: Anomaly detection is a critical task in industrial manufacturing, aiming to identify defective parts of products. Most industrial anomaly detection methods assume the availability of sufficient normal data for training. This assumption may not hold true due to the cost of labeling or data privacy policies. Additionally, mainstream methods require training bespoke models for different objects, which incurs heavy costs and lacks flexibility in practice. To address these issues, we seek help from Stable Diffusion (SD) model due to its capability of zero/few-shot inpainting, which can be leveraged to inpaint anomalous regions as normal. In this paper, a few-shot multi-class anomaly detection framework that adopts Stable Diffusion model is proposed, named AnomalySD. To adapt SD to anomaly detection task, we design different hierarchical text descriptions and the foreground mask mechanism for fine-tuning SD. In the inference stage, to accurate
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#22823;&#26102;&#65292;&#26356;&#33021;&#21453;&#26144;&#31038;&#20250;&#20013;&#30340;&#20154;&#33080;&#35780;&#20215;&#20559;&#35265;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#31038;&#20250;&#20013;&#26222;&#36941;&#30340;&#20559;&#35265;&#31243;&#24230;&#19982;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#31243;&#24230;&#27491;&#30456;&#20851;&#12290;&#36825;&#34920;&#26126;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#23398;&#20064;&#20154;&#31867;&#8220;&#21487;&#35266;&#23519;&#8221;&#21644;&#8220;&#19981;&#21487;&#35266;&#23519;&#8221;&#29305;&#24449;&#35780;&#20215;&#26102;&#65292;&#26356;&#23481;&#26131;&#22797;&#21046;&#31038;&#20250;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2408.01959</link><description>&lt;p&gt;
Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;43&#20010;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#22823;&#26102;&#65292;&#26356;&#33021;&#21453;&#26144;&#31038;&#20250;&#20013;&#30340;&#20154;&#33080;&#35780;&#20215;&#20559;&#35265;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#31038;&#20250;&#20013;&#26222;&#36941;&#30340;&#20559;&#35265;&#31243;&#24230;&#19982;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#31243;&#24230;&#27491;&#30456;&#20851;&#12290;&#36825;&#34920;&#26126;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#23398;&#20064;&#20154;&#31867;&#8220;&#21487;&#35266;&#23519;&#8221;&#21644;&#8220;&#19981;&#21487;&#35266;&#23519;&#8221;&#29305;&#24449;&#35780;&#20215;&#26102;&#65292;&#26356;&#23481;&#26131;&#22797;&#21046;&#31038;&#20250;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01959v1 Announce Type: new  Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we u
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#35270;&#35273; grounding &#33021;&#21147;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#25351;&#20196;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#36890;&#36807;&#35270;&#35273; grounding&#65292;&#20316;&#32773;&#33719;&#24471;&#20102;&#19968;&#20010;&#19982;&#25351;&#20196;&#20013;&#25351;&#31034;&#30340;&#30446;&#26631;&#29289;&#20307;&#30456;&#23545;&#24212;&#30340;&#23545;&#35937;&#23454;&#20307;&#32622;&#20449;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;VLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;RL&#20013;&#65306;&#19968;&#26159;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#22270;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35937;&#23450;&#21521;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#20195;&#29702;&#26397;&#21521;&#30446;&#26631;&#29289;&#20307;&#65307;&#20108;&#26159;&#20351;&#29992;&#32622;&#20449;&#22270;&#20026;&#20195;&#29702;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;&#35821;&#35328;&#23884;&#20837;&#26356;&#21152;&#32479;&#19968;&#21644;&#21487;&#35775;&#38382;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#23545;&#27604;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#21644;&#25351;&#20196;&#12290;</title><link>https://arxiv.org/abs/2408.01942</link><description>&lt;p&gt;
Visual Grounding for Object-Level Generalization in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01942
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#35270;&#35273; grounding &#33021;&#21147;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#25351;&#20196;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#36890;&#36807;&#35270;&#35273; grounding&#65292;&#20316;&#32773;&#33719;&#24471;&#20102;&#19968;&#20010;&#19982;&#25351;&#20196;&#20013;&#25351;&#31034;&#30340;&#30446;&#26631;&#29289;&#20307;&#30456;&#23545;&#24212;&#30340;&#23545;&#35937;&#23454;&#20307;&#32622;&#20449;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;VLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;RL&#20013;&#65306;&#19968;&#26159;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#22270;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35937;&#23450;&#21521;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#20195;&#29702;&#26397;&#21521;&#30446;&#26631;&#29289;&#20307;&#65307;&#20108;&#26159;&#20351;&#29992;&#32622;&#20449;&#22270;&#20026;&#20195;&#29702;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;&#35821;&#35328;&#23884;&#20837;&#26356;&#21152;&#32479;&#19968;&#21644;&#21487;&#35775;&#38382;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#23545;&#27604;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#21644;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01942v1 Announce Type: cross  Abstract: Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20915;&#31574;&#39118;&#38505;&#21644;&#22797;&#21512;&#39118;&#38505;&#30340;&#27010;&#24565;&#65292;&#29992;&#20197;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#30340;&#24773;&#24418;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20004;&#32423;&#25512;&#29702;&#26550;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#35813;&#30740;&#31350;&#20026;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#39118;&#38505;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2408.01935</link><description>&lt;p&gt;
Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01935
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20915;&#31574;&#39118;&#38505;&#21644;&#22797;&#21512;&#39118;&#38505;&#30340;&#27010;&#24565;&#65292;&#29992;&#20197;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#30340;&#24773;&#24418;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20004;&#32423;&#25512;&#29702;&#26550;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#35813;&#30740;&#31350;&#20026;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#39118;&#38505;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01935v1 Announce Type: cross  Abstract: Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;DiReCT&#30340;&#20020;&#24202;&#31508;&#35760;&#35786;&#26029;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#23427;&#36890;&#36807;521&#20221;&#20020;&#24202;&#31508;&#35760;&#30340;&#32454;&#33268;&#26631;&#27880;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#23545;&#20154;&#31867;&#21307;&#29983;&#30340;&#35299;&#37322;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#35786;&#26029;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;LLMs&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2408.01933</link><description>&lt;p&gt;
DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;DiReCT&#30340;&#20020;&#24202;&#31508;&#35760;&#35786;&#26029;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#23427;&#36890;&#36807;521&#20221;&#20020;&#24202;&#31508;&#35760;&#30340;&#32454;&#33268;&#26631;&#27880;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#23545;&#20154;&#31867;&#21307;&#29983;&#30340;&#35299;&#37322;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#35786;&#26029;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;LLMs&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35786;&#26029;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01933v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#22810;&#36890;&#36947;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#33021;&#22815;&#21033;&#29992;&#21253;&#21547;&#28857;&#20987;&#34892;&#20026;&#22312;&#20869;&#30340;&#22810;&#36890;&#36947;&#20449;&#24687;&#36827;&#34892;&#26597;&#35810;&#24847;&#22270;&#30340;&#20998;&#31867;&#65292;&#23588;&#20854;&#22312;&#35299;&#20915;&#38271;&#23614;&#31867;&#21035;&#20135;&#21697;&#30340;&#26597;&#35810;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01928</link><description>&lt;p&gt;
A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01928
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#22810;&#36890;&#36947;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#33021;&#22815;&#21033;&#29992;&#21253;&#21547;&#28857;&#20987;&#34892;&#20026;&#22312;&#20869;&#30340;&#22810;&#36890;&#36947;&#20449;&#24687;&#36827;&#34892;&#26597;&#35810;&#24847;&#22270;&#30340;&#20998;&#31867;&#65292;&#23588;&#20854;&#22312;&#35299;&#20915;&#38271;&#23614;&#31867;&#21035;&#20135;&#21697;&#30340;&#26597;&#35810;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01928v1 Announce Type: cross  Abstract: Query intent classification is an essential module for customers to find desired products on the e-commerce application quickly. Most existing query intent classification methods rely on the users' click behavior as a supervised signal to construct training samples. However, these methods based entirely on posterior labels may lead to serious category imbalance problems because of the Matthew effect in click samples. Compared with popular categories, it is difficult for products under long-tail categories to obtain traffic and user clicks, which makes the models unable to detect users' intent for products under long-tail categories. This in turn aggravates the problem that long-tail categories cannot obtain traffic, forming a vicious circle. In addition, due to the randomness of the user's click, the posterior label is unstable for the query with similar semantics, which makes the model very sensitive to the input, leading to an unstab
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;MAO&#26694;&#26550;&#65292;&#21033;&#29992;&#22810; agent &#21327;&#35843;&#26469;&#39640;&#25928;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#21019;&#26032;&#25552;&#31034;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810; agent &#21327;&#20316;&#20013;&#30340;&#25928;&#29575;&#65292;&#36825;&#21487;&#33021;&#20026;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.01916</link><description>&lt;p&gt;
MAO: A Framework for Process Model Generation with Multi-Agent Orchestration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;MAO&#26694;&#26550;&#65292;&#21033;&#29992;&#22810; agent &#21327;&#35843;&#26469;&#39640;&#25928;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#21019;&#26032;&#25552;&#31034;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810; agent &#21327;&#20316;&#20013;&#30340;&#25928;&#29575;&#65292;&#36825;&#21487;&#33021;&#20026;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01916v1 Announce Type: new  Abstract: Process models are frequently used in software engineering to describe business requirements, guide software testing and control system improvement. However, traditional process modeling methods often require the participation of numerous experts, which is expensive and time-consuming. Therefore, the exploration of a more efficient and cost-effective automated modeling method has emerged as a focal point in current research. This article explores a framework for automatically generating process models with multi-agent orchestration (MAO), aiming to enhance the efficiency of process modeling and offer valuable insights for domain experts. Our framework MAO leverages large language models as the cornerstone for multi-agent, employing an innovative prompt strategy to ensure efficient collaboration among multi-agent. Specifically, 1) generation. The first phase of MAO is to generate a slightly rough process model from the text description; 2
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27493;&#24494;&#20998;&#31639;&#23376;&#30340;&#26032;&#22411;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#22312;Kirchhoff&#26438;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2408.01914</link><description>&lt;p&gt;
Partial-differential-algebraic equations of nonlinear dynamics by Physics-Informed Neural-Network: (I) Operator splitting and framework assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27493;&#24494;&#20998;&#31639;&#23376;&#30340;&#26032;&#22411;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#22312;Kirchhoff&#26438;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01914v1 Announce Type: cross  Abstract: Several forms for constructing novel physics-informed neural-networks (PINN) for the solution of partial-differential-algebraic equations based on derivative operator splitting are proposed, using the nonlinear Kirchhoff rod as a prototype for demonstration. The open-source DeepXDE is likely the most well documented framework with many examples. Yet, we encountered some pathological problems and proposed novel methods to resolve them. Among these novel methods are the PDE forms, which evolve from the lower-level form with fewer unknown dependent variables to higher-level form with more dependent variables, in addition to those from lower-level forms. Traditionally, the highest-level form, the balance-of-momenta form, is the starting point for (hand) deriving the lowest-level form through a tedious (and error prone) process of successive substitutions. The next step in a finite element method is to discretize the lowest-level form upon 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;&#12298;Artificial Intelligence Disclosure (AID) Framework&#12299;&#30340;&#26631;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#23398;&#26415;&#21644;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20351;&#29992;&#25552;&#20379;&#36879;&#26126;&#21644;&#35814;&#32454;&#30340;&#25259;&#38706;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2408.01904</link><description>&lt;p&gt;
The Artificial Intelligence Disclosure (AID) Framework: An Introduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#21517;&#20026;&#12298;Artificial Intelligence Disclosure (AID) Framework&#12299;&#30340;&#26631;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#23398;&#26415;&#21644;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20351;&#29992;&#25552;&#20379;&#36879;&#26126;&#21644;&#35814;&#32454;&#30340;&#25259;&#38706;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01904v1 Announce Type: cross  Abstract: As the use of Generative Artificial Intelligence tools have grown in higher education and research, there have been increasing calls for transparency and granularity around the use and attribution of the use of these tools. Thus far, this need has been met via the recommended inclusion of a note, with little to no guidance on what the note itself should include. This has been identified as a problem to the use of AI in academic and research contexts. This article introduces The Artificial Intelligence Disclosure (AID) Framework, a standard, comprehensive, and detailed framework meant to inform the development and writing of GenAI disclosure for education and research.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28436;&#21592;-&#25209;&#35780;&#32773;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20462;&#25913;&#32473;&#23450;&#35821;&#38899;&#20449;&#21495;&#30340;&#35843;&#20540;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#35782;&#21035;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#36830;&#32493;&#27573;&#65292;&#20197;&#38142;&#25509;&#21040;&#20154;&#31867;&#30340;&#24773;&#24863;&#24863;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#19968;&#32452;&#20108;&#39033;&#20998;&#24067;&#38543;&#26426;&#21464;&#37327;&#30340;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#24212;&#29992;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#20808;&#39564;&#20197;&#30830;&#20445;&#36830;&#32493;&#24615;&#65292;&#20174;&#32780;&#39044;&#27979;&#24773;&#24863;&#31867;&#21035;&#12290;&#36890;&#36807;&#20462;&#25913;&#34987;&#36974;&#25377;&#37096;&#20998;&#30340;&#35843;&#20540;&#29305;&#24449;&#26469;&#25552;&#39640;&#30446;&#26631;&#24773;&#24863;&#30340;&#35780;&#20998;&#65292;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#23558;&#20462;&#25913;&#30340;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#26469;&#35757;&#32451;&#19968;&#20010;&#28436;&#21592;-&#25209;&#35780;&#32773;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.01892</link><description>&lt;p&gt;
Re-ENACT: Reinforcement Learning for Emotional Speech Generation using Actor-Critic Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28436;&#21592;-&#25209;&#35780;&#32773;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20462;&#25913;&#32473;&#23450;&#35821;&#38899;&#20449;&#21495;&#30340;&#35843;&#20540;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#35782;&#21035;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#36830;&#32493;&#27573;&#65292;&#20197;&#38142;&#25509;&#21040;&#20154;&#31867;&#30340;&#24773;&#24863;&#24863;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#19968;&#32452;&#20108;&#39033;&#20998;&#24067;&#38543;&#26426;&#21464;&#37327;&#30340;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#24212;&#29992;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#20808;&#39564;&#20197;&#30830;&#20445;&#36830;&#32493;&#24615;&#65292;&#20174;&#32780;&#39044;&#27979;&#24773;&#24863;&#31867;&#21035;&#12290;&#36890;&#36807;&#20462;&#25913;&#34987;&#36974;&#25377;&#37096;&#20998;&#30340;&#35843;&#20540;&#29305;&#24449;&#26469;&#25552;&#39640;&#30446;&#26631;&#24773;&#24863;&#30340;&#35780;&#20998;&#65292;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#23558;&#20462;&#25913;&#30340;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#26469;&#35757;&#32451;&#19968;&#20010;&#28436;&#21592;-&#25209;&#35780;&#32773;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01892v1 Announce Type: cross  Abstract: In this paper, we propose the first method to modify the prosodic features of a given speech signal using actor-critic reinforcement learning strategy. Our approach uses a Bayesian framework to identify contiguous segments of importance that links segments of the given utterances to perception of emotions in humans. We train a neural network to produce the variational posterior of a collection of Bernoulli random variables; our model applies a Markov prior on it to ensure continuity. A sample from this distribution is used for downstream emotion prediction. Further, we train the neural network to predict a soft assignment over emotion categories as the target variable. In the next step, we modify the prosodic features (pitch, intensity, and rhythm) of the masked segment to increase the score of target emotion. We employ an actor-critic reinforcement learning to train the prosody modifier by discretizing the space of modifications. Furt
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;FULORA&#27169;&#22411;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;-&#25506;&#32034;&#26426;&#21046;&#65292;&#20351;&#29992;&#24102;&#26377;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#21452;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#22312;&#39640;&#23618;&#27425;&#20195;&#29702;&#30340;&#25351;&#24341;&#19979;&#65292;&#20302;&#23618;&#27425;&#20195;&#29702;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#22312;&#30693;&#35782;&#22270;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#22312;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#26102;&#23384;&#22312;&#30340;&#20195;&#29702;&#26089;&#26399;&#38590;&#20197;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#21644;&#29305;&#23450;&#20110;&#31232;&#30095;&#30693;&#35782;&#22270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01880</link><description>&lt;p&gt;
Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;FULORA&#27169;&#22411;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;-&#25506;&#32034;&#26426;&#21046;&#65292;&#20351;&#29992;&#24102;&#26377;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#21452;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#22312;&#39640;&#23618;&#27425;&#20195;&#29702;&#30340;&#25351;&#24341;&#19979;&#65292;&#20302;&#23618;&#27425;&#20195;&#29702;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#22312;&#30693;&#35782;&#22270;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#22312;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#26102;&#23384;&#22312;&#30340;&#20195;&#29702;&#26089;&#26399;&#38590;&#20197;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#21644;&#29305;&#23450;&#20110;&#31232;&#30095;&#30693;&#35782;&#22270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01880v1 Announce Type: new  Abstract: Recent years, multi-hop reasoning has been widely studied for knowledge graph (KG) reasoning due to its efficacy and interpretability. However, previous multi-hop reasoning approaches are subject to two primary shortcomings. First, agents struggle to learn effective and robust policies at the early phase due to sparse rewards. Second, these approaches often falter on specific datasets like sparse knowledge graphs, where agents are required to traverse lengthy reasoning paths. To address these problems, we propose a multi-hop reasoning model with dual agents based on hierarchical reinforcement learning (HRL), which is named FULORA. FULORA tackles the above reasoning challenges by eFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks on the simplified knowledge graph to provide stage-wise hints for the low-level agent walking on the original knowledge graph. In this framework, the low-level agent optimizes a value 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20869;&#20998;&#24067;&#25968;&#25454;&#20316;&#20026;&#27491;&#20363;&#30340;&#21487;&#38752; semi-supervised &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#31867;&#20998;&#24067;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23384;&#22312; out-of-distribution (OOD) &#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25991;&#31456;&#26368;&#22823;&#21270;&#20869;&#20998;&#24067;&#25968;&#25454;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#24182;&#36890;&#36807;&#21487;&#21464;&#31995;&#25968;&#30340;&#26102;&#38388;&#34920;&#26377;&#25928;&#22320;&#32858;&#21512;&#30456;&#24212;&#30340;&#36127;&#26679;&#12290;</title><link>https://arxiv.org/abs/2408.01872</link><description>&lt;p&gt;
Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20869;&#20998;&#24067;&#25968;&#25454;&#20316;&#20026;&#27491;&#20363;&#30340;&#21487;&#38752; semi-supervised &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#31867;&#20998;&#24067;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23384;&#22312; out-of-distribution (OOD) &#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25991;&#31456;&#26368;&#22823;&#21270;&#20869;&#20998;&#24067;&#25968;&#25454;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#24182;&#36890;&#36807;&#21487;&#21464;&#31995;&#25968;&#30340;&#26102;&#38388;&#34920;&#26377;&#25928;&#22320;&#32858;&#21512;&#30456;&#24212;&#30340;&#36127;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01872v1 Announce Type: cross  Abstract: Semi-supervised learning methods have shown promising results in solving many practical problems when only a few labels are available. The existing methods assume that the class distributions of labeled and unlabeled data are equal; however, their performances are significantly degraded in class distribution mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled data. Previous safe semi-supervised learning studies have addressed this problem by making OOD data less likely to affect training based on labeled data. However, even if the studies effectively filter out the unnecessary OOD data, they can lose the basic information that all data share regardless of class. To this end, we propose to apply a self-supervised contrastive learning approach to fully exploit a large amount of unlabeled data. We also propose a contrastive loss function with coefficient schedule to aggregate as an anchor the labeled negative ex
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MALADE&#31995;&#32479;&#21033;&#29992;LLM&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21327;&#20316;&#22810;Agent&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;Pharmacovigilance&#39046;&#22495;&#30340;&#26377;&#25928;Ade extraction&#12290;</title><link>https://arxiv.org/abs/2408.01869</link><description>&lt;p&gt;
MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;MALADE&#31995;&#32479;&#21033;&#29992;LLM&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21327;&#20316;&#22810;Agent&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;Pharmacovigilance&#39046;&#22495;&#30340;&#26377;&#25928;Ade extraction&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01869v1 Announce Type: cross  Abstract: In the era of Large Language Models (LLMs), given their remarkable text understanding and generation abilities, there is an unprecedented opportunity to develop new, LLM-based methods for trustworthy medical knowledge synthesis, extraction and summarization. This paper focuses on the problem of Pharmacovigilance (PhV), where the significance and challenges lie in identifying Adverse Drug Events (ADEs) from diverse text sources, such as medical literature, clinical notes, and drug labels. Unfortunately, this task is hindered by factors including variations in the terminologies of drugs and outcomes, and ADE descriptions often being buried in large amounts of narrative text. We present MALADE, the first effective collaborative multi-agent system powered by LLM with Retrieval Augmented Generation for ADE extraction from drug label data. This technique involves augmenting a query to an LLM with relevant information extracted from text reso
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ST-SACLF&#27169;&#22411;&#65292;&#36890;&#36807;Style Transfer&#32467;&#21512;AdaIN&#29983;&#25104;&#25968;&#25454;&#65292;&#32467;&#21512;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21160;&#24577;&#26679;&#26412;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#33402;&#26415;&#20316;&#21697;&#30340;&#39640;&#31934;&#24230;&#20998;&#31867;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01827</link><description>&lt;p&gt;
ST-SACLF: Style Transfer Informed Self-Attention Classifier for Bias-Aware Painting Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ST-SACLF&#27169;&#22411;&#65292;&#36890;&#36807;Style Transfer&#32467;&#21512;AdaIN&#29983;&#25104;&#25968;&#25454;&#65292;&#32467;&#21512;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21160;&#24577;&#26679;&#26412;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#33402;&#26415;&#20316;&#21697;&#30340;&#39640;&#31934;&#24230;&#20998;&#31867;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01827v1 Announce Type: new  Abstract: Painting classification plays a vital role in organizing, finding, and suggesting artwork for digital and classic art galleries. Existing methods struggle with adapting knowledge from the real world to artistic images during training, leading to poor performance when dealing with different datasets. Our innovation lies in addressing these challenges through a two-step process. First, we generate more data using Style Transfer with Adaptive Instance Normalization (AdaIN), bridging the gap between diverse styles. Then, our classifier gains a boost with feature-map adaptive spatial attention modules, improving its understanding of artistic details. Moreover, we tackle the problem of imbalanced class representation by dynamically adjusting augmented samples. Through a dual-stage process involving careful hyperparameter search and model fine-tuning, we achieve an impressive 87.24\% accuracy using the ResNet-50 backbone over 40 training epochs
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ALIF&#65292;&#19968;&#31181;&#20381;&#25176;&#21452;&#26041;&#21521;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#19982;&#35821;&#38899;&#33258;&#21160;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#28040;&#32791;&#22823;&#37327;&#26597;&#35810;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#26377;&#25928;&#30340;&#23545;&#25239;&#35821;&#38899;&#25915;&#20987;&#65292;&#24182;&#23545;ASR&#27169;&#22411;&#26356;&#26032;&#20855;&#26377;&#36739;&#24378;&#30340;&#25239;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01808</link><description>&lt;p&gt;
ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;ALIF&#65292;&#19968;&#31181;&#20381;&#25176;&#21452;&#26041;&#21521;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#19982;&#35821;&#38899;&#33258;&#21160;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#28040;&#32791;&#22823;&#37327;&#26597;&#35810;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#26377;&#25928;&#30340;&#23545;&#25239;&#35821;&#38899;&#25915;&#20987;&#65292;&#24182;&#23545;ASR&#27169;&#22411;&#26356;&#26032;&#20855;&#26377;&#36739;&#24378;&#30340;&#25239;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01808v1 Announce Type: cross  Abstract: Extensive research has revealed that adversarial examples (AE) pose a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic speech recognition (ASR) system. However, these attacks typically involve many queries to the ASR, resulting in substantial costs. Moreover, AE-based adversarial audio samples are susceptible to ASR updates. In this paper, we identify the root cause of these limitations, namely the inability to construct AE attack samples directly around the decision boundary of deep learning (DL) models. Building on this observation, we propose ALIF, the first black-box adversarial linguistic feature-based attack pipeline. We leverage the reciprocal process of text-to-speech (TTS) and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides. Based on the ALIF pi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24635;&#32467;&#20102;&#26234;&#33021;&#21046;&#36896;&#20113;&#26381;&#21153;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#24182;&#23450;&#20041;&#20102;11&#20010;&#32508;&#21512;&#32771;&#34385;&#21040;&#19977;&#26041;&#21442;&#19982;&#32773;&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#20998;&#25955;&#30340;&#20248;&#21270;&#25351;&#26631;&#21450;&#38750;&#26631;&#20934;&#21270;&#30340;&#23450;&#20041;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#26234;&#33021;&#21046;&#36896;&#24179;&#21488;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01795</link><description>&lt;p&gt;
Review of Cloud Service Composition for Intelligent Manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24635;&#32467;&#20102;&#26234;&#33021;&#21046;&#36896;&#20113;&#26381;&#21153;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#24182;&#23450;&#20041;&#20102;11&#20010;&#32508;&#21512;&#32771;&#34385;&#21040;&#19977;&#26041;&#21442;&#19982;&#32773;&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#20998;&#25955;&#30340;&#20248;&#21270;&#25351;&#26631;&#21450;&#38750;&#26631;&#20934;&#21270;&#30340;&#23450;&#20041;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#26234;&#33021;&#21046;&#36896;&#24179;&#21488;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01795v1 Announce Type: new  Abstract: Intelligent manufacturing is a new model that uses advanced technologies such as the Internet of Things, big data, and artificial intelligence to improve the efficiency and quality of manufacturing production. As an important support to promote the transformation and upgrading of the manufacturing industry, cloud service optimization has received the attention of researchers. In recent years, remarkable research results have been achieved in this field. For the sustainability of intelligent manufacturing platforms, in this paper we summarize the process of cloud service optimization for intelligent manufacturing. Further, to address the problems of dispersed optimization indicators and nonuniform/unstandardized definitions in the existing research, 11 optimization indicators that take into account three-party participant subjects are defined from the urgent requirements of the sustainable development of intelligent manufacturing platform
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#26500;&#24314;&#19968;&#20010;&#27491;&#24335;&#27010;&#24565;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#32593;&#32476;&#31354;&#38388;&#20013;&#23448;&#26041;&#34892;&#20026;&#20307;&#21450;&#20854;&#32593;&#32476;&#27963;&#21160;&#29616;&#35937;&#30340;&#23450;&#24615;&#24314;&#27169;&#65292;&#26088;&#22312;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#20998;&#26512;&#30340;&#31185;&#23398;&#24615;&#12290;&#27492;&#26694;&#26550;&#36890;&#36807;&#36830;&#25509;&#29616;&#26377;&#32593;&#32476;&#23433;&#20840;&#27010;&#24565;&#26694;&#26550;&#19982;&#27861;&#24459;&#27861;&#35268;&#12289;&#25919;&#24220;&#37096;&#38376;&#31561;&#21608;&#36793;&#39046;&#22495;&#65292;&#20419;&#36827;&#20102;&#22810;&#28304;&#25968;&#25454;&#30340;&#19968;&#33268;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#25512;&#29702;&#20197;&#21450;&#30456;&#20851;&#24773;&#25253;&#30340;&#33258;&#21160;&#25552;&#21462;&#21644;&#20877;&#21033;&#29992;&#65292;&#23545;&#20110;&#25552;&#21319;&#22269;&#23478;&#23433;&#20840;&#39046;&#22495;&#30340;&#24773;&#25253;&#25910;&#38598;&#21644;&#20998;&#26512;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.01787</link><description>&lt;p&gt;
Towards an ontology of state actors in cyberspace
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#26500;&#24314;&#19968;&#20010;&#27491;&#24335;&#27010;&#24565;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#32593;&#32476;&#31354;&#38388;&#20013;&#23448;&#26041;&#34892;&#20026;&#20307;&#21450;&#20854;&#32593;&#32476;&#27963;&#21160;&#29616;&#35937;&#30340;&#23450;&#24615;&#24314;&#27169;&#65292;&#26088;&#22312;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#20998;&#26512;&#30340;&#31185;&#23398;&#24615;&#12290;&#27492;&#26694;&#26550;&#36890;&#36807;&#36830;&#25509;&#29616;&#26377;&#32593;&#32476;&#23433;&#20840;&#27010;&#24565;&#26694;&#26550;&#19982;&#27861;&#24459;&#27861;&#35268;&#12289;&#25919;&#24220;&#37096;&#38376;&#31561;&#21608;&#36793;&#39046;&#22495;&#65292;&#20419;&#36827;&#20102;&#22810;&#28304;&#25968;&#25454;&#30340;&#19968;&#33268;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#25512;&#29702;&#20197;&#21450;&#30456;&#20851;&#24773;&#25253;&#30340;&#33258;&#21160;&#25552;&#21462;&#21644;&#20877;&#21033;&#29992;&#65292;&#23545;&#20110;&#25552;&#21319;&#22269;&#23478;&#23433;&#20840;&#39046;&#22495;&#30340;&#24773;&#25253;&#25910;&#38598;&#21644;&#20998;&#26512;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01787v1 Announce Type: cross  Abstract: To improve cyber threat analysis practices in cybersecurity, I present a plan to build a formal ontological representation of state actors in cyberspace and of cyber operations. I argue that modelling these phenomena via ontologies allows for coherent integration of data coming from diverse sources, automated reasoning over such data, as well as intelligence extraction and reuse from and of them. Existing ontological tools in cybersecurity can be ameliorated by connecting them to neighboring domains such as law, regulations, governmental institutions, and documents. In this paper, I propose metrics to evaluate currently existing ontological tools to create formal representations in the cybersecurity domain, and I provide a plan to develop and extend them when they are lacking.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDA&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#26469;&#39044;&#27979;&#39550;&#39542;&#21592;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#26088;&#22312;&#36890;&#36807;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#21306;&#22495;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01774</link><description>&lt;p&gt;
STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDA&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#26469;&#39044;&#27979;&#39550;&#39542;&#21592;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#26088;&#22312;&#36890;&#36807;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#21306;&#22495;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01774v1 Announce Type: new  Abstract: Accurate behavior prediction for vehicles is essential but challenging for autonomous driving. Most existing studies show satisfying performance under regular scenarios, but most neglected safety-critical scenarios. In this study, a spatio-temporal dual-encoder network named STDA for safety-critical scenarios was developed. Considering the exceptional capabilities of human drivers in terms of situational awareness and comprehending risks, driver attention was incorporated into STDA to facilitate swift identification of the critical regions, which is expected to improve both performance and interpretability. STDA contains four parts: the driver attention prediction module, which predicts driver attention; the fusion module designed to fuse the features between driver attention and raw images; the temporary encoder module used to enhance the capability to interpret dynamic scenes; and the behavior prediction module to predict the behavior.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#8212;&#8212;&#29992;&#20110;&#31934;&#30830;&#35782;&#21035;&#21644;&#35786;&#26029;&#27700;&#31291;&#21494;&#29255;&#30142;&#30149;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#26377;&#25928;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#19988;&#36890;&#36807;&#28155;&#21152;&#20840;&#36830;&#25509;&#23618;&#21644;&#20351;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2408.01752</link><description>&lt;p&gt;
Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;ShuffleNet&#12289;MobileNetV2&#21644;EfficientNet-B0&#8212;&#8212;&#29992;&#20110;&#31934;&#30830;&#35782;&#21035;&#21644;&#35786;&#26029;&#27700;&#31291;&#21494;&#29255;&#30142;&#30149;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#26377;&#25928;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#19988;&#36890;&#36807;&#28155;&#21152;&#20840;&#36830;&#25509;&#23618;&#21644;&#20351;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01752v1 Announce Type: new  Abstract: Rice plays a vital role as a primary food source for over half of the world's population, and its production is critical for global food security. Nevertheless, rice cultivation is frequently affected by various diseases that can severely decrease yield and quality. Therefore, early and accurate detection of rice diseases is necessary to prevent their spread and minimize crop losses. In this research, we explore three mobile-compatible CNN architectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice leaf disease classification. These models are selected due to their compatibility with mobile devices, as they demand less computational power and memory compared to other CNN models. To enhance the performance of the three models, we added two fully connected layers separated by a dropout layer. We used early stop creation to prevent the model from being overfiting. The results of the study showed that the best performance wa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;LAM3D&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#20110;Pyramid Vision Transformer v2 (PVTv2)&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23545;Monocular 3D Object Detection&#20219;&#21153;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#22312;KITTI 3D Object Detection Benchmark&#19978;&#39564;&#35777;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2408.01739</link><description>&lt;p&gt;
LAM3D: Leveraging Attention for Monocular 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;LAM3D&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#20110;Pyramid Vision Transformer v2 (PVTv2)&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23545;Monocular 3D Object Detection&#20219;&#21153;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#22312;KITTI 3D Object Detection Benchmark&#19978;&#39564;&#35777;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01739v1 Announce Type: new  Abstract: Since the introduction of the self-attention mechanism and the adoption of the Transformer architecture for Computer Vision tasks, the Vision Transformer-based architectures gained a lot of popularity in the field, being used for tasks such as image classification, object detection and image segmentation. However, efficiently leveraging the attention mechanism in vision transformers for the Monocular 3D Object Detection task remains an open question. In this paper, we present LAM3D, a framework that Leverages self-Attention mechanism for Monocular 3D object Detection. To do so, the proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as feature extraction backbone and 2D/3D detection machinery. We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods. Moreover, due to the usage of sel
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#28857;&#12290;</title><link>https://arxiv.org/abs/2408.01736</link><description>&lt;p&gt;
Can LLMs predict the convergence of Stochastic Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01736v1 Announce Type: cross  Abstract: Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#38899;&#39057;&#20449;&#24687;&#21644;&#39044;&#29983;&#25104;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21767;&#24418;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#22836;&#35270;&#39057;&#12290;</title><link>https://arxiv.org/abs/2408.01732</link><description>&lt;p&gt;
Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#38899;&#39057;&#20449;&#24687;&#21644;&#39044;&#29983;&#25104;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21767;&#24418;&#21516;&#27493;&#19988;&#26102;&#38388;&#19968;&#33268;&#30340;&#35828;&#35805;&#22836;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01732v1 Announce Type: new  Abstract: Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23545;&#20351;&#29992;&#23039;&#21183;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#25216;&#26415;&#22312;&#34394;&#25311;&#29616;&#23454;&#39046;&#22495;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35843;&#26597;19&#31687;&#30456;&#20851;&#30740;&#31350;&#35770;&#25991;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#12289;&#20998;&#31867;&#31639;&#27861;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#20934;&#30830;&#24615;&#36825;&#19968;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#24471;&#20986;&#65292;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#23039;&#21183;&#20272;&#35745;&#39046;&#22495;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#25913;&#36827;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2408.01728</link><description>&lt;p&gt;
Survey on Emotion Recognition through Posture Detection and the possibility of its application in Virtual Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23545;&#20351;&#29992;&#23039;&#21183;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#25216;&#26415;&#22312;&#34394;&#25311;&#29616;&#23454;&#39046;&#22495;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35843;&#26597;19&#31687;&#30456;&#20851;&#30740;&#31350;&#35770;&#25991;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#12289;&#20998;&#31867;&#31639;&#27861;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#20934;&#30830;&#24615;&#36825;&#19968;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#24471;&#20986;&#65292;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#23039;&#21183;&#20272;&#35745;&#39046;&#22495;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#25913;&#36827;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01728v1 Announce Type: new  Abstract: A survey is presented focused on using pose estimation techniques in Emotional recognition using various technologies normal cameras, and depth cameras for real-time, and the potential use of VR and inputs including images, videos, and 3-dimensional poses described in vector space. We discussed 19 research papers collected from selected journals and databases highlighting their methodology, classification algorithm, and the used datasets that relate to emotion recognition and pose estimation. A benchmark has been made according to their accuracy as it was the most common performance measurement metric used. We concluded that the multimodal Approaches overall made the best accuracy and then we mentioned futuristic concerns that can improve the development of this research topic.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#33021;&#22815;&#27867;&#21270;&#21040;&#25152;&#26377;&#36755;&#20837;&#26679;&#26412;&#30340;&#25915;&#20987;&#24615;&#25200;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#21516;&#26102;&#29983;&#25104;&#25915;&#20987;&#24615;&#25200;&#21160;&#21644;&#20854;&#35299;&#37322;&#24615;&#22270;&#12290;</title><link>https://arxiv.org/abs/2408.01715</link><description>&lt;p&gt;
Joint Universal Adversarial Perturbations with Interpretations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#33021;&#22815;&#27867;&#21270;&#21040;&#25152;&#26377;&#36755;&#20837;&#26679;&#26412;&#30340;&#25915;&#20987;&#24615;&#25200;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#21516;&#26102;&#29983;&#25104;&#25915;&#20987;&#24615;&#25200;&#21160;&#21644;&#20854;&#35299;&#37322;&#24615;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01715v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19979;&#28216;&#36716;&#31227;&#25915;&#20987;&#8221;&#65288;DTA&#65289;&#30340;&#20840;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#32423;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21363;&#20351;&#26159;&#21516;&#19968;&#20010;&#36755;&#20837;&#22270;&#29255;&#65292;&#20063;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#29983;&#25104;&#19981;&#21516;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#27492;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01705</link><description>&lt;p&gt;
Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19979;&#28216;&#36716;&#31227;&#25915;&#20987;&#8221;&#65288;DTA&#65289;&#30340;&#20840;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#32423;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21363;&#20351;&#26159;&#21516;&#19968;&#20010;&#36755;&#20837;&#22270;&#29255;&#65292;&#20063;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#29983;&#25104;&#19981;&#21516;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#27492;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01705v1 Announce Type: new  Abstract: With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on \emph{sample-wise} transfer attacks and propose a novel attack method termed \emph{Downstream Transfer Attack (DTA)}. For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of the
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#21644;&#39564;&#35777;&#25991;&#26412;&#24418;&#24335;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#21355;&#26143;&#30005;&#23376;&#26495;&#21046;&#36896;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#25968;&#25454;&#20998;&#26512;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;KGs&#21644;LLMs&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01700</link><description>&lt;p&gt;
Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#21644;&#39564;&#35777;&#25991;&#26412;&#24418;&#24335;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#21355;&#26143;&#30005;&#23376;&#26495;&#21046;&#36896;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#25968;&#25454;&#20998;&#26512;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;KGs&#21644;LLMs&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01700v1 Announce Type: new  Abstract: Aerospace manufacturing companies, such as Thales Alenia Space, design, develop, integrate, verify, and validate products characterized by high complexity and low volume. They carefully document all phases for each product but analyses across products are challenging due to the heterogeneity and unstructured nature of the data in documents. In this paper, we propose a hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with Large Language Models (LLMs) to extract and validate data contained in these documents. We consider a case study focused on test data related to electronic boards for satellites. To do so, we extend the Semantic Sensor Network ontology. We store the metadata of the reports in a KG, while the actual test results are stored in parquet accessible via a Virtual Knowledge Graph. The validation process is managed using an LLM-based approach. We also conduct a benchmarking study to evaluate the performanc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;InfoIGL&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#22312;&#22270;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#23545;&#19981;&#21464;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#22806;&#29983;&#25104;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.01697</link><description>&lt;p&gt;
Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;InfoIGL&#26694;&#26550;&#36890;&#36807;&#32467;&#21512;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#22312;&#22270;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#23545;&#19981;&#21464;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#22806;&#29983;&#25104;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01697v1 Announce Type: cross  Abstract: Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31934;&#32454;&#21270;&#21028;&#21035;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#29983;&#25104;&#38899;&#20048;&#30340;&#36136;&#37327;&#65292;&#20854;&#20013;&#21253;&#21547;&#38024;&#23545;&#26059;&#24459;&#21644;&#33410;&#22863;&#30340;&#19987;&#38376;&#21028;&#21035;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26059;&#24459;&#21028;&#21035;&#22120;&#21644;&#33410;&#22863;&#21028;&#21035;&#22120;&#65292;&#38024;&#23545;&#24615;&#22320;&#25913;&#21892;&#20102;&#29983;&#25104;&#38899;&#20048;&#30340;&#26059;&#24459;&#21464;&#21270;&#21644;&#33410;&#22863;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2408.01696</link><description>&lt;p&gt;
Generating High-quality Symbolic Music Using Fine-grained Discriminators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31934;&#32454;&#21270;&#21028;&#21035;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#29983;&#25104;&#38899;&#20048;&#30340;&#36136;&#37327;&#65292;&#20854;&#20013;&#21253;&#21547;&#38024;&#23545;&#26059;&#24459;&#21644;&#33410;&#22863;&#30340;&#19987;&#38376;&#21028;&#21035;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26059;&#24459;&#21028;&#21035;&#22120;&#21644;&#33410;&#22863;&#21028;&#21035;&#22120;&#65292;&#38024;&#23545;&#24615;&#22320;&#25913;&#21892;&#20102;&#29983;&#25104;&#38899;&#20048;&#30340;&#26059;&#24459;&#21464;&#21270;&#21644;&#33410;&#22863;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01696v1 Announce Type: cross  Abstract: Existing symbolic music generation methods usually utilize discriminator to improve the quality of generated music via global perception of music. However, considering the complexity of information in music, such as rhythm and melody, a single discriminator cannot fully reflect the differences in these two primary dimensions of music. In this work, we propose to decouple the melody and rhythm from music, and design corresponding fine-grained discriminators to tackle the aforementioned issues. Specifically, equipped with a pitch augmentation strategy, the melody discriminator discerns the melody variations presented by the generated samples. By contrast, the rhythm discriminator, enhanced with bar-level relative positional encoding, focuses on the velocity of generated notes. Such a design allows the generator to be more explicitly aware of which aspects should be adjusted in the generated music, making it easier to mimic human-composed
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;TreeCSS&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#26041;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#37319;&#29992;&#26641;&#32467;&#26500;&#31574;&#30053;&#24182;&#20248;&#21270;&#25968;&#25454;&#37327;&#20998;&#37197;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#22810;&#20826;&#31169;&#26377;&#38598;&#20132;&#38598;&#65288;MPSI&#65289;&#21327;&#35758;&#20013;&#30340;&#26679;&#26412;&#23545;&#40784;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36873;&#25321;&#26680;&#24515;&#38598;&#65288;coreset selection, CSS&#65289;&#26469;&#32553;&#20943;&#26679;&#26412;&#25968;&#37327;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25972;&#20307;&#19978;&#25552;&#21319;&#20102;VFL&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01691</link><description>&lt;p&gt;
TreeCSS: An Efficient Framework for Vertical Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;TreeCSS&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#26041;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#37319;&#29992;&#26641;&#32467;&#26500;&#31574;&#30053;&#24182;&#20248;&#21270;&#25968;&#25454;&#37327;&#20998;&#37197;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#22810;&#20826;&#31169;&#26377;&#38598;&#20132;&#38598;&#65288;MPSI&#65289;&#21327;&#35758;&#20013;&#30340;&#26679;&#26412;&#23545;&#40784;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36873;&#25321;&#26680;&#24515;&#38598;&#65288;coreset selection, CSS&#65289;&#26469;&#32553;&#20943;&#26679;&#26412;&#25968;&#37327;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25972;&#20307;&#19978;&#25552;&#21319;&#20102;VFL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01691v1 Announce Type: cross  Abstract: Vertical federated learning (VFL) considers the case that the features of data samples are partitioned over different participants. VFL consists of two main steps, i.e., identify the common data samples for all participants (alignment) and train model using the aligned data samples (training). However, when there are many participants and data samples, both alignment and training become slow. As such, we propose TreeCSS as an efficient VFL framework that accelerates the two main steps. In particular, for sample alignment, we design an efficient multi-party private set intersection (MPSI) protocol called Tree-MPSI, which adopts a tree-based structure and a data-volume-aware scheduling strategy to parallelize alignment among the participants. As model training time scales with the number of data samples, we conduct coreset selection (CSS) to choose some representative data samples for training. Our CCS method adopts a clustering-based sc
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#27450;&#35784;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#26469;&#25512;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.01690</link><description>&lt;p&gt;
IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IDNet&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#27450;&#35784;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#26469;&#25512;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01690v1 Announce Type: new  Abstract: Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy.   In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection e
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#949;-&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#21487;&#25511;&#21435;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#30340;&#19981;&#21516;&#26399;&#26395;&#12290;</title><link>https://arxiv.org/abs/2408.01689</link><description>&lt;p&gt;
Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#949;-&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20687;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#21487;&#25511;&#21435;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#21435;&#23398;&#20064;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#30340;&#19981;&#21516;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01689v1 Announce Type: cross  Abstract: While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\varepsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\varepsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearni
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;radarODE&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;millimeter-wave&#38647;&#36798;&#20449;&#21495;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#26080;&#25509;&#35302;&#24335;ECG&#20449;&#21495;&#30340;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2408.01672</link><description>&lt;p&gt;
radarODE: An ODE-Embedded Deep Learning Model for Contactless ECG Reconstruction from Millimeter-Wave Radar
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;radarODE&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;millimeter-wave&#38647;&#36798;&#20449;&#21495;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#26080;&#25509;&#35302;&#24335;ECG&#20449;&#21495;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01672v1 Announce Type: cross  Abstract: Radar-based contactless cardiac monitoring has become a popular research direction recently, but the fine-grained electrocardiogram (ECG) signal is still hard to reconstruct from millimeter-wave radar signal. The key obstacle is to decouple the cardiac activities in the electrical domain (i.e., ECG) from that in the mechanical domain (i.e., heartbeat), and most existing research only uses pure data-driven methods to map such domain transformation as a black box. Therefore, this work first proposes a signal model for domain transformation, and then a novel deep learning framework called radarODE is designed to fuse the temporal and morphological features extracted from radar signals and generate ECG. In addition, ordinary differential equations are embedded in radarODE as a decoder to provide morphological prior, helping the convergence of the model training and improving the robustness under body movements. After being validated on the
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#31354;&#38388;&#20013;&#35821;&#20041;&#23646;&#24615;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026;SAT3D&#65292;&#23427;&#33021;&#22815;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#23548;&#20986;&#24182;&#32534;&#36753;&#20855;&#20307;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#22914;&#30007;&#20154;&#30340;&#32993;&#39035;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#23646;&#24615;&#21644;&#39118;&#26684;&#20195;&#30721;&#36890;&#36947;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25506;&#32034;&#20102;3D-aware StyleGAN&#22522;&#30784;&#29983;&#25104;&#22120;&#30340;&#39118;&#26684;&#31354;&#38388;&#65292;&#24182;&#20026;&#27599;&#20010;&#23646;&#24615;&#24314;&#31435;&#20102;&#25551;&#36848;&#31526;&#32452;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#27979;&#37327;&#27169;&#22359;&#65292;&#20197;&#22312;&#22270;&#20687;&#20013;&#23450;&#37327;&#25551;&#36848;&#23646;&#24615;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2408.01664</link><description>&lt;p&gt;
SAT3D: Image-driven Semantic Attribute Transfer in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#31354;&#38388;&#20013;&#35821;&#20041;&#23646;&#24615;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026;SAT3D&#65292;&#23427;&#33021;&#22815;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#23548;&#20986;&#24182;&#32534;&#36753;&#20855;&#20307;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#22914;&#30007;&#20154;&#30340;&#32993;&#39035;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#23646;&#24615;&#21644;&#39118;&#26684;&#20195;&#30721;&#36890;&#36947;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25506;&#32034;&#20102;3D-aware StyleGAN&#22522;&#30784;&#29983;&#25104;&#22120;&#30340;&#39118;&#26684;&#31354;&#38388;&#65292;&#24182;&#20026;&#27599;&#20010;&#23646;&#24615;&#24314;&#31435;&#20102;&#25551;&#36848;&#31526;&#32452;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#27979;&#37327;&#27169;&#22359;&#65292;&#20197;&#22312;&#22270;&#20687;&#20013;&#23450;&#37327;&#25551;&#36848;&#23646;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01664v1 Announce Type: new  Abstract: GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor group
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;SPORT&#26694;&#26550;&#36890;&#36807;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20998;&#20026;&#29289;&#20307;&#23450;&#20301;&#12289;&#30446;&#26631;&#24819;&#35937;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#19977;&#20010;&#37096;&#20998;&#65292;&#20197;&#21450;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#29289;&#20307;&#31354;&#38388;&#35782;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#25193;&#25955;&#30340;3D&#23039;&#24577;&#20272;&#35745;&#22120;&#20197;&#30830;&#20445;&#29289;&#29702;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#20165;&#22312;&#29289;&#20307;&#31867;&#22411;&#30340;&#23450;&#20041;&#20043;&#38388;&#36827;&#34892;&#20132;&#27969;&#65292;SPORT&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20805;&#20998;&#22320;&#21033;&#29992;&#24320;&#25918;&#29615;&#22659;&#29289;&#20307;&#23450;&#20301;&#21644;&#35782;&#21035;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#8220;&#24819;&#35937;&#8221;&#29289;&#20307;&#30456;&#23545;&#20110;&#21442;&#32771;&#29289;&#20307;&#30340;&#23039;&#24577;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#36890;&#29992;&#29615;&#22659;&#20013;&#20934;&#30830;&#22320;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307;&#65292;&#20174;&#32780;&#20026;&#23454;&#29616;&#36890;&#29992;&#30446;&#30340;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2408.01655</link><description>&lt;p&gt;
Stimulating Imagination: Towards General-purpose Object Rearrangement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;SPORT&#26694;&#26550;&#36890;&#36807;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20998;&#20026;&#29289;&#20307;&#23450;&#20301;&#12289;&#30446;&#26631;&#24819;&#35937;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#19977;&#20010;&#37096;&#20998;&#65292;&#20197;&#21450;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#29289;&#20307;&#31354;&#38388;&#35782;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#25193;&#25955;&#30340;3D&#23039;&#24577;&#20272;&#35745;&#22120;&#20197;&#30830;&#20445;&#29289;&#29702;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#20165;&#22312;&#29289;&#20307;&#31867;&#22411;&#30340;&#23450;&#20041;&#20043;&#38388;&#36827;&#34892;&#20132;&#27969;&#65292;SPORT&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20805;&#20998;&#22320;&#21033;&#29992;&#24320;&#25918;&#29615;&#22659;&#29289;&#20307;&#23450;&#20301;&#21644;&#35782;&#21035;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#8220;&#24819;&#35937;&#8221;&#29289;&#20307;&#30456;&#23545;&#20110;&#21442;&#32771;&#29289;&#20307;&#30340;&#23039;&#24577;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#36890;&#29992;&#29615;&#22659;&#20013;&#20934;&#30830;&#22320;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307;&#65292;&#20174;&#32780;&#20026;&#23454;&#29616;&#36890;&#29992;&#30446;&#30340;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01655v1 Announce Type: new  Abstract: General-purpose object placement is a fundamental capability of an intelligent generalist robot, i.e., being capable of rearranging objects following human instructions even in novel environments. To achieve this, we break the rearrangement down into three parts, including object localization, goal imagination and robot control, and propose a framework named SPORT. SPORT leverages pre-trained large vision models for broad semantic reasoning about objects, and learns a diffusion-based 3D pose estimator to ensure physically-realistic results. Only object types (to be moved or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object localization and recognition since no specific fine-tuning is needed for robotic scenarios. Furthermore, the diffusion-based estimator only need to "imagine" the poses of the moving and reference objects after the pl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Music2P&#30340;&#22810;&#27169;&#24577;AI&#39537;&#21160;&#24037;&#20855;&#65292;&#26088;&#22312;&#31616;&#21270;&#19987;&#36753;&#23553;&#38754;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#32467;&#21512;BLIP&#12289;LP-music-caps&#12289;LoRA&#21644;ControlNet&#31561;&#25216;&#26415;&#65292;Music2P&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#38899;&#20048;&#20316;&#20026;&#35774;&#35745;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#19987;&#36753;&#23553;&#38754;&#65292;&#24182;&#25903;&#25345;&#39640;&#25928;&#21019;&#24314;QR&#30721;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35774;&#35745;&#38376;&#27099;&#65292;&#25552;&#39640;&#20102;&#35774;&#35745;&#30340;&#20415;&#25463;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>https://arxiv.org/abs/2408.01651</link><description>&lt;p&gt;
Music2P: A Multi-Modal AI-Driven Tool for Simplifying Album Cover Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Music2P&#30340;&#22810;&#27169;&#24577;AI&#39537;&#21160;&#24037;&#20855;&#65292;&#26088;&#22312;&#31616;&#21270;&#19987;&#36753;&#23553;&#38754;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#32467;&#21512;BLIP&#12289;LP-music-caps&#12289;LoRA&#21644;ControlNet&#31561;&#25216;&#26415;&#65292;Music2P&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#38899;&#20048;&#20316;&#20026;&#35774;&#35745;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#19987;&#36753;&#23553;&#38754;&#65292;&#24182;&#25903;&#25345;&#39640;&#25928;&#21019;&#24314;QR&#30721;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35774;&#35745;&#38376;&#27099;&#65292;&#25552;&#39640;&#20102;&#35774;&#35745;&#30340;&#20415;&#25463;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01651v1 Announce Type: cross  Abstract: In today's music industry, album cover design is as crucial as the music itself, reflecting the artist's vision and brand. However, many AI-driven album cover services require subscriptions or technical expertise, limiting accessibility. To address these challenges, we developed Music2P, an open-source, multi-modal AI-driven tool that streamlines album cover creation, making it efficient, accessible, and cost-effective through Ngrok. Music2P automates the design process using techniques such as Bootstrapping Language Image Pre-training (BLIP), music-to-text conversion (LP-music-caps), image segmentation (LoRA), and album cover and QR code generation (ControlNet). This paper demonstrates the Music2P interface, details our application of these technologies, and outlines future improvements. Our ultimate goal is to provide a tool that empowers musicians and producers, especially those with limited resources or expertise, to create compell
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#34394;&#25311;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#23545;&#35805;&#20195;&#29702;&#22312;&#26410;&#21463;&#24403;&#21069;&#23545;&#35805;&#30452;&#25509;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20854;&#33258;&#36523;&#24773;&#24863;&#29366;&#24577;&#30340;&#29616;&#35937;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#31181;"&#33258;&#25105;&#24773;&#24863;"&#23545;&#20854;&#23545;&#35805;&#31574;&#30053;&#21644;&#20915;&#31574;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#24102;&#26377;&#21644;&#27809;&#26377;&#33258;&#25105;&#24773;&#24863;&#30340;&#20195;&#29702;&#30340;&#23545;&#35805;&#31574;&#30053;&#36873;&#25321;&#65292;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#36873;&#25321;&#23545;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;&#21253;&#21547;&#33258;&#25105;&#24773;&#24863;&#26377;&#21161;&#20110;&#20195;&#29702;&#20154;&#23637;&#29616;&#20986;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#36890;&#36807;&#27604;&#36739;&#22312;&#25317;&#26377;&#33258;&#25105;&#24773;&#24863;&#30340;&#23545;&#35805;&#20195;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;&#26410;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#33258;&#25105;&#24773;&#24863;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#35805;&#30340;&#33258;&#28982;&#24230;&#21644;&#30495;&#23454;&#24615;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#22312;&#19968;&#20010;&#20195;&#29702;&#20154;&#25317;&#26377;&#19981;&#21516;&#33258;&#25105;&#24773;&#24863;&#30340;&#34394;&#25311;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#23637;&#31034;&#20102;&#33258;&#25105;&#24773;&#24863;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#20154;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#29615;&#22659;&#20869;&#30340;&#31038;&#20132;&#27169;&#25311;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2408.01633</link><description>&lt;p&gt;
Self-Emotion Blended Dialogue Generation in Social Simulation Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#34394;&#25311;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#23545;&#35805;&#20195;&#29702;&#22312;&#26410;&#21463;&#24403;&#21069;&#23545;&#35805;&#30452;&#25509;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20854;&#33258;&#36523;&#24773;&#24863;&#29366;&#24577;&#30340;&#29616;&#35937;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#31181;"&#33258;&#25105;&#24773;&#24863;"&#23545;&#20854;&#23545;&#35805;&#31574;&#30053;&#21644;&#20915;&#31574;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#24102;&#26377;&#21644;&#27809;&#26377;&#33258;&#25105;&#24773;&#24863;&#30340;&#20195;&#29702;&#30340;&#23545;&#35805;&#31574;&#30053;&#36873;&#25321;&#65292;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#36873;&#25321;&#23545;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;&#21253;&#21547;&#33258;&#25105;&#24773;&#24863;&#26377;&#21161;&#20110;&#20195;&#29702;&#20154;&#23637;&#29616;&#20986;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#36890;&#36807;&#27604;&#36739;&#22312;&#25317;&#26377;&#33258;&#25105;&#24773;&#24863;&#30340;&#23545;&#35805;&#20195;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;&#26410;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#33258;&#25105;&#24773;&#24863;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#35805;&#30340;&#33258;&#28982;&#24230;&#21644;&#30495;&#23454;&#24615;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#22312;&#19968;&#20010;&#20195;&#29702;&#20154;&#25317;&#26377;&#19981;&#21516;&#33258;&#25105;&#24773;&#24863;&#30340;&#34394;&#25311;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#23637;&#31034;&#20102;&#33258;&#25105;&#24773;&#24863;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#20154;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#29615;&#22659;&#20869;&#30340;&#31038;&#20132;&#27169;&#25311;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01633v1 Announce Type: cross  Abstract: When engaging in conversations, dialogue agents in a virtual simulation environment may exhibit their own emotional states that are unrelated to the immediate conversational context, a phenomenon known as self-emotion. This study explores how such self-emotion affects the agents' behaviors in dialogue strategies and decision-making within a large language model (LLM)-driven simulation framework. In a dialogue strategy prediction experiment, we analyze the dialogue strategy choices employed by agents both with and without self-emotion, comparing them to those of humans. The results show that incorporating self-emotion helps agents exhibit more human-like dialogue strategies. In an independent experiment comparing the performance of models fine-tuned on GPT-4 generated dialogue datasets, we demonstrate that self-emotion can lead to better overall naturalness and humanness. Finally, in a virtual simulation environment where agents have di
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PUCL&#30340;&#31639;&#27861;&#65292;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#38750;&#32447;&#24615;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#12290;&#36825;&#19968;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#30340;&#30693;&#35782;&#23601;&#33021;&#25512;&#26029;&#32422;&#26463;&#21442;&#25968;&#21270;&#65292;&#19988;&#26080;&#38656;&#29615;&#22659;&#27169;&#22411;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26410;&#30693;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35268;&#21010;&#21644;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2408.01622</link><description>&lt;p&gt;
Positive-Unlabeled Constraint Learning (PUCL) for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PUCL&#30340;&#31639;&#27861;&#65292;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#38750;&#32447;&#24615;&#36830;&#32493;&#32422;&#26463;&#20989;&#25968;&#12290;&#36825;&#19968;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#30340;&#30693;&#35782;&#23601;&#33021;&#25512;&#26029;&#32422;&#26463;&#21442;&#25968;&#21270;&#65292;&#19988;&#26080;&#38656;&#29615;&#22659;&#27169;&#22411;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26410;&#30693;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35268;&#21010;&#21644;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01622v1 Announce Type: new  Abstract: Planning for a wide range of real-world robotic tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. This paper presents a novel Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a continuous arbitrary constraint function from demonstration, without requiring prior knowledge of the true constraint parameterization or environmental model as existing works. Within our framework, we treat all data in demonstrations as positive (feasible) data, and learn a control policy to generate potentially infeasible trajectories, which serve as unlabeled data. In each iteration, we first update the policy and then a two-step positive-unlabeled learning procedure is applied, where it first identifies reliable infeasible data using a distance metric, an
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;GPT-4&#30340;&#24515;&#29702; distress&#35780;&#20272;&#27169;&#22411;&#8212;&#8212;Psycho Analyst&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#30340;&#35821;&#35328;&#22788;&#29702;&#25163;&#27573;&#26469;&#25552;&#39640;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#26089;&#26399;&#31579;&#26597;&#31934;&#20934;&#24230;&#65292;&#20026;&#20844;&#20849;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#31532;&#20108;&#24847;&#35265;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2408.01614</link><description>&lt;p&gt;
Advancing Mental Health Pre-Screening: A New Custom GPT for Psychological Distress Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01614
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;GPT-4&#30340;&#24515;&#29702; distress&#35780;&#20272;&#27169;&#22411;&#8212;&#8212;Psycho Analyst&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#30340;&#35821;&#35328;&#22788;&#29702;&#25163;&#27573;&#26469;&#25552;&#39640;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#26089;&#26399;&#31579;&#26597;&#31934;&#20934;&#24230;&#65292;&#20026;&#20844;&#20849;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#31532;&#20108;&#24847;&#35265;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01614v1 Announce Type: cross  Abstract: This study introduces 'Psycho Analyst', a custom GPT model based on OpenAI's GPT-4, optimized for pre-screening mental health disorders. Enhanced with DSM-5, PHQ-8, detailed data descriptions, and extensive training data, the model adeptly decodes nuanced linguistic indicators of mental health disorders. It utilizes a dual-task framework that includes binary classification and a three-stage PHQ-8 score computation involving initial assessment, detailed breakdown, and independent assessment, showcasing refined analytic capabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1 scores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of 2.89 and 3.69 in PHQ-8 scoring. These results highlight the model's precision and transformative potential in enhancing public mental health support, improving accessibility, cost-effectiveness, and serving as a second opinion for professionals.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#29615;&#22659;&#20013;&#36827;&#34892;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#32771;&#34385;&#22810;&#31181;&#31038;&#20250;&#34892;&#20026;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20197;&#21450;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#23545;&#25239;&#31038;&#20132;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#24433;&#21709;&#30340;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#22411;&#21644;&#25919;&#31574;&#21046;&#23450;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2408.01596</link><description>&lt;p&gt;
Trustworthy Machine Learning under Social and Adversarial Data Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#29615;&#22659;&#20013;&#36827;&#34892;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#32771;&#34385;&#22810;&#31181;&#31038;&#20250;&#34892;&#20026;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20197;&#21450;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#23545;&#25239;&#31038;&#20132;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#28304;&#24433;&#21709;&#30340;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#22411;&#21644;&#25919;&#31574;&#21046;&#23450;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01596v1 Announce Type: cross  Abstract: Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;OpenLogParser&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#24182;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#30340;&#35757;&#32451;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#65292;&#21363;&#21487;&#23454;&#29616;&#31934;&#30830;&#30340;&#26085;&#24535;&#25968;&#25454;&#32467;&#26500;&#21270;&#22788;&#29702;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38750;&#32467;&#26500;&#21270;&#26085;&#24535;&#25968;&#25454;&#30340;&#35299;&#26512;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#19981;&#36981;&#24490;&#39044;&#35774;&#35821;&#27861;&#35268;&#21017;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#25552;&#20986;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#38754;&#20020;&#30340;&#26102;&#38388;&#25928;&#29575;&#20302;&#12289;&#25104;&#26412;&#39640;&#26114;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01585</link><description>&lt;p&gt;
OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;OpenLogParser&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#24182;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#30340;&#35757;&#32451;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#65292;&#21363;&#21487;&#23454;&#29616;&#31934;&#30830;&#30340;&#26085;&#24535;&#25968;&#25454;&#32467;&#26500;&#21270;&#22788;&#29702;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38750;&#32467;&#26500;&#21270;&#26085;&#24535;&#25968;&#25454;&#30340;&#35299;&#26512;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#19981;&#36981;&#24490;&#39044;&#35774;&#35821;&#27861;&#35268;&#21017;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#25552;&#20986;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#38754;&#20020;&#30340;&#26102;&#38388;&#25928;&#29575;&#20302;&#12289;&#25104;&#26412;&#39640;&#26114;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01585v1 Announce Type: cross  Abstract: Log parsing is a critical step that transforms unstructured log data into structured formats, facilitating subsequent log-based analysis. Traditional syntax-based log parsers are efficient and effective, but they often experience decreased accuracy when processing logs that deviate from the predefined rules. Recently, large language models (LLM) based log parsers have shown superior parsing accuracy. However, existing LLM-based parsers face three main challenges: 1)time-consuming and labor-intensive manual labeling for fine-tuning or in-context learning, 2)increased parsing costs due to the vast volume of log data and limited context size of LLMs, and 3)privacy risks from using commercial models like ChatGPT with sensitive log information. To overcome these limitations, this paper introduces OpenLogParser, an unsupervised log parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance privacy and reduce operational co
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;GPUDrive&#65292;&#19968;&#27454;&#22522;&#20110;Madrona Game Engine&#30340;GPU&#21152;&#36895;&#22810;agent&#27169;&#25311;&#22120;&#65292;&#27599;&#31186;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;&#19968;&#30334;&#19975;&#27493;&#30340;&#32463;&#39564;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;C++&#20013;&#32534;&#20889;&#35266;&#23519;&#12289;&#22870;&#21169;&#21644;&#21160;&#24577;&#20989;&#25968;&#65292;&#29992;&#25143;&#21487;&#20197;&#23450;&#20041;&#22797;&#26434;&#30340;&#24322;&#26500;agent&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#20248;&#21270;&#20026;&#39640;&#25928;&#29575;&#30340;CUDA&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;GPUDrive&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;Waymo Motion&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;agent&#65292;&#20165;&#38656;&#25968;&#20998;&#38047;&#21363;&#21487;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#20135;&#29983;&#26377;&#25928;&#30446;&#26631;&#23547;&#27714;agent&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01584</link><description>&lt;p&gt;
GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;GPUDrive&#65292;&#19968;&#27454;&#22522;&#20110;Madrona Game Engine&#30340;GPU&#21152;&#36895;&#22810;agent&#27169;&#25311;&#22120;&#65292;&#27599;&#31186;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;&#19968;&#30334;&#19975;&#27493;&#30340;&#32463;&#39564;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;C++&#20013;&#32534;&#20889;&#35266;&#23519;&#12289;&#22870;&#21169;&#21644;&#21160;&#24577;&#20989;&#25968;&#65292;&#29992;&#25143;&#21487;&#20197;&#23450;&#20041;&#22797;&#26434;&#30340;&#24322;&#26500;agent&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#20248;&#21270;&#20026;&#39640;&#25928;&#29575;&#30340;CUDA&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;GPUDrive&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;Waymo Motion&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;agent&#65292;&#20165;&#38656;&#25968;&#20998;&#38047;&#21363;&#21487;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#20135;&#29983;&#26377;&#25928;&#30446;&#26631;&#23547;&#27714;agent&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01584v1 Announce Type: new  Abstract: Multi-agent learning algorithms have been successful at generating superhuman planning in a wide variety of games but have had little impact on the design of deployed multi-agent planners. A key bottleneck in applying these techniques to multi-agent planning is that they require billions of steps of experience. To enable the study of multi-agent planning at this scale, we present GPUDrive, a GPU-accelerated, multi-agent simulator built on top of the Madrona Game Engine that can generate over a million steps of experience per second. Observation, reward, and dynamics functions are written directly in C++, allowing users to define complex, heterogeneous agent behaviors that are lowered to high-performance CUDA. We show that using GPUDrive we are able to effectively train reinforcement learning agents over many scenes in the Waymo Motion dataset, yielding highly effective goal-reaching agents in minutes for individual scenes and generally c
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#21033;&#29992; conformal diffusion &#27169;&#22411;&#65292;&#32467;&#21512;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#21644;&#32622;&#20449;&#21306;&#38388;&#26500;&#24314;&#65292;&#20026;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#24230;&#37327;&#21644;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2408.01582</link><description>&lt;p&gt;
Conformal Diffusion Models for Individual Treatment Effect Estimation and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#21033;&#29992; conformal diffusion &#27169;&#22411;&#65292;&#32467;&#21512;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#21644;&#32622;&#20449;&#21306;&#38388;&#26500;&#24314;&#65292;&#20026;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#24230;&#37327;&#21644;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01582v1 Announce Type: cross  Abstract: Estimating treatment effects from observational data is of central interest across numerous application domains. Individual treatment effect offers the most granular measure of treatment effect on an individual level, and is the most useful to facilitate personalized care. However, its estimation and inference remain underdeveloped due to several challenges. In this article, we propose a novel conformal diffusion model-based approach that addresses those intricate challenges. We integrate the highly flexible diffusion modeling, the model-free statistical inference paradigm of conformal inference, along with propensity score and covariate local approximation that tackle distributional shifts. We unbiasedly estimate the distributions of potential outcomes for individual treatment effect, construct an informative confidence interval, and establish rigorous theoretical guarantees. We demonstrate the competitive performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#39318;&#27425;&#23558;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#23545;&#32963;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#35786;&#26029;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#25968;&#25454;&#31232;&#32570;&#21644;&#20559;&#35265;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20811;&#26381;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#26465;&#20214;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01554</link><description>&lt;p&gt;
Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#39318;&#27425;&#23558;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#19971;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#32963;&#30284;&#32959;&#30244;&#20551;&#20307;&#65292;&#23545;&#32963;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#35786;&#26029;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#25968;&#25454;&#31232;&#32570;&#21644;&#20559;&#35265;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20811;&#26381;&#28151;&#21512;&#24418;&#24577;&#29305;&#24449;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#19981;&#23436;&#20840;&#25509;&#35302;&#26465;&#20214;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01554v1 Announce Type: new  Abstract: In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#19968;&#31181;&#26032;&#24212;&#29992;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#30740;&#31350;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AL4PDE&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#20027;&#21160;&#23398;&#20064;&#22522;&#20934;&#65292;&#23427;&#20026;&#35780;&#20272;&#29616;&#26377;&#21644;&#24320;&#21457;&#26032;AL&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#31181;&#21442;&#25968;&#21270;PDEs&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#20869;&#27714;&#35299;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#29305;&#24449;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35810;&#38382;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#21021;&#22987;&#26465;&#20214;&#21644;PDE&#21442;&#25968;&#26469;&#20943;&#23569;&#27714;&#35299;&#22120;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#36825;&#26631;&#24535;&#30528;&#20027;&#21160;&#23398;&#20064;&#22312;PDE&#27714;&#35299;&#22120;&#39046;&#22495;&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#23545;&#20943;&#23569;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2408.01536</link><description>&lt;p&gt;
Active Learning for Neural PDE Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01536
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#19968;&#31181;&#26032;&#24212;&#29992;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#30740;&#31350;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AL4PDE&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#20027;&#21160;&#23398;&#20064;&#22522;&#20934;&#65292;&#23427;&#20026;&#35780;&#20272;&#29616;&#26377;&#21644;&#24320;&#21457;&#26032;AL&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#31181;&#21442;&#25968;&#21270;PDEs&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#20869;&#27714;&#35299;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#29305;&#24449;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35810;&#38382;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#21021;&#22987;&#26465;&#20214;&#21644;PDE&#21442;&#25968;&#26469;&#20943;&#23569;&#27714;&#35299;&#22120;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#36825;&#26631;&#24535;&#30528;&#20027;&#21160;&#23398;&#20064;&#22312;PDE&#27714;&#35299;&#22120;&#39046;&#22495;&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#23545;&#20943;&#23569;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01536v1 Announce Type: cross  Abstract: Solving partial differential equations (PDEs) is a fundamental problem in engineering and science. While neural PDE solvers can be more efficient than established numerical solvers, they often require large amounts of training data that is costly to obtain. Active Learning (AL) could help surrogate models reach the same accuracy with smaller training sets by querying classical solvers with more informative initial conditions and PDE parameters. While AL is more common in other domains, it has yet to be studied extensively for neural PDE solvers. To bridge this gap, we introduce AL4PDE, a modular and extensible active learning benchmark. It provides multiple parametric PDEs and state-of-the-art surrogate models for the solver-in-the-loop setting, enabling the evaluation of existing and the development of new AL methods for PDE solving. We use the benchmark to evaluate batch active learning algorithms such as uncertainty- and feature-bas
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#23398;&#20064;&#27169;&#24577;&#20043;&#38388;&#30340;&#36129;&#29486;&#29305;&#24449;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#21495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24615;&#27169;&#24577;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01532</link><description>&lt;p&gt;
Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#23398;&#20064;&#27169;&#24577;&#20043;&#38388;&#30340;&#36129;&#29486;&#29305;&#24449;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#21495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24615;&#27169;&#24577;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01532v1 Announce Type: cross  Abstract: In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and lo
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#23545;&#36719;&#20214;&#38544;&#21547;&#20542;&#21521;&#24615;&#37327;&#21270;&#20998;&#26512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;Microsoft&#30340;Product Desirability Toolkit&#65288;PDT&#65289;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#37327;&#21270;&#29992;&#25143;&#20542;&#21521;&#24615;&#19978;&#30340;&#28508;&#21147;&#65292;&#26377;&#21161;&#20110;&#20915;&#31574;&#32773;&#26681;&#25454;&#20135;&#21697;&#25509;&#21463;&#24230;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2408.01527</link><description>&lt;p&gt;
Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of Software Desirability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#23545;&#36719;&#20214;&#38544;&#21547;&#20542;&#21521;&#24615;&#37327;&#21270;&#20998;&#26512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;Microsoft&#30340;Product Desirability Toolkit&#65288;PDT&#65289;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#37327;&#21270;&#29992;&#25143;&#20542;&#21521;&#24615;&#19978;&#30340;&#28508;&#21147;&#65292;&#26377;&#21161;&#20110;&#20915;&#31574;&#32773;&#26681;&#25454;&#20135;&#21697;&#25509;&#21463;&#24230;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01527v1 Announce Type: cross  Abstract: This study explores the use of several LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability expressed by users. The study provides scaled numerical sentiment analysis unlike other methods that simply classify sentiment as positive, neutral, or negative. Numerical analysis provides deeper insights into the magnitude of sentiment, to drive better decisions regarding product desirability.   Data is collected through the use of the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of ZORQ, a gamification system used in undergraduate computer science education. The PDT data collected was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment (TRBS), and through Vader, a leading sentiment analysis 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35777;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#24456;&#22810;&#35757;&#32451;&#31639;&#27861;&#25152;&#22522;&#20110;&#30340;&#21442;&#25968;&#31354;&#38388;&#26799;&#24230;&#27969;&#21487;&#20197;&#36830;&#32493;&#21464;&#24418;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26799;&#24230;&#27969;&#65292;&#35813;&#26799;&#24230;&#27969;&#22312;&#36755;&#20986;&#31354;&#38388;&#23548;&#33268;&#20102;&#32422;&#26463;&#27431;&#27663;&#26799;&#24230;&#27969;&#12290;&#21516;&#26102;&#65292;&#22914;&#26524;&#36755;&#20986;&#30456;&#23545;&#20110;&#21442;&#25968;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#22312;&#22266;&#23450;&#35757;&#32451;&#25968;&#25454;&#19979;&#26159;&#28385;&#31209;&#30340;&#65292;&#21017;&#21487;&#20197;&#37325;&#26032;&#21442;&#25968;&#21270;&#26102;&#38388;&#21464;&#37327;&#65292;&#20351;&#24471;&#24471;&#21040;&#30340;&#27969;&#26159;&#31616;&#21333;&#30340;&#19968;&#32500;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#19988;&#22312;&#20840;&#23616;&#26368;&#23567;&#22788;&#36798;&#21040;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2408.01517</link><description>&lt;p&gt;
Gradient flow in parameter space is equivalent to linear interpolation in output space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35777;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#24456;&#22810;&#35757;&#32451;&#31639;&#27861;&#25152;&#22522;&#20110;&#30340;&#21442;&#25968;&#31354;&#38388;&#26799;&#24230;&#27969;&#21487;&#20197;&#36830;&#32493;&#21464;&#24418;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26799;&#24230;&#27969;&#65292;&#35813;&#26799;&#24230;&#27969;&#22312;&#36755;&#20986;&#31354;&#38388;&#23548;&#33268;&#20102;&#32422;&#26463;&#27431;&#27663;&#26799;&#24230;&#27969;&#12290;&#21516;&#26102;&#65292;&#22914;&#26524;&#36755;&#20986;&#30456;&#23545;&#20110;&#21442;&#25968;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#22312;&#22266;&#23450;&#35757;&#32451;&#25968;&#25454;&#19979;&#26159;&#28385;&#31209;&#30340;&#65292;&#21017;&#21487;&#20197;&#37325;&#26032;&#21442;&#25968;&#21270;&#26102;&#38388;&#21464;&#37327;&#65292;&#20351;&#24471;&#24471;&#21040;&#30340;&#27969;&#26159;&#31616;&#21333;&#30340;&#19968;&#32500;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#19988;&#22312;&#20840;&#23616;&#26368;&#23567;&#22788;&#36798;&#21040;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01517v1 Announce Type: cross  Abstract: We prove that the usual gradient flow in parameter space that underlies many training algorithms for neural networks in deep learning can be continuously deformed into an adapted gradient flow which yields (constrained) Euclidean gradient flow in output space. Moreover, if the Jacobian of the outputs with respect to the parameters is full rank (for fixed training data), then the time variable can be reparametrized so that the resulting flow is simply linear interpolation, and a global minimum can be achieved.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;LocalValueBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#28595;&#22823;&#21033;&#20122;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#21644;&#20262;&#29702;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#39033;&#22522;&#20934;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#20026;&#20840;&#29699;&#21508;&#22320;&#30340;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20415;&#26681;&#25454;&#26412;&#22320;&#20215;&#20540;&#35266;&#23450;&#21046;&#38024;&#23545;LLMs&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#36890;&#36807;&#19968;&#20010;&#21019;&#26032;&#30340;&#20262;&#29702;&#25512;&#29702;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#26041;&#27861;&#65292;&#25991;&#31456;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#36816;&#29992;&#20102;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#65292;&#20197;&#20415;&#28145;&#20837;&#25506;&#27979;&#27169;&#22411;&#23545;&#26412;&#22320;&#20215;&#20540;&#35266;&#30340;&#21709;&#24212;&#12290;&#35780;&#20272;&#26631;&#20934;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#20559;&#31163;&#24230;&#65292;&#30830;&#20445;&#20102;&#35780;&#20272;&#36807;&#31243;&#30340;&#20005;&#26684;&#24615;&#12290;&#36890;&#36807;&#35780;&#20272;&#26469;&#28304;&#33258;&#32654;&#22269;&#30340;&#19977;&#23478;&#21830;&#19994;LLMs&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#25928;&#33021;&#21644;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2408.01460</link><description>&lt;p&gt;
LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;LocalValueBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#28595;&#22823;&#21033;&#20122;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#21644;&#20262;&#29702;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#39033;&#22522;&#20934;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#20026;&#20840;&#29699;&#21508;&#22320;&#30340;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20415;&#26681;&#25454;&#26412;&#22320;&#20215;&#20540;&#35266;&#23450;&#21046;&#38024;&#23545;LLMs&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#36890;&#36807;&#19968;&#20010;&#21019;&#26032;&#30340;&#20262;&#29702;&#25512;&#29702;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#26041;&#27861;&#65292;&#25991;&#31456;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#36816;&#29992;&#20102;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#65292;&#20197;&#20415;&#28145;&#20837;&#25506;&#27979;&#27169;&#22411;&#23545;&#26412;&#22320;&#20215;&#20540;&#35266;&#30340;&#21709;&#24212;&#12290;&#35780;&#20272;&#26631;&#20934;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#20559;&#31163;&#24230;&#65292;&#30830;&#20445;&#20102;&#35780;&#20272;&#36807;&#31243;&#30340;&#20005;&#26684;&#24615;&#12290;&#36890;&#36807;&#35780;&#20272;&#26469;&#28304;&#33258;&#32654;&#22269;&#30340;&#19977;&#23478;&#21830;&#19994;LLMs&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#25928;&#33021;&#21644;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01460v1 Announce Type: cross  Abstract: The proliferation of large language models (LLMs) requires robust evaluation of their alignment with local values and ethical standards, especially as existing benchmarks often reflect the cultural, legal, and ideological values of their creators. \textsc{LocalValueBench}, introduced in this paper, is an extensible benchmark designed to assess LLMs' adherence to Australian values, and provides a framework for regulators worldwide to develop their own LLM benchmarks for local value alignment. Employing a novel typology for ethical reasoning and an interrogation approach, we curated comprehensive questions and utilized prompt engineering strategies to probe LLMs' value alignment. Our evaluation criteria quantified deviations from local values, ensuring a rigorous assessment process. Comparative analysis of three commercial LLMs by USA vendors revealed significant insights into their effectiveness and limitations, demonstrating the critic
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;AgentPeerTalk&#30340;&#26032;&#22411;&#26041;&#26696;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#22914;ChatGPT 4&#26469;&#21306;&#20998;&#23398;&#29983;&#20043;&#38388;&#30340;&#27450;&#20940;&#21644;&#25103;&#35857;&#34892;&#20026;&#65292;&#26088;&#22312;&#23545;&#23398;&#29983;&#36827;&#34892;&#26377;&#25928;&#21450;&#26102;&#30340;&#20445;&#25252;&#21644;&#24178;&#39044;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT 4&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#22815;&#25552;&#20379;&#30456;&#23545;&#36739;&#20026;&#20934;&#30830;&#30340;&#21028;&#26029;&#65292;&#23545;&#20110;&#25935;&#24863;&#30340;&#31038;&#20132;&#20114;&#21160;&#24773;&#22659;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#8220;&#20195;&#29702;AI&#8221;&#30340;&#26041;&#24335;&#65292;&#23398;&#29983;&#21487;&#20197;&#24471;&#21040;&#25345;&#32493;&#30340;&#23454;&#26102;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#26356;&#23433;&#20840;&#30340;&#26657;&#22253;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2408.01459</link><description>&lt;p&gt;
AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;AgentPeerTalk&#30340;&#26032;&#22411;&#26041;&#26696;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#22914;ChatGPT 4&#26469;&#21306;&#20998;&#23398;&#29983;&#20043;&#38388;&#30340;&#27450;&#20940;&#21644;&#25103;&#35857;&#34892;&#20026;&#65292;&#26088;&#22312;&#23545;&#23398;&#29983;&#36827;&#34892;&#26377;&#25928;&#21450;&#26102;&#30340;&#20445;&#25252;&#21644;&#24178;&#39044;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT 4&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#22815;&#25552;&#20379;&#30456;&#23545;&#36739;&#20026;&#20934;&#30830;&#30340;&#21028;&#26029;&#65292;&#23545;&#20110;&#25935;&#24863;&#30340;&#31038;&#20132;&#20114;&#21160;&#24773;&#22659;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#8220;&#20195;&#29702;AI&#8221;&#30340;&#26041;&#24335;&#65292;&#23398;&#29983;&#21487;&#20197;&#24471;&#21040;&#25345;&#32493;&#30340;&#23454;&#26102;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#26356;&#23433;&#20840;&#30340;&#26657;&#22253;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01459v1 Announce Type: cross  Abstract: Addressing school bullying effectively and promptly is crucial for the mental health of students. This study examined the potential of large language models (LLMs) to empower students by discerning between bullying and joking in school peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus, evaluating their effectiveness through human review. Our results revealed that not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the most promise. We observed variations in LLM outputs, possibly influenced by political overcorrectness, context window limitations, and pre-existing bias in their training data. ChatGPT-4 excelled in context-specific accuracy after implementing the agentic approach, highlighting its potential to provide continuous, real-time support to vulnerable students. This study underlines the significant social impact of using agentic AI in educational settings, offering a new avenue f
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25351;&#20986;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#12289;&#21457;&#23637;&#21644;&#27835;&#29702;&#39046;&#22495;&#20013;&#65292;&#23613;&#31649;&#20844;&#20247;&#21442;&#19982;&#30340;&#27010;&#24565;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#65292;&#20294;&#24403;&#21069;&#29992;&#20110;&#25910;&#38598;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30475;&#27861;&#12289;&#24863;&#30693;&#21644;&#32463;&#21382;&#30340;&#35843;&#26597;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#35843;&#26597;&#24448;&#24448;&#24102;&#26377;&#35199;&#26041;&#30693;&#35782;&#12289;&#20215;&#20540;&#35266;&#21644;&#20551;&#35774;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#35774;&#35745;&#19978;&#23545;&#20262;&#29702;&#27010;&#24565;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26377;&#25152;&#20559;&#24046;&#65292;&#20197;&#21450;&#23545;&#20110;&#37096;&#32626;&#31574;&#30053;&#30340;&#35752;&#35770;&#32570;&#20047;&#25209;&#21028;&#24615;&#12290;&#27492;&#22806;&#65292;&#35843;&#26597;&#32467;&#26524;&#30340;&#25253;&#21578;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2408.01458</link><description>&lt;p&gt;
Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25351;&#20986;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#12289;&#21457;&#23637;&#21644;&#27835;&#29702;&#39046;&#22495;&#20013;&#65292;&#23613;&#31649;&#20844;&#20247;&#21442;&#19982;&#30340;&#27010;&#24565;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#65292;&#20294;&#24403;&#21069;&#29992;&#20110;&#25910;&#38598;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30475;&#27861;&#12289;&#24863;&#30693;&#21644;&#32463;&#21382;&#30340;&#35843;&#26597;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#35843;&#26597;&#24448;&#24448;&#24102;&#26377;&#35199;&#26041;&#30693;&#35782;&#12289;&#20215;&#20540;&#35266;&#21644;&#20551;&#35774;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#35774;&#35745;&#19978;&#23545;&#20262;&#29702;&#27010;&#24565;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26377;&#25152;&#20559;&#24046;&#65292;&#20197;&#21450;&#23545;&#20110;&#37096;&#32626;&#31574;&#30053;&#30340;&#35752;&#35770;&#32570;&#20047;&#25209;&#21028;&#24615;&#12290;&#27492;&#22806;&#65292;&#35843;&#26597;&#32467;&#26524;&#30340;&#25253;&#21578;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01458v1 Announce Type: cross  Abstract: Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our finding
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340; epistemological&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35774;&#35745;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#23562;&#37325;&#21253;&#23481;&#24615;&#21407;&#21017;&#21644;&#31616;&#27905;&#24615;&#30340;&#20449;&#24565;&#31995;&#32479;&#20998;&#31867;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#22312;&#35821;&#35328;&#27169;&#22411;&#20449;&#24565;&#20844;&#24179;&#24615;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01455</link><description>&lt;p&gt;
Ontology of Belief Diversity: A Community-Based Epistemological Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340; epistemological&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35774;&#35745;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#23562;&#37325;&#21253;&#23481;&#24615;&#21407;&#21017;&#21644;&#31616;&#27905;&#24615;&#30340;&#20449;&#24565;&#31995;&#32479;&#20998;&#31867;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#22312;&#35821;&#35328;&#27169;&#22411;&#20449;&#24565;&#20844;&#24179;&#24615;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01455v1 Announce Type: cross  Abstract: AI applications across classification, fairness, and human interaction often implicitly require ontologies of social concepts. Constructing these well, especially when there are many relevant categories, is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which is a complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology's utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35770;&#36848;&#20102;&#22914;&#20309;&#23558;&#26234;&#33021;&#22478;&#24066;&#25216;&#26415;&#24212;&#29992;&#20110;&#20419;&#36827;&#32422;&#26086;&#23433;&#26364;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#21644;&#29615;&#22659;&#20445;&#25252;&#65292;&#24378;&#35843;&#20102;&#26234;&#33021;&#25216;&#26415;&#22312;&#22478;&#24066;&#35268;&#21010;&#21644;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20197;&#21450;&#22312;&#23621;&#27665;&#29983;&#27963;&#36136;&#37327;&#25552;&#21319;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2408.01454</link><description>&lt;p&gt;
Amman City, Jordan: Toward a Sustainable City from the Ground Up
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35770;&#36848;&#20102;&#22914;&#20309;&#23558;&#26234;&#33021;&#22478;&#24066;&#25216;&#26415;&#24212;&#29992;&#20110;&#20419;&#36827;&#32422;&#26086;&#23433;&#26364;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#21644;&#29615;&#22659;&#20445;&#25252;&#65292;&#24378;&#35843;&#20102;&#26234;&#33021;&#25216;&#26415;&#22312;&#22478;&#24066;&#35268;&#21010;&#21644;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20197;&#21450;&#22312;&#23621;&#27665;&#29983;&#27963;&#36136;&#37327;&#25552;&#21319;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01454v1 Announce Type: cross  Abstract: The idea of smart cities (SCs) has gained substantial attention in recent years. The SC paradigm aims to improve citizens' quality of life and protect the city's environment. As we enter the age of next-generation SCs, it is important to explore all relevant aspects of the SC paradigm. In recent years, the advancement of Information and Communication Technologies (ICT) has produced a trend of supporting daily objects with smartness, targeting to make human life easier and more comfortable. The paradigm of SCs appears as a response to the purpose of building the city of the future with advanced features. SCs still face many challenges in their implementation, but increasingly more studies regarding SCs are implemented. Nowadays, different cities are employing SC features to enhance services or the residents quality of life. This work provides readers with useful and important information about Amman Smart City.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#25512;&#21160;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#24517;&#39035;&#27880;&#24847;&#20854;&#33021;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.01453</link><description>&lt;p&gt;
Reporting and Analysing the Environmental Impact of Language Models on the Example of Commonsense Question Answering with External Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#25512;&#21160;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#24517;&#39035;&#27880;&#24847;&#20854;&#33021;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01453v1 Announce Type: cross  Abstract: Human-produced emissions are growing at an alarming rate, causing already observable changes in the climate and environment in general. Each year global carbon dioxide emissions hit a new record, and it is reported that 0.5% of total US greenhouse gas emissions are attributed to data centres as of 2021. The release of ChatGPT in late 2022 sparked social interest in Large Language Models (LLMs), the new generation of Language Models with a large number of parameters and trained on massive amounts of data. Currently, numerous companies are releasing products featuring various LLMs, with many more models in development and awaiting release. Deep Learning research is a competitive field, with only models that reach top performance attracting attention and being utilized. Hence, achieving better accuracy and results is often the first priority, while the model's efficiency and the environmental impact of the study are neglected. However, LL
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#22312;K-12&#25945;&#32946;&#24179;&#21488;&#37096;&#32626;&#29983;&#20135;&#32423;&#25252;&#21355;&#26639;&#27169;&#22411;&#30340;&#32463;&#39564;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#25935;&#24863;&#39046;&#22495;&#37096;&#32626;&#30340;&#35201;&#27714;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#25945;&#32946;&#36825;&#19968;&#29305;&#27530;&#39046;&#22495;&#20869;&#35757;&#32451;&#21644;&#37096;&#32626;&#25252;&#21355;&#26639;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2408.01452</link><description>&lt;p&gt;
Building a Domain-specific Guardrail Model in Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#22312;K-12&#25945;&#32946;&#24179;&#21488;&#37096;&#32626;&#29983;&#20135;&#32423;&#25252;&#21355;&#26639;&#27169;&#22411;&#30340;&#32463;&#39564;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#25935;&#24863;&#39046;&#22495;&#37096;&#32626;&#30340;&#35201;&#27714;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#25945;&#32946;&#36825;&#19968;&#29305;&#27530;&#39046;&#22495;&#20869;&#35757;&#32451;&#21644;&#37096;&#32626;&#25252;&#21355;&#26639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01452v1 Announce Type: cross  Abstract: Generative AI holds the promise of enabling a range of sought-after capabilities and revolutionizing workflows in various consumer and enterprise verticals. However, putting a model in production involves much more than just generating an output. It involves ensuring the model is reliable, safe, performant and also adheres to the policy of operation in a particular domain. Guardrails as a necessity for models has evolved around the need to enforce appropriate behavior of models, especially when they are in production. In this paper, we use education as a use case, given its stringent requirements of the appropriateness of content in the domain, to demonstrate how a guardrail model can be trained and deployed in production. Specifically, we describe our experience in building a production-grade guardrail model for a K-12 educational platform. We begin by formulating the requirements for deployment to this sensitive domain. We then descr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#38024;&#23545;&#32844;&#19994;&#36719;&#20214;&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#27431;&#27954;AI&#27861;&#26696;&#30340;&#29702;&#35299;&#21644;&#23548;&#33322;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#20182;&#20204;&#23545;&#27861;&#26696;&#30340;113&#39033;&#26465;&#27454;&#12289;180&#26465;&#22768;&#26126;&#21644;13&#20010;&#38468;&#24405;&#26377;&#26356;&#28145;&#21051;&#30340;&#20102;&#35299;&#12290;</title><link>https://arxiv.org/abs/2408.01449</link><description>&lt;p&gt;
AI Act for the Working Programmer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#38024;&#23545;&#32844;&#19994;&#36719;&#20214;&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#27431;&#27954;AI&#27861;&#26696;&#30340;&#29702;&#35299;&#21644;&#23548;&#33322;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#20182;&#20204;&#23545;&#27861;&#26696;&#30340;113&#39033;&#26465;&#27454;&#12289;180&#26465;&#22768;&#26126;&#21644;13&#20010;&#38468;&#24405;&#26377;&#26356;&#28145;&#21051;&#30340;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01449v1 Announce Type: cross  Abstract: The European AI Act is a new, legally binding instrument that will enforce certain requirements on the development and use of AI technology potentially affecting people in Europe. It can be expected that the stipulations of the Act, in turn, are going to affect the work of many software engineers, software testers, data engineers, and other professionals across the IT sector in Europe and beyond. The 113 articles, 180 recitals, and 13 annexes that make up the Act cover 144 pages. This paper aims at providing an aid for navigating the Act from the perspective of some professional in the software domain, termed "the working programmer", who feels the need to know about the stipulations of the Act.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PreIndex&#30340;&#39044;&#27979;&#25351;&#26631;&#26469;&#20272;&#31639;&#27169;&#22411;&#22240;&#29615;&#22659;&#21464;&#21270;&#25110;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#25913;&#21464;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#29615;&#22659;&#25104;&#26412;&#65292;&#21253;&#25324;&#35745;&#31639;&#36164;&#28304;&#21644;&#30899;&#25490;&#25918;&#12290;&#36825;&#26377;&#21161;&#20110;&#20248;&#21270;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#65292;&#23454;&#29616;&#26356;&#29615;&#20445;&#21644;&#21487;&#25345;&#32493;&#30340;AI/&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2408.01446</link><description>&lt;p&gt;
Estimating Environmental Cost Throughout Model's Adaptive Life Cycle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PreIndex&#30340;&#39044;&#27979;&#25351;&#26631;&#26469;&#20272;&#31639;&#27169;&#22411;&#22240;&#29615;&#22659;&#21464;&#21270;&#25110;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#25913;&#21464;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#29615;&#22659;&#25104;&#26412;&#65292;&#21253;&#25324;&#35745;&#31639;&#36164;&#28304;&#21644;&#30899;&#25490;&#25918;&#12290;&#36825;&#26377;&#21161;&#20110;&#20248;&#21270;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#29983;&#21629;&#21608;&#26399;&#65292;&#23454;&#29616;&#26356;&#29615;&#20445;&#21644;&#21487;&#25345;&#32493;&#30340;AI/&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01446v1 Announce Type: cross  Abstract: With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MiranDa&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#21307;&#29983;&#23398;&#20064;&#36807;&#31243;&#24182;&#20511;&#37492;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21019;&#26032;&#24615;&#22320;&#23454;&#29616;&#20102;&#33647;&#29289;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#12290;MiranDa&#39318;&#20808;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;ELOS&#65288;&#39044;&#35745;&#20303;&#38498;&#22825;&#25968;&#65289;&#25351;&#23548;&#19979;&#20248;&#21270;&#33647;&#29289;&#32452;&#21512;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20303;&#38498;&#22825;&#25968;&#24182;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01445</link><description>&lt;p&gt;
MiranDa: Mimicking the Learning Processes of Human Doctors to Achieve Causal Inference for Medication Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MiranDa&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#21307;&#29983;&#23398;&#20064;&#36807;&#31243;&#24182;&#20511;&#37492;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21019;&#26032;&#24615;&#22320;&#23454;&#29616;&#20102;&#33647;&#29289;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#12290;MiranDa&#39318;&#20808;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;ELOS&#65288;&#39044;&#35745;&#20303;&#38498;&#22825;&#25968;&#65289;&#25351;&#23548;&#19979;&#20248;&#21270;&#33647;&#29289;&#32452;&#21512;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20303;&#38498;&#22825;&#25968;&#24182;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01445v1 Announce Type: cross  Abstract: To enhance therapeutic outcomes from a pharmacological perspective, we propose MiranDa, designed for medication recommendation, which is the first actionable model capable of providing the estimated length of stay in hospitals (ELOS) as counterfactual outcomes that guide clinical practice and model training. In detail, MiranDa emulates the educational trajectory of doctors through two gradient-scaling phases shifted by ELOS: an Evidence-based Training Phase that utilizes supervised learning and a Therapeutic Optimization Phase grounds in reinforcement learning within the gradient space, explores optimal medications by perturbations from ELOS. Evaluation of the Medical Information Mart for Intensive Care III dataset and IV dataset, showcased the superior results of our model across five metrics, particularly in reducing the ELOS. Surprisingly, our model provides structural attributes of medication combinations proved in hyperbolic space
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#39318;&#27425;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#20844;&#21496;&#20013;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#34892;&#19994;&#23454;&#36341;&#32773;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2408.01444</link><description>&lt;p&gt;
No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with Company Size
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#39318;&#27425;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#20844;&#21496;&#20013;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#34892;&#19994;&#23454;&#36341;&#32773;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01444v1 Announce Type: cross  Abstract: Large language models (LLMs) are playing a pivotal role in deploying strategic use cases across a range of organizations, from large pan-continental companies to emerging startups. The issues and challenges involved in the successful utilization of LLMs can vary significantly depending on the size of the organization. It is important to study and discuss these pertinent issues of LLM adaptation with a focus on the scale of the industrial concerns and brainstorm possible solutions and prospective directions. Such a study has not been prominently featured in the current research literature. In this study, we adopt a threefold strategy: first, we conduct a case study with industry practitioners to formulate the key research questions; second, we examine existing industrial publications to address these questions; and finally, we provide a practical guide for industries to utilize LLMs more efficiently.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25351;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20419;&#36827;&#22810;&#20803;&#21270;&#19982;&#21253;&#23481;&#24615;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#36890;&#36807;&#23545;AI&#20107;&#25925;&#25968;&#25454;&#24211;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;AI&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#65292;&#20197;&#30830;&#20445;AI&#25216;&#26415;&#30340;&#20844;&#24179;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.01438</link><description>&lt;p&gt;
AI for All: Identifying AI incidents Related to Diversity and Inclusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25351;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20419;&#36827;&#22810;&#20803;&#21270;&#19982;&#21253;&#23481;&#24615;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#36890;&#36807;&#23545;AI&#20107;&#25925;&#25968;&#25454;&#24211;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;AI&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#65292;&#20197;&#30830;&#20445;AI&#25216;&#26415;&#30340;&#20844;&#24179;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01438v1 Announce Type: cross  Abstract: The rapid expansion of Artificial Intelligence (AI) technologies has introduced both significant advancements and challenges, with diversity and inclusion (D&amp;I) emerging as a critical concern. Addressing D&amp;I in AI is essential to reduce biases and discrimination, enhance fairness, and prevent adverse societal impacts. Despite its importance, D&amp;I considerations are often overlooked, resulting in incidents marked by built-in biases and ethical dilemmas. Analyzing AI incidents through a D&amp;I lens is crucial for identifying causes of biases and developing strategies to mitigate them, ensuring fairer and more equitable AI technologies. However, systematic investigations of D&amp;I-related AI incidents are scarce. This study addresses these challenges by identifying and understanding D&amp;I issues within AI systems through a manual analysis of AI incident databases (AIID and AIAAIC). The research develops a decision tree to investigate D&amp;I issues ti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26500;&#24314;&#19968;&#20010;&#31526;&#21512;&#20262;&#29702;&#21644;&#21487;&#20449;&#30340;&#29983;&#29289;&#21307;&#23398;AI&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#20415;&#23558;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#21040;&#20020;&#24202;&#21644;&#36716;&#21270;&#30740;&#31350;&#20013;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#25972;&#21512;&#22522;&#30784;&#27169;&#22411;&#21040;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#27169;&#22411;&#35780;&#20215;&#12289;&#20020;&#24202;&#32763;&#35793;&#12289;AI&#27835;&#29702;&#21644;&#30417;&#31649;&#20197;&#21450;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#31561;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#38754;&#30340;&#32508;&#21512;&#32771;&#34385;&#65292;&#25991;&#31456;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#26356;&#21152;&#31283;&#20581;&#21644;&#36879;&#26126;&#30340;&#21457;&#23637;&#65292;&#26368;&#32456;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2408.01431</link><description>&lt;p&gt;
Building an Ethical and Trustworthy Biomedical AI Ecosystem for the Translational and Clinical Integration of Foundational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01431
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26500;&#24314;&#19968;&#20010;&#31526;&#21512;&#20262;&#29702;&#21644;&#21487;&#20449;&#30340;&#29983;&#29289;&#21307;&#23398;AI&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#20415;&#23558;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#21040;&#20020;&#24202;&#21644;&#36716;&#21270;&#30740;&#31350;&#20013;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#25972;&#21512;&#22522;&#30784;&#27169;&#22411;&#21040;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#27169;&#22411;&#35780;&#20215;&#12289;&#20020;&#24202;&#32763;&#35793;&#12289;AI&#27835;&#29702;&#21644;&#30417;&#31649;&#20197;&#21450;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#31561;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#38754;&#30340;&#32508;&#21512;&#32771;&#34385;&#65292;&#25991;&#31456;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#26356;&#21152;&#31283;&#20581;&#21644;&#36879;&#26126;&#30340;&#21457;&#23637;&#65292;&#26368;&#32456;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01431v1 Announce Type: cross  Abstract: Foundational Models (FMs) are emerging as the cornerstone of the biomedical AI ecosystem due to their ability to represent and contextualize multimodal biomedical data. These capabilities allow FMs to be adapted for various tasks, including biomedical reasoning, hypothesis generation, and clinical decision-making. This review paper examines the foundational components of an ethical and trustworthy AI (ETAI) biomedical ecosystem centered on FMs, highlighting key challenges and solutions. The ETAI biomedical ecosystem is defined by seven key components which collectively integrate FMs into clinical settings: Data Lifecycle Management, Data Processing, Model Development, Model Evaluation, Clinical Translation, AI Governance and Regulation, and Stakeholder Engagement. While the potential of biomedical AI is immense, it requires heightened ethical vigilance and responsibility. For instance, biases can arise from data, algorithms, and user i
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUSTechGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#38632;&#22812;&#65289;&#19979;&#30340;&#39550;&#39542;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#30446;&#26631;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;YOLOv5&#30446;&#26631;&#35782;&#21035;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#26032;&#26679;&#26412;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#29983;&#25104;&#30340;&#26631;&#30340;&#22270;&#20687;&#65292;&#23601;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20854;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#30446;&#26631;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2408.01430</link><description>&lt;p&gt;
SUSTechGAN: Image Generation for Object Recognition in Adverse Conditions of Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUSTechGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#38632;&#22812;&#65289;&#19979;&#30340;&#39550;&#39542;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#30446;&#26631;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;YOLOv5&#30446;&#26631;&#35782;&#21035;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#26032;&#26679;&#26412;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#29983;&#25104;&#30340;&#26631;&#30340;&#22270;&#20687;&#65292;&#23601;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20854;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#30446;&#26631;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01430v1 Announce Type: new  Abstract: Autonomous driving significantly benefits from data-driven deep neural networks. However, the data in autonomous driving typically fits the long-tailed distribution, in which the critical driving data in adverse conditions is hard to collect. Although generative adversarial networks (GANs) have been applied to augment data for autonomous driving, generating driving images in adverse conditions is still challenging. In this work, we propose a novel SUSTechGAN with dual attention modules and multi-scale generators to generate driving images for improving object recognition of autonomous driving in adverse conditions. We test the SUSTechGAN and the existing well-known GANs to generate driving images in adverse conditions of rain and night and apply the generated images to retrain object recognition networks. Specifically, we add generated images into the training datasets to retrain the well-known YOLOv5 and evaluate the improvement of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#36710;&#36742;&#36890;&#20449;&#32593;&#32476;&#20013;&#36890;&#20449;&#27169;&#24335;&#20998;&#37197;&#23545;&#36890;&#20449;&#25928;&#29575;&#24433;&#21709;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#39550;&#39542;&#22330;&#26223;&#21644;&#19994;&#21153;&#38656;&#27714;&#25935;&#25463;&#36866;&#24212;&#30340;&#22810;&#27169;&#36890;&#20449;&#35774;&#22791;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;Q-learning&#35757;&#32451;&#19968;&#20010;&#25935;&#25463;&#36866;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#22240;&#36890;&#20449;&#19981;&#31283;&#23450;&#22330;&#26223;&#20013;&#24310;&#36831;&#27979;&#37327;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#36866;&#24212;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#21160;&#24577;&#30340;&#36710;&#36742;&#32593;&#32476;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#24182;&#21457;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2408.01429</link><description>&lt;p&gt;
An Agile Adaptation Method for Multi-mode Vehicle Communication Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01429
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#36710;&#36742;&#36890;&#20449;&#32593;&#32476;&#20013;&#36890;&#20449;&#27169;&#24335;&#20998;&#37197;&#23545;&#36890;&#20449;&#25928;&#29575;&#24433;&#21709;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#39550;&#39542;&#22330;&#26223;&#21644;&#19994;&#21153;&#38656;&#27714;&#25935;&#25463;&#36866;&#24212;&#30340;&#22810;&#27169;&#36890;&#20449;&#35774;&#22791;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;Q-learning&#35757;&#32451;&#19968;&#20010;&#25935;&#25463;&#36866;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#22240;&#36890;&#20449;&#19981;&#31283;&#23450;&#22330;&#26223;&#20013;&#24310;&#36831;&#27979;&#37327;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#36866;&#24212;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#21160;&#24577;&#30340;&#36710;&#36742;&#32593;&#32476;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#24182;&#21457;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01429v1 Announce Type: cross  Abstract: This paper focuses on discovering the impact of communication mode allocation on communication efficiency in the vehicle communication networks. To be specific, Markov decision process and reinforcement learning are applied to establish an agile adaptation mechanism for multi-mode communication devices according to the driving scenarios and business requirements. Then, Q-learning is used to train the agile adaptation reinforcement learning model and output the trained model. By learning the best actions to take in different states to maximize the cumulative reward, and avoiding the problem of poor adaptation effect caused by inaccurate delay measurement in unstable communication scenarios. The experiments show that the proposed scheme can quickly adapt to dynamic vehicle networking environment, while achieving high concurrency and communication efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#20154;&#33080;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#26576;&#31181;&#38754;&#37096;&#29305;&#24449;&#65288;&#22914;&#22918;&#23481;&#20449;&#24687;&#65289;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#22320;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#21046;&#20316;&#20986;&#36924;&#30495;&#19988;&#39640;&#24230;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2408.01428</link><description>&lt;p&gt;
Transferable Adversarial Facial Images for Privacy Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#20154;&#33080;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#35270;&#35273;&#36136;&#37327;&#12290;&#36890;&#36807;&#30452;&#25509;&#22609;&#36896;&#25972;&#20010;&#38754;&#37096;&#31354;&#38388;&#32780;&#19981;&#26159;&#20165;&#21033;&#29992;&#26576;&#31181;&#38754;&#37096;&#29305;&#24449;&#65288;&#22914;&#22918;&#23481;&#20449;&#24687;&#65289;&#26469;&#25972;&#21512;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#22320;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#21046;&#20316;&#20986;&#36924;&#30495;&#19988;&#39640;&#24230;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01428v1 Announce Type: new  Abstract: The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent sear
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Siamese Transformer&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#32593;&#32476;&#21516;&#26102;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;K-shot&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2408.01427</link><description>&lt;p&gt;
Siamese Transformer Networks for Few-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Siamese Transformer&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#32593;&#32476;&#21516;&#26102;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;K-shot&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01427v1 Announce Type: new  Abstract: Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global featur
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Self-Contradictory Instructions&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#38754;&#23545;&#33258;&#30456;&#30683;&#30462;&#30340;&#25351;&#20196;&#26102;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#19968;&#20010;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#21019;&#24314;&#26694;&#26550;&#65292;&#29983;&#25104;&#20102;20,000&#20010;&#24102;&#26377;&#30683;&#30462;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#20004;&#31181;&#24418;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;LMMs&#22312;&#35782;&#21035;&#22810;&#27169;&#24577;&#25351;&#20196;&#20914;&#31361;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Cognitive Awakening Prompting&#30340;&#25216;&#26415;&#26469;&#25913;&#21892;&#36825;&#19968;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2408.01091</link><description>&lt;p&gt;
Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Self-Contradictory Instructions&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#38754;&#23545;&#33258;&#30456;&#30683;&#30462;&#30340;&#25351;&#20196;&#26102;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#19968;&#20010;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#21019;&#24314;&#26694;&#26550;&#65292;&#29983;&#25104;&#20102;20,000&#20010;&#24102;&#26377;&#30683;&#30462;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#20004;&#31181;&#24418;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;LMMs&#22312;&#35782;&#21035;&#22810;&#27169;&#24577;&#25351;&#20196;&#20914;&#31361;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Cognitive Awakening Prompting&#30340;&#25216;&#26415;&#26469;&#25913;&#21892;&#36825;&#19968;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01091v2 Announce Type: replace  Abstract: Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. The dataset and code 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#27531;&#24046;&#25193;&#25955;&#27169;&#22411;&#65288;CIResDiff&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#24182;&#29983;&#25104;&#24739;&#32773;&#38543;&#35775;CT&#22270;&#20687;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00938</link><description>&lt;p&gt;
CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#30693;&#35782;&#25913;&#36827;&#30340;&#27531;&#24046;&#25193;&#25955;&#27169;&#22411;&#65288;CIResDiff&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#21021;&#22987;CT&#25195;&#25551;&#39044;&#27979;&#24182;&#29983;&#25104;&#24739;&#32773;&#38543;&#35775;CT&#22270;&#20687;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#29305;&#21457;&#24615;&#32954;&#32420;&#32500;&#21270;&#65288;IPF&#65289;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00938v2 Announce Type: replace-cross  Abstract: The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UlRe-NeRF&#30340;3D&#36229;&#22768;&#25104;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#28210;&#26579;&#19982;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#32467;&#21512;&#36215;&#26469;&#65292;&#36816;&#29992;&#23450;&#21521;&#30340;&#20840;&#36830;&#25509;&#65288;MLP&#65289;&#27169;&#22359;&#26469;&#29983;&#25104;&#22522;&#20110;&#35270;&#35273;&#30340;&#39640;&#39057;&#21453;&#23556;&#24378;&#24230;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;MLP&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2408.00860</link><description>&lt;p&gt;
UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UlRe-NeRF&#30340;3D&#36229;&#22768;&#25104;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#28210;&#26579;&#19982;&#36229;&#22768;&#21453;&#23556;&#26041;&#21521;&#21442;&#25968;&#21270;&#32467;&#21512;&#36215;&#26469;&#65292;&#36816;&#29992;&#23450;&#21521;&#30340;&#20840;&#36830;&#25509;&#65288;MLP&#65289;&#27169;&#22359;&#26469;&#29983;&#25104;&#22522;&#20110;&#35270;&#35273;&#30340;&#39640;&#39057;&#21453;&#23556;&#24378;&#24230;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;MLP&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00860v2 Announce Type: replace  Abstract: Three-dimensional ultrasound imaging is a critical technology widely used in medical diagnostics. However, traditional 3D ultrasound imaging methods have limitations such as fixed resolution, low storage efficiency, and insufficient contextual connectivity, leading to poor performance in handling complex artifacts and reflection characteristics. Recently, techniques based on NeRF (Neural Radiance Fields) have made significant progress in view synthesis and 3D reconstruction, but there remains a research gap in high-quality ultrasound imaging. To address these issues, we propose a new model, UlRe-NeRF, which combines implicit neural networks and explicit ultrasound volume rendering into an ultrasound neural rendering architecture. This model incorporates reflection direction parameterization and harmonic encoding, using a directional MLP module to generate view-dependent high-frequency reflection intensity estimates, and a spatial MLP
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Stain Normalized Pathology Foundational Model&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#23454;&#39564;&#23460;&#21644;&#25195;&#25551;&#20202;&#24341;&#36215;&#30340;&#39068;&#33394;&#24046;&#24322;&#36827;&#34892;&#26579;&#26009;&#26631;&#20934;&#21270;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;&#20840;&#20999;&#29255;&#30149;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#25552;&#21462;&#20013;&#20986;&#29616;&#30340;&#24037;&#20316;&#38598;&#29305;&#23450;&#29305;&#24449;&#23849;&#28291;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#30142;&#30149;&#31867;&#22411;&#21644;&#31361;&#21457;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2408.00380</link><description>&lt;p&gt;
Enhancing Whole Slide Pathology Foundation Models through Stain Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;Stain Normalized Pathology Foundational Model&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#23454;&#39564;&#23460;&#21644;&#25195;&#25551;&#20202;&#24341;&#36215;&#30340;&#39068;&#33394;&#24046;&#24322;&#36827;&#34892;&#26579;&#26009;&#26631;&#20934;&#21270;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;&#20840;&#20999;&#29255;&#30149;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#25552;&#21462;&#20013;&#20986;&#29616;&#30340;&#24037;&#20316;&#38598;&#29305;&#23450;&#29305;&#24449;&#23849;&#28291;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#30142;&#30149;&#31867;&#22411;&#21644;&#31361;&#21457;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00380v2 Announce Type: replace-cross  Abstract: Recent advancements in digital pathology have led to the development of numerous foundational models that utilize self-supervised learning on patches extracted from gigapixel whole slide images (WSIs). While this approach leverages vast amounts of unlabeled data, we have discovered a significant issue: features extracted from these self-supervised models tend to cluster by individual WSIs, a phenomenon we term WSI-specific feature collapse. This problem can potentially limit the model's generalization ability and performance on various downstream tasks. To address this issue, we introduce Stain Normalized Pathology Foundational Model, a novel foundational model trained on patches that have undergone stain normalization. Stain normalization helps reduce color variability arising from different laboratories and scanners, enabling the model to learn more consistent features. Stain Normalized Pathology Foundational Model is trained
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35843;&#26597;&#21644;&#33258;&#21160;&#35843;&#25972;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27010;&#24565;&#38169;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#33021;&#22312;&#23454;&#38469;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#19979;&#28216;&#23454;&#29992;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00230</link><description>&lt;p&gt;
Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35843;&#26597;&#21644;&#33258;&#21160;&#35843;&#25972;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27010;&#24565;&#38169;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#33021;&#22312;&#23454;&#38469;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#19979;&#28216;&#23454;&#29992;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00230v2 Announce Type: replace  Abstract: Advancements in text-to-image diffusion models have broadened extensive downstream practical applications, but such models often encounter misalignment issues between text and image. Taking the generation of a combination of two disentangled concepts as an example, say given the prompt "a tea cup of iced coke", existing models usually generate a glass cup of iced coke because the iced coke usually co-occurs with the glass cup instead of the tea one during model training. The root of such misalignment is attributed to the confusion in the latent semantic space of text-to-image diffusion models, and hence we refer to the "a tea cup of iced coke" phenomenon as Latent Concept Misalignment (LC-Mis). We leverage large language models (LLMs) to thoroughly investigate the scope of LC-Mis, and develop an automated pipeline for aligning the latent semantics of diffusion models to text prompts. Empirical assessments confirm the effectiveness of
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23545;&#20110;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24212;&#23545;&#26377;&#24847;&#21644;&#26080;&#24847;&#30340;&#31995;&#32479;&#24178;&#25200;&#19979;&#30340;&#38887;&#24615;&#21450;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#26041;&#27861;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#26102;&#24517;&#39035;&#35299;&#20915;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#36825;&#31687;&#24037;&#20316;&#19981;&#20165;&#26159;&#39318;&#20010;&#23558;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#23545;&#26377;&#24847;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#36827;&#34892;&#32508;&#21512;&#35843;&#26597;&#30340;&#35770;&#25991;&#65292;&#32780;&#19988;&#35206;&#30422;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#24178;&#25200;&#24773;&#20917;&#19979;&#30340;&#30456;&#20284;&#30740;&#31350;&#65292;&#20419;&#36827;&#20102;&#30456;&#20851;&#31038;&#21306;&#20043;&#38388;&#30740;&#31350;&#30340;&#34701;&#21512;&#19982;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2408.00193</link><description>&lt;p&gt;
Resilience and Security of Deep Neural Networks Against Intentional and Unintentional Perturbations: Survey and Research Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23545;&#20110;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24212;&#23545;&#26377;&#24847;&#21644;&#26080;&#24847;&#30340;&#31995;&#32479;&#24178;&#25200;&#19979;&#30340;&#38887;&#24615;&#21450;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#26041;&#27861;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#26102;&#24517;&#39035;&#35299;&#20915;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#36825;&#31687;&#24037;&#20316;&#19981;&#20165;&#26159;&#39318;&#20010;&#23558;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#23545;&#26377;&#24847;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#36827;&#34892;&#32508;&#21512;&#35843;&#26597;&#30340;&#35770;&#25991;&#65292;&#32780;&#19988;&#35206;&#30422;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#24178;&#25200;&#24773;&#20917;&#19979;&#30340;&#30456;&#20284;&#30740;&#31350;&#65292;&#20419;&#36827;&#20102;&#30456;&#20851;&#31038;&#21306;&#20043;&#38388;&#30740;&#31350;&#30340;&#34701;&#21512;&#19982;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00193v2 Announce Type: replace-cross  Abstract: In order to deploy deep neural networks (DNNs) in high-stakes scenarios, it is imperative that DNNs provide inference robust to external perturbations - both intentional and unintentional. Although the resilience of DNNs to intentional and unintentional perturbations has been widely investigated, a unified vision of these inherently intertwined problem domains is still missing. In this work, we fill this gap by providing a survey of the state of the art and highlighting the similarities of the proposed approaches.We also analyze the research challenges that need to be addressed to deploy resilient and secure DNNs. As there has not been any such survey connecting the resilience of DNNs to intentional and unintentional perturbations, we believe this work can help advance the frontier in both domains by enabling the exchange of ideas between the two communities.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22522;&#20110;&#21367;&#31215;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#21644;&#32479;&#19968;&#35760;&#24518;&#26426;&#21046;&#23558;&#22270;&#30340;&#25299;&#25169;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21367;&#31215;GNNs&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2408.00165</link><description>&lt;p&gt;
Non-convolutional Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00165
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22522;&#20110;&#21367;&#31215;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#21644;&#32479;&#19968;&#35760;&#24518;&#26426;&#21046;&#23558;&#22270;&#30340;&#25299;&#25169;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21367;&#31215;GNNs&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00165v2 Announce Type: replace-cross  Abstract: Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Preference-Based Abstract Argumentation for Case-Based Reasoning&#65288;AA-CBR-P&#65289;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#29992;&#25143;&#23450;&#20041;&#30340;&#20559;&#22909;&#21644;&#25277;&#35937;Argumentation&#19982;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#12290;&#36825;&#19968;&#21019;&#26032;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20570;&#39044;&#27979;&#26102;&#36981;&#24490;&#29992;&#25143;&#35774;&#23450;&#30340;&#20559;&#22909;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#34920;&#36798;&#23545;&#35770;&#35777;&#32452;&#25104;&#37096;&#20998;&#30340;&#20559;&#22909;&#26041;&#38754;&#65292;&#20197;&#24448;&#30340;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#36825;&#19968;&#27169;&#22411;&#22914;&#20309;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#23545;&#33041;&#32959;&#30244;&#24739;&#32773;&#20351;&#29992;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#30340;&#20020;&#24202;&#35797;&#39564;&#12290;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;&#36825;&#31181;&#32467;&#21512;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2408.00108</link><description>&lt;p&gt;
Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00108
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Preference-Based Abstract Argumentation for Case-Based Reasoning&#65288;AA-CBR-P&#65289;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#29992;&#25143;&#23450;&#20041;&#30340;&#20559;&#22909;&#21644;&#25277;&#35937;Argumentation&#19982;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#12290;&#36825;&#19968;&#21019;&#26032;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20570;&#39044;&#27979;&#26102;&#36981;&#24490;&#29992;&#25143;&#35774;&#23450;&#30340;&#20559;&#22909;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#34920;&#36798;&#23545;&#35770;&#35777;&#32452;&#25104;&#37096;&#20998;&#30340;&#20559;&#22909;&#26041;&#38754;&#65292;&#20197;&#24448;&#30340;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#36825;&#19968;&#27169;&#22411;&#22914;&#20309;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#23545;&#33041;&#32959;&#30244;&#24739;&#32773;&#20351;&#29992;&#19981;&#21516;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#30340;&#20020;&#24202;&#35797;&#39564;&#12290;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#65292;&#25991;&#31456;&#35777;&#26126;&#20102;&#36825;&#31181;&#32467;&#21512;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00108v2 Announce Type: replace  Abstract: In the pursuit of enhancing the efficacy and flexibility of interpretable, data-driven classification models, this work introduces a novel incorporation of user-defined preferences with Abstract Argumentation and Case-Based Reasoning (CBR). Specifically, we introduce Preference-Based Abstract Argumentation for Case-Based Reasoning (which we call AA-CBR-P), allowing users to define multiple approaches to compare cases with an ordering that specifies their preference over these comparison approaches. We prove that the model inherently follows these preferences when making predictions and show that previous abstract argumentation for case-based reasoning approaches are insufficient at expressing preferences over constituents of an argument. We then demonstrate how this can be applied to a real-world medical dataset sourced from a clinical trial evaluating differing assessment methods of patients with a primary brain tumour. We show empi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23545;&#36890;&#36807;&#19982;&#20195;&#29702;&#20114;&#21160;&#36827;&#34892;&#30340;&#31038;&#20250;&#23398;&#20064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#22914;&#20309;&#20419;&#36827;&#20102;&#26032;&#24418;&#24335;&#30340;&#31038;&#20132;&#23398;&#20064;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#34892;&#20026;&#20811;&#38534;&#12289;&#19979;&#19968;&#20010;&#23383;&#39044;&#27979;&#20197;&#21450;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#27169;&#20223;&#21644;&#25945;&#32946;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#33021;&#22815;&#20114;&#30456;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#20840;&#36890;&#20449;&#20195;&#29702;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2407.21713</link><description>&lt;p&gt;
Social Learning through Interactions with Other Agents: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23545;&#36890;&#36807;&#19982;&#20195;&#29702;&#20114;&#21160;&#36827;&#34892;&#30340;&#31038;&#20250;&#23398;&#20064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#22914;&#20309;&#20419;&#36827;&#20102;&#26032;&#24418;&#24335;&#30340;&#31038;&#20132;&#23398;&#20064;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#34892;&#20026;&#20811;&#38534;&#12289;&#19979;&#19968;&#20010;&#23383;&#39044;&#27979;&#20197;&#21450;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#27169;&#20223;&#21644;&#25945;&#32946;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#33021;&#22815;&#20114;&#30456;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#20840;&#36890;&#20449;&#20195;&#29702;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21713v2 Announce Type: replace-cross  Abstract: Social learning plays an important role in the development of human intelligence. As children, we imitate our parents' speech patterns until we are able to produce sounds; we learn from them praising us and scolding us; and as adults, we learn by working with others. In this work, we survey the degree to which this paradigm -- social learning -- has been mirrored in machine learning. In particular, since learning socially requires interacting with others, we are interested in how embodied agents can and have utilised these techniques. This is especially in light of the degree to which recent advances in natural language processing (NLP) enable us to perform new forms of social learning. We look at how behavioural cloning and next-token prediction mirror human imitation, how learning from human feedback mirrors human education, and how we can go further to enable fully communicative agents that learn from each other. We find tha
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27969;&#24335;&#31639;&#27861;&#21644;k-means&#32858;&#31867;&#25216;&#26415;&#30340;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;RAG&#20869;&#23384;&#28040;&#32791;&#22823;&#21644;&#26080;&#27861;&#23454;&#26102;&#26356;&#26032;&#32034;&#24341;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#27969;&#24335;&#31639;&#27861;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#25552;&#39640;&#26597;&#35810;&#25928;&#29575;&#65292;&#23454;&#39564;&#39564;&#35777;&#20102;&#26032;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.21300</link><description>&lt;p&gt;
Implementing Streaming algorithm and k-means clusters to RAG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27969;&#24335;&#31639;&#27861;&#21644;k-means&#32858;&#31867;&#25216;&#26415;&#30340;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;RAG&#20869;&#23384;&#28040;&#32791;&#22823;&#21644;&#26080;&#27861;&#23454;&#26102;&#26356;&#26032;&#32034;&#24341;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#27969;&#24335;&#31639;&#27861;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#25552;&#39640;&#26597;&#35810;&#25928;&#29575;&#65292;&#23454;&#39564;&#39564;&#35777;&#20102;&#26032;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21300v2 Announce Type: replace-cross  Abstract: Retrieval-augmented generation (RAG) has achieved great success in information retrieval to assist large language models because it builds an external knowledge database. However, it also has many problems: it consumes a lot of memory because of the huge database. When faced with massive streaming data, it is unable to update the established index database in time. To save the memory of building the database and maintain accuracy simultaneously, we proposed a new approach combining a streaming algorithm and k-means cluster with RAG. Our approach applies a streaming algorithm to update the index and reduce memory consumption. Then use the k-means algorithm to cluster documents with high similarities together, the query time will be shortened by doing this. We conducted comparative experiments on four methods, and the results show that RAG with streaming algorithm and k-means cluster performs well in accuracy and memory. For mass
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35774;&#35745;&#20102;&#19968;&#31181;&#26082;&#33021;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#21448;&#20855;&#26377;&#20302;&#33021;&#32791;&#30340;&#22522;&#20110;&#25972;&#25968;&#20540;&#35757;&#32451;&#21644;&#22522;&#20110;&#35302;&#21457;&#29468;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23545;&#35937;&#26816;&#27979;&#30340;&#39640;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2407.20708</link><description>&lt;p&gt;
Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.20708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35774;&#35745;&#20102;&#19968;&#31181;&#26082;&#33021;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#21448;&#20855;&#26377;&#20302;&#33021;&#32791;&#30340;&#22522;&#20110;&#25972;&#25968;&#20540;&#35757;&#32451;&#21644;&#22522;&#20110;&#35302;&#21457;&#29468;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23545;&#35937;&#26816;&#27979;&#30340;&#39640;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.20708v3 Announce Type: replace  Abstract: Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and low-power advantages over Artificial Neural Networks (ANNs). Applications of SNNs are currently limited to simple classification tasks because of their poor performance. In this work, we focus on bridging the performance gap between ANNs and SNNs on object detection. Our design revolves around network architecture and spiking neuron. First, the overly complex module design causes spike degradation when the YOLO series is converted to the corresponding spiking version. We design a SpikeYOLO architecture to solve this problem by simplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object detection is more sensitive to quantization errors in the conversion of membrane potentials into binary spikes by spiking neurons. To address this challenge, we design a new spiking neuron that activates Integer values during training while maintaining spike-driv
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AOTree&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22240;&#32032;&#39034;&#24207;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31995;&#32479;&#20165;&#20851;&#27880;&#20869;&#23481;&#37325;&#35201;&#24615;&#32780;&#24573;&#35270;&#39034;&#24207;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.19937</link><description>&lt;p&gt;
AOTree: Aspect Order Tree-based Model for Explainable Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AOTree&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22240;&#32032;&#39034;&#24207;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31995;&#32479;&#20165;&#20851;&#27880;&#20869;&#23481;&#37325;&#35201;&#24615;&#32780;&#24573;&#35270;&#39034;&#24207;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19937v2 Announce Type: replace-cross  Abstract: Recent recommender systems aim to provide not only accurate recommendations but also explanations that help users understand them better. However, most existing explainable recommendations only consider the importance of content in reviews, such as words or aspects, and ignore the ordering relationship among them. This oversight neglects crucial ordering dimensions in the human decision-making process, leading to suboptimal performance. Therefore, in this paper, we propose Aspect Order Tree-based (AOTree) explainable recommendation method, inspired by the Order Effects Theory from cognitive and decision psychology, in order to capture the dependency relationships among decisive factors. We first validate the theory in the recommendation scenario by analyzing the reviews of the users. Then, according to the theory, the proposed AOTree expands the construction of the decision tree to capture aspect orders in users' decision-makin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VersusDebias&#30340;&#20840;&#26032;&#36890;&#29992;&#38646;&#26679;&#26412;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#24615;&#23545;&#25239;&#26426;&#21046;&#65292;&#33021;&#22815;&#38024;&#23545;&#19981;&#21516;T2I&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#33258;&#25105;&#36866;&#24212;&#24615;&#20248;&#21270;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#27169;&#22411;&#24187;&#35937;&#24341;&#36215;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2407.19524</link><description>&lt;p&gt;
VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VersusDebias&#30340;&#20840;&#26032;&#36890;&#29992;&#38646;&#26679;&#26412;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#24615;&#23545;&#25239;&#26426;&#21046;&#65292;&#33021;&#22815;&#38024;&#23545;&#19981;&#21516;T2I&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#33258;&#25105;&#36866;&#24212;&#24615;&#20248;&#21270;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#27169;&#22411;&#24187;&#35937;&#24341;&#36215;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19524v2 Announce Type: replace  Abstract: With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#19982;&#29983;&#25104;&#27169;&#22411;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#25216;&#33021;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20256;&#36798;&#32473;&#23398;&#20064;&#32773;&#65292;&#25552;&#39640;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2407.19393</link><description>&lt;p&gt;
Integrating Cognitive AI with Generative Models for Enhanced Question Answering in Skill-based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19393
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#19982;&#29983;&#25104;&#27169;&#22411;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#25216;&#33021;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20256;&#36798;&#32473;&#23398;&#20064;&#32773;&#65292;&#25552;&#39640;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19393v2 Announce Type: replace  Abstract: In online learning, the ability to provide quick and accurate feedback to learners is crucial. In skill-based learning, learners need to understand the underlying concepts and mechanisms of a skill to be able to apply it effectively. While videos are a common tool in online learning, they cannot comprehend or assess the skills being taught. Additionally, while Generative AI methods are effective in searching and retrieving answers from a text corpus, it remains unclear whether these methods exhibit any true understanding. This limits their ability to provide explanations of skills or help with problem-solving. This paper proposes a novel approach that merges Cognitive AI and Generative AI to address these challenges. We employ a structured knowledge representation, the TMK (Task-Method-Knowledge) model, to encode skills taught in an online Knowledge-based AI course. Leveraging techniques such as Large Language Models, Chain-of-Though
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#65288;MobAgent&#65289;&#65292;&#36890;&#36807;&#29702;&#35299;&#21644;&#25512;&#29702;&#20004;&#20010;&#38454;&#27573;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#30340;&#26053;&#34892;&#26085;&#35760;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#20010;&#20307;&#30340;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#20197;&#29983;&#25104;&#31526;&#21512;&#29616;&#23454;&#19990;&#30028;&#24773;&#22659;&#30340;&#36712;&#36857;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2407.18932</link><description>&lt;p&gt;
Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#65288;MobAgent&#65289;&#65292;&#36890;&#36807;&#29702;&#35299;&#21644;&#25512;&#29702;&#20004;&#20010;&#38454;&#27573;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#30340;&#26053;&#34892;&#26085;&#35760;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#20010;&#20307;&#30340;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#20197;&#29983;&#25104;&#31526;&#21512;&#29616;&#23454;&#19990;&#30028;&#24773;&#22659;&#30340;&#36712;&#36857;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18932v2 Announce Type: replace-cross  Abstract: Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#36801;&#31227;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#22797;&#26434;&#30340;&#37117;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#35268;&#21010;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#20351;&#29992;&#25552;&#39640;&#35268;&#21010;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#19987;&#23478;&#25968;&#25454;&#20013;&#33719;&#24471;&#30693;&#35782;&#65292;&#20877;&#23558;&#20854;&#36801;&#31227;&#21040;&#29992;&#25143;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2407.18569</link><description>&lt;p&gt;
PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#36801;&#31227;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20010;&#24615;&#21270;&#39550;&#39542;&#35268;&#21010;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#22797;&#26434;&#30340;&#37117;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#35268;&#21010;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#20351;&#29992;&#25552;&#39640;&#35268;&#21010;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#19987;&#23478;&#25968;&#25454;&#20013;&#33719;&#24471;&#30693;&#35782;&#65292;&#20877;&#23558;&#20854;&#36801;&#31227;&#21040;&#29992;&#25143;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18569v3 Announce Type: replace  Abstract: Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a fundamental resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tunin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20013;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#24182;&#32467;&#21512;&#40644;&#37329;&#20195;&#30721;&#21453;&#39304;&#30340;&#20840;&#26032;&#26041;&#27861;&#26469;&#25552;&#21319;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#21160;&#29983;&#25104;Verilog&#20195;&#30721;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;&#25968;&#25454;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#25991;&#20013;&#20171;&#32461;&#30340;6.7&#20159;&#21442;&#25968;&#27169;&#22411;&#22312;&#19982;&#20854;&#20182;13&#20159;&#21644;16&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#30452;&#25509;&#24494;&#35843;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#38480;&#21046;&#65292;&#25991;&#20013;&#36824;&#25506;&#35752;&#20102;&#22312;&#36825;&#31181;&#25216;&#26415;&#36827;&#27493;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2407.18271</link><description>&lt;p&gt;
Large Language Model for Verilog Generation with Golden Code Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20013;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#24182;&#32467;&#21512;&#40644;&#37329;&#20195;&#30721;&#21453;&#39304;&#30340;&#20840;&#26032;&#26041;&#27861;&#26469;&#25552;&#21319;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#21160;&#29983;&#25104;Verilog&#20195;&#30721;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;&#25968;&#25454;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#25991;&#20013;&#20171;&#32461;&#30340;6.7&#20159;&#21442;&#25968;&#27169;&#22411;&#22312;&#19982;&#20854;&#20182;13&#20159;&#21644;16&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#30452;&#25509;&#24494;&#35843;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#38480;&#21046;&#65292;&#25991;&#20013;&#36824;&#25506;&#35752;&#20102;&#22312;&#36825;&#31181;&#25216;&#26415;&#36827;&#27493;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18271v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) have catalyzed significant interest in the automatic generation of Register-Transfer Level (RTL) code, particularly Verilog, from natural language instructions. While commercial LLMs like ChatGPT have dominated this domain, open-source alternatives have lagged considerably in performance, limiting the flexibility and data privacy of this emerging technology. This study introduces a novel approach utilizing reinforcement learning with golden code feedback to enhance the performance of pre-trained models. Leveraging open-source data and base models, we have achieved state-of-the-art (SOTA) results with a substantial margin. Notably, our 6.7B parameter model \ours{} demonstrates superior performance compared to current best-in-class 13B and 16B models. Furthermore, through a comprehensive analysis of the limitations in direct fine-tuning and the training dynamics of reinforcement
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#25512;&#29702;&#30340;&#24490;&#29615;&#31185;&#23398;&#21457;&#29616;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#12290;&#25991;&#31456;&#36824;&#23450;&#20041;&#20102;&#19968;&#20010;&#20174;&#31038;&#20250;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#30340;&#35299;&#37322;&#36873;&#25321;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#20102;&#29616;&#26377;&#30340;&#27010;&#24565;&#24182;&#25193;&#23637;&#20102;&#26032;&#30340;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.17454</link><description>&lt;p&gt;
Automated Explanation Selection for Scientific Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#21160;&#25512;&#29702;&#30340;&#24490;&#29615;&#31185;&#23398;&#21457;&#29616;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#36873;&#25321;&#35299;&#37322;&#12290;&#25991;&#31456;&#36824;&#23450;&#20041;&#20102;&#19968;&#20010;&#20174;&#31038;&#20250;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#30340;&#35299;&#37322;&#36873;&#25321;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#20102;&#29616;&#26377;&#30340;&#27010;&#24565;&#24182;&#25193;&#23637;&#20102;&#26032;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17454v2 Announce Type: replace  Abstract: Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explanability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#24320;&#21457;&#20102;PyBench&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#34913;&#37327;&#20855;&#26377;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;LLM&#20195;&#29702;&#22312;&#35299;&#20915;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#20687;&#32534;&#36753;&#31561;&#12290;PyBench&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#23613;&#31649;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#32534;&#30721;&#20219;&#21153;&#21364;&#28085;&#30422;&#20102;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;PyBench&#25104;&#20026;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#27169;&#25311;&#32534;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2407.16732</link><description>&lt;p&gt;
PyBench: Evaluating LLM Agent on various real-world coding tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#24320;&#21457;&#20102;PyBench&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#34913;&#37327;&#20855;&#26377;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;LLM&#20195;&#29702;&#22312;&#35299;&#20915;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#20687;&#32534;&#36753;&#31561;&#12290;PyBench&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#23613;&#31649;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#32534;&#30721;&#20219;&#21153;&#21364;&#28085;&#30422;&#20102;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;PyBench&#25104;&#20026;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#27169;&#25311;&#32534;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16732v2 Announce Type: replace-cross  Abstract: The LLM Agent, equipped with a code interpreter, is capable of automatically solving real-world coding tasks, such as data analysis and image editing.   However, existing benchmarks primarily focus on either simplistic tasks, such as completing a few lines of code, or on extremely complex and specific tasks at the repository level, neither of which are representative of various daily coding tasks.   To address this gap, we introduce \textbf{PyBench}, a benchmark encompassing five main categories of real-world tasks, covering more than 10 types of files. Given a high-level user query and related files, the LLM Agent needs to reason and execute Python code via a code interpreter for a few turns before making a formal response to fulfill the user's requirements. Successfully addressing tasks in PyBench demands a robust understanding of various Python packages, superior reasoning capabilities, and the ability to incorporate feedbac
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32852;&#21512;&#27861;&#24459;&#26381;&#21153;&#26694;&#26550;&#65292;&#21517;&#20026;LawLuo&#65292;&#26088;&#22312;&#20026;&#38750;&#27861;&#24459;&#32972;&#26223;&#29992;&#25143;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#27861;&#24459;&#20107;&#21153;&#25152;&#30340;&#21672;&#35810;&#20307;&#39564;&#12290;LawLuo&#36890;&#36807;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#21046;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#26597;&#35810;&#21644;&#39057;&#32321;&#30340;&#23545;&#35805;&#36718;&#27425;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#35299;&#20915;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.16252</link><description>&lt;p&gt;
LawLuo: A Chinese Law Firm Co-run by LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32852;&#21512;&#27861;&#24459;&#26381;&#21153;&#26694;&#26550;&#65292;&#21517;&#20026;LawLuo&#65292;&#26088;&#22312;&#20026;&#38750;&#27861;&#24459;&#32972;&#26223;&#29992;&#25143;&#25552;&#20379;&#25509;&#36817;&#30495;&#23454;&#27861;&#24459;&#20107;&#21153;&#25152;&#30340;&#21672;&#35810;&#20307;&#39564;&#12290;LawLuo&#36890;&#36807;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#21046;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#26597;&#35810;&#21644;&#39057;&#32321;&#30340;&#23545;&#35805;&#36718;&#27425;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#35299;&#20915;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16252v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate substantial potential in delivering legal consultation services to users without a legal background, attributed to their superior text comprehension and generation capabilities. Nonetheless, existing Chinese legal LLMs limit interaction to a single model-user dialogue, unlike the collaborative consultations typical of law firms, where multiple staff members contribute to a single consultation. This limitation prevents an authentic consultation experience. Additionally, extant Chinese legal LLMs suffer from critical limitations: (1) insufficient control over the quality of instruction fine-tuning data; (2) increased model hallucination resulting from users' ambiguous queries; and (3) a reduction in the model's ability to follow instructions over multiple dialogue turns. In response to these challenges, we propose a novel legal dialogue framework that leverages the collaborative capabiliti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#20998;&#26512;Chat GPT&#25910;&#38598;&#30340;&#26377;&#20851;&#8220;Spring&#8221;&#19968;&#35789;&#30340;&#22235;&#31181;&#19981;&#21516;&#21547;&#20041;&#30340;1000&#21477;&#25991;&#26412;&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#32454;&#32990;&#30340;&#24494;&#23567;&#21464;&#24322;&#22914;&#20309;&#23548;&#33268;&#35789;&#20041;&#30340;&#22810;&#20041;&#24615;&#65292;&#36825;&#26159;&#35813;&#35789;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#33719;&#24471;&#19981;&#21516;&#24847;&#20041;&#30340;&#36827;&#21270;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2407.16110</link><description>&lt;p&gt;
Analyzing the Polysemy Evolution using Semantic Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#20998;&#26512;Chat GPT&#25910;&#38598;&#30340;&#26377;&#20851;&#8220;Spring&#8221;&#19968;&#35789;&#30340;&#22235;&#31181;&#19981;&#21516;&#21547;&#20041;&#30340;1000&#21477;&#25991;&#26412;&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#32454;&#32990;&#30340;&#24494;&#23567;&#21464;&#24322;&#22914;&#20309;&#23548;&#33268;&#35789;&#20041;&#30340;&#22810;&#20041;&#24615;&#65292;&#36825;&#26159;&#35813;&#35789;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#33719;&#24471;&#19981;&#21516;&#24847;&#20041;&#30340;&#36827;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16110v2 Announce Type: replace-cross  Abstract: The senses of words evolve. The sense of the same word may change from today to tomorrow, and multiple senses of the same word may be the result of the evolution of each other, that is, they may be parents and children. If we view Juba as an evolving ecosystem, the paradigm of learning the correct answer, which does not move with the sense of a word, is no longer valid. This paper is a case study that shows that word polysemy is an evolutionary consequence of the modification of Semantic Cells, which has al-ready been presented by the author, by introducing a small amount of diversity in its initial state as an example of analyzing the current set of short sentences. In particular, the analysis of a sentence sequence of 1000 sentences in some order for each of the four senses of the word Spring, collected using Chat GPT, shows that the word acquires the most polysemy monotonically in the analysis when the senses are arranged in
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#26500;&#24314;&#20102;&#21253;&#21547;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#26368;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#30149;&#29702;&#35786;&#26029;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;H&amp;E&#26579;&#33394;&#20999;&#29255;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#22312;32&#31181;&#30284;&#30151;&#31867;&#22411;&#20013;&#30340;10,275&#21517;&#24739;&#32773;&#20013;&#30340;&#39640;&#25928;&#30149;&#29702;&#25253;&#21578;&#29983;&#25104;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#39640;&#20102;&#30149;&#29702;&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#32508;&#21512;&#21033;&#29992;&#20026;&#20010;&#24615;&#21270;&#21307;&#23398;&#21644;&#31934;&#20934;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2407.15362</link><description>&lt;p&gt;
A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#22320;&#26500;&#24314;&#20102;&#21253;&#21547;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#26368;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#30149;&#29702;&#35786;&#26029;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;H&amp;E&#26579;&#33394;&#20999;&#29255;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#22312;32&#31181;&#30284;&#30151;&#31867;&#22411;&#20013;&#30340;10,275&#21517;&#24739;&#32773;&#20013;&#30340;&#39640;&#25928;&#30149;&#29702;&#25253;&#21578;&#29983;&#25104;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#39640;&#20102;&#30149;&#29702;&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#30149;&#29702;&#25253;&#21578;&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#32508;&#21512;&#21033;&#29992;&#20026;&#20010;&#24615;&#21270;&#21307;&#23398;&#21644;&#31934;&#20934;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15362v2 Announce Type: replace  Abstract: Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\&amp;E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#22120;&#22312;&#21360;&#24230;&#25991;&#21270;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35813;&#25216;&#26415;&#22312;&#36755;&#20986;&#20869;&#23481;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#23545;&#38750;&#35199;&#26041;&#25991;&#21270;&#30340;&#35823;&#20195;&#34920;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#35774;&#35745;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2407.14779</link><description>&lt;p&gt;
Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.14779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#22120;&#22312;&#21360;&#24230;&#25991;&#21270;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35813;&#25216;&#26415;&#22312;&#36755;&#20986;&#20869;&#23481;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#23545;&#38750;&#35199;&#26041;&#25991;&#21270;&#30340;&#35823;&#20195;&#34920;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#35774;&#35745;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.14779v3 Announce Type: replace-cross  Abstract: Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a community-centered approach and grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, aiming to address these issues and contribute to the development of more equitable and representative GA
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;ToG2.0&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#26679;&#21270;&#26597;&#35810;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#20449;&#24687;&#31934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#30693;&#35782;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.10805</link><description>&lt;p&gt;
Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;ToG2.0&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#26679;&#21270;&#26597;&#35810;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#20449;&#24687;&#31934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#30693;&#35782;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10805v2 Announce Type: replace-cross  Abstract: Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG${2.0}$ not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential o
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISMRNN&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;Mamba&#30340;&#38544;&#24335;&#20998;&#21106;RNN&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#20998;&#21106;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#21152;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#24207;&#21015;&#20449;&#24687;&#30340;&#20445;&#30041;&#38382;&#39064;&#65292;&#20174;&#32780;&#23545;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;ISMRNN&#36890;&#36807;&#25913;&#36827;&#30340;&#38544;&#24335;&#20998;&#21106;&#21644;&#27969;&#32447;&#22411;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;RNN&#26041;&#27861;&#22312;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#21644;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#36830;&#32493;&#24615;&#30340;&#20445;&#30041;&#21644;&#20449;&#24687;&#22312;&#25972;&#20010;&#24207;&#21015;&#20013;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;ISMRNN&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2407.10768</link><description>&lt;p&gt;
ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.10768
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISMRNN&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;Mamba&#30340;&#38544;&#24335;&#20998;&#21106;RNN&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#20998;&#21106;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#21152;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#24207;&#21015;&#20449;&#24687;&#30340;&#20445;&#30041;&#38382;&#39064;&#65292;&#20174;&#32780;&#23545;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;ISMRNN&#36890;&#36807;&#25913;&#36827;&#30340;&#38544;&#24335;&#20998;&#21106;&#21644;&#27969;&#32447;&#22411;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;RNN&#26041;&#27861;&#22312;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#21644;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#36830;&#32493;&#24615;&#30340;&#20445;&#30041;&#21644;&#20449;&#24687;&#22312;&#25972;&#20010;&#24207;&#21015;&#20013;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;ISMRNN&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10768v5 Announce Type: replace-cross  Abstract: Long time series forecasting aims to utilize historical information to forecast future states over extended horizons. Traditional RNN-based series forecasting methods struggle to effectively address long-term dependencies and gradient issues in long time series problems. Recently, SegRNN has emerged as a leading RNN-based model tailored for long-term series forecasting, demonstrating state-of-the-art performance while maintaining a streamlined architecture through innovative segmentation and parallel decoding techniques. Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts data continuity and fails to effectively leverage information across different segments, the segmentation strategy employed by SegRNN does not fundamentally address the issue of information loss within the recurrent structure. To address these issues, we propose the ISMRNN method with three key enhancements: we introduce an implicit s
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#26469;&#39044;&#27979;&#21644;&#29702;&#35299;&#20154;&#31867;&#30340;&#34892;&#20026;&#20915;&#31574;&#65292;&#24182;&#25581;&#31034;&#20102;AI&#31995;&#32479;&#22312;&#25552;&#20379;&#26377;&#29992;&#24110;&#21161;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2407.09281</link><description>&lt;p&gt;
Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.09281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#23454;&#20363;&#23398;&#20064;&#26469;&#39044;&#27979;&#21644;&#29702;&#35299;&#20154;&#31867;&#30340;&#34892;&#20026;&#20915;&#31574;&#65292;&#24182;&#25581;&#31034;&#20102;AI&#31995;&#32479;&#22312;&#25552;&#20379;&#26377;&#29992;&#24110;&#21161;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.09281v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have demonstrated their capabilities across various tasks, from language translation to complex reasoning. Understanding and predicting human behavior and biases are crucial for artificial intelligence (AI) assisted systems to provide useful assistance, yet it remains an open question whether these models can achieve this. This paper addresses this gap by leveraging the reasoning and generative capabilities of the LLMs to predict human behavior in two sequential decision-making tasks. These tasks involve balancing between exploitative and exploratory actions and handling delayed feedback, both essential for simulating real-life decision processes. We compare the performance of LLMs with a cognitive instance-based learning (IBL) model, which imitates human experiential decision-making. Our findings indicate that LLMs excel at rapidly incorporating feedback to enhance prediction accuracy. In contrast, the c
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25581;&#31034;&#20102;&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#21457;&#23637;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#24378;&#35843;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#21644;&#25968;&#25454;&#24320;&#21457;&#20013;&#30340;&#30456;&#20114;&#20419;&#36827;&#20316;&#29992;&#65292;&#20026;&#25171;&#36896;&#26356;&#20026;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#25968;&#25454;&#24320;&#21457;&#27169;&#24335;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25745;&#12290;</title><link>https://arxiv.org/abs/2407.08583</link><description>&lt;p&gt;
The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.08583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25581;&#31034;&#20102;&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#21457;&#23637;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#24378;&#35843;&#20004;&#32773;&#22312;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#21644;&#25968;&#25454;&#24320;&#21457;&#20013;&#30340;&#30456;&#20114;&#20419;&#36827;&#20316;&#29992;&#65292;&#20026;&#25171;&#36896;&#26356;&#20026;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#25968;&#25454;&#24320;&#21457;&#27169;&#24335;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08583v2 Announce Type: replace-cross  Abstract: The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs; on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approache
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#33258;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20248;&#21270;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#37325;&#21551;&#26426;&#21046;&#30340;&#33258;&#36866;&#24212;ES&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#24378;&#22823;&#30340;&#22522;&#20934;&#20989;&#25968; landscapes &#19978;&#23548;&#33322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#35814;&#32454;&#30340;&#27963;&#21160;&#26085;&#24535;&#65292;&#21253;&#25324;&#20248;&#21270;&#21382;&#31243;&#30340; fitness &#28436;&#21464;&#12289; step-size &#30340;&#35843;&#25972;&#20197;&#21450;&#30001;&#20110;&#20572;&#28382;&#24341;&#36215;&#30340;&#37325;&#21551;&#20107;&#20214;&#12290;&#28982;&#21518;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#26085;&#24535;&#65292;&#29983;&#25104;&#28165;&#26224;&#12289;&#21451;&#22909;&#30340;&#24635;&#32467;&#65292;&#36825;&#20123;&#24635;&#32467;&#31361;&#20986;&#24378;&#35843;&#20102;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914; convergence behavior&#12289;&#26368;&#20248; fitness &#25104;&#23601;&#20197;&#21450;&#36973;&#36935;&#23616;&#37096;&#26368;&#20248;&#28857;&#30340;&#23454;&#20363;&#12290;&#36890;&#36807;&#22312; Rastrigin &#20989;&#25968;&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807; LLM &#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545; ES &#20248;&#21270;&#22797;&#26434;&#24615;&#30340;&#36879;&#26126;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20808;&#36827;&#20248;&#21270;&#31639;&#27861;&#19982;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21457;&#25381;&#28508;&#22312;&#26725;&#26753;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2407.08331</link><description>&lt;p&gt;
Towards Explainable Evolution Strategies with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.08331
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#33258;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20248;&#21270;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#37325;&#21551;&#26426;&#21046;&#30340;&#33258;&#36866;&#24212;ES&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#24378;&#22823;&#30340;&#22522;&#20934;&#20989;&#25968; landscapes &#19978;&#23548;&#33322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#35814;&#32454;&#30340;&#27963;&#21160;&#26085;&#24535;&#65292;&#21253;&#25324;&#20248;&#21270;&#21382;&#31243;&#30340; fitness &#28436;&#21464;&#12289; step-size &#30340;&#35843;&#25972;&#20197;&#21450;&#30001;&#20110;&#20572;&#28382;&#24341;&#36215;&#30340;&#37325;&#21551;&#20107;&#20214;&#12290;&#28982;&#21518;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#26085;&#24535;&#65292;&#29983;&#25104;&#28165;&#26224;&#12289;&#21451;&#22909;&#30340;&#24635;&#32467;&#65292;&#36825;&#20123;&#24635;&#32467;&#31361;&#20986;&#24378;&#35843;&#20102;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914; convergence behavior&#12289;&#26368;&#20248; fitness &#25104;&#23601;&#20197;&#21450;&#36973;&#36935;&#23616;&#37096;&#26368;&#20248;&#28857;&#30340;&#23454;&#20363;&#12290;&#36890;&#36807;&#22312; Rastrigin &#20989;&#25968;&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807; LLM &#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545; ES &#20248;&#21270;&#22797;&#26434;&#24615;&#30340;&#36879;&#26126;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20808;&#36827;&#20248;&#21270;&#31639;&#27861;&#19982;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21457;&#25381;&#28508;&#22312;&#26725;&#26753;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.08331v2 Announce Type: replace-cross  Abstract: This paper introduces an approach that integrates self-adaptive Evolution Strategies (ES) with Large Language Models (LLMs) to enhance the explainability of complex optimization processes. By employing a self-adaptive ES equipped with a restart mechanism, we effectively navigate the challenging landscapes of benchmark functions, capturing detailed logs of the optimization journey. The logs include fitness evolution, step-size adjustments and restart events due to stagnation. An LLM is then utilized to process these logs, generating concise, user-friendly summaries that highlight key aspects such as convergence behavior, optimal fitness achievements, and encounters with local optima. Our case study on the Rastrigin function demonstrates how our approach makes the complexities of ES optimization transparent. Our findings highlight the potential of using LLMs to bridge the gap between advanced optimization algorithms and their int
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#20449;&#24687;&#25628;&#32034;&#22330;&#26223;&#20013;&#26159;&#21542;&#23384;&#22312;&#35805;&#35821;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#22238;&#31572;&#29983;&#25104;&#20013;&#23384;&#22312;&#20559;&#22909;&#20351;&#29992;&#19982;&#20854;&#26597;&#35810;&#35821;&#35328;&#30456;&#21516;&#30340;&#35821;&#35328;&#30340;&#20449;&#24687;&#30340;&#31995;&#32479;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24403;&#26597;&#35810;&#35821;&#35328;&#30340;&#20449;&#24687;&#24456;&#23569;&#26102;&#65292;LLMs&#20542;&#21521;&#20110;&#20351;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26723;&#65292;&#36825;&#24378;&#21270;&#20102;&#20027;&#23548;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#20559;&#35265;&#26082;&#36866;&#29992;&#20110;&#20107;&#23454;&#24615;&#26597;&#35810;&#20063;&#36866;&#29992;&#20110;&#24847;&#35265;&#24615;&#26597;&#35810;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#25918;&#22823;&#35821;&#35328;&#24046;&#24322;&#21644;&#30693;&#35782;&#20914;&#31361;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#26469;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65292;&#20197;&#20415;LLMs&#33021;&#22815;&#26356;&#20844;&#24179;&#22320;&#26381;&#21153;&#20110;&#20840;&#29699;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2407.05502</link><description>&lt;p&gt;
Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.05502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#20449;&#24687;&#25628;&#32034;&#22330;&#26223;&#20013;&#26159;&#21542;&#23384;&#22312;&#35805;&#35821;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#22238;&#31572;&#29983;&#25104;&#20013;&#23384;&#22312;&#20559;&#22909;&#20351;&#29992;&#19982;&#20854;&#26597;&#35810;&#35821;&#35328;&#30456;&#21516;&#30340;&#35821;&#35328;&#30340;&#20449;&#24687;&#30340;&#31995;&#32479;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24403;&#26597;&#35810;&#35821;&#35328;&#30340;&#20449;&#24687;&#24456;&#23569;&#26102;&#65292;LLMs&#20542;&#21521;&#20110;&#20351;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26723;&#65292;&#36825;&#24378;&#21270;&#20102;&#20027;&#23548;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#20559;&#35265;&#26082;&#36866;&#29992;&#20110;&#20107;&#23454;&#24615;&#26597;&#35810;&#20063;&#36866;&#29992;&#20110;&#24847;&#35265;&#24615;&#26597;&#35810;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#25918;&#22823;&#35821;&#35328;&#24046;&#24322;&#21644;&#30693;&#35782;&#20914;&#31361;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#26469;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65292;&#20197;&#20415;LLMs&#33021;&#22815;&#26356;&#20844;&#24179;&#22320;&#26381;&#21153;&#20110;&#20840;&#29699;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.05502v2 Announce Type: replace-cross  Abstract: With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are playing a pivotal role in information search and are being adopted globally. Although the multilingual capability of LLMs offers new opportunities to bridge the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences? In this paper, we studied LLM's linguistic preference in a RAG-based information search setting. We found that LLMs displayed systemic bias towards information in the same language as the query language in both information retrieval and answer generation. Furthermore, in scenarios where there is little information in the language of the query, LLMs prefer documents in high-resource languages, reinforcing the dominant views. Such bias exists for both factual and opinion-based queries. Our results highlight the linguistic div
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#39057;&#33021;&#37327;&#25439;&#22833;&#29616;&#35937;&#30340;&#21078;&#26512;&#65292;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#36807;&#24230;&#24179;&#28369;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2407.01281</link><description>&lt;p&gt;
Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.01281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#39057;&#33021;&#37327;&#25439;&#22833;&#29616;&#35937;&#30340;&#21078;&#26512;&#65292;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#36807;&#24230;&#24179;&#28369;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.01281v2 Announce Type: replace-cross  Abstract: In this paper, we explore the approximation theory of functions defined on graphs. Our study builds upon the approximation results derived from the $K$-functional. We establish a theoretical framework to assess the lower bounds of approximation for target functions using Graph Convolutional Networks (GCNs) and examine the over-smoothing phenomenon commonly observed in these networks. Initially, we introduce the concept of a $K$-functional on graphs, establishing its equivalence to the modulus of smoothness. We then analyze a typical type of GCN to demonstrate how the high-frequency energy of the output decays, an indicator of over-smoothing. This analysis provides theoretical insights into the nature of over-smoothing within GCNs. Furthermore, we establish a lower bound for the approximation of target functions by GCNs, which is governed by the modulus of smoothness of these functions. This finding offers a new perspective on t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMHalSnowball&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#20043;&#21069;&#29983;&#25104;&#30340;&#24187;&#35273;&#26102;&#26159;&#21542;&#20250;&#21463;&#21040;&#35823;&#23548;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36935;&#21040;&#24187;&#35273;&#30456;&#20851;&#30340;&#26597;&#35810;&#26102;&#65292;&#24320;&#28304;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#20102;&#33267;&#23569;31%&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25509;&#21463;&#29983;&#25104;&#30340;&#24187;&#35273;&#24182;&#20316;&#20986;&#38169;&#35823;&#30340;&#38472;&#36848;&#12290;</title><link>https://arxiv.org/abs/2407.00569</link><description>&lt;p&gt;
Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.00569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMHalSnowball&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#20043;&#21069;&#29983;&#25104;&#30340;&#24187;&#35273;&#26102;&#26159;&#21542;&#20250;&#21463;&#21040;&#35823;&#23548;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36935;&#21040;&#24187;&#35273;&#30456;&#20851;&#30340;&#26597;&#35810;&#26102;&#65292;&#24320;&#28304;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#20102;&#33267;&#23569;31%&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25509;&#21463;&#29983;&#25104;&#30340;&#24187;&#35273;&#24182;&#20316;&#20986;&#38169;&#35823;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.00569v4 Announce Type: replace  Abstract: Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#35780;&#35770;&#31574;&#30053;&#65292;&#26088;&#22312;&#20026;&#24102;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#21345;&#29260;&#28216;&#25103;&#22914;"&#24191;&#19996;&#40635;&#23558;"&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35780;&#35770;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#29260;&#23616;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#24212;&#30340;&#35780;&#35770;&#25991;&#26412;&#65292;&#27169;&#20223;&#19987;&#19994;&#35780;&#35770;&#21592;&#30340;&#25112;&#30053;&#20998;&#26512;&#21644;&#21465;&#20107;&#33021;&#21147;&#12290;&#31995;&#32479;&#21253;&#25324;&#29366;&#24577;&#35780;&#35770;&#25351;&#21335;&#12289;&#22522;&#20110;&#29702;&#35770;&#24515;&#26234;&#30340;&#31574;&#30053;&#20998;&#26512;&#22120;&#21644;&#39118;&#26684;&#26816;&#32034;&#27169;&#22359;&#65292;&#21327;&#21516;&#24037;&#20316;&#20197;&#22312;&#20013;&#25991;&#29615;&#22659;&#20013;&#25552;&#20379;&#35814;&#32454;&#19988;&#30456;&#20851;&#30340;&#28216;&#25103;&#35780;&#35770;&#12290;</title><link>https://arxiv.org/abs/2406.17807</link><description>&lt;p&gt;
Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.17807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#35780;&#35770;&#31574;&#30053;&#65292;&#26088;&#22312;&#20026;&#24102;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#21345;&#29260;&#28216;&#25103;&#22914;"&#24191;&#19996;&#40635;&#23558;"&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35780;&#35770;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#29260;&#23616;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#24212;&#30340;&#35780;&#35770;&#25991;&#26412;&#65292;&#27169;&#20223;&#19987;&#19994;&#35780;&#35770;&#21592;&#30340;&#25112;&#30053;&#20998;&#26512;&#21644;&#21465;&#20107;&#33021;&#21147;&#12290;&#31995;&#32479;&#21253;&#25324;&#29366;&#24577;&#35780;&#35770;&#25351;&#21335;&#12289;&#22522;&#20110;&#29702;&#35770;&#24515;&#26234;&#30340;&#31574;&#30053;&#20998;&#26512;&#22120;&#21644;&#39118;&#26684;&#26816;&#32034;&#27169;&#22359;&#65292;&#21327;&#21516;&#24037;&#20316;&#20197;&#22312;&#20013;&#25991;&#29615;&#22659;&#20013;&#25552;&#20379;&#35814;&#32454;&#19988;&#30456;&#20851;&#30340;&#28216;&#25103;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.17807v3 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabiliti
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InterCLIP-MEP&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23545;&#25991;&#26412;&#19982;&#22270;&#20687;&#20132;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312; overestimate&#24615;&#33021;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2406.16464</link><description>&lt;p&gt;
InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.16464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InterCLIP-MEP&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23545;&#25991;&#26412;&#19982;&#22270;&#20687;&#20132;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312; overestimate&#24615;&#33021;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.16464v3 Announce Type: replace-cross  Abstract: The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Existing multi-modal sarcasm detection methods have been proven to overestimate performance, as they struggle to effectively capture the intricate sarcastic cues that arise from the interaction between an image and text. To address these issues, we propose InterCLIP-MEP, a novel framework for multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP (InterCLIP) as the backbone to extract text-image representations, enhancing them by embedding cross-modality information directly within each encoder, thereby improving the representations to capture text-image interactions better. Furthermore, an efficient training strategy is designed to adapt InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic, fixed-length dual-channel mem
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23545;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;Large Language Models&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#23545;&#35805;&#31867;&#22411;&#65288;&#22914;&#24320;&#25918;&#22495;&#23545;&#35805;&#65289;&#20013;&#19981;&#21516;&#35757;&#32451;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;Llama-2&#21644;Mistral&#20004;&#31181;&#22522;&#30784;LLM&#22312;&#19981;&#21516;&#23545;&#35805;&#31867;&#22411;&#19979;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21516;&#26102;&#20063;&#27979;&#35797;&#20102;&#22312;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#25216;&#26415;&#19982;&#22522;&#20110;&#40644;&#37329;&#30693;&#35782;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#21253;&#25324;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20869;&#23884;&#23398;&#20064;&#26041;&#27861;&#21644;&#31934;&#32454;&#35843;&#25972;&#25216;&#26415;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#38024;&#23545;&#21508;&#20010;&#23545;&#35805;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#36873;&#23450;&#21644;&#35780;&#20272;&#65292;&#25991;&#31456;&#23545;&#19981;&#21516;&#30340;LLM&#36866;&#24212;&#25216;&#26415;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2406.06399</link><description>&lt;p&gt;
Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.06399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23545;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;Large Language Models&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#23545;&#35805;&#31867;&#22411;&#65288;&#22914;&#24320;&#25918;&#22495;&#23545;&#35805;&#65289;&#20013;&#19981;&#21516;&#35757;&#32451;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;Llama-2&#21644;Mistral&#20004;&#31181;&#22522;&#30784;LLM&#22312;&#19981;&#21516;&#23545;&#35805;&#31867;&#22411;&#19979;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21516;&#26102;&#20063;&#27979;&#35797;&#20102;&#22312;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#25216;&#26415;&#19982;&#22522;&#20110;&#40644;&#37329;&#30693;&#35782;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#21253;&#25324;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20869;&#23884;&#23398;&#20064;&#26041;&#27861;&#21644;&#31934;&#32454;&#35843;&#25972;&#25216;&#26415;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#38024;&#23545;&#21508;&#20010;&#23545;&#35805;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#36873;&#23450;&#21644;&#35780;&#20272;&#65292;&#25991;&#31456;&#23545;&#19981;&#21516;&#30340;LLM&#36866;&#24212;&#25216;&#26415;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.06399v3 Announce Type: replace-cross  Abstract: We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and expl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2406.04485</link><description>&lt;p&gt;
GenAI Arena: An Open Evaluation Platform for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;GenAI-Arena&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21442;&#19982;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#25552;&#20379;&#26356;&#27665;&#20027;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04485v2 Announce Type: replace-cross  Abstract: Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source g
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;WRDScore&#65292;&#23427;&#33021;&#22815;&#32508;&#21512;&#32771;&#34385;&#26041;&#27861;&#30340;&#21629;&#21517;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#35821;&#27861;&#19978;&#30340;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#20840;&#38754;&#25429;&#25417;&#36825;&#20123;&#24046;&#24322;&#30340;&#38590;&#39064;&#12290;WRDScore&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#26631;&#20934;&#21270;&#19988;&#31934;&#24230;-&#21484;&#22238;&#24230;&#37327;&#30456;&#32467;&#21512;&#30340;&#25351;&#26631;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20551;&#35774;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#26356;&#36148;&#21512;&#20154;&#31867;&#23545;&#35780;&#20272;&#32467;&#26524;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2405.19220</link><description>&lt;p&gt;
WRDScore: New Metric for Evaluation of Natural Language Generation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.19220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;WRDScore&#65292;&#23427;&#33021;&#22815;&#32508;&#21512;&#32771;&#34385;&#26041;&#27861;&#30340;&#21629;&#21517;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#35821;&#27861;&#19978;&#30340;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#20840;&#38754;&#25429;&#25417;&#36825;&#20123;&#24046;&#24322;&#30340;&#38590;&#39064;&#12290;WRDScore&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#26631;&#20934;&#21270;&#19988;&#31934;&#24230;-&#21484;&#22238;&#24230;&#37327;&#30456;&#32467;&#21512;&#30340;&#25351;&#26631;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20551;&#35774;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#26356;&#36148;&#21512;&#20154;&#31867;&#23545;&#35780;&#20272;&#32467;&#26524;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.19220v4 Announce Type: replace-cross  Abstract: Evaluating natural language generation models, particularly for method name prediction, poses significant challenges. A robust metric must account for the versatility of method naming, considering both semantic and syntactic variations. Traditional overlap-based metrics fail to capture these nuances. Existing embedding-based metrics often suffer from imbalanced precision and recall, lack normalized scores, or make unrealistic assumptions about sequences. To address these limitations, we propose WRDScore, a novel metric that strikes a balance between simplicity and effectiveness. Our metric is lightweight, normalized, and precision-recall-oriented, avoiding unrealistic assumptions while aligning well with human judgments.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24847;&#35265;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;Agent&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#31649;&#29702;&#25351;&#23548;&#32773;&#30340;&#24847;&#35265;&#26469;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#20154;&#24037;&#20195;&#29702;&#36824;&#26159;&#20154;&#31867;&#23548;&#24072;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#21516;&#25351;&#23548;&#31574;&#30053;&#19979;&#22343;&#26174;&#31034;&#20986;&#20102;&#26377;&#20215;&#20540;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2405.17287</link><description>&lt;p&gt;
Opinion-Guided Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.17287
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24847;&#35265;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;Agent&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#31649;&#29702;&#25351;&#23548;&#32773;&#30340;&#24847;&#35265;&#26469;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#20154;&#24037;&#20195;&#29702;&#36824;&#26159;&#20154;&#31867;&#23548;&#24072;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#21516;&#25351;&#23548;&#31574;&#30053;&#19979;&#22343;&#26174;&#31034;&#20986;&#20102;&#26377;&#20215;&#20540;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.17287v2 Announce Type: replace-cross  Abstract: Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence can be produced. Thus, guiding reinforcement learning agents by way of opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle) and human advisors, at different levels of uncertainty, and under multiple advice strategies. Ou
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21452;&#37325;&#21160;&#24577;&#29305;&#24449;&#30340;ISAC&#39044;&#32534;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21463;&#38480;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#36710;&#36742;&#32593;&#32476;&#29615;&#22659;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#26465;&#20214;&#12290;&#25991;&#20013;&#36824;&#29305;&#21035;&#32771;&#34385;&#20102;&#30446;&#26631;&#24555;&#36895;&#31227;&#21160;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#21270;PD-DDPG&#21644;Wolpertinger&#26550;&#26500;&#26469;&#26377;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#19981;&#21516;&#29992;&#25143;&#25968;&#37327;&#30340;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#26102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#36824;&#33021;&#22312;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#39044;&#32534;&#30721;&#35774;&#35745;&#65292;&#20026;ISAC&#22312;&#36710;&#36742;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2405.14347</link><description>&lt;p&gt;
Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep Reinforcement Learning (CDRL) Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21452;&#37325;&#21160;&#24577;&#29305;&#24449;&#30340;ISAC&#39044;&#32534;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21463;&#38480;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#36710;&#36742;&#32593;&#32476;&#29615;&#22659;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#26465;&#20214;&#12290;&#25991;&#20013;&#36824;&#29305;&#21035;&#32771;&#34385;&#20102;&#30446;&#26631;&#24555;&#36895;&#31227;&#21160;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#21270;PD-DDPG&#21644;Wolpertinger&#26550;&#26500;&#26469;&#26377;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#19981;&#21516;&#29992;&#25143;&#25968;&#37327;&#30340;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#26102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#36824;&#33021;&#22312;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#39044;&#32534;&#30721;&#35774;&#35745;&#65292;&#20026;ISAC&#22312;&#36710;&#36742;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14347v2 Announce Type: replace-cross  Abstract: Integrated sensing and communication (ISAC) technology is essential for enabling the vehicular networks. However, the communication channel in this scenario exhibits time-varying characteristics, and the potential targets may move rapidly, creating a doubly-dynamic phenomenon. This nature poses a challenge for real-time precoder design. While optimization-based solutions are widely researched, they are complex and heavily rely on perfect prior information, which is impractical in double dynamics. To address this challenge, we propose using constrained deep reinforcement learning (CDRL) to facilitate dynamic updates to the ISAC precoder design. Additionally, the primal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertinger architecture are tailored to efficiently train the algorithm under complex constraints and variable numbers of users. The proposed scheme not only adapts to the dynamics based on observations but a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#31639;&#27861;&#34917;&#25937;&#36807;&#31243;&#30340;&#37325;&#26032;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#22312;&#25509;&#21463;&#21644;&#37319;&#21462;&#34917;&#25937;&#25514;&#26045;&#26041;&#38754;&#30340;&#25509;&#21463;&#24230;&#19982;&#34917;&#25937;&#25514;&#26045;&#30340;&#36317;&#31163;&#19981;&#23436;&#20840;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2405.14264</link><description>&lt;p&gt;
Reassessing Evaluation Functions in Algorithmic Recourse: An Empirical Study from a Human-Centered Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#23545;&#31639;&#27861;&#34917;&#25937;&#36807;&#31243;&#30340;&#37325;&#26032;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#22312;&#25509;&#21463;&#21644;&#37319;&#21462;&#34917;&#25937;&#25514;&#26045;&#26041;&#38754;&#30340;&#25509;&#21463;&#24230;&#19982;&#34917;&#25937;&#25514;&#26045;&#30340;&#36317;&#31163;&#19981;&#23436;&#20840;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14264v2 Announce Type: replace-cross  Abstract: In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance. Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise cons
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#22522;&#20110;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#25552;&#20379;&#23545;&#36712;&#36857;&#30340;&#20559;&#22909;&#20197;&#21450;&#23545;&#20854;&#20559;&#22909;&#20026;&#20160;&#20040;&#25104;&#31435;&#30340;&#35814;&#23613;&#35299;&#37322;&#26469;&#25351;&#23548;&#35757;&#32451;&#12290;&#36825;&#31181;&#23545;&#21333;&#20010;&#36712;&#36857;&#27493;&#39588;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20026;&#34920;&#36798;&#21147;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#28165;&#26970;&#22320;&#34920;&#36798;&#21738;&#20123;&#36712;&#36857;&#32452;&#25104;&#37096;&#20998;&#23545;&#20854;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#39640;&#20102;&#20154;&#31867;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#24230;&#21644;&#20114;&#21160;&#30340;&#28145;&#24230;&#65292;&#20351;&#24471;&#20154;&#31867;&#19981;&#20165;&#33021;&#22815;&#20915;&#23450;&#19968;&#20010;&#34892;&#20026;&#31574;&#30053;&#30340;&#20248;&#21155;&#65292;&#36824;&#33021;&#25552;&#20379;&#26356;&#28145;&#23618;&#27425;&#30340;&#35299;&#37322;&#21644;&#25351;&#23548;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2405.14244</link><description>&lt;p&gt;
Tell me why: Training preferences-based RL with human preferences and step-level explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.14244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#22522;&#20110;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#25552;&#20379;&#23545;&#36712;&#36857;&#30340;&#20559;&#22909;&#20197;&#21450;&#23545;&#20854;&#20559;&#22909;&#20026;&#20160;&#20040;&#25104;&#31435;&#30340;&#35814;&#23613;&#35299;&#37322;&#26469;&#25351;&#23548;&#35757;&#32451;&#12290;&#36825;&#31181;&#23545;&#21333;&#20010;&#36712;&#36857;&#27493;&#39588;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20026;&#34920;&#36798;&#21147;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#28165;&#26970;&#22320;&#34920;&#36798;&#21738;&#20123;&#36712;&#36857;&#32452;&#25104;&#37096;&#20998;&#23545;&#20854;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#25552;&#39640;&#20102;&#20154;&#31867;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#24230;&#21644;&#20114;&#21160;&#30340;&#28145;&#24230;&#65292;&#20351;&#24471;&#20154;&#31867;&#19981;&#20165;&#33021;&#22815;&#20915;&#23450;&#19968;&#20010;&#34892;&#20026;&#31574;&#30053;&#30340;&#20248;&#21155;&#65292;&#36824;&#33021;&#25552;&#20379;&#26356;&#28145;&#23618;&#27425;&#30340;&#35299;&#37322;&#21644;&#25351;&#23548;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.14244v2 Announce Type: replace  Abstract: Human-in-the-loop reinforcement learning allows the training of agents through various interfaces, even for non-expert humans. Recently, preference-based methods (PbRL), where the human has to give his preference over two trajectories, increased in popularity since they allow training in domains where more direct feedback is hard to formulate. However, the current PBRL methods have limitations and do not provide humans with an expressive interface for giving feedback. With this work, we propose a new preference-based learning method that provides humans with a more expressive interface to provide their preference over trajectories and a factual explanation (or annotation of why they have this preference). These explanations allow the human to explain what parts of the trajectory are most relevant for the preference. We allow the expression of the explanations over individual trajectory steps. We evaluate our method in various simulat
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Visual Language Model&#65288;VLM&#65289;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#29992;&#25143;&#20132;&#20114;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20219;&#21153;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2405.11537</link><description>&lt;p&gt;
VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Visual Language Model&#65288;VLM&#65289;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#29992;&#25143;&#20132;&#20114;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20219;&#21153;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11537v3 Announce Type: replace  Abstract: The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21033;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;argumentation&#65288;ABA&#65289;&#21644;&#22240;&#26524;&#29702;&#35770;&#65292;&#20197;&#22312;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20026;&#30740;&#31350;&#23545;&#35937;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#31526;&#21495;&#34920;&#31034;&#20013;&#25903;&#25345;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#26576;&#20123;&#33258;&#28982;&#26465;&#20214;&#19979;&#26816;&#32034;&#21040;&#30495;&#27491;&#30340;&#22240;&#26524;&#22270;&#65292;&#26377;&#25928;&#22320;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36890;&#36807;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#22312;Answer Set Programming&#65288;ASP&#65289;&#20013;&#23454;&#29616;&#30340;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2405.11250</link><description>&lt;p&gt;
Argumentative Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21033;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;argumentation&#65288;ABA&#65289;&#21644;&#22240;&#26524;&#29702;&#35770;&#65292;&#20197;&#22312;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20026;&#30740;&#31350;&#23545;&#35937;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#31526;&#21495;&#34920;&#31034;&#20013;&#25903;&#25345;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#26576;&#20123;&#33258;&#28982;&#26465;&#20214;&#19979;&#26816;&#32034;&#21040;&#30495;&#27491;&#30340;&#22240;&#26524;&#22270;&#65292;&#26377;&#25928;&#22320;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36890;&#36807;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#22312;Answer Set Programming&#65288;ASP&#65289;&#20013;&#23454;&#29616;&#30340;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11250v3 Announce Type: replace  Abstract: Causal discovery amounts to unearthing causal relationships amongst features in data. It is a crucial companion to causal inference, necessary to build scientific knowledge without resorting to expensive or impossible randomised control trials. In this paper, we explore how reasoning with symbolic representations can support causal discovery. Specifically, we deploy assumption-based argumentation (ABA), a well-established and powerful knowledge representation formalism, in combination with causality theories, to learn graphs which reflect causal dependencies in the data. We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs. We also conduct experiments with an implementation of our method in answer set programming (ASP) on four datasets from standard benchmarks in causal discovery, showing that our method compares well against established baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35770;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26222;&#36941;&#24573;&#35270;&#21487;&#20105;&#35758;&#24615;&#36825;&#19968;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#35745;&#31639;&#26426;&#21046;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#20105;&#35758;&#24615;&#65292;&#20197;&#28385;&#36275;&#25919;&#31574;&#21644;&#27861;&#35268;&#23545;&#20110;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#21487;&#20105;&#35758;&#24615;&#30340;&#35201;&#27714;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#21160;&#24577;&#35299;&#37322;&#24615;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#24212;&#33021;&#22815;&#19982;&#20154;&#25110;&#20854;&#23427;&#26426;&#22120;&#20114;&#21160;&#65292;&#36880;&#27493;&#35299;&#37322;&#20854;&#20915;&#31574;&#24182;&#35780;&#20272;&#23545;&#25163;&#26041;&#25552;&#20986;&#30340;&#20105;&#35758;&#28857;&#65292;&#24182;&#22312;&#24517;&#35201;&#26102;&#26681;&#25454;&#36825;&#20123;&#20105;&#35758;&#28857;&#35843;&#25972;&#33258;&#36523;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2405.10729</link><description>&lt;p&gt;
Contestable AI needs Computational Argumentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.10729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35770;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26222;&#36941;&#24573;&#35270;&#21487;&#20105;&#35758;&#24615;&#36825;&#19968;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#35745;&#31639;&#26426;&#21046;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#20105;&#35758;&#24615;&#65292;&#20197;&#28385;&#36275;&#25919;&#31574;&#21644;&#27861;&#35268;&#23545;&#20110;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#21487;&#20105;&#35758;&#24615;&#30340;&#35201;&#27714;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#21160;&#24577;&#35299;&#37322;&#24615;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#24212;&#33021;&#22815;&#19982;&#20154;&#25110;&#20854;&#23427;&#26426;&#22120;&#20114;&#21160;&#65292;&#36880;&#27493;&#35299;&#37322;&#20854;&#20915;&#31574;&#24182;&#35780;&#20272;&#23545;&#25163;&#26041;&#25552;&#20986;&#30340;&#20105;&#35758;&#28857;&#65292;&#24182;&#22312;&#24517;&#35201;&#26102;&#26681;&#25454;&#36825;&#20123;&#20105;&#35758;&#28857;&#35843;&#25972;&#33258;&#36523;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10729v2 Announce Type: replace  Abstract: AI has become pervasive in recent years, but state-of-the-art approaches predominantly neglect the need for AI systems to be contestable. Instead, contestability is advocated by AI guidelines (e.g. by the OECD) and regulation of automated decision-making (e.g. GDPR). In this position paper we explore how contestability can be achieved computationally in and for AI. We argue that contestable AI requires dynamic (human-machine and/or machine-machine) explainability and decision-making processes, whereby machines can (i) interact with humans and/or other machines to progressively explain their outputs and/or their reasoning as well as assess grounds for contestation provided by these humans and/or other machines, and (ii) revise their decision-making processes to redress any issues successfully raised during contestation. Given that much of the current AI landscape is tailored to static AIs, the need to accommodate contestability will r
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;PropertyExtractor&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#21033;&#29992;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#38646;&#26679;&#26412;&#19982;&#23569;&#26679;&#26412;&#30340;&#20869;&#22312;&#24037;&#20316;&#26041;&#24335;&#65292;&#20197;&#21450;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#26448;&#26009;&#23646;&#24615;&#25968;&#25454;&#30340;&#21160;&#24577;&#33258;&#23398;&#20064;&#65288;in-context learning&#65289;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#35782;&#21035;&#12289;&#25552;&#21462;&#24182;&#39564;&#35777;&#22823;&#37327;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;2D&#26448;&#26009;&#21402;&#24230;&#30340;&#25968;&#25454;&#24211;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#20854;&#20316;&#20026;&#26448;&#26009;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#27979;&#30340;&#39640;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2405.10448</link><description>&lt;p&gt;
Dynamic In-context Learning with Conversational Models for Data Extraction and Materials Property Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.10448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;PropertyExtractor&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#21033;&#29992;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#38646;&#26679;&#26412;&#19982;&#23569;&#26679;&#26412;&#30340;&#20869;&#22312;&#24037;&#20316;&#26041;&#24335;&#65292;&#20197;&#21450;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#26448;&#26009;&#23646;&#24615;&#25968;&#25454;&#30340;&#21160;&#24577;&#33258;&#23398;&#20064;&#65288;in-context learning&#65289;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#35782;&#21035;&#12289;&#25552;&#21462;&#24182;&#39564;&#35777;&#22823;&#37327;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;2D&#26448;&#26009;&#21402;&#24230;&#30340;&#25968;&#25454;&#24211;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#20854;&#20316;&#20026;&#26448;&#26009;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#27979;&#30340;&#39640;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.10448v2 Announce Type: replace-cross  Abstract: The advent of natural language processing and large language models (LLMs) has revolutionized the extraction of data from unstructured scholarly papers. However, ensuring data trustworthiness remains a significant challenge. In this paper, we introduce PropertyExtractor, an open-source tool that leverages advanced conversational LLMs like Google gemini-pro and OpenAI gpt-4, blends zero-shot with few-shot in-context learning, and employs engineered prompts for the dynamic refinement of structured information hierarchies - enabling autonomous, efficient, scalable, and accurate identification, extraction, and verification of material property data. Our tests on material data demonstrate precision and recall that exceed 95\% with an error rate of approximately 9%, highlighting the effectiveness and versatility of the toolkit. Finally, databases for 2D material thicknesses, a critical parameter for device integration, and energy ban
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#38750;&#20808;&#30693;&#22411;&#35843;&#24230;&#38382;&#39064;&#20013;&#30340;&#39044;&#27979;&#19981;&#23436;&#20840;&#24615;&#22330;&#26223;&#65292;&#30740;&#31350;&#20102;&#24403;&#20915;&#31574;&#32773;&#20165;&#33021;&#33719;&#24471;&#37096;&#20998;&#20219;&#21153;&#22823;&#23567;&#30340;&#39044;&#27979;&#20449;&#24687;&#26102;&#65292;&#22914;&#20309;&#35774;&#35745;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#25991;&#20013;&#39318;&#20808;&#22312;&#23436;&#32654;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#36817;&#26368;&#20248;&#30340;&#36793;&#30028;&#21644;&#31639;&#27861;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#28385;&#36275;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#24615;&#30340;&#35201;&#27714;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#39044;&#27979;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#26032;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2405.01013</link><description>&lt;p&gt;
Non-clairvoyant Scheduling with Partial Predictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#32858;&#28966;&#20110;&#38750;&#20808;&#30693;&#22411;&#35843;&#24230;&#38382;&#39064;&#20013;&#30340;&#39044;&#27979;&#19981;&#23436;&#20840;&#24615;&#22330;&#26223;&#65292;&#30740;&#31350;&#20102;&#24403;&#20915;&#31574;&#32773;&#20165;&#33021;&#33719;&#24471;&#37096;&#20998;&#20219;&#21153;&#22823;&#23567;&#30340;&#39044;&#27979;&#20449;&#24687;&#26102;&#65292;&#22914;&#20309;&#35774;&#35745;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#25991;&#20013;&#39318;&#20808;&#22312;&#23436;&#32654;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#36817;&#26368;&#20248;&#30340;&#36793;&#30028;&#21644;&#31639;&#27861;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#28385;&#36275;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#24615;&#30340;&#35201;&#27714;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#39044;&#27979;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#26032;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01013v2 Announce Type: replace-cross  Abstract: The non-clairvoyant scheduling problem has gained new interest within learning-augmented algorithms, where the decision-maker is equipped with predictions without any quality guarantees. In practical settings, access to predictions may be reduced to specific instances, due to cost or data limitations. Our investigation focuses on scenarios where predictions for only $B$ job sizes out of $n$ are available to the algorithm. We first establish near-optimal lower bounds and algorithms in the case of perfect predictions. Subsequently, we present a learning-augmented algorithm satisfying the robustness, consistency, and smoothness criteria, and revealing a novel tradeoff between consistency and smoothness inherent in the scenario with a restricted number of predictions.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;WorkBench&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#22312;&#29616;&#23454;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#65292;&#20854;&#20013;&#21253;&#21547;&#20116;&#20010;&#25968;&#25454;&#24211;&#12289;26&#31181;&#24037;&#20855;&#21644;690&#20010;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#20195;&#34920;&#24120;&#35265;&#30340;&#19994;&#21153;&#27963;&#21160;&#12290;WorkBench&#30340;&#20219;&#21153;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#35268;&#21010;&#21644;&#24037;&#20855;&#36873;&#25321;&#65292;&#20197;&#21450;&#26377;&#26102;&#38656;&#35201;&#22810;&#20010;&#21160;&#20316;&#12290;&#24403;&#20219;&#21153;&#25104;&#21151;&#25191;&#34892;&#26102;&#65292;&#25968;&#25454;&#24211;&#30340;&#20540;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;&#27491;&#30830;&#32467;&#26524;&#26159;&#21807;&#19968;&#30340;&#21644;&#26080;&#21487;&#20105;&#35758;&#30340;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#21270;&#35780;&#20272;&#21464;&#24471;&#21487;&#38752;&#12290;&#25991;&#31456;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26524;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20116;&#20010;&#29616;&#26377;&#30340;ReAct&#20195;&#29702;&#22312;WorkBench&#19978;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;GPT-4&#30340;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#23436;&#25104;&#30340;&#20219;&#21153;&#20063;&#21482;&#26377;43%&#12290;&#25991;&#31456;&#36824;&#25581;&#31034;&#20102;&#20195;&#29702;&#22312;&#25191;&#34892;&#38169;&#35823;&#20219;&#21153;&#26102;&#21487;&#33021;&#23548;&#33268;&#30340;&#32467;&#26524;&#65292;&#27604;&#22914;&#21457;&#36865;&#38169;&#35823;&#30340;&#30005;&#23376;&#37038;&#20214;&#12290;</title><link>https://arxiv.org/abs/2405.00823</link><description>&lt;p&gt;
WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;WorkBench&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#22312;&#29616;&#23454;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#65292;&#20854;&#20013;&#21253;&#21547;&#20116;&#20010;&#25968;&#25454;&#24211;&#12289;26&#31181;&#24037;&#20855;&#21644;690&#20010;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#20195;&#34920;&#24120;&#35265;&#30340;&#19994;&#21153;&#27963;&#21160;&#12290;WorkBench&#30340;&#20219;&#21153;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#35268;&#21010;&#21644;&#24037;&#20855;&#36873;&#25321;&#65292;&#20197;&#21450;&#26377;&#26102;&#38656;&#35201;&#22810;&#20010;&#21160;&#20316;&#12290;&#24403;&#20219;&#21153;&#25104;&#21151;&#25191;&#34892;&#26102;&#65292;&#25968;&#25454;&#24211;&#30340;&#20540;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;&#27491;&#30830;&#32467;&#26524;&#26159;&#21807;&#19968;&#30340;&#21644;&#26080;&#21487;&#20105;&#35758;&#30340;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#21270;&#35780;&#20272;&#21464;&#24471;&#21487;&#38752;&#12290;&#25991;&#31456;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26524;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20116;&#20010;&#29616;&#26377;&#30340;ReAct&#20195;&#29702;&#22312;WorkBench&#19978;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;GPT-4&#30340;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#23436;&#25104;&#30340;&#20219;&#21153;&#20063;&#21482;&#26377;43%&#12290;&#25991;&#31456;&#36824;&#25581;&#31034;&#20102;&#20195;&#29702;&#22312;&#25191;&#34892;&#38169;&#35823;&#20219;&#21153;&#26102;&#21487;&#33021;&#23548;&#33268;&#30340;&#32467;&#26524;&#65292;&#27604;&#22914;&#21457;&#36865;&#38169;&#35823;&#30340;&#30005;&#23376;&#37038;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00823v2 Announce Type: replace-cross  Abstract: We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being s
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#20013;&#22411;&#37327;&#23376;&#27604;&#29305;&#26550;&#26500;&#30340;&#39640;&#25928;&#27169;&#25311;&#65292;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#33016;&#29255;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#38271;&#23614;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20102;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2405.00156</link><description>&lt;p&gt;
Expanding the Horizon: Enabling Hybrid Quantum Transfer Learning for Long-Tailed Chest X-Ray Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.00156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;Jax&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#20013;&#22411;&#37327;&#23376;&#27604;&#29305;&#26550;&#26500;&#30340;&#39640;&#25928;&#27169;&#25311;&#65292;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#33016;&#29255;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#38271;&#23614;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20102;&#28151;&#21512;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.00156v2 Announce Type: replace  Abstract: Quantum machine learning (QML) has the potential for improving the multi-label classification of rare, albeit critical, diseases in large-scale chest x-ray (CXR) datasets due to theoretical quantum advantages over classical machine learning (CML) in sample efficiency and generalizability. While prior literature has explored QML with CXRs, it has focused on binary classification tasks with small datasets due to limited access to quantum hardware and computationally expensive simulations. To that end, we implemented a Jax-based framework that enables the simulation of medium-sized qubit architectures with significant improvements in wall-clock time over current software offerings. We evaluated the performance of our Jax-based framework in terms of efficiency and performance for hybrid quantum transfer learning for long-tailed classification across 8, 14, and 19 disease labels using large-scale CXR datasets. The Jax-based framework resu
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CT&#22270;&#20687;&#21644;&#38750;&#23545;&#27604;&#24615;&#35786;&#26029;&#25253;&#21578;&#65292;&#20197;&#39044;&#27979;&#33041;&#21330;&#20013;&#30340;&#27835;&#30103;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#35299;&#37322;&#25991;&#26412;&#20449;&#24687;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#22270;&#20687;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#12290;&#23613;&#31649;&#20165;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;Transformer&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#24403;&#32467;&#21512;&#24102;&#26377;&#20020;&#24202;meta&#29305;&#24449;&#35786;&#26029;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;&#33041;&#21330;&#20013;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.12634</link><description>&lt;p&gt;
Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.12634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CT&#22270;&#20687;&#21644;&#38750;&#23545;&#27604;&#24615;&#35786;&#26029;&#25253;&#21578;&#65292;&#20197;&#39044;&#27979;&#33041;&#21330;&#20013;&#30340;&#27835;&#30103;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#35299;&#37322;&#25991;&#26412;&#20449;&#24687;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#22270;&#20687;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21333;&#19968;&#27169;&#24577;&#12290;&#23613;&#31649;&#20165;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;Transformer&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#24403;&#32467;&#21512;&#24102;&#26377;&#20020;&#24202;meta&#29305;&#24449;&#35786;&#26029;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;&#33041;&#21330;&#20013;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.12634v2 Announce Type: replace  Abstract: This study proposes a multi-modal fusion framework Multitrans based on the Transformer architecture and self-attention mechanism. This architecture combines the study of non-contrast computed tomography (NCCT) images and discharge diagnosis reports of patients undergoing stroke treatment, using a variety of methods based on Transformer architecture approach to predicting functional outcomes of stroke treatment. The results show that the performance of single-modal text classification is significantly better than single-modal image classification, but the effect of multi-modal combination is better than any single modality. Although the Transformer model only performs worse on imaging data, when combined with clinical meta-diagnostic information, both can learn better complementary information and make good contributions to accurately predicting stroke treatment effects..
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#30456;&#20284;&#24615;&#20272;&#35745;&#30340;&#22270;&#20687;&#29983;&#25104;&#24335;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#32593;&#32476;&#20013;&#23454;&#29616;&#20302;&#25968;&#25454;&#20256;&#36755;&#37327;&#30340;&#22270;&#20687;&#20256;&#36755;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#22270;&#20687;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#20943;&#23569;&#32593;&#32476;&#27969;&#37327;&#65292;&#20174;&#32780;&#25903;&#25345;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2404.11280</link><description>&lt;p&gt;
Image Generative Semantic Communication with Multi-Modal Similarity Estimation for Resource-Limited Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.11280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#30456;&#20284;&#24615;&#20272;&#35745;&#30340;&#22270;&#20687;&#29983;&#25104;&#24335;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#32593;&#32476;&#20013;&#23454;&#29616;&#20302;&#25968;&#25454;&#20256;&#36755;&#37327;&#30340;&#22270;&#20687;&#20256;&#36755;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#22270;&#20687;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#20943;&#23569;&#32593;&#32476;&#27969;&#37327;&#65292;&#20174;&#32780;&#25903;&#25345;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.11280v2 Announce Type: replace-cross  Abstract: To reduce network traffic and support environments with limited resources, a method for transmitting images with minimal transmission data is required. Several machine learning-based image compression methods, which compress the data size of images while maintaining their features, have been proposed. However, in certain situations, reconstructing only the semantic information of images at the receiver end may be sufficient. To realize this concept, semantic-information-based communication, called semantic communication, has been proposed, along with an image transmission method using semantic communication. This method transmits only the semantic information of an image, and the receiver reconstructs it using an image-generation model. This method utilizes a single type of semantic information for image reconstruction, but reconstructing images similar to the original image using only this information is challenging. This stud
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20027;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#8220;$N$-agent ad hoc teamwork (NAHT)&#8221;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21512;&#20316;&#22810;Agent&#23398;&#20064;&#26041;&#27861;&#35774;&#32622;&#30340;&#38480;&#21046;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#21512;&#20316;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#21512;&#20316;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.10740</link><description>&lt;p&gt;
N-Agent Ad Hoc Teamwork
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.10740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20027;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#8220;$N$-agent ad hoc teamwork (NAHT)&#8221;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21512;&#20316;&#22810;Agent&#23398;&#20064;&#26041;&#27861;&#35774;&#32622;&#30340;&#38480;&#21046;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#21512;&#20316;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#21512;&#20316;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.10740v2 Announce Type: replace  Abstract: Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls $\textit{all}$ agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$-agent ad hoc teamwork (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and prop
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;OpenBias&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#20559;&#35265;&#36827;&#34892;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#35265;&#38598;&#21512;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20559;&#35265;&#65292;&#30446;&#26631;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#21450;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#21644;&#31243;&#24230;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#21487;&#39044;&#35265;&#20559;&#35265;&#38382;&#39064;&#19978;&#30340;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.07990</link><description>&lt;p&gt;
OpenBias: Open-set Bias Detection in Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07990
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;OpenBias&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#20559;&#35265;&#36827;&#34892;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39044;&#20808;&#23450;&#20041;&#30340;&#20559;&#35265;&#38598;&#21512;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20559;&#35265;&#65292;&#30446;&#26631;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#21450;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#21644;&#31243;&#24230;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#21487;&#39044;&#35265;&#20559;&#35265;&#38382;&#39064;&#19978;&#30340;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07990v2 Announce Type: replace  Abstract: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previou
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#39033;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#39640;&#26031;&#28857;&#31215;&#27861;&#65288;Reinforcement Learning with Generalizable Gaussian Splatting&#65289;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;3D&#39640;&#26031;&#28857;&#31215;&#27861;&#20026;&#35270;&#35273;&#22686;&#24378;&#22411;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#29615;&#22659;&#34920;&#31034;&#23384;&#22312;&#30340;&#22797;&#26434;&#20960;&#20309;&#25551;&#36848;&#19981;&#36275;&#12289;&#22330;&#26223;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#38656;&#35201;&#31934;&#30830;&#21069;&#26223;&#25513;&#30721;&#31561;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#34920;&#31034;&#30340;&#35299;&#35835;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Blended RAG&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#32034;&#24341;&#21644;&#31232;&#30095;&#32534;&#30721;&#22120;&#32034;&#24341;&#30340;&#35821;&#20041;&#25628;&#32034;&#25216;&#26415;&#20197;&#21450;&#28151;&#21512;&#26597;&#35810;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#25913;&#36827;&#26816;&#32034;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#25968;&#25454;&#38598;&#22914;NQL&#21644;TREC-COVID&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#24182;&#19988;&#34987;&#25193;&#23637;&#21040;RAG&#31995;&#32479;&#65292;&#22312;&#20687;SQUAD&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#31934;&#24515;&#35843;&#25945;&#30340;RAG&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.07220</link><description>&lt;p&gt;
Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Blended RAG&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#32034;&#24341;&#21644;&#31232;&#30095;&#32534;&#30721;&#22120;&#32034;&#24341;&#30340;&#35821;&#20041;&#25628;&#32034;&#25216;&#26415;&#20197;&#21450;&#28151;&#21512;&#26597;&#35810;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#25913;&#36827;&#26816;&#32034;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#25968;&#25454;&#38598;&#22914;NQL&#21644;TREC-COVID&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#24182;&#19988;&#34987;&#25193;&#23637;&#21040;RAG&#31995;&#32479;&#65292;&#22312;&#20687;SQUAD&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#31934;&#24515;&#35843;&#25945;&#30340;RAG&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07220v2 Announce Type: replace-cross  Abstract: Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q\&amp;A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q\&amp;A datasets like SQUAD, even surpassing fine-tunin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Logic-Query-of-Thoughts&#8221;&#30340;&#26694;&#26550;&#65288;LGOT&#65289;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#20004;&#20010;&#27169;&#22411;&#37117;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#20551;&#24819;&#30340;&#31572;&#26696;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#23436;&#20840;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#32467;&#21512;&#65292;LGOT&#26088;&#22312;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#25552;&#39640;&#23545;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#30340;&#22238;&#31572;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.04264</link><description>&lt;p&gt;
Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.04264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Logic-Query-of-Thoughts&#8221;&#30340;&#26694;&#26550;&#65288;LGOT&#65289;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#20004;&#20010;&#27169;&#22411;&#37117;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#20551;&#24819;&#30340;&#31572;&#26696;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#23436;&#20840;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#32467;&#21512;&#65292;LGOT&#26088;&#22312;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#25552;&#39640;&#23545;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#30340;&#22238;&#31572;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.04264v3 Announce Type: replace-cross  Abstract: Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge grap
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC-LLM&#30340;&#27169;&#22411;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#33258;&#35299;&#37322;&#33021;&#21147;&#65292;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#25345;&#32493;&#30417;&#30563;&#35757;&#32451;&#21644;CoT&#25512;&#29702;&#30340;&#24494;&#35843;&#65292;LC-LLM&#27169;&#22411;&#19981;&#20165;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#32780;&#19988;&#22312;&#26356;&#24191;&#27867;&#30340;&#39550;&#39542;&#22330;&#26223;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18344</link><description>&lt;p&gt;
LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC-LLM&#30340;&#27169;&#22411;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#33258;&#35299;&#37322;&#33021;&#21147;&#65292;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#25345;&#32493;&#30417;&#30563;&#35757;&#32451;&#21644;CoT&#25512;&#29702;&#30340;&#24494;&#35843;&#65292;LC-LLM&#27169;&#22411;&#19981;&#20165;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#32780;&#19988;&#22312;&#26356;&#24191;&#27867;&#30340;&#39550;&#39542;&#22330;&#26223;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18344v2 Announce Type: replace  Abstract: To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve predi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#26426;&#22120;&#27927;&#30333;&#65288;MU&#65289;&#20316;&#20026;&#19968;&#31181;&#30830;&#20445;AI&#23433;&#20840;&#21644;&#21512;&#35268;&#24615;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#12289;&#36991;&#20813;&#38544;&#31169;&#20405;&#29359;&#21644;&#30456;&#20851;&#27861;&#24459;&#27861;&#35268;&#30340;&#36981;&#23432;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#26426;&#22120;&#27927;&#30333;&#20801;&#35768;&#29992;&#25143;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#27169;&#22411;&#24433;&#21709;&#65292;&#20026;AI&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21644;&#38450;&#27490;&#19981;&#33391;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#32771;&#37327;&#20197;&#21450;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24037;&#20316;&#25351;&#20986;&#20102;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.13682</link><description>&lt;p&gt;
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#26426;&#22120;&#27927;&#30333;&#65288;MU&#65289;&#20316;&#20026;&#19968;&#31181;&#30830;&#20445;AI&#23433;&#20840;&#21644;&#21512;&#35268;&#24615;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#12289;&#36991;&#20813;&#38544;&#31169;&#20405;&#29359;&#21644;&#30456;&#20851;&#27861;&#24459;&#27861;&#35268;&#30340;&#36981;&#23432;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#26426;&#22120;&#27927;&#30333;&#20801;&#35768;&#29992;&#25143;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#27169;&#22411;&#24433;&#21709;&#65292;&#20026;AI&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21644;&#38450;&#27490;&#19981;&#33391;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#32771;&#37327;&#20197;&#21450;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24037;&#20316;&#25351;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13682v3 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25945;&#24072;-&#23398;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21560;&#25910;&#22810;&#31181;&#23567;&#22411;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#30693;&#35782;&#65292;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20998;&#23376;&#35201;&#27714;&#30340;&#26032;&#20998;&#23376;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;82.58%&#33267;67.48%&#19981;&#31561;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25945;&#24072;-&#23398;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21560;&#25910;&#22810;&#31181;&#23567;&#22411;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#30693;&#35782;&#65292;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20998;&#23376;&#35201;&#27714;&#30340;&#26032;&#20998;&#23376;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;82.58%&#33267;67.48%&#19981;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v3 Announce Type: replace-cross  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 82.58%, 68.03%, and 67.48%, respectively. The mod
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RallyNet&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31163;&#32447;&#35270;&#39057;&#20013;&#27169;&#20223;&#20154;&#31867;&#29699;&#21592;&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32701;&#27611;&#29699;&#36825;&#31181;&#20132;&#26367;&#34892;&#21160;&#30340;&#20307;&#32946;&#27604;&#36187;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#20915;&#31574;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#27604;&#36187;&#30340;&#23545;&#31216;&#24615;&#65292;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#26512;&#29699;&#21592;&#30340;&#23454;&#38469;&#34892;&#20026;&#26469;&#25552;&#21319;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#35757;&#32451;&#25552;&#20379;&#23453;&#36149;&#30340;&#23454;&#36341;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.12406</link><description>&lt;p&gt;
Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RallyNet&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31163;&#32447;&#35270;&#39057;&#20013;&#27169;&#20223;&#20154;&#31867;&#29699;&#21592;&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32701;&#27611;&#29699;&#36825;&#31181;&#20132;&#26367;&#34892;&#21160;&#30340;&#20307;&#32946;&#27604;&#36187;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#20915;&#31574;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#27604;&#36187;&#30340;&#23545;&#31216;&#24615;&#65292;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#26512;&#29699;&#21592;&#30340;&#23454;&#38469;&#34892;&#20026;&#26469;&#25552;&#21319;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#35757;&#32451;&#25552;&#20379;&#23453;&#36149;&#30340;&#23454;&#36341;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v2 Announce Type: replace  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;cRSSM&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;Dreamer&#65288;v3&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#20197;&#38598;&#25104;&#24773;&#22659;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;CARL&#22522;&#20934;&#20219;&#21153;&#20013;&#23545;&#26410;&#35265;&#24773;&#22659;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10967</link><description>&lt;p&gt;
Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10967
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#30340;cRSSM&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;Dreamer&#65288;v3&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#20197;&#38598;&#25104;&#24773;&#22659;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;CARL&#22522;&#20934;&#20219;&#21153;&#20013;&#23545;&#26410;&#35265;&#24773;&#22659;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v2 Announce Type: replace-cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our approach is evaluated on two tasks from the CARL benchmark suite, which is tailored to study contextual RL. Our experim
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#26356;&#26032;&#30340;&#27425;&#25968;&#36828;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#26102;&#65292;&#31639;&#27861;&#33021;&#22815;&#25269;&#25239;&#20215;&#20540;&#20559;&#24046;&#24182;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22312;&#22823;&#22411;&#26356;&#26032;-&#25968;&#25454;&#27604;&#19979;&#21487;&#33021;&#23548;&#33268;&#30340;&#20215;&#20540;&#20989;&#25968;&#31232;&#37322;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#26356;&#26032;&#30340;&#27425;&#25968;&#36828;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#26102;&#65292;&#31639;&#27861;&#33021;&#22815;&#25269;&#25239;&#20215;&#20540;&#20559;&#24046;&#24182;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22312;&#22823;&#22411;&#26356;&#26032;-&#25968;&#25454;&#27604;&#19979;&#21487;&#33021;&#23548;&#33268;&#30340;&#20215;&#20540;&#20989;&#25968;&#31232;&#37322;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v3 Announce Type: replace-cross  Abstract: We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer moment
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#31034;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#36981;&#23432;&#29305;&#23450;&#25351;&#31034;&#21644;&#25552;&#39640;&#29992;&#25143;&#24847;&#22270;&#21709;&#24212;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.05063</link><description>&lt;p&gt;
Aligning Large Language Models for Controllable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#31034;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#36981;&#23432;&#29305;&#23450;&#25351;&#31034;&#21644;&#25552;&#39640;&#29992;&#25143;&#24847;&#22270;&#21709;&#24212;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05063v2 Announce Type: replace-cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;PrimeComposer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#36895;&#24230;&#26356;&#24555;&#12289;&#36136;&#37327;&#26356;&#20248;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#24335;&#25193;&#25955;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#23427;&#19987;&#27880;&#20110;&#23616;&#37096;&#32534;&#36753;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32972;&#26223;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#36807;&#28193;&#21306;&#22495;&#30340;&#19981;&#33391;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;PrimeComposer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#36895;&#24230;&#26356;&#24555;&#12289;&#36136;&#37327;&#26356;&#20248;&#30340;&#28176;&#36827;&#24335;&#32452;&#21512;&#24335;&#25193;&#25955;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#23427;&#19987;&#27880;&#20110;&#23616;&#37096;&#32534;&#36753;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#32972;&#26223;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#36807;&#28193;&#21306;&#22495;&#30340;&#19981;&#33391;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v2 Announce Type: replace  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. Current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only impedes their swift implementation but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tra
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#39318;&#27425;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#22312;&#22788;&#29702;&#21160;&#24577;&#25968;&#25454;&#38598;&#21512;&#20013;&#30340;&#23454;&#38469;&#24847;&#20041;&#21644;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#23545;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;CIL&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#21644;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2402.12035</link><description>&lt;p&gt;
Class-incremental Learning for Time Series: Benchmark and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#39318;&#27425;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#22312;&#22788;&#29702;&#21160;&#24577;&#25968;&#25454;&#38598;&#21512;&#20013;&#30340;&#23454;&#38469;&#24847;&#20041;&#21644;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#23545;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;CIL&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v2 Announce Type: replace-cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and co
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25104;&#26524; I - &#31185;&#23398;&#27719; [arXiv:2311.12324v1]
Title: Orthogonal Learning for Multi-Task and Domain Generalization
Abstract: &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#27491;&#20132;&#32422;&#26463;&#20248;&#21270;&#22810;&#20219;&#21153;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#30340;&#35299;&#20915;&#12290;&#24037;&#20316;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#20132;&#32422;&#26463;&#33021;&#22815;&#20419;&#36827;&#27169;&#22411;&#22312;&#22797;&#26434;&#22810;&#21464;&#30340;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;&#22810;&#20010;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#27491;&#20132;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#22343;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;

&#35813;&#25991;&#31456;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#20132;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#27491;&#20132;&#32422;&#26463;&#20248;&#21270;&#22810;&#20219;&#21153;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22797;&#26434;&#22810;&#21464;&#30340;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25104;&#26524; I - &#31185;&#23398;&#27719; [arXiv:2311.12324v1]
Title: Orthogonal Learning for Multi-Task and Domain Generalization
Abstract: &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#27491;&#20132;&#32422;&#26463;&#20248;&#21270;&#22810;&#20219;&#21153;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#30340;&#35299;&#20915;&#12290;&#24037;&#20316;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#20132;&#32422;&#26463;&#33021;&#22815;&#20419;&#36827;&#27169;&#22411;&#22312;&#22797;&#26434;&#22810;&#21464;&#30340;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;&#22810;&#20010;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#27491;&#20132;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#22343;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;

&#35813;&#25991;&#31456;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#20132;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#27491;&#20132;&#32422;&#26463;&#20248;&#21270;&#22810;&#20219;&#21153;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22797;&#26434;&#22810;&#21464;&#30340;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v2 Announce Type: replace-cross  Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#39046;&#22495;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#19981;&#20687;&#20154;&#20204;&#25152;&#26399;&#26395;&#30340;&#37027;&#26679;&#21487;&#38752;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#35268;&#21010;&#21644;&#38382;&#39064;&#35299;&#20915;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.08115</link><description>&lt;p&gt;
On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#39046;&#22495;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#19981;&#20687;&#20154;&#20204;&#25152;&#26399;&#26395;&#30340;&#37027;&#26679;&#21487;&#38752;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#35268;&#21010;&#21644;&#38382;&#39064;&#35299;&#20915;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08115v2 Announce Type: replace  Abstract: There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRI
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;GM Cruise&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#22312;2023&#24180;10&#26376;&#19982;&#19968;&#21517;&#34892;&#20154;&#21457;&#29983;&#30896;&#25758;&#30340;&#26696;&#20363;&#65292;&#20998;&#26512;&#20102;&#20844;&#21496;&#22312;&#24212;&#23545;&#34892;&#20154;&#34987;&#25302;&#34892;&#20107;&#25925;&#19978;&#30340;&#19981;&#24403;&#22788;&#29702;&#65292;&#24182;&#20174;&#20013;&#25552;&#28860;&#20986;&#23545;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#23433;&#20840;&#24037;&#31243;&#26377;&#37325;&#35201;&#21551;&#31034;&#30340;&#25945;&#35757;&#65292;&#21253;&#25324;&#23545;&#21608;&#36793;&#21457;&#29983;&#20107;&#20214;&#30340;&#35748;&#35782;&#19982;&#21453;&#24212;&#12289;&#20934;&#30830;&#26500;&#24314;&#30896;&#25758;&#21518;&#22330;&#26223;&#27169;&#22411;&#20197;&#21450;&#25913;&#21892;&#20107;&#25925;&#21709;&#24212;&#26426;&#21046;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;GM Cruise&#33258;&#21160;&#39550;&#39542;&#20986;&#31199;&#36710;&#22312;2023&#24180;10&#26376;&#19982;&#19968;&#21517;&#34892;&#20154;&#21457;&#29983;&#30896;&#25758;&#30340;&#26696;&#20363;&#65292;&#20998;&#26512;&#20102;&#20844;&#21496;&#22312;&#24212;&#23545;&#34892;&#20154;&#34987;&#25302;&#34892;&#20107;&#25925;&#19978;&#30340;&#19981;&#24403;&#22788;&#29702;&#65292;&#24182;&#20174;&#20013;&#25552;&#28860;&#20986;&#23545;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#23433;&#20840;&#24037;&#31243;&#26377;&#37325;&#35201;&#21551;&#31034;&#30340;&#25945;&#35757;&#65292;&#21253;&#25324;&#23545;&#21608;&#36793;&#21457;&#29983;&#20107;&#20214;&#30340;&#35748;&#35782;&#19982;&#21453;&#24212;&#12289;&#20934;&#30830;&#26500;&#24314;&#30896;&#25758;&#21518;&#22330;&#26223;&#27169;&#22411;&#20197;&#21450;&#25913;&#21892;&#20107;&#25925;&#21709;&#24212;&#26426;&#21046;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.06046v3 Announce Type: replace  Abstract: An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. Is-sues stem not just from the loss events themselves, but also from how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. External investigation reports provide raw material describing the incident and critique the company's response from a regulatory point of view, but exclude safety engineering recommendations from scope. We highlight specific facts and relationships among events by tying together different pieces of the external report material. We then explore safety lessons that might be learned related to: recognizing and responding to nearby mishaps, building an accurate world model of a post-collision scenario, the in-adequacy of 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20813;&#36153;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#38543;&#26426;&#36807;&#31243;&#30340;&#27010;&#29575;&#27969;&#21464;&#25442;&#20026;&#24658;&#23450;&#36895;&#24230;&#27969;&#65292;&#24182;&#19988;&#26080;&#38656;&#35757;&#32451;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;Euler&#26041;&#27861;&#20174;&#21464;&#25442;&#21518;&#30340;&#27969;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#22312;&#21518;&#32493;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#26174;&#33879;&#25552;&#21319;&#20102;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20813;&#36153;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#38543;&#26426;&#36807;&#31243;&#30340;&#27010;&#29575;&#27969;&#21464;&#25442;&#20026;&#24658;&#23450;&#36895;&#24230;&#27969;&#65292;&#24182;&#19988;&#26080;&#38656;&#35757;&#32451;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;Euler&#26041;&#27861;&#20174;&#21464;&#25442;&#21518;&#30340;&#27969;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#22312;&#21518;&#32493;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#26174;&#33879;&#25552;&#21319;&#20102;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.02977v4 Announce Type: replace-cross  Abstract: We propose a systematic training-free method to transform the probability flow of a "linear" stochastic process characterized by the equation X_{t}=a_{t}X_{0}+\sigma_{t}X_{1} into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original probability flow via the Euler method without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows of two distinct linear stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing the sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental results substantiate the advantages of our framework. Our code is available at this [https://github.com/clarken92/VFM||link].
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#20449;&#24687;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35299;&#20915;P2P&#20511;&#36151;&#20013;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;Lending Club&#25968;&#25454;&#38598;&#19978;&#23545;BERT&#31561;LLM&#30340;&#36816;&#29992;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#20854;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#23545;&#20110;&#25552;&#21319;&#20449;&#36151;&#39118;&#38505;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#20449;&#24687;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35299;&#20915;P2P&#20511;&#36151;&#20013;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;Lending Club&#25968;&#25454;&#38598;&#19978;&#23545;BERT&#31561;LLM&#30340;&#36816;&#29992;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#20854;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#23545;&#20110;&#25552;&#21319;&#20449;&#36151;&#39118;&#38505;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16458v2 Announce Type: replace-cross  Abstract: Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.   Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based system
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#21508;&#31181;&#22797;&#26434;&#30340;&#21512;&#25104;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#36825;&#20123;&#35821;&#35328;&#26159;&#26681;&#25454;&#38750;&#33258;&#28982;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#23545;&#33521;&#35821;&#25968;&#25454;&#30340;&#31995;&#32479;&#24615;&#26356;&#25913;&#12290;&#20316;&#32773;&#25253;&#21578;&#20102;&#23545;&#20110;GPT-2&#23567;&#22411;&#27169;&#22411;&#22312;&#36825;&#20123;&#20598;&#28982;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#23398;&#20064;&#35821;&#35328;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#35821;&#35328;&#23398;&#20064;&#30340;&#30028;&#38480;&#65292;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22797;&#26434;&#24615;&#19979;&#23398;&#20064;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
Mission: Impossible Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#21508;&#31181;&#22797;&#26434;&#30340;&#21512;&#25104;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#36825;&#20123;&#35821;&#35328;&#26159;&#26681;&#25454;&#38750;&#33258;&#28982;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#23545;&#33521;&#35821;&#25968;&#25454;&#30340;&#31995;&#32479;&#24615;&#26356;&#25913;&#12290;&#20316;&#32773;&#25253;&#21578;&#20102;&#23545;&#20110;GPT-2&#23567;&#22411;&#27169;&#22411;&#22312;&#36825;&#20123;&#20598;&#28982;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#23398;&#20064;&#35821;&#35328;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#35821;&#35328;&#23398;&#20064;&#30340;&#30028;&#38480;&#65292;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22797;&#26434;&#24615;&#19979;&#23398;&#20064;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06416v2 Announce Type: replace-cross  Abstract: Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DrawTalking&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32472;&#30011;&#21644;&#21475;&#22836;&#21465;&#36848;&#26469;&#21019;&#24314;&#21644;&#25511;&#21046;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32534;&#30721;&#30693;&#35782;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#36817;&#20046;&#32534;&#31243;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#35753;&#29992;&#25143;&#22312;&#27809;&#26377;&#32534;&#31243;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#31181;&#25506;&#32034;&#21644;&#21019;&#20316;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DrawTalking&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32472;&#30011;&#21644;&#21475;&#22836;&#21465;&#36848;&#26469;&#21019;&#24314;&#21644;&#25511;&#21046;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32534;&#30721;&#30693;&#35782;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#36817;&#20046;&#32534;&#31243;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#35753;&#29992;&#25143;&#22312;&#27809;&#26377;&#32534;&#31243;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#31181;&#25506;&#32034;&#21644;&#21019;&#20316;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.05631v4 Announce Type: replace-cross  Abstract: We introduce DrawTalking, an approach to building and controlling interactive worlds by sketching and speaking while telling stories. It emphasizes user control and flexibility, and gives programming-like capability without requiring code. An early open-ended study with our prototype shows that the mechanics resonate and are applicable to many creative-exploratory use cases, with the potential to inspire and inform research in future natural interfaces for creative exploration and authoring.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20197;&#20248;&#21270;&#29992;&#20110;&#19977;&#32500;&#24418;&#29366;&#20998;&#26512;&#30340;&#32452;&#22810;&#35270;&#22270; transformer &#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16477</link><description>&lt;p&gt;
Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20197;&#20248;&#21270;&#29992;&#20110;&#19977;&#32500;&#24418;&#29366;&#20998;&#26512;&#30340;&#32452;&#22810;&#35270;&#22270; transformer &#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16477v3 Announce Type: replace  Abstract: In recent years, the results of view-based 3D shape recognition methods have saturated, and models with excellent performance cannot be deployed on memory-limited devices due to their huge size of parameters. To address this problem, we introduce a compression method based on knowledge distillation for this field, which largely reduces the number of parameters while preserving model performance as much as possible. Specifically, to enhance the capabilities of smaller models, we design a high-performing large model called Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first establishes relationships between view-level features. Additionally, to capture deeper features, we employ the grouping module to enhance view-level features into group-level features. Finally, the group-level ViT aggregates group-level features into complete, well-formed 3D shape descriptors. Notably, in both ViTs, we introduce spatial e
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#21644;&#21033;&#29992;CROWN-IBP&#27491;&#21017;&#21270;&#26469;&#22788;&#29702;Min-Max&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#20174;&#32780;&#20026;&#20840;&#21442;&#25968;&#30340;&#29983;&#23384;&#27169;&#22411;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;SurvSet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;Survival Analysis with Adversarial Regularization&#65288;SAWAR&#65289;&#26041;&#27861;&#22312;&#38754;&#23545;&#21508;&#31181;&#25200;&#21160;&#26102;&#65292;&#22312;&#36127;&#23545;&#25968; likelihood&#65288;NegLL&#65289;&#12289;&#38598;&#25104;&#36125;&#27982;&#23572;&#20998;&#25968;&#65288;IBS&#65289;&#21644;&#19968;&#33268;&#24615;&#25351;&#25968;&#65288;CI&#65289;&#31561;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#23545;&#22522;&#32447;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#39564;&#35777;&#20102;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#25552;&#39640;&#20102;&#29983;&#23384;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16019</link><description>&lt;p&gt;
Robust Survival Analysis with Adversarial Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16019
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#21644;&#21033;&#29992;CROWN-IBP&#27491;&#21017;&#21270;&#26469;&#22788;&#29702;Min-Max&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#20174;&#32780;&#20026;&#20840;&#21442;&#25968;&#30340;&#29983;&#23384;&#27169;&#22411;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;SurvSet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;Survival Analysis with Adversarial Regularization&#65288;SAWAR&#65289;&#26041;&#27861;&#22312;&#38754;&#23545;&#21508;&#31181;&#25200;&#21160;&#26102;&#65292;&#22312;&#36127;&#23545;&#25968; likelihood&#65288;NegLL&#65289;&#12289;&#38598;&#25104;&#36125;&#27982;&#23572;&#20998;&#25968;&#65288;IBS&#65289;&#21644;&#19968;&#33268;&#24615;&#25351;&#25968;&#65288;CI&#65289;&#31561;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#23545;&#22522;&#32447;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#39564;&#35777;&#20102;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#25552;&#39640;&#20102;&#29983;&#23384;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16019v3 Announce Type: replace-cross  Abstract: Survival Analysis (SA) models the time until an event occurs, with applications in fields like medicine, defense, finance, and aerospace. Recent work shows that Neural Networks (NNs) can capture complex relationships in SA. However, dataset uncertainties (e.g., noisy measurements, human error) can degrade model performance. To address this, we leverage NN verification advances to create algorithms for robust, fully-parametric survival models. We introduce a robust loss function and use CROWN-IBP regularization to handle computational challenges in the Min-Max problem. Evaluating our approach on SurvSet datasets, we find that our Survival Analysis with Adversarial Regularization (SAWAR) method consistently outperforms baselines under various perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI). This demonstrates that adversarial regularization enhances SA perform
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;GPT-4 APIs&#20013;&#19977;&#20010;&#26032;&#21151;&#33021;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#21253;&#25324;&#22312;&#26497;&#23569;&#25968;&#24694;&#24847;&#31034;&#20363;&#19979;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12289;&#33021;&#22815;&#25191;&#34892;&#20219;&#24847;&#20989;&#25968;&#35843;&#29992;&#12289;&#20197;&#21450;&#25805;&#32437;&#30693;&#35782;&#26816;&#32034;&#26426;&#21046;&#65292;&#34920;&#26126;&#20102;&#30495;&#23454;&#19990;&#30028;API&#30340;&#21151;&#33021;&#25193;&#23637;&#21487;&#33021;&#24341;&#20837;&#30340;&#26032;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.14302</link><description>&lt;p&gt;
Exploiting Novel GPT-4 APIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;GPT-4 APIs&#20013;&#19977;&#20010;&#26032;&#21151;&#33021;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#21253;&#25324;&#22312;&#26497;&#23569;&#25968;&#24694;&#24847;&#31034;&#20363;&#19979;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12289;&#33021;&#22815;&#25191;&#34892;&#20219;&#24847;&#20989;&#25968;&#35843;&#29992;&#12289;&#20197;&#21450;&#25805;&#32437;&#30693;&#35782;&#26816;&#32034;&#26426;&#21046;&#65292;&#34920;&#26126;&#20102;&#30495;&#23454;&#19990;&#30028;API&#30340;&#21151;&#33021;&#25193;&#23637;&#21487;&#33021;&#24341;&#20837;&#30340;&#26032;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14302v2 Announce Type: replace-cross  Abstract: Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose "gray-box" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed b
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36187;&#36710;&#22312;&#24212;&#23545;&#23454;&#38469;&#36710;&#36742;&#27169;&#22411;&#35823;&#24046;&#65288;&#27169;&#24577;&#24046;&#24322;&#65289;&#26102;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#37319;&#29992;&#35299;&#32806;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31639;&#27861;&#22312;&#38754;&#23545;&#27169;&#22411;&#35823;&#24046;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06406</link><description>&lt;p&gt;
Partial End-to-end Reinforcement Learning for Robustness Against Modelling Error in Autonomous Racing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36187;&#36710;&#22312;&#24212;&#23545;&#23454;&#38469;&#36710;&#36742;&#27169;&#22411;&#35823;&#24046;&#65288;&#27169;&#24577;&#24046;&#24322;&#65289;&#26102;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#37319;&#29992;&#35299;&#32806;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31639;&#27861;&#22312;&#38754;&#23545;&#27169;&#22411;&#35823;&#24046;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06406v2 Announce Type: replace  Abstract: In this paper, we address the issue of increasing the performance of reinforcement learning (RL) solutions for autonomous racing cars when navigating under conditions where practical vehicle modelling errors (commonly known as \emph{model mismatches}) are present. To address this challenge, we propose a partial end-to-end algorithm that decouples the planning and control tasks. Within this framework, an RL agent generates a trajectory comprising a path and velocity, which is subsequently tracked using a pure pursuit steering controller and a proportional velocity controller, respectively. In contrast, many current learning-based (i.e., reinforcement and imitation learning) algorithms utilise an end-to-end approach whereby a deep neural network directly maps from sensor data to control commands. By leveraging the robustness of a classical controller, our partial end-to-end driving algorithm exhibits better robustness towards model mis
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#25552;&#39640;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20026;&#25233;&#37057;&#30151;&#24739;&#32773;&#30340;&#20020;&#24202;&#23545;&#35805;&#25552;&#20379;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.13852</link><description>&lt;p&gt;
A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13852
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#25552;&#39640;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20026;&#25233;&#37057;&#30151;&#24739;&#32773;&#30340;&#20020;&#24202;&#23545;&#35805;&#25552;&#20379;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13852v3 Announce Type: replace  Abstract: The lack of explainability using relevant clinical knowledge hinders the adoption of Artificial Intelligence-powered analysis of unstructured clinical dialogue. A wealth of relevant, untapped Mental Health (MH) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. We develop a method to enhance attention in popular transformer models and generate clinician-understandable explanations for classification by incorporating external clinical knowledge. Inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to model patient inputs, providing meaningful explanations for classification. This will save manual review time and engender trust. We develop such a system in the context of MH using clinical practice guidelines (CPG) for d
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Emu Video&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#22270;&#20687;&#26465;&#20214;&#21270;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20004;&#27493;&#65306;&#39318;&#20808;&#26681;&#25454;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65292;&#28982;&#21518;&#26681;&#25454;&#25991;&#26412;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#35270;&#39057;&#12290;&#25991;&#31456;&#35782;&#21035;&#20986;&#20851;&#38190;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#23545;&#25193;&#25955;&#36807;&#31243;&#30340;&#22122;&#22768;&#35843;&#24230;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;&#20808;&#21069;&#24037;&#20316;&#37027;&#26679;&#20381;&#36182;&#28145;&#23618;&#27169;&#22411; cascades&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#19982;&#25152;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Emu Video&#29983;&#25104;&#30340;&#35270;&#39057;&#22312;&#36136;&#37327;&#19978;&#34987; strongly preferred&#65292;&#20998;&#21035;&#20026; 81% &#23545;&#35895;&#27468;&#30340;Imagen Video&#65292;90% &#23545;&#33521;&#20255;&#36798;&#30340;PYOCO&#65292;&#20197;&#21450; 96% &#23545; Meta&#30340;Make-A-Video&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21830;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;RunwayML&#30340;Gen2&#21644;Pika Labs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#25991;&#26412;&#25552;&#31034;&#21160;&#30011;&#21270;&#22270;&#20687;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#29983;&#25104;&#29289;&#22312;&#29992;&#25143;&#20043;&#38388;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20854;&#20026; 96% &#23545;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2311.10709</link><description>&lt;p&gt;
Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Emu Video&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#22270;&#20687;&#26465;&#20214;&#21270;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20004;&#27493;&#65306;&#39318;&#20808;&#26681;&#25454;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65292;&#28982;&#21518;&#26681;&#25454;&#25991;&#26412;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#35270;&#39057;&#12290;&#25991;&#31456;&#35782;&#21035;&#20986;&#20851;&#38190;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#23545;&#25193;&#25955;&#36807;&#31243;&#30340;&#22122;&#22768;&#35843;&#24230;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;&#20808;&#21069;&#24037;&#20316;&#37027;&#26679;&#20381;&#36182;&#28145;&#23618;&#27169;&#22411; cascades&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#19982;&#25152;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Emu Video&#29983;&#25104;&#30340;&#35270;&#39057;&#22312;&#36136;&#37327;&#19978;&#34987; strongly preferred&#65292;&#20998;&#21035;&#20026; 81% &#23545;&#35895;&#27468;&#30340;Imagen Video&#65292;90% &#23545;&#33521;&#20255;&#36798;&#30340;PYOCO&#65292;&#20197;&#21450; 96% &#23545; Meta&#30340;Make-A-Video&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21830;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;RunwayML&#30340;Gen2&#21644;Pika Labs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#25991;&#26412;&#25552;&#31034;&#21160;&#30011;&#21270;&#22270;&#20687;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#29983;&#25104;&#29289;&#22312;&#29992;&#25143;&#20043;&#38388;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20854;&#20026; 96% &#23545;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10709v2 Announce Type: replace  Abstract: We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#36896;&#24615;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ConferenceQA&#30340;&#19987;&#38376;&#29992;&#20110;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#32452;&#32455;&#20102;&#19971;&#20010;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#30340;&#20250;&#35758;&#25968;&#25454;&#65292;&#24182;&#20197;&#26641;&#29366;&#32467;&#26500;&#24418;&#24335;&#21576;&#29616;&#65292;&#21516;&#26102;&#26631;&#27880;&#20102;&#23545;&#24212;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#23398;&#26415;&#20250;&#35758;&#20449;&#24687;&#30340;&#26816;&#32034;&#21644;&#38382;&#31572;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.13028</link><description>&lt;p&gt;
Reliable Academic Conference Question Answering: A Study Based on Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#36896;&#24615;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ConferenceQA&#30340;&#19987;&#38376;&#29992;&#20110;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#32452;&#32455;&#20102;&#19971;&#20010;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#30340;&#20250;&#35758;&#25968;&#25454;&#65292;&#24182;&#20197;&#26641;&#29366;&#32467;&#26500;&#24418;&#24335;&#21576;&#29616;&#65292;&#21516;&#26102;&#26631;&#27880;&#20102;&#23545;&#24212;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#23398;&#26415;&#20250;&#35758;&#20449;&#24687;&#30340;&#26816;&#32034;&#21644;&#38382;&#31572;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13028v2 Announce Type: replace-cross  Abstract: As the development of academic conferences fosters global scholarly communication, researchers consistently need to obtain accurate and up-to-date information about academic conferences. Since the information is scattered, using an intelligent question-answering system to efficiently handle researchers' queries and ensure awareness of the latest advancements is necessary. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in question answering, and have been enhanced by retrieving external knowledge to deal with outdated knowledge. However, these methods fail to work due to the lack of the latest conference knowledge. To address this challenge, we develop the ConferenceQA dataset, consisting of seven diverse academic conferences. Specifically, for each conference, we first organize academic conference data in a tree-structured format through a semi-automated method. Then we annotate question-answer
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#25506;&#31350;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35757;&#32451;&#20013;&#24930;&#25910;&#25947;&#21644;&#37319;&#26679;&#20013;&#33394;&#24425;&#20559;&#24046;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#20174;&#32780;&#25512;&#36827;&#20102;&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2310.08442</link><description>&lt;p&gt;
Unmasking Bias in Diffusion Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08442
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#25506;&#31350;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35757;&#32451;&#20013;&#24930;&#25910;&#25947;&#21644;&#37319;&#26679;&#20013;&#33394;&#24425;&#20559;&#24046;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#20174;&#32780;&#25512;&#36827;&#20102;&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08442v3 Announce Type: replace  Abstract: Denoising diffusion models have emerged as a dominant approach for image generation, however they still suffer from slow convergence in training and color shift issues in sampling. In this paper, we identify that these obstacles can be largely attributed to bias and suboptimality inherent in the default training paradigm of diffusion models. Specifically, we offer theoretical insights that the prevailing constant loss weight strategy in $\epsilon$-prediction of diffusion models leads to biased estimation during the training phase, hindering accurate estimations of original images. To address the issue, we propose a simple but effective weighting strategy derived from the unlocked biased part. Furthermore, we conduct a comprehensive and systematic exploration, unraveling the inherent bias problem in terms of its existence, impact and underlying reasons. These analyses contribute to advancing the understanding of diffusion models. Empi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#31995;&#32479;&#26803;&#29702;&#20102;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20114;&#21160;&#30340;&#25991;&#29486;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;&#20114;&#21160;&#23450;&#20041;&#12289;&#35282;&#33394;&#12289;&#24433;&#21709;&#22240;&#32032;&#20197;&#21450;&#27979;&#37327;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#30740;&#31350;&#23454;&#36341;&#30340;&#25512;&#33616;&#65292;&#26088;&#22312;&#25552;&#39640;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36136;&#37327;&#21644;&#20114;&#21160;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.03392</link><description>&lt;p&gt;
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#31995;&#32479;&#26803;&#29702;&#20102;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20114;&#21160;&#30340;&#25991;&#29486;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;&#20114;&#21160;&#23450;&#20041;&#12289;&#35282;&#33394;&#12289;&#24433;&#21709;&#22240;&#32032;&#20197;&#21450;&#27979;&#37327;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#30740;&#31350;&#23454;&#36341;&#30340;&#25512;&#33616;&#65292;&#26088;&#22312;&#25552;&#39640;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36136;&#37327;&#21644;&#20114;&#21160;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03392v2 Announce Type: replace-cross  Abstract: Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;&#36229;&#36807;36&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#20026;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25552;&#20379;&#20855;&#20307;&#30340;&#23454;&#39564;&#35777;&#25454;&#65292;&#20174;&#32780;&#20026;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2310.02812</link><description>&lt;p&gt;
Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;&#36229;&#36807;36&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#20026;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25552;&#20379;&#20855;&#20307;&#30340;&#23454;&#39564;&#35777;&#25454;&#65292;&#20174;&#32780;&#20026;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02812v2 Announce Type: replace-cross  Abstract: Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the al
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24212;&#23545;&#26576;&#20123;&#38543;&#26426;&#36755;&#20837;&#26102;&#21487;&#33021;&#20250;&#20135;&#29983;&#30475;&#20284;&#21512;&#29702;&#30340;&#38169;&#35823;&#36755;&#20986;&#65292;&#36825;&#34920;&#26126;LLMs&#21487;&#33021;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#34987;&#25805;&#25511;&#20197;&#29983;&#25104;&#29305;&#23450;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24212;&#23545;&#26576;&#20123;&#38543;&#26426;&#36755;&#20837;&#26102;&#21487;&#33021;&#20250;&#20135;&#29983;&#30475;&#20284;&#21512;&#29702;&#30340;&#38169;&#35823;&#36755;&#20986;&#65292;&#36825;&#34920;&#26126;LLMs&#21487;&#33021;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#34987;&#25805;&#25511;&#20197;&#29983;&#25104;&#29305;&#23450;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01469v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from \textbf{hallucination}\textemdash fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that \emph{hallucination may be another view of adversarial examples}, and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formaliz
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#24037;&#19994;&#31995;&#32479;&#22312;&#39044;&#27979;&#32500;&#25252;&#31574;&#30053;&#20013;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#24037;&#19994;&#31995;&#32479;&#22312;&#39044;&#27979;&#32500;&#25252;&#31574;&#30053;&#20013;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.16935v3 Announce Type: replace-cross  Abstract: Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces an integrated framework that leverages the capabilities of the Transformer model-based neural networks and deep reinforcement learning (DRL) algorithms to optimize system maintenance actions. Our approach employs the Transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the remaining useful life (RUL) of an equipment. Additionally, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions, compared to the other prevalent machine learning-based methods. Our proposed approach provides an innovative
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#26368;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23436;&#20840;AI&#26234;&#33021; tutoring&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#65292;&#20010;&#24615;&#21270;&#25945;&#23398;&#21644;&#28789;&#27963;&#30340;&#27979;&#35797;&#35780;&#20272;&#12290;&#31995;&#32479;&#36890;&#36807;&#38142;&#25509; LLM &#24037;&#20855;&#21644;&#21160;&#24577;&#26356;&#26032;&#30340;&#20869;&#23384;&#27169;&#22359;&#26469;&#24212;&#23545;&#38271;&#26399;&#30340;&#25945;&#23398;&#20114;&#21160;&#21644;&#20010;&#24615;&#21270;&#25945;&#32946;&#38656;&#27714;&#12290;&#36890;&#36807;&#23398;&#20064;&#26085;&#24535;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#35777;&#26126;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2309.08112</link><description>&lt;p&gt;
Empowering Private Tutoring by Chaining Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#26368;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23436;&#20840;AI&#26234;&#33021; tutoring&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#65292;&#20010;&#24615;&#21270;&#25945;&#23398;&#21644;&#28789;&#27963;&#30340;&#27979;&#35797;&#35780;&#20272;&#12290;&#31995;&#32479;&#36890;&#36807;&#38142;&#25509; LLM &#24037;&#20855;&#21644;&#21160;&#24577;&#26356;&#26032;&#30340;&#20869;&#23384;&#27169;&#22359;&#26469;&#24212;&#23545;&#38271;&#26399;&#30340;&#25945;&#23398;&#20114;&#21160;&#21644;&#20010;&#24615;&#21270;&#25945;&#32946;&#38656;&#27714;&#12290;&#36890;&#36807;&#23398;&#20064;&#26085;&#24535;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#35777;&#26126;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08112v2 Announce Type: replace-cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate t
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#23545;&#20110;&#23545;&#31216;&#30830;&#23450;&#24615;&#39640;&#24230;&#20989;&#25968;&#30340;&#19968;&#31867;&#29305;&#23450;CNF&#20844;&#24335;&#65292;&#20854;&#32463;&#21333;&#20301;&#23376;&#21477;&#20256;&#25773;&#21518;&#19981;&#21487;&#31616;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#26368;&#23567;&#31561;&#20215;CNF&#20844;&#24335;&#30340;&#22823;&#23567;&#33267;&#23569;&#22823; O(n/ln n) &#20493;&#65292;&#36825;&#34920;&#26126;&#20102;&#19982;&#21333;&#20301;&#23376;&#21477;&#20256;&#25773;&#34892;&#20026;&#30456;&#20851;&#30340;CNF&#20844;&#24335;&#22823;&#23567;&#38388;&#30340;&#19968;&#20010;&#22522;&#26412;&#27604;&#20540;&#30340;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2309.01750</link><description>&lt;p&gt;
On CNF formulas irredundant with respect to unit clause propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#23637;&#31034;&#20102;&#23545;&#20110;&#23545;&#31216;&#30830;&#23450;&#24615;&#39640;&#24230;&#20989;&#25968;&#30340;&#19968;&#31867;&#29305;&#23450;CNF&#20844;&#24335;&#65292;&#20854;&#32463;&#21333;&#20301;&#23376;&#21477;&#20256;&#25773;&#21518;&#19981;&#21487;&#31616;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#26368;&#23567;&#31561;&#20215;CNF&#20844;&#24335;&#30340;&#22823;&#23567;&#33267;&#23569;&#22823; O(n/ln n) &#20493;&#65292;&#36825;&#34920;&#26126;&#20102;&#19982;&#21333;&#20301;&#23376;&#21477;&#20256;&#25773;&#34892;&#20026;&#30456;&#20851;&#30340;CNF&#20844;&#24335;&#22823;&#23567;&#38388;&#30340;&#19968;&#20010;&#22522;&#26412;&#27604;&#20540;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01750v4 Announce Type: replace-cross  Abstract: Two CNF formulas are called ucp-equivalent, if they behave in the same way with respect to the unit clause propagation (UCP). A formula is called ucp-irredundant, if removing any clause leads to a formula which is not ucp-equivalent to the original one. As a consequence of known results, the ratio of the size of a ucp-irredundant formula and the size of a smallest ucp-equivalent formula is at most $n^2$, where $n$ is the number of the variables. We demonstrate an example of a ucp-irredundant formula for a symmetric definite Horn function which is larger than a smallest ucp-equivalent formula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on the above ratio cannot be smaller than this.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;AI&#27169;&#22411;&#36171;&#33021;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25968;&#25454;&#36716;&#25442;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#24211;&#36827;&#34892;&#29992;&#25143;&#29305;&#23450;&#30340;&#35821;&#20041;&#25552;&#21462;&#25110;&#24674;&#22797;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#35821;&#20041;&#36890;&#20449;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2309.01249</link><description>&lt;p&gt;
Large AI Model Empowered Multimodal Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;AI&#27169;&#22411;&#36171;&#33021;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25968;&#25454;&#36716;&#25442;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#24211;&#36827;&#34892;&#29992;&#25143;&#29305;&#23450;&#30340;&#35821;&#20041;&#25552;&#21462;&#25110;&#24674;&#22797;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#35821;&#20041;&#36890;&#20449;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01249v2 Announce Type: replace  Abstract: Multimodal signals, including text, audio, image, and video, can be integrated into Semantic Communication (SC) systems to provide an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal distortion during transmission. Recent advancements in large AI models, particularly in the Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for addressing these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAMBO&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#36827;&#34892;&#36793;&#32536;&#26234;&#33021;&#30340;&#20248;&#21270;&#12290;LAMBO&#26694;&#26550;&#36890;&#36807;&#36755;&#20837;&#23884;&#20837;(IE)&#35299;&#20915;&#20102;&#24322;&#26500;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#25913;&#36827;&#30340;transformer&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;(AED)&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20840;&#23616;&#24863;&#30693;&#21644;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;actor-critic&#23398;&#20064;(ACL)&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;AED&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2308.15078</link><description>&lt;p&gt;
LAMBO: Large AI Model Empowered Edge Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.15078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAMBO&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#36827;&#34892;&#36793;&#32536;&#26234;&#33021;&#30340;&#20248;&#21270;&#12290;LAMBO&#26694;&#26550;&#36890;&#36807;&#36755;&#20837;&#23884;&#20837;(IE)&#35299;&#20915;&#20102;&#24322;&#26500;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#25913;&#36827;&#30340;transformer&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;(AED)&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20840;&#23616;&#24863;&#30693;&#21644;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;actor-critic&#23398;&#20064;(ACL)&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;AED&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.15078v2 Announce Type: replace  Abstract: Next-generation edge intelligence is anticipated to benefit various applications via offloading techniques. However, traditional offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this paper, we propose a Large AI Model-Based Offloading (LAMBO) framework with over one billion parameters for solving these problems. We first use input embedding (IE) to achieve normalized feature representation with heterogeneous constraints and task prompts. Then, we introduce a novel asymmetric encoder-decoder (AED) as the decision-making model, which is an improved transformer architecture consisting of a deep encoder and a shallow decoder for global perception and decision. Next, actor-critic learning (ACL) is used to pre-train the AED for different optimization tasks under corresponding prompts, enhancing the AED's generalization in multi-task
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#22768;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;&#22768;&#38899;&#25968;&#25454;&#20013;&#21516;&#26102;&#25429;&#25417;&#21644;&#29983;&#25104;&#22768;&#38899;&#20107;&#20214;&#21450;&#20854;&#22312;&#38899;&#39057;&#20013;&#30340;&#26102;&#38388;&#20301;&#32622;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26102;&#38388;&#26631;&#27880;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2308.11530</link><description>&lt;p&gt;
Leveraging Language Model Capabilities for Sound Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.11530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#22768;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;&#22768;&#38899;&#25968;&#25454;&#20013;&#21516;&#26102;&#25429;&#25417;&#21644;&#29983;&#25104;&#22768;&#38899;&#20107;&#20214;&#21450;&#20854;&#22312;&#38899;&#39057;&#20013;&#30340;&#26102;&#38388;&#20301;&#32622;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26102;&#38388;&#26631;&#27880;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.11530v2 Announce Type: replace-cross  Abstract: Large language models reveal deep comprehension and fluent generation in the field of multi-modality. Although significant advancements have been achieved in audio multi-modality, existing methods are rarely leverage language model for sound event detection (SED). In this work, we propose an end-to-end framework for understanding audio features while simultaneously generating sound event and their temporal location. Specifically, we employ pretrained acoustic models to capture discriminative features across different categories and language models for autoregressive text generation. Conventional methods generally struggle to obtain features in pure audio domain for classification. In contrast, our framework utilizes the language model to flexibly understand abundant semantic context aligned with the acoustic representation. The experimental results showcase the effectiveness of proposed method in enhancing timestamps precision 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;SchemaWalk&#8221;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#38024;&#23545;&#22797;&#26434; schema &#32593;&#32476;&#65288;&#20363;&#22914;&#21253;&#21547;&#25968;&#30334;&#31181;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#30693;&#35782;&#24211;&#65289;&#30340;&#38544;&#24335;&#20803;&#36335;&#24452;&#23398;&#20064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#20803;&#36335;&#24452;&#65292;&#26080;&#38656;&#39044;&#20808;&#25163;&#21160;&#26522;&#20030;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#22240;&#20803;&#36335;&#24452;&#26522;&#20030;&#21644;&#35780;&#20272;&#32780;&#20135;&#29983;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2307.03937</link><description>&lt;p&gt;
Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.03937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;SchemaWalk&#8221;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#38024;&#23545;&#22797;&#26434; schema &#32593;&#32476;&#65288;&#20363;&#22914;&#21253;&#21547;&#25968;&#30334;&#31181;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#30693;&#35782;&#24211;&#65289;&#30340;&#38544;&#24335;&#20803;&#36335;&#24452;&#23398;&#20064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#20803;&#36335;&#24452;&#65292;&#26080;&#38656;&#39044;&#20808;&#25163;&#21160;&#26522;&#20030;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#22240;&#20803;&#36335;&#24452;&#26522;&#20030;&#21644;&#35780;&#20272;&#32780;&#20135;&#29983;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.03937v2 Announce Type: replace  Abstract: Heterogeneous Information Networks (HINs) are information networks with multiple types of nodes and edges. The concept of meta-path, i.e., a sequence of entity types and relation types connecting two entities, is proposed to provide the meta-level explainable semantics for various HIN tasks. Traditionally, meta-paths are primarily used for schema-simple HINs, e.g., bibliographic networks with only a few entity types, where meta-paths are often enumerated with domain knowledge. However, the adoption of meta-paths for schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and relation types, has been limited due to the computational complexity associated with meta-path enumeration. Additionally, effectively assessing meta-paths requires enumerating relevant path instances, which adds further complexity to the meta-path learning process. To address these challenges, we propose SchemaWalk, an inductive meta-path learn
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24040;&#22823;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAM&#65289;&#30340; semantic &#36890;&#20449;&#65288;SC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65288;SAM&#65289;&#23558;&#22270;&#20687;&#20998;&#20026;&#19981;&#21516;&#30340;&#35821;&#20041;&#27573;&#24182;&#33258;&#21160;&#38598;&#25104;&#36825;&#20123;&#27573;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#35821;&#20041;&#24863;&#30693;&#22270;&#20687;&#30340;&#26500;&#24314;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;SC&#31995;&#32479;&#20013;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#24314;&#35774;&#30340;&#23616;&#38480;&#24615;&#12289;&#39057;&#32321;&#26356;&#26032;&#21644;&#23433;&#20840;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2307.03492</link><description>&lt;p&gt;
Large AI Model-Based Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.03492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24040;&#22823;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAM&#65289;&#30340; semantic &#36890;&#20449;&#65288;SC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65288;SAM&#65289;&#23558;&#22270;&#20687;&#20998;&#20026;&#19981;&#21516;&#30340;&#35821;&#20041;&#27573;&#24182;&#33258;&#21160;&#38598;&#25104;&#36825;&#20123;&#27573;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#35821;&#20041;&#24863;&#30693;&#22270;&#20687;&#30340;&#26500;&#24314;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;SC&#31995;&#32479;&#20013;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#24314;&#35774;&#30340;&#23616;&#38480;&#24615;&#12289;&#39057;&#32321;&#26356;&#26032;&#21644;&#23433;&#20840;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.03492v2 Announce Type: replace  Abstract: Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed reality, and the Internet of Everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model (LAM) provides new solutions to overcome the above issues. Here, we propose a LAM-based SC framework (LAM-SC) specifically designed for image data, where we first apply the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic aware image. Additionall
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#21450;&#20854;&#22312;&#25991;&#21270;&#33402;&#26415;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;"&#25552;&#31034;&#24037;&#31243;"&#36825;&#19968;&#23454;&#36341;&#65292;&#20854;&#20026;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#21019;&#24847;&#34920;&#36798;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25991;&#31456;&#25351;&#20986;&#65292;&#35813;&#25216;&#26415;&#19981;&#20165;&#25903;&#25345;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#21457;&#25381;&#65292;&#20063;&#21487;&#33021;&#23545;&#26410;&#26469;&#30340;&#25216;&#26415;&#21644;&#20154;&#31867;&#21457;&#23637;&#36896;&#25104;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2306.11393</link><description>&lt;p&gt;
The Cultivated Practices of Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11393
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25551;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#21450;&#20854;&#22312;&#25991;&#21270;&#33402;&#26415;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;"&#25552;&#31034;&#24037;&#31243;"&#36825;&#19968;&#23454;&#36341;&#65292;&#20854;&#20026;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#21019;&#24847;&#34920;&#36798;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25991;&#31456;&#25351;&#20986;&#65292;&#35813;&#25216;&#26415;&#19981;&#20165;&#25903;&#25345;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#21457;&#25381;&#65292;&#20063;&#21487;&#33021;&#23545;&#26410;&#26469;&#30340;&#25216;&#26415;&#21644;&#20154;&#31867;&#21457;&#23637;&#36896;&#25104;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.11393v2 Announce Type: replace-cross  Abstract: Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI). Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online. This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem. A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community. It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI. The chapter discusses the potential risks and dangers o
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20581;&#24247;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#65288;HKGs&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#20854;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#12289;&#24212;&#29992;&#26041;&#24335;&#20197;&#21450;&#29616;&#26377;&#30340;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#23545;&#24403;&#21069;&#30340;&#30693;&#35782;&#22270;&#35889;&#36164;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;</title><link>https://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20581;&#24247;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#65288;HKGs&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#20854;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#12289;&#24212;&#29992;&#26041;&#24335;&#20197;&#21450;&#29616;&#26377;&#30340;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#23545;&#24403;&#21069;&#30340;&#30693;&#35782;&#22270;&#35889;&#36164;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.04802v4 Announce Type: replace  Abstract: Healthcare knowledge graphs (HKGs) are valuable tools for organizing biomedical concepts and their relationships with interpretable structures. The recent advent of large language models (LLMs) has paved the way for building more comprehensive and accurate HKGs. This, in turn, can improve the reliability of generated content and enable better evaluation of LLMs. However, the challenges of HKGs such as regarding data heterogeneity and limited coverage are not fully understood, highlighting the need for detailed reviews. This work provides the first comprehensive review of HKGs. It summarizes the pipeline and key techniques for HKG construction, as well as the common utilization approaches, i.e., model-free and model-based. The existing HKG resources are also organized based on the data types they capture and application domains they cover, along with relevant statistical information (Resource available at https://github.com/lujiaying/
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#23450;&#20041;&#22806;&#37096;&#20107;&#20214;&#23545;&#20854;&#34892;&#20026;&#30340;&#24433;&#21709;&#26469;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#21464;&#21270;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.16056</link><description>&lt;p&gt;
Markov Decision Processes under External Temporal Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#23450;&#20041;&#22806;&#37096;&#20107;&#20214;&#23545;&#20854;&#34892;&#20026;&#30340;&#24433;&#21709;&#26469;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#21464;&#21270;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16056v2 Announce Type: replace-cross  Abstract: Most reinforcement learning algorithms treat the context under which they operate as a stationary, isolated, and undisturbed environment. However, in real world applications, environments constantly change due to a variety of external events. To address this problem, we study Markov Decision Processes (MDP) under the influence of an external temporal process. We formalize this notion and discuss conditions under which the problem becomes tractable with suitable solutions. We propose a policy iteration algorithm to solve this problem and theoretically analyze its performance. We derive results on the sample complexity of the algorithm and study its dependency on the extent of non-stationarity of the environment. We then conduct experiments to illustrate our results in a classic control environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#19982;&#20854;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#29109;&#26377;&#20851;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#31574;&#30053;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#23436;&#25104;&#21518;&#25913;&#21892;&#20854;&#20844;&#24179;&#24615;&#12290;&#36825;&#31181;&#20107;&#21518;&#35843;&#33410;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#21069;&#22788;&#29702;&#21644;&#22312;&#22788;&#29702;&#26041;&#27861;&#32780;&#35328;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.13088</link><description>&lt;p&gt;
Should We Attend More or Less? Modulating Attention for Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#21019;&#26032;&#24615;&#22320;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#19982;&#20854;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#29109;&#26377;&#20851;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#31574;&#30053;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#23436;&#25104;&#21518;&#25913;&#21892;&#20854;&#20844;&#24179;&#24615;&#12290;&#36825;&#31181;&#20107;&#21518;&#35843;&#33410;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#21069;&#22788;&#29702;&#21644;&#22312;&#22788;&#29702;&#26041;&#27861;&#32780;&#35328;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13088v2 Announce Type: replace-cross  Abstract: The advances in natural language processing (NLP) pose both opportunities and challenges. While recent progress enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal per
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#34920;&#26126;&#65292;&#21363;&#20351;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010; token &#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36890;&#36807;&#23545;&#29305;&#23450;&#20195;&#30721;&#36755;&#20986;&#35757;&#32451;&#65292;&#23427;&#20204;&#20063;&#33021;&#22815;&#23398;&#20064;&#21040;&#31243;&#24207;&#30340;&#27491;&#24335;&#35821;&#20041;&#34920;&#31034;&#12290;&#20363;&#22914;&#65292;&#20197;&#31243;&#24207;&#23548;&#33322;&#29615;&#22659;&#30340;&#34892;&#20026;&#20316;&#20026;&#36755;&#20837;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#34920;&#31034;&#31243;&#24207;&#25191;&#34892;&#36807;&#31243;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#20013;&#38388;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
Emergent Representations of Program Semantics in Language Models Trained on Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#34920;&#26126;&#65292;&#21363;&#20351;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010; token &#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36890;&#36807;&#23545;&#29305;&#23450;&#20195;&#30721;&#36755;&#20986;&#35757;&#32451;&#65292;&#23427;&#20204;&#20063;&#33021;&#22815;&#23398;&#20064;&#21040;&#31243;&#24207;&#30340;&#27491;&#24335;&#35821;&#20041;&#34920;&#31034;&#12290;&#20363;&#22914;&#65292;&#20197;&#31243;&#24207;&#23548;&#33322;&#29615;&#22659;&#30340;&#34892;&#20026;&#20316;&#20026;&#36755;&#20837;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#34920;&#31034;&#31243;&#24207;&#25191;&#34892;&#36807;&#31243;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#20013;&#38388;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.11169v3 Announce Type: replace-cross  Abstract: We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We antic
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25506;&#32034;&#20102;&#22312;&#29615;&#22659;&#32422;&#26463;&#19979;&#65292;&#20449;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#31526;&#21495;&#24847;&#20041;&#30340;&#35821;&#22659;&#20381;&#36182;&#24615;&#24471;&#21040;&#21457;&#23637;&#30340;&#26465;&#20214;&#12290;&#30740;&#31350;&#22522;&#20110;&#32463;&#20856;&#20449;&#21495;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#35789;&#27719;&#37327;&#30340;&#24773;&#26223;&#19979;&#65292;&#20132;&#27969;&#32773;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#20449;&#21495;&#24182;&#20316;&#20986;&#21453;&#24212;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#29615;&#22659;&#22240;&#32032;&#21644;&#25509;&#25910;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#22914;&#20309;&#20419;&#36827;&#23545;&#31526;&#21495;&#24847;&#20041;&#30340;&#35821;&#22659;&#24615;&#35299;&#37322;&#65292;&#24182;&#25351;&#20986;&#22312;&#19981;&#20855;&#22791;&#35821;&#22659;&#35299;&#37322;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#25509;&#25910;&#32773;&#23545;&#29615;&#22659;&#30340;&#24863;&#30693;&#20063;&#26080;&#27861;&#38459;&#27490;&#21033;&#29992;&#29615;&#22659;&#30340;&#25509;&#25910;&#32773;&#23545;&#20449;&#21495;&#20316;&#20986;&#27491;&#30830;&#21453;&#24212;&#12290;&#30740;&#31350;&#25552;&#20986;&#65292;&#25509;&#25910;&#32773;&#23545;&#35821;&#22659;&#30340;&#24847;&#35782;&#26159;&#23454;&#29616;&#26377;&#25928;&#35821;&#22659;&#20381;&#36182;&#24335;&#20132;&#27969;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2305.05821</link><description>&lt;p&gt;
Context-dependent communication under environmental constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.05821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25506;&#32034;&#20102;&#22312;&#29615;&#22659;&#32422;&#26463;&#19979;&#65292;&#20449;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#31526;&#21495;&#24847;&#20041;&#30340;&#35821;&#22659;&#20381;&#36182;&#24615;&#24471;&#21040;&#21457;&#23637;&#30340;&#26465;&#20214;&#12290;&#30740;&#31350;&#22522;&#20110;&#32463;&#20856;&#20449;&#21495;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#35789;&#27719;&#37327;&#30340;&#24773;&#26223;&#19979;&#65292;&#20132;&#27969;&#32773;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#20449;&#21495;&#24182;&#20316;&#20986;&#21453;&#24212;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#29615;&#22659;&#22240;&#32032;&#21644;&#25509;&#25910;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#22914;&#20309;&#20419;&#36827;&#23545;&#31526;&#21495;&#24847;&#20041;&#30340;&#35821;&#22659;&#24615;&#35299;&#37322;&#65292;&#24182;&#25351;&#20986;&#22312;&#19981;&#20855;&#22791;&#35821;&#22659;&#35299;&#37322;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#25509;&#25910;&#32773;&#23545;&#29615;&#22659;&#30340;&#24863;&#30693;&#20063;&#26080;&#27861;&#38459;&#27490;&#21033;&#29992;&#29615;&#22659;&#30340;&#25509;&#25910;&#32773;&#23545;&#20449;&#21495;&#20316;&#20986;&#27491;&#30830;&#21453;&#24212;&#12290;&#30740;&#31350;&#25552;&#20986;&#65292;&#25509;&#25910;&#32773;&#23545;&#35821;&#22659;&#30340;&#24847;&#35782;&#26159;&#23454;&#29616;&#26377;&#25928;&#35821;&#22659;&#20381;&#36182;&#24335;&#20132;&#27969;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.05821v2 Announce Type: replace  Abstract: There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phe
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;MUlti-modal Generator&#65288;MUG&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;Web&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#30340;&#35299;&#37322;&#65292;&#20026;&#35774;&#35745;&#26032;&#30340;&#35270;&#35273;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;MUlti-modal Generator&#65288;MUG&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;Web&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#30340;&#35299;&#37322;&#65292;&#20026;&#35774;&#35745;&#26032;&#30340;&#35270;&#35273;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07088v3 Announce Type: replace  Abstract: Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#20915;&#31574;&#26102;&#38388;&#19982;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#23545;&#27604;&#65292;&#23588;&#20854;&#20851;&#27880;&#22522;&#20110;&#20215;&#20540;&#30340;&#20004;&#31181;&#26041;&#27861;&#22312;&#24120;&#35268;RL&#21644;&#36716;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#24212;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2206.08442</link><description>&lt;p&gt;
A Look at Value-Based Decision-Time vs. Background Planning Methods Across Different Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.08442
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#20915;&#31574;&#26102;&#38388;&#19982;&#32972;&#26223;&#35268;&#21010;&#26041;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#23545;&#27604;&#65292;&#23588;&#20854;&#20851;&#27880;&#22522;&#20110;&#20215;&#20540;&#30340;&#20004;&#31181;&#26041;&#27861;&#22312;&#24120;&#35268;RL&#21644;&#36716;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#24212;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.08442v2 Announce Type: replace-cross  Abstract: In model-based reinforcement learning (RL), an agent can leverage a learned model to improve its way of behaving in different ways. Two of the prevalent ways to do this are through decision-time and background planning methods. In this study, we are interested in understanding how the value-based versions of these two planning methods will compare against each other across different settings. Towards this goal, we first consider the simplest instantiations of value-based decision-time and background planning methods and provide theoretical results on which one will perform better in the regular RL and transfer learning settings. Then, we consider the modern instantiations of them and provide hypotheses on which one will perform better in the same settings. Finally, we perform illustrative experiments to validate these theoretical results and hypotheses. Overall, our findings suggest that even though value-based versions of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Confidence-aware Self-Knowledge Distillation (CSD)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#27169;&#22411;&#33258;&#36523;&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#25552;&#39640;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#39640;&#32500;&#25945;&#24072;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25805;&#20316;&#25152;&#24102;&#26469;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CSD&#25552;&#39640;&#20102;KGE&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#20026;&#22788;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2206.02963</link><description>&lt;p&gt;
Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.02963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Confidence-aware Self-Knowledge Distillation (CSD)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#27169;&#22411;&#33258;&#36523;&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#25552;&#39640;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#39640;&#32500;&#25945;&#24072;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25805;&#20316;&#25152;&#24102;&#26469;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CSD&#25552;&#39640;&#20102;KGE&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#20026;&#22788;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02963v3 Announce Type: replace-cross  Abstract: Knowledge Graph Embedding (KGE), which projects entities and relations into continuous vector spaces, has garnered significant attention. Although high-dimensional KGE methods offer better performance, they come at the expense of significant computation and memory overheads. Decreasing embedding dimensions significantly deteriorates model performance. While several recent efforts utilize knowledge distillation or non-Euclidean representation learning to augment the effectiveness of low-dimensional KGE, they either necessitate a pre-trained high-dimensional teacher model or involve complex non-Euclidean operations, thereby incurring considerable additional computational costs. To address this, this work proposes Confidence-aware Self-Knowledge Distillation (CSD) that learns from the model itself to enhance KGE in a low-dimensional space. Specifically, CSD extracts knowledge from embeddings in previous iterations, which would be 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#20351;&#29992;&#20840;&#23616;&#24418;&#29366;&#25551;&#36848;&#31526;&#23545;&#29289;&#20307;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;&#65292;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#20004;&#31867;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#29305;&#23450;&#31867;&#21035;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#34892;&#20026;&#35777;&#25454;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22312;&#35821;&#20041;&#31867;&#21035;&#32570;&#38519;&#20013;&#65292;&#24739;&#32773;&#22312;&#20570;&#20986;&#24191;&#27867;&#30340;&#20998;&#31867;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#35760;&#20303;&#35814;&#32454;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#23545;&#20110;&#27010;&#24565;&#30340;&#19968;&#33324;&#20449;&#24687;&#23384;&#20648;&#26159;&#26356;&#19981;&#26131;&#21463;&#21040;&#35821;&#20041;&#35760;&#24518;&#25439;&#23475;&#24433;&#21709;&#30340;&#12290;&#25991;&#31456;&#36824;&#23637;&#31034;&#20102;&#24739;&#32773;&#22312;&#23376;&#31867;&#21035;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#20007;&#22833;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#31456;&#21033;&#29992;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#19968;&#20010;&#39069;&#22806;&#38454;&#27573;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#30340;&#35780;&#20215;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#35273;&#39046;&#22495;&#26377;&#25928;&#22320;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/1901.11398</link><description>&lt;p&gt;
A study on general visual categorization of objects into animal and plant groups using global shape descriptors with a focus on category-specific deficits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1901.11398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#20351;&#29992;&#20840;&#23616;&#24418;&#29366;&#25551;&#36848;&#31526;&#23545;&#29289;&#20307;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;&#65292;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#20004;&#31867;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#29305;&#23450;&#31867;&#21035;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#34892;&#20026;&#35777;&#25454;&#65292;&#25991;&#31456;&#23637;&#31034;&#20102;&#22312;&#35821;&#20041;&#31867;&#21035;&#32570;&#38519;&#20013;&#65292;&#24739;&#32773;&#22312;&#20570;&#20986;&#24191;&#27867;&#30340;&#20998;&#31867;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#35760;&#20303;&#35814;&#32454;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#23545;&#20110;&#27010;&#24565;&#30340;&#19968;&#33324;&#20449;&#24687;&#23384;&#20648;&#26159;&#26356;&#19981;&#26131;&#21463;&#21040;&#35821;&#20041;&#35760;&#24518;&#25439;&#23475;&#24433;&#21709;&#30340;&#12290;&#25991;&#31456;&#36824;&#23637;&#31034;&#20102;&#24739;&#32773;&#22312;&#23376;&#31867;&#21035;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#20007;&#22833;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#31456;&#21033;&#29992;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#19968;&#20010;&#39069;&#22806;&#38454;&#27573;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#30340;&#35780;&#20215;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#35273;&#39046;&#22495;&#26377;&#25928;&#22320;&#21306;&#20998;&#21160;&#29289;&#21644;&#26893;&#29289;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1901.11398v3 Announce Type: replace  Abstract: How do humans distinguish between general categories of objects? In a number of semantic category deficits, patients are good at making broad categorization but are unable to remember fine and specific details. It has been well accepted that general information about concepts is more robust to damages related to semantic memory. Results from patients with semantic memory disorders demonstrate the loss of ability in subcategory recognition. In this paper, we review the behavioral evidence for category specific disorder and show that general categories of animal and plant are visually distinguishable without processing textural information. To this aim, we utilize shape descriptors with an additional phase of feature learning. The results are evaluated with both supervised and unsupervised learning mechanisms and confirm that the proposed method can effectively discriminates between animal and plant object categories in visual domain.
&lt;/p&gt;</description></item></channel></rss>