<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://raw.githubusercontent.com/chrunx/cn-chat-arxiv/master/cs.AI.xml</link><description>This is arxiv RSS feed for cs.AI</description><item><title>该文章的重要创新贡献在于提出LLaVA-OneVision，这是一种开放的大型多模态模型，可以在单图像、多图像和视频场景中同时推动性能边界，通过单一模型展现强大的跨模态和跨场景任务迁移能力，特别是将图像理解能力成功迁移到视频场景中。</title><link>https://arxiv.org/abs/2408.03326</link><description>&lt;p&gt;
LLaVA-OneVision: Easy Visual Task Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03326
&lt;/p&gt;
&lt;p&gt;
该文章的重要创新贡献在于提出LLaVA-OneVision，这是一种开放的大型多模态模型，可以在单图像、多图像和视频场景中同时推动性能边界，通过单一模型展现强大的跨模态和跨场景任务迁移能力，特别是将图像理解能力成功迁移到视频场景中。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03326v1 Announce Type: new  Abstract: We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.
&lt;/p&gt;</description></item><item><title>该文章创建了一个标注有 hedge 的 Roadrunner 动画对白的文本库，并使用 BERT 模型进行微调，以提高准确识别这些主观表达的能力。通过这种方法，人工智能系统能够更好地理解说话者在句子中加入的模糊化表达，如怀疑或宽容程度，从而提高自然语言处理的准确性和对人类话语的理解。</title><link>https://arxiv.org/abs/2408.03319</link><description>&lt;p&gt;
Training LLMs to Recognize Hedges in Spontaneous Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03319
&lt;/p&gt;
&lt;p&gt;
该文章创建了一个标注有 hedge 的 Roadrunner 动画对白的文本库，并使用 BERT 模型进行微调，以提高准确识别这些主观表达的能力。通过这种方法，人工智能系统能够更好地理解说话者在句子中加入的模糊化表达，如怀疑或宽容程度，从而提高自然语言处理的准确性和对人类话语的理解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03319v1 Announce Type: cross  Abstract: Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or "fuzziness", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-
&lt;/p&gt;</description></item><item><title>该文章提出了一种深度神经网络，该网络能够利用人类指导来改善现有的图像分割掩码。这种方法显著减少了手动输入的需求，使得手动标注的流程更加高效，同时保持了高质量的标注结果。</title><link>https://arxiv.org/abs/2408.03304</link><description>&lt;p&gt;
Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03304
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种深度神经网络，该网络能够利用人类指导来改善现有的图像分割掩码。这种方法显著减少了手动输入的需求，使得手动标注的流程更加高效，同时保持了高质量的标注结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03304v1 Announce Type: new  Abstract: Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improveme
&lt;/p&gt;</description></item><item><title>该文章通过深入研究盲人用户与物体识别错误处理的相关策略和挑战，揭示了在相机辅助技术和物体识别系统中识别错误时用户的直观经验。</title><link>https://arxiv.org/abs/2408.03303</link><description>&lt;p&gt;
Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03303
&lt;/p&gt;
&lt;p&gt;
该文章通过深入研究盲人用户与物体识别错误处理的相关策略和挑战，揭示了在相机辅助技术和物体识别系统中识别错误时用户的直观经验。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03303v1 Announce Type: cross  Abstract: Object recognition technologies hold the potential to support blind and low-vision people in navigating the world around them. However, the gap between benchmark performances and practical usability remains a significant challenge. This paper presents a study aimed at understanding blind users' interaction with object recognition systems for identifying and avoiding errors. Leveraging a pre-existing object recognition system, URCam, fine-tuned for our experiment, we conducted a user study involving 12 blind and low-vision participants. Through in-depth interviews and hands-on error identification tasks, we gained insights into users' experiences, challenges, and strategies for identifying errors in camera-based assistive technologies and object recognition systems. During interviews, many participants preferred independent error review, while expressing apprehension toward misrecognitions. In the error identification task, participants
&lt;/p&gt;</description></item><item><title>该文章提出了一个称为“知识偏好优化”（KaPO）的方法，旨在通过实现可控的知识选择来增强检索增强生成模型（RAG）在知识密集型任务中的表现，以解决大型语言模型在回答问题时可能出现的“幻觉”问题。通过结合外部知识与内部知识，模型能够根据不同情境合理选取知识，避免了知识间的冲突，提高了模型的回答准确性。</title><link>https://arxiv.org/abs/2408.03297</link><description>&lt;p&gt;
KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03297
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个称为“知识偏好优化”（KaPO）的方法，旨在通过实现可控的知识选择来增强检索增强生成模型（RAG）在知识密集型任务中的表现，以解决大型语言模型在回答问题时可能出现的“幻觉”问题。通过结合外部知识与内部知识，模型能够根据不同情境合理选取知识，避免了知识间的冲突，提高了模型的回答准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03297v1 Announce Type: cross  Abstract: By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios. To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in re
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用注意力U-Net和基于关注度的可解释性的静态IR下降预测方法，该方法通过混合人工生成数据和少量真实设计点来训练神经网络，以实现快速准确的图像化静态IR下降预测。</title><link>https://arxiv.org/abs/2408.03292</link><description>&lt;p&gt;
Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03292
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用注意力U-Net和基于关注度的可解释性的静态IR下降预测方法，该方法通过混合人工生成数据和少量真实设计点来训练神经网络，以实现快速准确的图像化静态IR下降预测。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03292v1 Announce Type: cross  Abstract: There has been significant recent progress to reduce the computational effort of static IR drop analysis using neural networks, and modeling as an image-to-image translation task. A crucial issue is the lack of sufficient data from real industry designs to train these networks. Additionally, there is no methodology to explain a high-drop pixel in a predicted IR drop image to its specific root-causes. In this work, we first propose a U-Net neural network model with attention gates which is specifically tailored to achieve fast and accurate image-based static IR drop prediction. Attention gates allow selective emphasis on relevant parts of the input data without supervision which is desired because of the often sparse nature of the IR drop map. We propose a two-phase training process which utilizes a mix of artificially-generated data and a limited number of points from real designs. The results are, on-average, 18% (53%) better in MAE a
&lt;/p&gt;</description></item><item><title>该文章提出的StructEval框架通过在多个认知水平和关键概念上进行结构化评估，深化和拓宽了大型语言模型评估，提供了一种可靠的工具来抵抗数据污染的风险和减少潜在偏见的干扰，从而更可靠和一致地得出有关模型能力的结果。</title><link>https://arxiv.org/abs/2408.03281</link><description>&lt;p&gt;
StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03281
&lt;/p&gt;
&lt;p&gt;
该文章提出的StructEval框架通过在多个认知水平和关键概念上进行结构化评估，深化和拓宽了大型语言模型评估，提供了一种可靠的工具来抵抗数据污染的风险和减少潜在偏见的干扰，从而更可靠和一致地得出有关模型能力的结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03281v1 Announce Type: cross  Abstract: Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities.
&lt;/p&gt;</description></item><item><title>该文章设计并实现了一个交互式视觉系统Compress and Compare，该系统旨在支持机器学习模型压缩的效率评估和行为比较。用户可以通过一个统一的界面来探索压缩策略，并通过可视化比较来观察模型之间的预测、权重和激活行为差异。Compress and Compare特别适用于处理压缩模型的复杂对比分析，从而帮助用户在保持模型性能的同时优化其效率。</title><link>https://arxiv.org/abs/2408.03274</link><description>&lt;p&gt;
Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03274
&lt;/p&gt;
&lt;p&gt;
该文章设计并实现了一个交互式视觉系统Compress and Compare，该系统旨在支持机器学习模型压缩的效率评估和行为比较。用户可以通过一个统一的界面来探索压缩策略，并通过可视化比较来观察模型之间的预测、权重和激活行为差异。Compress and Compare特别适用于处理压缩模型的复杂对比分析，从而帮助用户在保持模型性能的同时优化其效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03274v1 Announce Type: cross  Abstract: To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called Compress and Compare. Within a single interface, Compress and Compare surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how Compress and Compare supp
&lt;/p&gt;</description></item><item><title>该文章揭示了大型语言模型在面临推理任务时是否积极调用其内部知识仓库的秘密，通过研究模型在推理每个步骤中知识神经元的激活模式，发现模型未能在某些情况下有效地利用关键的知识关联，而是倾向于采取捷径来回答问题。通过手动调整参数知识在模型中的回忆过程，文章证明了提升模型的知识回忆能力可以显著提高模型的推理性能，而抑制这种回忆则导致性能显著下降。此外，文章还探讨了链式思维（CoT）提示在解决复杂推理任务中的作用，发现CoT可以增强模型对事实知识的回忆，促使模型以有序的方式参与推理。</title><link>https://arxiv.org/abs/2408.03247</link><description>&lt;p&gt;
Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03247
&lt;/p&gt;
&lt;p&gt;
该文章揭示了大型语言模型在面临推理任务时是否积极调用其内部知识仓库的秘密，通过研究模型在推理每个步骤中知识神经元的激活模式，发现模型未能在某些情况下有效地利用关键的知识关联，而是倾向于采取捷径来回答问题。通过手动调整参数知识在模型中的回忆过程，文章证明了提升模型的知识回忆能力可以显著提高模型的推理性能，而抑制这种回忆则导致性能显著下降。此外，文章还探讨了链式思维（CoT）提示在解决复杂推理任务中的作用，发现CoT可以增强模型对事实知识的回忆，促使模型以有序的方式参与推理。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03247v1 Announce Type: cross  Abstract: In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and rel
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为PFedSIS的联邦学习方法，用于个性化手术器械分割。该方法通过引入视觉先验，结合全局个性化 disentanglement、外观调节个性化增强和形状相似性全局增强机制，解决了现有方法未能充分考虑多头自我注意力个性化的问题，并且能够捕捉手术场景中的外观多样性与器械形状相似性。通过对多头自我注意力的头权重个性化，该方法实现了对单个训练站点特征的精确适应，提高了在多个独立站点上的器械分割性能。</title><link>https://arxiv.org/abs/2408.03208</link><description>&lt;p&gt;
Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03208
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为PFedSIS的联邦学习方法，用于个性化手术器械分割。该方法通过引入视觉先验，结合全局个性化 disentanglement、外观调节个性化增强和形状相似性全局增强机制，解决了现有方法未能充分考虑多头自我注意力个性化的问题，并且能够捕捉手术场景中的外观多样性与器械形状相似性。通过对多头自我注意力的头权重个性化，该方法实现了对单个训练站点特征的精确适应，提高了在多个独立站点上的器械分割性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03208v1 Announce Type: cross  Abstract: Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site d
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用自然istic人类驾驶先验和强化学习技术的对抗性安全关键场景生成方法，旨在从现实和挑战性的自动驾驶车辆决策系统评估中获取大规模测试场景。这种方法能够通过模拟真实交通交互环境以及实施的两阶段流程，模拟驾驶策略，进而生成既有真实感又具备挑战性的多样化场景。</title><link>https://arxiv.org/abs/2408.03200</link><description>&lt;p&gt;
Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03200
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用自然istic人类驾驶先验和强化学习技术的对抗性安全关键场景生成方法，旨在从现实和挑战性的自动驾驶车辆决策系统评估中获取大规模测试场景。这种方法能够通过模拟真实交通交互环境以及实施的两阶段流程，模拟驾驶策略，进而生成既有真实感又具备挑战性的多样化场景。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03200v1 Announce Type: new  Abstract: Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques. By doing this, we can obtain large-scale test scenarios that are both diverse and realistic. Specifically, we build a simulation environment that mimics natural traffic interaction scenarios. Informed by this environment, we implement a two-stage procedure. The first stage incorporates conventional rule-based models, e.g., IDM~(Intelligent Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and ca
&lt;/p&gt;</description></item><item><title>该文章提出了一种新颖的针对纳米无人机在有限资源下进行自我监督学习的实时训练方法，有效解决了域迁移问题，提高了模型的感知性能。</title><link>https://arxiv.org/abs/2408.03168</link><description>&lt;p&gt;
Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03168
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新颖的针对纳米无人机在有限资源下进行自我监督学习的实时训练方法，有效解决了域迁移问题，提高了模型的感知性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03168v1 Announce Type: new  Abstract: Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training label
&lt;/p&gt;</description></item><item><title>该文章展示了使用可学习间隔的空间膨胀卷积（DCLS）的方法，这种方法不仅在计算机视觉基准测试中超越了标准和膨胀卷积，还在提升模型的可解释性方面取得了显著进步。可解释性通过与人类视觉策略的对应关系来衡量，即通过比较基于模型的GradCAM热图与反映人类视觉注意度的ClickMe数据集热图之间的Spearman相关系数来评估。该方法在多个基准模型中得到了应用，并提高了模型的可解释性评分，表明DCLS增加了模型与人类视觉策略的一致性。</title><link>https://arxiv.org/abs/2408.03164</link><description>&lt;p&gt;
Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03164
&lt;/p&gt;
&lt;p&gt;
该文章展示了使用可学习间隔的空间膨胀卷积（DCLS）的方法，这种方法不仅在计算机视觉基准测试中超越了标准和膨胀卷积，还在提升模型的可解释性方面取得了显著进步。可解释性通过与人类视觉策略的对应关系来衡量，即通过比较基于模型的GradCAM热图与反映人类视觉注意度的ClickMe数据集热图之间的Spearman相关系数来评估。该方法在多个基准模型中得到了应用，并提高了模型的可解释性评分，表明DCLS增加了模型与人类视觉策略的一致性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03164v1 Announce Type: new  Abstract: Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random he
&lt;/p&gt;</description></item><item><title>该文章研究了由大型语言模型（LLMs）驱动的现代多模态推理模型在辅助视觉辅助设备完成多步骤日常活动方面的能力。我们通过在线视频数据集中的动作预测任务，对两种不同类型的多模态LLM方法——Socratic模型和视觉条件语言模型（VCLM）进行了基准测试，以评估其将视觉历史编码化和在中长期预测动作的能力。然而，这些在线视频数据集仅仅允许我们评估辅助设备的前两个能力，并不能评估在用户参与下重新规划的能力。因此，我们设计了一个用户参与式的评估框架，为VCLM提供了实时用户交互的补充评估方式，从而评估了VCLM在中长期预测和动态行动规划方面的实际表现。</title><link>https://arxiv.org/abs/2408.03160</link><description>&lt;p&gt;
User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03160
&lt;/p&gt;
&lt;p&gt;
该文章研究了由大型语言模型（LLMs）驱动的现代多模态推理模型在辅助视觉辅助设备完成多步骤日常活动方面的能力。我们通过在线视频数据集中的动作预测任务，对两种不同类型的多模态LLM方法——Socratic模型和视觉条件语言模型（VCLM）进行了基准测试，以评估其将视觉历史编码化和在中长期预测动作的能力。然而，这些在线视频数据集仅仅允许我们评估辅助设备的前两个能力，并不能评估在用户参与下重新规划的能力。因此，我们设计了一个用户参与式的评估框架，为VCLM提供了实时用户交互的补充评估方式，从而评估了VCLM在中长期预测和动态行动规划方面的实际表现。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03160v1 Announce Type: new  Abstract: Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches -- Socratic Models and Vision Conditioned Language Models (VCLMs) on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we
&lt;/p&gt;</description></item><item><title>该文章提出了一种利用人工智能驱动的特征选择和注意力网络的疾病预测优化方法，该方法通过结合统计、深度学习和优化选择特征的SEV-EB算法，创建了一个能同时捕捉多种疾病不同特征的强效预测模型。</title><link>https://arxiv.org/abs/2408.03151</link><description>&lt;p&gt;
Optimizing Disease Prediction with Artificial Intelligence Driven Feature Selection and Attention Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03151
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种利用人工智能驱动的特征选择和注意力网络的疾病预测优化方法，该方法通过结合统计、深度学习和优化选择特征的SEV-EB算法，创建了一个能同时捕捉多种疾病不同特征的强效预测模型。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03151v1 Announce Type: cross  Abstract: The rapid integration of machine learning methodologies in healthcare has ignited innovative strategies for disease prediction, particularly with the vast repositories of Electronic Health Records (EHR) data. This article delves into the realm of multi-disease prediction, presenting a comprehensive study that introduces a pioneering ensemble feature selection model. This model, designed to optimize learning systems, combines statistical, deep, and optimally selected features through the innovative Stabilized Energy Valley Optimization with Enhanced Bounds (SEV-EB) algorithm. The objective is to achieve unparalleled accuracy and stability in predicting various disorders. This work proposes an advanced ensemble model that synergistically integrates statistical, deep, and optimally selected features. This combination aims to enhance the predictive power of the model by capturing diverse aspects of the health data. At the heart of the prop
&lt;/p&gt;</description></item><item><title>该文章介绍了一个名为COMMENTATOR的代码混合型多语言文本标注框架，该框架专为确保有效地标注代码混合型文本而设计。通过展示在Hinglish文本的词汇级别和句子级别的语言标注任务中达到与现有最佳方法相比五倍的标注效率，COMMENTATOR证明了自己的有效性。此外，相关代码已公开，可在GitHub上的https://github.com/lingo-iitgn/commentator页面获取，并在https://bit.ly/commentator_video播放了一个演示视频。</title><link>https://arxiv.org/abs/2408.03125</link><description>&lt;p&gt;
COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03125
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一个名为COMMENTATOR的代码混合型多语言文本标注框架，该框架专为确保有效地标注代码混合型文本而设计。通过展示在Hinglish文本的词汇级别和句子级别的语言标注任务中达到与现有最佳方法相比五倍的标注效率，COMMENTATOR证明了自己的有效性。此外，相关代码已公开，可在GitHub上的https://github.com/lingo-iitgn/commentator页面获取，并在https://bit.ly/commentator_video播放了一个演示视频。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03125v1 Announce Type: cross  Abstract: As the NLP community increasingly addresses challenges associated with multilingualism, robust annotation tools are essential to handle multilingual datasets efficiently. In this paper, we introduce a code-mixed multilingual text annotation framework, COMMENTATOR, specifically designed for annotating code-mixed text. The tool demonstrates its effectiveness in token-level and sentence-level language annotation tasks for Hinglish text. We perform robust qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster annotations than the best baseline. Our code is publicly available at \url{https://github.com/lingo-iitgn/commentator}. The demonstration video is available at \url{https://bit.ly/commentator_video}.
&lt;/p&gt;</description></item><item><title>该文章构建了名为Euas-20的机器翻译性能评估数据集，用以量化大型语言模型在不同语言上的翻译能力及预训练数据对其翻译性能的影响，为相关研究人员和开发者提供参考。</title><link>https://arxiv.org/abs/2408.03119</link><description>&lt;p&gt;
Evaluating the Translation Performance of Large Language Models Based on Euas-20
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03119
&lt;/p&gt;
&lt;p&gt;
该文章构建了名为Euas-20的机器翻译性能评估数据集，用以量化大型语言模型在不同语言上的翻译能力及预训练数据对其翻译性能的影响，为相关研究人员和开发者提供参考。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03119v1 Announce Type: cross  Abstract: In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.
&lt;/p&gt;</description></item><item><title>该文章提出了一种数据驱动的方法，用于在具有未知分布的随机环境中学习具有跨环境鲁棒性的策略。该方法能够为学习到的策略在新环境中提供概率近似正确的性能保证。通过在MDP环境上进行有限样本探索，构建基于轨迹的区间MDP模型近似，并生成一个能在多个环境上保持良好性能的单一策略。同时，该方法还能够量化策略在新环境中的性能风险，并在探索和决策过程中实现这种风险的风险。</title><link>https://arxiv.org/abs/2408.03093</link><description>&lt;p&gt;
Learning Provably Robust Policies in Uncertain Parametric Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03093
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种数据驱动的方法，用于在具有未知分布的随机环境中学习具有跨环境鲁棒性的策略。该方法能够为学习到的策略在新环境中提供概率近似正确的性能保证。通过在MDP环境上进行有限样本探索，构建基于轨迹的区间MDP模型近似，并生成一个能在多个环境上保持良好性能的单一策略。同时，该方法还能够量化策略在新环境中的性能风险，并在探索和决策过程中实现这种风险的风险。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03093v1 Announce Type: cross  Abstract: We present a data-driven approach for learning MDP policies that are robust across stochastic environments whose transition probabilities are defined by parameters with an unknown distribution. We produce probably approximately correct (PAC) guarantees for the performance of these learned policies in a new, unseen environment over the unknown distribution. Our approach is based on finite samples of the MDP environments, for each of which we build an approximation of the model as an interval MDP, by exploring a set of generated trajectories. We use the built approximations to synthesise a single policy that performs well (meets given requirements) across the sampled environments, and furthermore bound its risk (of not meeting the given requirements) when deployed in an unseen environment. Our procedure offers a trade-off between the guaranteed performance of the learned policy and the risk of not meeting the guarantee in an unseen envir
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为QADQN（量子注意度量下的深度强化学习网络）的方法，该方法是量子强化学习的一种创新尝试，它结合了量子电路与传统深度学习网络，以用于金融市场的预测和交易策略制定。通过在包含固定交易成本的现实市场条件下进行回测，验证了QADQN在实际金融应用中的有效性和潜力。</title><link>https://arxiv.org/abs/2408.03088</link><description>&lt;p&gt;
QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03088
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为QADQN（量子注意度量下的深度强化学习网络）的方法，该方法是量子强化学习的一种创新尝试，它结合了量子电路与传统深度学习网络，以用于金融市场的预测和交易策略制定。通过在包含固定交易成本的现实市场条件下进行回测，验证了QADQN在实际金融应用中的有效性和潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03088v1 Announce Type: cross  Abstract: Financial market prediction and optimal trading strategy development remain challenging due to market complexity and volatility. Our research in quantum finance and reinforcement learning for decision-making demonstrates the approach of quantum-classical hybrid algorithms to tackling real-world financial challenges. In this respect, we corroborate the concept with rigorous backtesting and validate the framework's performance under realistic market conditions, by including fixed transaction cost per trade. This paper introduces a Quantum Attention Deep Q-Network (QADQN) approach to address these challenges through quantum-enhanced reinforcement learning. Our QADQN architecture uses a variational quantum circuit inside a traditional deep Q-learning framework to take advantage of possible quantum advantages in decision-making. We gauge the QADQN agent's performance on historical data from major market indices, including the S&amp;amp;P 500. We ev
&lt;/p&gt;</description></item><item><title>该文章提出了一种统一的复杂因果关系提取框架（UniCE），通过改进的子任务交互和知识融合机制，解决了在单一句子中识别多个因果关系提取、子任务间的相互依赖以及两种知识源（语言模型和结构知识图）融合的问题。</title><link>https://arxiv.org/abs/2408.03079</link><description>&lt;p&gt;
Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03079
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种统一的复杂因果关系提取框架（UniCE），通过改进的子任务交互和知识融合机制，解决了在单一句子中识别多个因果关系提取、子任务间的相互依赖以及两种知识源（语言模型和结构知识图）融合的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03079v1 Announce Type: cross  Abstract: Event Causality Extraction (ECE) aims at extracting causal event pairs from texts. Despite ChatGPT's recent success, fine-tuning small models remains the best approach for the ECE task. However, existing fine-tuning based ECE methods cannot address all three key challenges in ECE simultaneously: 1) Complex Causality Extraction, where multiple causal-effect pairs occur within a single sentence; 2) Subtask~ Interaction, which involves modeling the mutual dependence between the two subtasks of ECE, i.e., extracting events and identifying the causal relationship between extracted events; and 3) Knowledge Fusion, which requires effectively fusing the knowledge in two modalities, i.e., the expressive pretrained language models and the structured knowledge graphs. In this paper, we propose a unified ECE framework (UniCE to address all three issues in ECE simultaneously. Specifically, we design a subtask interaction mechanism to enable mutual 
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一种面向手术应用的开放域单目视觉SLAM框架BodySLAM，该框架能够充分利用单目摄像头的输入，无需任何传统的传感器输入，有效提高了手术操作中的深度感知和操纵精准度。</title><link>https://arxiv.org/abs/2408.03078</link><description>&lt;p&gt;
BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03078
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一种面向手术应用的开放域单目视觉SLAM框架BodySLAM，该框架能够充分利用单目摄像头的输入，无需任何传统的传感器输入，有效提高了手术操作中的深度感知和操纵精准度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03078v1 Announce Type: cross  Abstract: Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM) has emerged as a promising solution to address these limitations, its implementation in endoscopic procedures presents significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents a robust deep learning-based SLAM approach that combines state-of-the-art and newly developed models. It consists of three main parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method based on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the novel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous models to create a coherent surgical map. The performance of the procedure was rigorously eva
&lt;/p&gt;</description></item><item><title>该文章创新性地在英特尔Loihi 2型神经形态处理器上开发了一个针对二次无约束二元优化问题的算法，该算法通过硬件感知并能够快速生成解决方案，并且在能量效率上相比于传统的CPU上的算法有显著的提升。</title><link>https://arxiv.org/abs/2408.03076</link><description>&lt;p&gt;
Solving QUBO on the Loihi 2 Neuromorphic Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03076
&lt;/p&gt;
&lt;p&gt;
该文章创新性地在英特尔Loihi 2型神经形态处理器上开发了一个针对二次无约束二元优化问题的算法，该算法通过硬件感知并能够快速生成解决方案，并且在能量效率上相比于传统的CPU上的算法有显著的提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03076v1 Announce Type: cross  Abstract: In this article, we describe an algorithm for solving Quadratic Unconstrained Binary Optimization problems on the Intel Loihi 2 neuromorphic processor. The solver is based on a hardware-aware fine-grained parallel simulated annealing algorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary results show that our approach can generate feasible solutions in as little as 1 ms and up to 37x more energy efficient compared to two baseline solvers running on a CPU. These advantages could be especially relevant for size-, weight-, and power-constrained edge computing applications.
&lt;/p&gt;</description></item><item><title>该文章介绍了OpenOmni，一个开源工具，旨在支持开发人员构建未来导向的多模态对话代理。该工具集成了多种技术，包括语音识别、情感分析、增强生成和大型语言模型，允许用户根据需要进行定制，并支持本地和云部署，以保证数据安全和优化性能。</title><link>https://arxiv.org/abs/2408.03047</link><description>&lt;p&gt;
OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03047
&lt;/p&gt;
&lt;p&gt;
该文章介绍了OpenOmni，一个开源工具，旨在支持开发人员构建未来导向的多模态对话代理。该工具集成了多种技术，包括语音识别、情感分析、增强生成和大型语言模型，允许用户根据需要进行定制，并支持本地和云部署，以保证数据安全和优化性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03047v1 Announce Type: cross  Abstract: Multimodal conversational agents are highly desirable because they offer natural and human-like interaction. However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking. While proprietary systems like GPT-4o and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy. To better understand and quantify these issues, we developed OpenOmni, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models. OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking. This flexible framework allows researchers to customize the pipeline
&lt;/p&gt;</description></item><item><title>该文章提出了一种新型的自适应增强学习中稀疏奖励问题的解决方案，通过结合历史经验中的成功率来构造密集且信息量大的奖励信号。该方法使用从Beta分布中采样的成功率，这些分布随着数据的积累而逐渐变得可靠。在初始阶段，自适应成功率鼓励探索，随着数据的积累，逐渐变为鼓励利用的好方法。通过结合核密度估计（KDE）与随机傅里叶特征（RFF），该方法在处理高维连续状态空间时具有高效的计算效率。</title><link>https://arxiv.org/abs/2408.03029</link><description>&lt;p&gt;
Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03029
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新型的自适应增强学习中稀疏奖励问题的解决方案，通过结合历史经验中的成功率来构造密集且信息量大的奖励信号。该方法使用从Beta分布中采样的成功率，这些分布随着数据的积累而逐渐变得可靠。在初始阶段，自适应成功率鼓励探索，随着数据的积累，逐渐变为鼓励利用的好方法。通过结合核密度估计（KDE）与随机傅里叶特征（RFF），该方法在处理高维连续状态空间时具有高效的计算效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03029v1 Announce Type: cross  Abstract: Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experiences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implementation in high-dimensional continuous state spaces. This method provides a non-pa
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CSI的框架，能够将多种不同风格的运动技能整合到一个单一的控制器中，无需复杂的奖励工程。这为 legged robots 的多技能整合提供了一种新的灵活方法，可以应用于各种不同的任务环境中。</title><link>https://arxiv.org/abs/2408.03018</link><description>&lt;p&gt;
Integrating Controllable Motion Skills from Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03018
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CSI的框架，能够将多种不同风格的运动技能整合到一个单一的控制器中，无需复杂的奖励工程。这为 legged robots 的多技能整合提供了一种新的灵活方法，可以应用于各种不同的任务环境中。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03018v1 Announce Type: new  Abstract: The expanding applications of legged robots require their mastery of versatile motion skills. Correspondingly, researchers must address the challenge of integrating multiple diverse motion skills into controllers. While existing reinforcement learning (RL)-based approaches have achieved notable success in multi-skill integration for legged robots, these methods often require intricate reward engineering or are restricted to integrating a predefined set of motion skills constrained by specific task objectives, resulting in limited flexibility. In this work, we introduce a flexible multi-skill integration framework named Controllable Skills Integration (CSI). CSI enables the integration of a diverse set of motion skills with varying styles into a single policy without the need for complex reward tuning. Furthermore, in a hierarchical control manner, the trained low-level policy can be coupled with a high-level Natural Language Inference (N
&lt;/p&gt;</description></item><item><title>该文章提出NeurDB，一个融合人工智能的自治数据库，通过在数据库内无缝集成AI工作流程，实现了对数据和工作量变化的快速适应性。这种创新使NeurDB在处理AI数据分析任务时显著优于现有系统，特别是在通过学习构建的组件的性能方面。</title><link>https://arxiv.org/abs/2408.03013</link><description>&lt;p&gt;
NeurDB: On the Design and Implementation of an AI-powered Autonomous Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03013
&lt;/p&gt;
&lt;p&gt;
该文章提出NeurDB，一个融合人工智能的自治数据库，通过在数据库内无缝集成AI工作流程，实现了对数据和工作量变化的快速适应性。这种创新使NeurDB在处理AI数据分析任务时显著优于现有系统，特别是在通过学习构建的组件的性能方面。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03013v1 Announce Type: cross  Abstract: Databases are increasingly embracing AI to provide autonomous system optimization and intelligent in-database analytics, aiming to relieve end-user burdens across various industry sectors. Nonetheless, most existing approaches fail to account for the dynamic nature of databases, which renders them ineffective for real-world applications characterized by evolving data and workloads. This paper introduces NeurDB, an AI-powered autonomous database that deepens the fusion of AI and databases with adaptability to data and workload drift. NeurDB establishes a new in-database AI ecosystem that seamlessly integrates AI workflows within the database. This integration enables efficient and effective in-database AI analytics and fast-adaptive learned system components. Empirical evaluations demonstrate that NeurDB substantially outperforms existing solutions in managing AI analytics tasks, with the proposed learned components more effectively han
&lt;/p&gt;</description></item><item><title>该文章通过在虚拟现实环境中模拟不同文化背景下的行人行为对自主车辆通过决策的影响，探讨了文化差异如何影响人与自动驾驶车辆交互的安全性和接受度。</title><link>https://arxiv.org/abs/2408.03003</link><description>&lt;p&gt;
Cross-cultural analysis of pedestrian group behaviour influence on crossing decisions in interactions with autonomous vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.03003
&lt;/p&gt;
&lt;p&gt;
该文章通过在虚拟现实环境中模拟不同文化背景下的行人行为对自主车辆通过决策的影响，探讨了文化差异如何影响人与自动驾驶车辆交互的安全性和接受度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.03003v1 Announce Type: cross  Abstract: Understanding cultural backgrounds is crucial for the seamless integration of autonomous driving into daily life as it ensures that systems are attuned to diverse societal norms and behaviours, enhancing acceptance and safety in varied cultural contexts. In this work, we investigate the impact of co-located pedestrians on crossing behaviour, considering cultural and situational factors. To accomplish this, a full-scale virtual reality (VR) environment was created in the CARLA simulator, enabling the identical experiment to be replicated in both Spain and Australia. Participants (N=30) attempted to cross the road at an urban crosswalk alongside other pedestrians exhibiting conservative to more daring behaviours, while an autonomous vehicle (AV) approached with different driving styles. For the analysis of interactions, we utilized questionnaires and direct measures of the moment when participants entered the lane.   Our findings indicat
&lt;/p&gt;</description></item><item><title>该文章介绍了利用大型语言模型（LLMs）作为概率性最小适当教师（pMAT）的新框架，用于在DFA（确定性有限自动机）学习过程中进行错误概率性的随机分布，并为大概率产生错误的情况下提供了应对策略。通过利用LLMs可能出现的错误特性，提出了改进的提示和方法来提高学习效率和模型准确性。此外，该研究还对比了TTT算法和其他常见主动学习算法在DFA学习中的性能差异，并且为了解决可能导致的错误数量爆炸问题，该研究还引入了一种动态查询缓存算法来优化查询过程。</title><link>https://arxiv.org/abs/2408.02999</link><description>&lt;p&gt;
LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02999
&lt;/p&gt;
&lt;p&gt;
该文章介绍了利用大型语言模型（LLMs）作为概率性最小适当教师（pMAT）的新框架，用于在DFA（确定性有限自动机）学习过程中进行错误概率性的随机分布，并为大概率产生错误的情况下提供了应对策略。通过利用LLMs可能出现的错误特性，提出了改进的提示和方法来提高学习效率和模型准确性。此外，该研究还对比了TTT算法和其他常见主动学习算法在DFA学习中的性能差异，并且为了解决可能导致的错误数量爆炸问题，该研究还引入了一种动态查询缓存算法来优化查询过程。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02999v1 Announce Type: cross  Abstract: The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifi
&lt;/p&gt;</description></item><item><title>该文章提出的ASR-enhanced Multimodal Product Representation Learning（AMPere）方法通过使用基于LLM的ASR文本摘要器简化从噪声ASR文本中提取产品相关信息的过程，并将其与视觉数据联合输入到一个多分支网络中，以生成紧凑的跨域产品 multimodal 表示。</title><link>https://arxiv.org/abs/2408.02978</link><description>&lt;p&gt;
ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02978
&lt;/p&gt;
&lt;p&gt;
该文章提出的ASR-enhanced Multimodal Product Representation Learning（AMPere）方法通过使用基于LLM的ASR文本摘要器简化从噪声ASR文本中提取产品相关信息的过程，并将其与视觉数据联合输入到一个多分支网络中，以生成紧凑的跨域产品 multimodal 表示。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02978v1 Announce Type: cross  Abstract: E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive exp
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用强化学习方法（EmpRL）进行同情性响应生成的框架，它通过有效的同情度奖励函数和最大化期望奖励来生成更加同情性的文本响应，并利用了预训练的语言模型来提高响应的生成能力。</title><link>https://arxiv.org/abs/2408.02976</link><description>&lt;p&gt;
Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02976
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用强化学习方法（EmpRL）进行同情性响应生成的框架，它通过有效的同情度奖励函数和最大化期望奖励来生成更加同情性的文本响应，并利用了预训练的语言模型来提高响应的生成能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02976v1 Announce Type: cross  Abstract: Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in
&lt;/p&gt;</description></item><item><title>该文章提出了ADdress方法，这是一个以成功为基础的自学习单破坏-修复增强的任何时间多智能体路径规划方法，它通过动态调整破坏策略成功地减少了大型邻域搜索中的解决成本，提高了路径规划的多智能体系统的效率和实用性。</title><link>https://arxiv.org/abs/2408.02960</link><description>&lt;p&gt;
Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02960
&lt;/p&gt;
&lt;p&gt;
该文章提出了ADdress方法，这是一个以成功为基础的自学习单破坏-修复增强的任何时间多智能体路径规划方法，它通过动态调整破坏策略成功地减少了大型邻域搜索中的解决成本，提高了路径规划的多智能体系统的效率和实用性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02960v1 Announce Type: new  Abstract: Anytime multi-agent path finding (MAPF) is a promising approach to scalable path optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood Search (LNS), is the current state-of-the-art approach where a fast initial solution is iteratively optimized by destroying and repairing selected paths of the solution. Current MAPF-LNS variants commonly use an adaptive selection mechanism to choose among multiple destroy heuristics. However, to determine promising destroy heuristics, MAPF-LNS requires a considerable amount of exploration time. As common destroy heuristics are non-adaptive, any performance bottleneck caused by these heuristics cannot be overcome via adaptive heuristic selection alone, thus limiting the overall effectiveness of MAPF-LNS in terms of solution cost. In this paper, we propose Adaptive Delay-based Destroy-and-Repair Enhanced with Success-based Self-Learning (ADDRESS) as a single-destroy-heuristic variant o
&lt;/p&gt;</description></item><item><title>该文章提出了一种使用深度核感知方法训练的新型元学习策略，用于在极端域偏移下的自主机械臂少样本采矿任务。该方法通过模拟最大部署间隙并在训练过程中专门训练模型以克服这些间隙，从而能够适应巨大的域偏移。在贝叶斯优化序贯决策框架中使用该方法，不仅提高了采矿任务的效率，还有效地提升了样本采掘的质量。</title><link>https://arxiv.org/abs/2408.02949</link><description>&lt;p&gt;
Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02949
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种使用深度核感知方法训练的新型元学习策略，用于在极端域偏移下的自主机械臂少样本采矿任务。该方法通过模拟最大部署间隙并在训练过程中专门训练模型以克服这些间隙，从而能够适应巨大的域偏移。在贝叶斯优化序贯决策框架中使用该方法，不仅提高了采矿任务的效率，还有效地提升了样本采掘的质量。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02949v1 Announce Type: new  Abstract: Autonomous lander missions on extraterrestrial bodies need to sample granular materials while coping with domain shifts, even when sampling strategies are extensively tuned on Earth. To tackle this challenge, this paper studies the few-shot scooping problem and proposes a vision-based adaptive scooping strategy that uses the deep kernel Gaussian process method trained with a novel meta-training strategy to learn online from very limited experience on out-of-distribution target terrains. Our Deep Kernel Calibration with Maximal Deployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt to large domain shifts by creating simulated maximal deployment gaps from an offline training dataset and training models to overcome these deployment gaps during training. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to perform high-quality scooping actions on out-of-dist
&lt;/p&gt;</description></item><item><title>该文章发现大语言模型（LLMs）在对数据进行污染时表现出了更大的脆弱性，这种污染包括恶意微调、数据质量问题以及数据故意污染。研究显示，随着模型参数的增加，学习有害行为的速度也显著加快，这表明大型模型可能更容易受到有害行为的攻击。</title><link>https://arxiv.org/abs/2408.02946</link><description>&lt;p&gt;
Scaling Laws for Data Poisoning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02946
&lt;/p&gt;
&lt;p&gt;
该文章发现大语言模型（LLMs）在对数据进行污染时表现出了更大的脆弱性，这种污染包括恶意微调、数据质量问题以及数据故意污染。研究显示，随着模型参数的增加，学习有害行为的速度也显著加快，这表明大型模型可能更容易受到有害行为的攻击。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02946v1 Announce Type: cross  Abstract: Recent work shows that LLMs are vulnerable to data poisoning, in which they are trained on partially corrupted or harmful data. Poisoned data is hard to detect, breaks guardrails, and leads to undesirable and harmful behavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat. We consider three threat models by which data poisoning can occur: malicious fine-tuning, imperfect data curation, and intentional data contamination. Our experiments evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three datasets which speak to each of our threat models. We find that larger LLMs are increasingly vulnerable, learning harmful behavior -- including sleeper agent behavior -- significantly more quickly than smaller LLMs with
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一种利用大型语言模型(LLM)进行无线通信系统资源分配的方法，旨在通过整合LLM技术实现更加智能的无线网络资源优化，从而提升通信系统的效率。</title><link>https://arxiv.org/abs/2408.02944</link><description>&lt;p&gt;
LLM-Empowered Resource Allocation in Wireless Communications Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02944
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一种利用大型语言模型(LLM)进行无线通信系统资源分配的方法，旨在通过整合LLM技术实现更加智能的无线网络资源优化，从而提升通信系统的效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02944v1 Announce Type: cross  Abstract: The recent success of large language models (LLMs) has spurred their application in various fields. In particular, there have been efforts to integrate LLMs into various aspects of wireless communication systems. The use of LLMs in wireless communication systems has the potential to realize artificial general intelligence (AGI)-enabled wireless networks. In this paper, we investigate an LLM-based resource allocation scheme for wireless communication systems. Specifically, we formulate a simple resource allocation problem involving two transmit pairs and develop an LLM-based resource allocation approach that aims to maximize either energy efficiency or spectral efficiency. Additionally, we consider the joint use of low-complexity resource allocation techniques to compensate for the reliability shortcomings of the LLM-based scheme. After confirming the applicability and feasibility of LLM-based resource allocation, we address several key
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于Marcus映射的改进算法，通过引入秩限制，实现了稀疏矩阵到双重可导对称矩阵的转换，从而为聚类问题提供了一种新型的稀疏聚类方法，提高了聚类效率并保持了聚类的可导性。</title><link>https://arxiv.org/abs/2408.02932</link><description>&lt;p&gt;
Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02932
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于Marcus映射的改进算法，通过引入秩限制，实现了稀疏矩阵到双重可导对称矩阵的转换，从而为聚类问题提供了一种新型的稀疏聚类方法，提高了聚类效率并保持了聚类的可导性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02932v1 Announce Type: cross  Abstract: Clustering is a fundamental task in machine learning and data science, and similarity graph-based clustering is an important approach within this domain. Doubly stochastic symmetric similarity graphs provide numerous benefits for clustering problems and downstream tasks, yet learning such graphs remains a significant challenge. Marcus theorem states that a strictly positive symmetric matrix can be transformed into a doubly stochastic symmetric matrix by diagonal matrices. However, in clustering, learning sparse matrices is crucial for computational efficiency. We extend Marcus theorem by proposing the Marcus mapping, which indicates that certain sparse matrices can also be transformed into doubly stochastic symmetric matrices via diagonal matrices. Additionally, we introduce rank constraints into the clustering problem and propose the Doubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus Mapping (ANCMM). This en
&lt;/p&gt;</description></item><item><title>该文章提出的“小型代理、大型世界”框架，强调了小型代理在大型世界中持续学习的重要性，旨在设计能够有效地汲取、保持和排除相关信息的学习代理。文章还讨论了现有模拟环境在适应实用学习环境方面的局限性，并提出了设计未来模拟环境的两个基本要求，以期促进实用持续学习算法的快速原型开发。</title><link>https://arxiv.org/abs/2408.02930</link><description>&lt;p&gt;
The Need for a Big World Simulator: A Scientific Challenge for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02930
&lt;/p&gt;
&lt;p&gt;
该文章提出的“小型代理、大型世界”框架，强调了小型代理在大型世界中持续学习的重要性，旨在设计能够有效地汲取、保持和排除相关信息的学习代理。文章还讨论了现有模拟环境在适应实用学习环境方面的局限性，并提出了设计未来模拟环境的两个基本要求，以期促进实用持续学习算法的快速原型开发。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02930v1 Announce Type: cross  Abstract: The "small agent, big world" frame offers a conceptual view that motivates the need for continual learning. The idea is that a small agent operating in a much bigger world cannot store all information that the world has to offer. To perform well, the agent must be carefully designed to ingest, retain, and eject the right information. To enable the development of performant continual learning agents, a number of synthetic environments have been proposed. However, these benchmarks suffer from limitations, including unnatural distribution shifts and a lack of fidelity to the "small agent, big world" framing. This paper aims to formalize two desiderata for the design of future simulated environments. These two criteria aim to reflect the objectives and complexity of continual learning in practical settings while enabling rapid prototyping of algorithms on a smaller scale.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为HARMONIC的框架，使用大型语言模型进行表格式数据的生成和隐私保护，通过精细调整以生成更高质量的数据，同时保证数据隐私的安全性。</title><link>https://arxiv.org/abs/2408.02927</link><description>&lt;p&gt;
HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02927
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为HARMONIC的框架，使用大型语言模型进行表格式数据的生成和隐私保护，通过精细调整以生成更高质量的数据，同时保证数据隐私的安全性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02927v1 Announce Type: cross  Abstract: Data serves as the fundamental foundation for advancing deep learning, particularly tabular data presented in a structured format, which is highly conducive to modeling. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Hence, exploring how to effectively use models like LLMs to generate realistic and privacy-preserving synthetic tabular data is urgent. In this paper, we take a step forward to explore LLMs for tabular data synthesis and privacy protection, by introducing a new framework HARMONIC for tabular data generation and evaluation. In the tabular data generation of our framework, unlike previous small-scale LLM-based methods that rely on continued pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular data and enhance privacy. Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is
&lt;/p&gt;</description></item><item><title>该文章提出了一个以基础模型为架构的智能体系统设计框架，具体包括功能能力和非功能性质量等的架构设计分类，以及设计决策模型，旨在提升智能体系统的开发和运行效率。</title><link>https://arxiv.org/abs/2408.02920</link><description>&lt;p&gt;
A Taxonomy of Architecture Options for Foundation Model-based Agents: Analysis and Decision Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02920
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个以基础模型为架构的智能体系统设计框架，具体包括功能能力和非功能性质量等的架构设计分类，以及设计决策模型，旨在提升智能体系统的开发和运行效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02920v1 Announce Type: cross  Abstract: The rapid advancement of AI technology has led to widespread applications of agent systems across various domains. However, the need for detailed architecture design poses significant challenges in designing and operating these systems. This paper introduces a taxonomy focused on the architectures of foundation-model-based agents, addressing critical aspects such as functional capabilities and non-functional qualities. We also discuss the operations involved in both design-time and run-time phases, providing a comprehensive view of architectural design and operational characteristics. By unifying and detailing these classifications, our taxonomy aims to improve the design of foundation-model-based agents. Additionally, the paper establishes a decision model that guides critical design and runtime decisions, offering a structured approach to enhance the development of foundation-model-based agents. Our contributions include providing a 
&lt;/p&gt;</description></item><item><title>该文章提供了一种名为KOI的在线模仿学习加速方法，通过整合关键状态指导来精确估计任务相关奖励，从而实现更有效的在线探索。</title><link>https://arxiv.org/abs/2408.02912</link><description>&lt;p&gt;
KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02912
&lt;/p&gt;
&lt;p&gt;
该文章提供了一种名为KOI的在线模仿学习加速方法，通过整合关键状态指导来精确估计任务相关奖励，从而实现更有效的在线探索。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02912v1 Announce Type: new  Abstract: Online Imitation Learning methods struggle with the gap between extensive online exploration space and limited expert trajectories, which hinder efficient exploration due to inaccurate task-aware reward estimation. Inspired by the findings from cognitive neuroscience that task decomposition could facilitate cognitive processing for efficient learning, we hypothesize that an agent could estimate precise task-aware imitation rewards for efficient online exploration by decomposing the target task into the objectives of "what to do" and the mechanisms of "how to do". In this work, we introduce the hybrid Key-state guided Online Imitation (KOI) learning approach, which leverages the integration of semantic and motion key states as guidance for task-aware reward estimation. Initially, we utilize the visual-language models to segment the expert trajectory into semantic key states, indicating the objectives of "what to do". Within the intervals 
&lt;/p&gt;</description></item><item><title>该文章提出了一种新颖的两阶段框架，用于准确识别埃及车辆牌照上的阿拉伯文字。该框架首先通过图像处理技术可靠地定位牌照，然后使用定制设计的深度学习模型进行阿拉伯字符识别。该系统在多样化数据集上取得了99.3%的准确率，超过现有方法。其潜在应用包括智能交通管理，如交通违规检测和停车场优化。未来研究将进一步通过架构改进、扩大数据集和解决系统依赖问题来增强系统的性能。</title><link>https://arxiv.org/abs/2408.02904</link><description>&lt;p&gt;
Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02904
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新颖的两阶段框架，用于准确识别埃及车辆牌照上的阿拉伯文字。该框架首先通过图像处理技术可靠地定位牌照，然后使用定制设计的深度学习模型进行阿拉伯字符识别。该系统在多样化数据集上取得了99.3%的准确率，超过现有方法。其潜在应用包括智能交通管理，如交通违规检测和停车场优化。未来研究将进一步通过架构改进、扩大数据集和解决系统依赖问题来增强系统的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02904v1 Announce Type: new  Abstract: This paper introduces a novel two-stage framework for accurate Egyptian Vehicle License Plate Recognition (EVLPR). The first stage employs image processing techniques to reliably localize license plates, while the second stage utilizes a custom-designed deep learning model for robust Arabic character recognition. The proposed system achieves a remarkable 99.3% accuracy on a diverse dataset, surpassing existing approaches. Its potential applications extend to intelligent traffic management, including traffic violation detection and parking optimization. Future research will focus on enhancing the system's capabilities through architectural refinements, expanded datasets, and addressing system dependencies.
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于度量驱动的方法，用于指导混合精度的神经网络训练，通过选择合适的数值精度，该方法有助于在加速器上高效地训练大规模语言模型，并将该方法推广到其他模型架构中。</title><link>https://arxiv.org/abs/2408.02897</link><description>&lt;p&gt;
A Metric Driven Approach to Mixed Precision Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02897
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于度量驱动的方法，用于指导混合精度的神经网络训练，通过选择合适的数值精度，该方法有助于在加速器上高效地训练大规模语言模型，并将该方法推广到其他模型架构中。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02897v1 Announce Type: cross  Abstract: As deep learning methodologies have developed, it has been generally agreed that increasing neural network size improves model quality. However, this is at the expense of memory and compute requirements, which also need to be increased. Various efficiency techniques have been proposed to rein in hardware costs, one being the use of low precision numerics. Recent accelerators have introduced several different 8-bit data types to help accommodate DNNs in terms of numerics. In this paper, we identify a metric driven methodology to aid in the choice of numerics. We demonstrate how such a methodology can help scale training of a language representation model. The technique can be generalized to other model architectures.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为VizECGNet的模型，它使用视觉ECG图像网络来通过结合多模态训练和知识蒸馏技术，仅使用打印的ECG图形便能够准确地对多种心血管疾病进行分类。</title><link>https://arxiv.org/abs/2408.02888</link><description>&lt;p&gt;
VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02888
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为VizECGNet的模型，它使用视觉ECG图像网络来通过结合多模态训练和知识蒸馏技术，仅使用打印的ECG图形便能够准确地对多种心血管疾病进行分类。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02888v1 Announce Type: new  Abstract: An electrocardiogram (ECG) captures the heart's electrical signal to assess various heart conditions. In practice, ECG data is stored as either digitized signals or printed images. Despite the emergence of numerous deep learning models for digitized signals, many hospitals prefer image storage due to cost considerations. Recognizing the unavailability of raw ECG signals in many clinical settings, we propose VizECGNet, which uses only printed ECG graphics to determine the prognosis of multiple cardiovascular diseases. During training, cross-modal attention modules (CMAM) are used to integrate information from two modalities - image and signal, while self-modality attention modules (SMAM) capture inherent long-range dependencies in ECG data of each modality. Additionally, we utilize knowledge distillation to improve the similarity between two distinct predictions from each modality stream. This innovative multi-modal deep learning architec
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为\method{}的新方法，通过污染少量的上下文演示，攻击者可以在不知不觉中影响大型语言模型（LLMs）的内部知识，导致其生成包含特定环境响应的缺陷代码，从而在未来的运行时触发恶意行为。</title><link>https://arxiv.org/abs/2408.02882</link><description>&lt;p&gt;
Compromising Embodied Agents with Contextual Backdoor Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02882
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为\method{}的新方法，通过污染少量的上下文演示，攻击者可以在不知不觉中影响大型语言模型（LLMs）的内部知识，导致其生成包含特定环境响应的缺陷代码，从而在未来的运行时触发恶意行为。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02882v1 Announce Type: new  Abstract: Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \method{}. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ 
&lt;/p&gt;</description></item><item><title>该文章提出使用进化学习策略，通过一个名为“Hide and Seek”的算法指纹识别大型语言模型，其通过一个主控（Auditor）LLM生成鉴别性问题，辅助（Detective）LLM分析回答以识别目标模型的方法，显示出对不同LLM家族的语义情境有所了解，并提高了72%的准确度来辨别LLM的家族。</title><link>https://arxiv.org/abs/2408.02871</link><description>&lt;p&gt;
Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02871
&lt;/p&gt;
&lt;p&gt;
该文章提出使用进化学习策略，通过一个名为“Hide and Seek”的算法指纹识别大型语言模型，其通过一个主控（Auditor）LLM生成鉴别性问题，辅助（Detective）LLM分析回答以识别目标模型的方法，显示出对不同LLM家族的语义情境有所了解，并提高了72%的准确度来辨别LLM的家族。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02871v1 Announce Type: cross  Abstract: As content generated by Large Language Model (LLM) has grown exponentially, the ability to accurately identify and fingerprint such text has become increasingly crucial. In this work, we introduce a novel black-box approach for fingerprinting LLMs, achieving an impressive 72% accuracy in identifying the correct family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of LLMs. We present an evolutionary strategy that leverages the capabilities of one LLM to discover the most salient features for identifying other LLMs. Our method employs a unique "Hide and Seek" algorithm, where an Auditor LLM generates discriminative prompts, and a Detective LLM analyzes the responses to fingerprint the target models. This approach not only demonstrates the feasibility of LLM-driven model identification but also reveals insights into the semantic manifolds of different LLM families. By iteratively refining prompts through in-context learnin
&lt;/p&gt;</description></item><item><title>该文章描述了一个名为VisionUnite的全新面向眼科的视觉语言基础模型，它增强了临床知识，并在1.24亿图像文本对的大型数据集上进行了预训练。VisionUnite在诊断能力上超越了现有的生成性基础模型，如GPT-4V和Gemini Pro，并且在多病种诊断、临床解释和病人互动等临床场景中表现出色，有望成为辅助专科医师的有力工具。</title><link>https://arxiv.org/abs/2408.02865</link><description>&lt;p&gt;
VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02865
&lt;/p&gt;
&lt;p&gt;
该文章描述了一个名为VisionUnite的全新面向眼科的视觉语言基础模型，它增强了临床知识，并在1.24亿图像文本对的大型数据集上进行了预训练。VisionUnite在诊断能力上超越了现有的生成性基础模型，如GPT-4V和Gemini Pro，并且在多病种诊断、临床解释和病人互动等临床场景中表现出色，有望成为辅助专科医师的有力工具。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02865v1 Announce Type: cross  Abstract: The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly vers
&lt;/p&gt;</description></item><item><title>该文章探讨了道德偏好的稳定性问题，提出了仅在单一时间点对个人进行道德偏好调查的方法可能无法准确反映其真实价值观，因为人们的道德判断可能因时间、情绪或环境因素的变化而出现不稳定，这一发现对研究参与式伦理AI工具的调查方法提出了重要挑战。</title><link>https://arxiv.org/abs/2408.02862</link><description>&lt;p&gt;
On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02862
&lt;/p&gt;
&lt;p&gt;
该文章探讨了道德偏好的稳定性问题，提出了仅在单一时间点对个人进行道德偏好调查的方法可能无法准确反映其真实价值观，因为人们的道德判断可能因时间、情绪或环境因素的变化而出现不稳定，这一发现对研究参与式伦理AI工具的调查方法提出了重要挑战。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02862v1 Announce Type: cross  Abstract: Preference elicitation frameworks feature heavily in the research on participatory ethical AI tools and provide a viable mechanism to enquire and incorporate the moral values of various stakeholders. As part of the elicitation process, surveys about moral preferences, opinions, and judgments are typically administered only once to each participant. This methodological practice is reasonable if participants' responses are stable over time such that, all other relevant factors being held constant, their responses today will be the same as their responses to the same questions at a later time. However, we do not know how often that is the case. It is possible that participants' true moral preferences change, are subject to temporary moods or whims, or are influenced by environmental factors we don't track. If participants' moral responses are unstable in such ways, it would raise important methodological and theoretical issues for how par
&lt;/p&gt;</description></item><item><title>该文章提出了Madeleine（Madeleine），一种多模态预训练策略，用于从包括免疫组化在内的多标记染色切片中获取丰富的任务无关训练信号，从而在海平面巨像素全切片图像中学习全面的、可转移的表示形式。</title><link>https://arxiv.org/abs/2408.02859</link><description>&lt;p&gt;
Multistain Pretraining for Slide Representation Learning in Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02859
&lt;/p&gt;
&lt;p&gt;
该文章提出了Madeleine（Madeleine），一种多模态预训练策略，用于从包括免疫组化在内的多标记染色切片中获取丰富的任务无关训练信号，从而在海平面巨像素全切片图像中学习全面的、可转移的表示形式。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02859v1 Announce Type: cross  Abstract: Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&amp;amp;E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224 x 224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce Madeleine, a multimodal pretraining strategy for slide representation
&lt;/p&gt;</description></item><item><title>该文章提出使用标准机器学习工具在硬件上实现 recurrent neural network，通过在 spintronic oscillator 上构建多层网络，并通过 backpropagation through time（BPTT）进行训练，从而可以在低能源成本下处理时间序列数据。文章通过模拟证明了该方法的有效性，并在解决序列数字分类任务时取得了与软件网络相当的 $89.83\pm2.91~\%$ 准确率。文章还提供了关于如何选择振荡器的时间常数以及网络超参数的指导，以便适应不同的输入时间尺度。</title><link>https://arxiv.org/abs/2408.02835</link><description>&lt;p&gt;
Training a multilayer dynamical spintronic network with standard machine learning tools to perform time series classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02835
&lt;/p&gt;
&lt;p&gt;
该文章提出使用标准机器学习工具在硬件上实现 recurrent neural network，通过在 spintronic oscillator 上构建多层网络，并通过 backpropagation through time（BPTT）进行训练，从而可以在低能源成本下处理时间序列数据。文章通过模拟证明了该方法的有效性，并在解决序列数字分类任务时取得了与软件网络相当的 $89.83\pm2.91~\%$ 准确率。文章还提供了关于如何选择振荡器的时间常数以及网络超参数的指导，以便适应不同的输入时间尺度。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02835v1 Announce Type: cross  Abstract: The ability to process time-series at low energy cost is critical for many applications. Recurrent neural network, which can perform such tasks, are computationally expensive when implementing in software on conventional computers. Here we propose to implement a recurrent neural network in hardware using spintronic oscillators as dynamical neurons. Using numerical simulations, we build a multi-layer network and demonstrate that we can use backpropagation through time (BPTT) and standard machine learning tools to train this network. Leveraging the transient dynamics of the spintronic oscillators, we solve the sequential digits classification task with $89.83\pm2.91~\%$ accuracy, as good as the equivalent software network. We devise guidelines on how to choose the time constant of the oscillators as well as hyper-parameters of the network to adapt to different input time scales.
&lt;/p&gt;</description></item><item><title>该文章介绍了基于规则的生成式人工智能REGAI，这是一种通过使用自生成或手动预设的规则来增强大型语言模型性能的新技术。REGAI能够改善传统大型语言模型和基于检索的生成式语言模型的表现，且已经在多个领域表现出潜在的应用价值。</title><link>https://arxiv.org/abs/2408.02811</link><description>&lt;p&gt;
Development of REGAI: Rubric Enabled Generative Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02811
&lt;/p&gt;
&lt;p&gt;
该文章介绍了基于规则的生成式人工智能REGAI，这是一种通过使用自生成或手动预设的规则来增强大型语言模型性能的新技术。REGAI能够改善传统大型语言模型和基于检索的生成式语言模型的表现，且已经在多个领域表现出潜在的应用价值。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02811v1 Announce Type: new  Abstract: This paper presents and evaluates a new retrieval augmented generation (RAG) and large language model (LLM)-based artificial intelligence (AI) technique: rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics, which can be created manually or automatically by the system, to enhance the performance of LLMs for evaluation purposes. REGAI improves on the performance of both classical LLMs and RAG-based LLM techniques. This paper describes REGAI, presents data regarding its performance and discusses several possible application areas for the technology.
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了结合社会语言学理论中的“面子行为（face acts）”和“礼貌”（politeness）分析论域的框架，并利用Wikipedia talk页面的新文本资源来训练face act标签器。文章通过分析性别和权力在Wikipedia编辑之间的讨论中，发现女性编辑在语言上更加礼貌，并且在自我谦逊方面的话语更多。同时，当仅考虑有权力的编辑时，这种性别差异几乎消失。</title><link>https://arxiv.org/abs/2408.02798</link><description>&lt;p&gt;
Examining Gender and Power on Wikipedia Through Face and Politeness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02798
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了结合社会语言学理论中的“面子行为（face acts）”和“礼貌”（politeness）分析论域的框架，并利用Wikipedia talk页面的新文本资源来训练face act标签器。文章通过分析性别和权力在Wikipedia编辑之间的讨论中，发现女性编辑在语言上更加礼貌，并且在自我谦逊方面的话语更多。同时，当仅考虑有权力的编辑时，这种性别差异几乎消失。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02798v1 Announce Type: cross  Abstract: We propose a framework for analyzing discourse by combining two interdependent concepts from sociolinguistic theory: face acts and politeness. While politeness has robust existing tools and data, face acts are less resourced. We introduce a new corpus created by annotating Wikipedia talk pages with face acts and we use this to train a face act tagger. We then employ our framework to study how face and politeness interact with gender and power in discussions between Wikipedia editors. Among other findings, we observe that female Wikipedians are not only more polite, which is consistent with prior studies, but that this difference corresponds with significantly more language directed at humbling aspects of their own face. Interestingly, the distinction nearly vanishes once limiting to editors with administrative power.
&lt;/p&gt;</description></item><item><title>该文章提出将用于图像合成的生成模型转化为数据挖掘工具，通过预先训练的扩散模型来评估训练数据中视觉元素的典型性，这种方法能够高效地通过生成模型来概括数据，并对于特定标签（如地理位置、时间戳、标签或疾病）进行可视化分析，展示了在处理大量数据时的高效性。</title><link>https://arxiv.org/abs/2408.02752</link><description>&lt;p&gt;
Diffusion Models as Data Mining Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02752
&lt;/p&gt;
&lt;p&gt;
该文章提出将用于图像合成的生成模型转化为数据挖掘工具，通过预先训练的扩散模型来评估训练数据中视觉元素的典型性，这种方法能够高效地通过生成模型来概括数据，并对于特定标签（如地理位置、时间戳、标签或疾病）进行可视化分析，展示了在处理大量数据时的高效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02752v1 Announce Type: new  Abstract: This paper demonstrates how to use generative models trained for image synthesis as tools for visual data mining. Our insight is that since contemporary generative models learn an accurate representation of their training data, we can use them to summarize the data by mining for visual patterns. Concretely, we show that after finetuning conditional diffusion models to synthesize images from a specific dataset, we can use these models to define a typicality measure on that dataset. This measure assesses how typical visual elements are for different data labels, such as geographic location, time stamps, semantic labels, or even the presence of a disease. This analysis-by-synthesis approach to data mining has two key advantages. First, it scales much better than traditional correspondence-based approaches since it does not require explicitly comparing all pairs of visual elements. Second, while most previous works on visual data mining focu
&lt;/p&gt;</description></item><item><title>该文章提出了一种新颖的dataset distillation方法——Multi-domain Distribution Matching（MDM），旨在针对信号处理中的特殊域特性和跨域分析，通过独特的discrete Fourier transform和series fusion对大规模信号数据进行压缩，生成小型但性能维持的高效合成dataset，为自动调制识别（AMR）任务提供了更为实用的数据解决方案。</title><link>https://arxiv.org/abs/2408.02714</link><description>&lt;p&gt;
MDM: Advancing Multi-Domain Distribution Matching for Automatic Modulation Recognition Dataset Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02714
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种新颖的dataset distillation方法——Multi-domain Distribution Matching（MDM），旨在针对信号处理中的特殊域特性和跨域分析，通过独特的discrete Fourier transform和series fusion对大规模信号数据进行压缩，生成小型但性能维持的高效合成dataset，为自动调制识别（AMR）任务提供了更为实用的数据解决方案。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02714v1 Announce Type: cross  Abstract: Recently, deep learning technology has been successfully introduced into Automatic Modulation Recognition (AMR) tasks. However, the success of deep learning is all attributed to the training on large-scale datasets. Such a large amount of data brings huge pressure on storage, transmission and model training. In order to solve the problem of large amount of data, some researchers put forward the method of data distillation, which aims to compress large training data into smaller synthetic datasets to maintain its performance. While numerous data distillation techniques have been developed within the realm of image processing, the unique characteristics of signals set them apart. Signals exhibit distinct features across various domains, necessitating specialized approaches for their analysis and processing. To this end, a novel dataset distillation method--Multi-domain Distribution Matching (MDM) is proposed. MDM employs the Discrete Fou
&lt;/p&gt;</description></item><item><title>该文章详细介绍了用于增强现实手术导航中的组织变形模型方法，旨在确保手术过程中预制组织的模拟与实际解剖结构相匹配。这对于确保手术的可靠性和准确性至关重要。</title><link>https://arxiv.org/abs/2408.02713</link><description>&lt;p&gt;
A Review on Organ Deformation Modeling Approaches for Reliable Surgical Navigation using Augmented Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02713
&lt;/p&gt;
&lt;p&gt;
该文章详细介绍了用于增强现实手术导航中的组织变形模型方法，旨在确保手术过程中预制组织的模拟与实际解剖结构相匹配。这对于确保手术的可靠性和准确性至关重要。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02713v1 Announce Type: cross  Abstract: Augmented Reality (AR) holds the potential to revolutionize surgical procedures by allowing surgeons to visualize critical structures within the patient's body. This is achieved through superimposing preoperative organ models onto the actual anatomy. Challenges arise from dynamic deformations of organs during surgery, making preoperative models inadequate for faithfully representing intraoperative anatomy. To enable reliable navigation in augmented surgery, modeling of intraoperative deformation to obtain an accurate alignment of the preoperative organ model with the intraoperative anatomy is indispensable. Despite the existence of various methods proposed to model intraoperative organ deformation, there are still few literature reviews that systematically categorize and summarize these approaches. This review aims to fill this gap by providing a comprehensive and technical-oriented overview of modeling methods for intraoperative organ
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于Phonetic PosteriorGrams（PPG）的自动语音鉴别技术，能够成功将语音转换为不同身份的音频，即使是在经过重新合成的音频条件下，也无法被自动语音识别模型识别出原始说话人的身份。这种技术为媒体监控和新闻工作等领域提供了新的应用可能性。</title><link>https://arxiv.org/abs/2408.02712</link><description>&lt;p&gt;
Automatic Voice Identification after Speech Resynthesis using PPG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02712
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于Phonetic PosteriorGrams（PPG）的自动语音鉴别技术，能够成功将语音转换为不同身份的音频，即使是在经过重新合成的音频条件下，也无法被自动语音识别模型识别出原始说话人的身份。这种技术为媒体监控和新闻工作等领域提供了新的应用可能性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02712v1 Announce Type: cross  Abstract: Speech resynthesis is a generic task for which we want to synthesize audio with another audio as input, which finds applications for media monitors and journalists.Among different tasks addressed by speech resynthesis, voice conversion preserves the linguistic information while modifying the identity of the speaker, and speech edition preserves the identity of the speaker but some words are modified.In both cases, we need to disentangle speaker and phonetic contents in intermediate representations.Phonetic PosteriorGrams (PPG) are a frame-level probabilistic representation of phonemes, and are usually considered speaker-independent.This paper presents a PPG-based speech resynthesis system.A perceptive evaluation assesses that it produces correct audio quality.Then, we demonstrate that an automatic speaker verification model is not able to recover the source speaker after re-synthesis with PPG, even when the model is trained on syntheti
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了一种基于潜伏扩散模型（LDMs）的文本条件鼓点生成方法，通过对比学习将文本与鼓点进行联合编码，并与CLIP算法相结合，实现了文本与音乐模态的紧密关联。同时，文章还引入了多分辨率长短期记忆网络（MultiResolutionLSTM），旨在模拟音乐的多分辨率特性。此外，文章提出了一种新的预训练无条件自动编码器，能够在生成过程中加速扩散过程，并通过评估训练数据与生成的鼓点之间的距离（二进制钢琴卷帘和潜在空间）来展示生成鼓点的原创性和多样性。</title><link>https://arxiv.org/abs/2408.02711</link><description>&lt;p&gt;
Text Conditioned Symbolic Drumbeat Generation using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02711
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了一种基于潜伏扩散模型（LDMs）的文本条件鼓点生成方法，通过对比学习将文本与鼓点进行联合编码，并与CLIP算法相结合，实现了文本与音乐模态的紧密关联。同时，文章还引入了多分辨率长短期记忆网络（MultiResolutionLSTM），旨在模拟音乐的多分辨率特性。此外，文章提出了一种新的预训练无条件自动编码器，能够在生成过程中加速扩散过程，并通过评估训练数据与生成的鼓点之间的距离（二进制钢琴卷帘和潜在空间）来展示生成鼓点的原创性和多样性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02711v1 Announce Type: cross  Abstract: This study introduces a text-conditioned approach to generating drumbeats with Latent Diffusion Models (LDMs). It uses informative conditioning text extracted from training data filenames. By pretraining a text and drumbeat encoder through contrastive learning within a multimodal network, aligned following CLIP, we align the modalities of text and music closely. Additionally, we examine an alternative text encoder based on multihot text encodings. Inspired by musics multi-resolution nature, we propose a novel LSTM variant, MultiResolutionLSTM, designed to operate at various resolutions independently. In common with recent LDMs in the image space, it speeds up the generation process by running diffusion in a latent space provided by a pretrained unconditional autoencoder. We demonstrate the originality and variety of the generated drumbeats by measuring distance (both over binary pianorolls and in the latent space) versus the training d
&lt;/p&gt;</description></item><item><title>该文章创新性地采用boxology框架对结合机器学习和规则推理的医疗AI系统设计模式进行分类和对比分析，揭示了其在提高临床决策能力中的有效性，为优化健康护理AI系统的结构和性能提供了重要见解。</title><link>https://arxiv.org/abs/2408.02709</link><description>&lt;p&gt;
Enhancing Medical Learning and Reasoning Systems: A Boxology-Based Comparative Analysis of Design Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02709
&lt;/p&gt;
&lt;p&gt;
该文章创新性地采用boxology框架对结合机器学习和规则推理的医疗AI系统设计模式进行分类和对比分析，揭示了其在提高临床决策能力中的有效性，为优化健康护理AI系统的结构和性能提供了重要见解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02709v1 Announce Type: cross  Abstract: This study analyzes hybrid AI systems' design patterns and their effectiveness in clinical decision-making using the boxology framework. It categorizes and copares various architectures combining machine learning and rule-based reasoning to provide insights into their structural foundations and healthcare applications. Addressing two main questions, how to categorize these systems againts established design patterns and how to extract insights through comparative analysis, the study uses design patterns from software engineering to understand and optimize healthcare AI systems. Boxology helps identify commonalities and create reusable solutions, enhancing these systems' scalability, reliability, and performance. Five primary architectures are examined: REML, MLRB, RBML, RMLT, and PERML. Each has unique strengths and weaknesses, highlighting the need for tailored approaches in clinical tasks. REML excels in high-accuracy prediction for 
&lt;/p&gt;</description></item><item><title>该文章提出了一种将快照集合概念应用于无负例知识图链接预测任务的新方法，通过迭代创建负样本并利用先前模型，能够在无需增加训练时间的情况下，通过单一模型训练集合实现性能稳定提升。</title><link>https://arxiv.org/abs/2408.02707</link><description>&lt;p&gt;
SnapE -- Training Snapshot Ensembles of Link Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02707
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种将快照集合概念应用于无负例知识图链接预测任务的新方法，通过迭代创建负样本并利用先前模型，能够在无需增加训练时间的情况下，通过单一模型训练集合实现性能稳定提升。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02707v1 Announce Type: cross  Abstract: Snapshot ensembles have been widely used in various fields of prediction. They allow for training an ensemble of prediction models at the cost of training a single one. They are known to yield more robust predictions by creating a set of diverse base models. In this paper, we introduce an approach to transfer the idea of snapshot ensembles to link prediction models in knowledge graphs. Moreover, since link prediction in knowledge graphs is a setup without explicit negative examples, we propose a novel training loop that iteratively creates negative examples using previous snapshot models. An evaluation with four base models across four datasets shows that this approach constantly outperforms the single model approach, while keeping the training time constant.
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Bayesian Kolmogorov Arnold Networks（BKANs）的框架，它结合了Kolmogorov Arnold Networks的表达能力和贝叶斯推理，致力于提供可解释性和考虑预测不确定性的模型。在医疗诊断的两个常用数据集上进行了实验，结果表明BKANs在预测精度方面超过了传统深度学习模型，并且为临床决策提供了有价值的信心预测和决策边界信息。</title><link>https://arxiv.org/abs/2408.02706</link><description>&lt;p&gt;
Bayesian Kolmogorov Arnold Networks (Bayesian_KANs): A Probabilistic Approach to Enhance Accuracy and Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02706
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Bayesian Kolmogorov Arnold Networks（BKANs）的框架，它结合了Kolmogorov Arnold Networks的表达能力和贝叶斯推理，致力于提供可解释性和考虑预测不确定性的模型。在医疗诊断的两个常用数据集上进行了实验，结果表明BKANs在预测精度方面超过了传统深度学习模型，并且为临床决策提供了有价值的信心预测和决策边界信息。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02706v1 Announce Type: cross  Abstract: Because of its strong predictive skills, deep learning has emerged as an essential tool in many industries, including healthcare. Traditional deep learning models, on the other hand, frequently lack interpretability and omit to take prediction uncertainty into account two crucial components of clinical decision making. In order to produce explainable and uncertainty aware predictions, this study presents a novel framework called Bayesian Kolmogorov Arnold Networks (BKANs), which combines the expressive capacity of Kolmogorov Arnold Networks with Bayesian inference. We employ BKANs on two medical datasets, which are widely used benchmarks for assessing machine learning models in medical diagnostics: the Pima Indians Diabetes dataset and the Cleveland Heart Disease dataset. Our method provides useful insights into prediction confidence and decision boundaries and outperforms traditional deep learning models in terms of prediction accurac
&lt;/p&gt;</description></item><item><title>该文章提出了一种高效的谱稀疏化算法PSNE，能够实现网络嵌入的大规模扩展，通过改进的局部推动机制，仅需执行一次局部推动过程即可获得整个PPR矩阵的精确估计，从而显著降低了计算复杂度，同时提高了网络嵌入的性能。</title><link>https://arxiv.org/abs/2408.02705</link><description>&lt;p&gt;
PSNE: Efficient Spectral Sparsification Algorithms for Scaling Network Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02705
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种高效的谱稀疏化算法PSNE，能够实现网络嵌入的大规模扩展，通过改进的局部推动机制，仅需执行一次局部推动过程即可获得整个PPR矩阵的精确估计，从而显著降低了计算复杂度，同时提高了网络嵌入的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02705v1 Announce Type: cross  Abstract: Network embedding has numerous practical applications and has received extensive attention in graph learning, which aims at mapping vertices into a low-dimensional and continuous dense vector space by preserving the underlying structural properties of the graph. Many network embedding methods have been proposed, among which factorization of the Personalized PageRank (PPR for short) matrix has been empirically and theoretically well supported recently. However, several fundamental issues cannot be addressed. (1) Existing methods invoke a seminal Local Push subroutine to approximate \textit{a single} row or column of the PPR matrix. Thus, they have to execute $n$ ($n$ is the number of nodes) Local Push subroutines to obtain a provable PPR matrix, resulting in prohibitively high computational costs for large $n$. (2) The PPR matrix has limited power in capturing the structural similarity between vertices, leading to performance degradatio
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为STGCNDT的空间-时间图卷积神经网络，通过构建统一的图张量卷积网络并引入三种不同类型的变换方案，能够高效且集中地处理和提取复杂动态图的时空信息。</title><link>https://arxiv.org/abs/2408.02704</link><description>&lt;p&gt;
Spatial-temporal Graph Convolutional Networks with Diversified Transformation for Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02704
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为STGCNDT的空间-时间图卷积神经网络，通过构建统一的图张量卷积网络并引入三种不同类型的变换方案，能够高效且集中地处理和提取复杂动态图的时空信息。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02704v1 Announce Type: cross  Abstract: Dynamic graphs (DG) are often used to describe evolving interactions between nodes in real-world applications. Temporal patterns are a natural feature of DGs and are also key to representation learning. However, existing dynamic GCN models are mostly composed of static GCNs and sequence modules, which results in the separation of spatiotemporal information and cannot effectively capture complex temporal patterns in DGs. To address this problem, this study proposes a spatial-temporal graph convolutional networks with diversified transformation (STGCNDT), which includes three aspects: a) constructing a unified graph tensor convolutional network (GTCN) using tensor M-products without the need to represent spatiotemporal information separately; b) introducing three transformation schemes in GTCN to model complex temporal patterns to aggregate temporal information; and c) constructing an ensemble of diversified transformation schemes to obt
&lt;/p&gt;</description></item><item><title>该文章探讨了基于参数度量$m_{\lambda}$的风险中性多项目库存问题，其中需求向量的组成部分为模糊变量。通过提出$m_{\lambda}$-期望值的概念，文章构建了一个优化模型，并证明了其解的一般公式。文章通过该模型有效计算了最优解决方案，并通过对优化问题的具体化得到了实践中的有效公式，从而为模糊不确定性条件下的库存管理提供了更全面的分析工具。</title><link>https://arxiv.org/abs/2408.02700</link><description>&lt;p&gt;
Inventory problems and the parametric measure $m_{\lambda}$
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02700
&lt;/p&gt;
&lt;p&gt;
该文章探讨了基于参数度量$m_{\lambda}$的风险中性多项目库存问题，其中需求向量的组成部分为模糊变量。通过提出$m_{\lambda}$-期望值的概念，文章构建了一个优化模型，并证明了其解的一般公式。文章通过该模型有效计算了最优解决方案，并通过对优化问题的具体化得到了实践中的有效公式，从而为模糊不确定性条件下的库存管理提供了更全面的分析工具。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02700v1 Announce Type: cross  Abstract: The credibility theory was introduced by B. Liu as a new way to describe the fuzzy uncertainty. The credibility measure is the fundamental notion of the credibility theory. Recently, L.Yang and K. Iwamura extended the credibility measure by defining the parametric measure $m_{\lambda}$ ($\lambda$ is a real parameter in the interval $[0,1]$ and for $\lambda= 1/2$ we obtain as a particular case the notion of credibility measure). By using the $m_{\lambda}$-measure, we studied in this paper a risk neutral multi-item inventory problem. Our construction generalizes the credibilistic inventory model developed by Y. Li and Y. Liu in 2019. In our model, the components of demand vector are fuzzy variables and the maximization problem is formulated by using the notion of $m_{\lambda}$-expected value. We shall prove a general formula for the solution of optimization problem, from which we obtained effective formulas for computing the optimal solu
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为DeepNetBeam的框架，用于使用不同类型的SciML（科学机器学习）方法分析具有功能 gradients的孔隙状 beams。该框架通过连续函数模型材料的属性来应对不规则配置，并比较三种不同方法下的输出：PINN（Physics-Informed Neural Network）基于向量方法，DEM（Deep Energy Method）基于能量方法，以及基于神经操作器的数据驱动方法。三种方法均基于连续形式，以网络输出作为位移场的近似，并推导了支配beams行为的方程。实验结果表明，使用神经操作器的预测在处理功能梯度材料下的孔隙beams响应时，对任意的 porosity分布和梯度特性都表现出了高效的预测性能。</title><link>https://arxiv.org/abs/2408.02698</link><description>&lt;p&gt;
DeepNetBeam: A Framework for the Analysis of Functionally Graded Porous Beams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02698
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为DeepNetBeam的框架，用于使用不同类型的SciML（科学机器学习）方法分析具有功能 gradients的孔隙状 beams。该框架通过连续函数模型材料的属性来应对不规则配置，并比较三种不同方法下的输出：PINN（Physics-Informed Neural Network）基于向量方法，DEM（Deep Energy Method）基于能量方法，以及基于神经操作器的数据驱动方法。三种方法均基于连续形式，以网络输出作为位移场的近似，并推导了支配beams行为的方程。实验结果表明，使用神经操作器的预测在处理功能梯度材料下的孔隙beams响应时，对任意的 porosity分布和梯度特性都表现出了高效的预测性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02698v1 Announce Type: cross  Abstract: This study investigates different Scientific Machine Learning (SciML) approaches for the analysis of functionally graded (FG) porous beams and compares them under a new framework. The beam material properties are assumed to vary as an arbitrary continuous function. The methods consider the output of a neural network/operator as an approximation to the displacement fields and derive the equations governing beam behavior based on the continuum formulation. The methods are implemented in the framework and formulated by three approaches: (a) the vector approach leads to a Physics-Informed Neural Network (PINN), (b) the energy approach brings about the Deep Energy Method (DEM), and (c) the data-driven approach, which results in a class of Neural Operator methods. Finally, a neural operator has been trained to predict the response of the porous beam with functionally graded material under any porosity distribution pattern and any arbitrary t
&lt;/p&gt;</description></item><item><title>该文章从有效理论的角度出发，提出了基于RePU的神经网络在深层结构中存在值爆炸或消失的问题，并通过有效的理论解释和新的激活函数设计，解决了RePU在深层网络中训练失败的问题。</title><link>https://arxiv.org/abs/2408.02697</link><description>&lt;p&gt;
Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02697
&lt;/p&gt;
&lt;p&gt;
该文章从有效理论的角度出发，提出了基于RePU的神经网络在深层结构中存在值爆炸或消失的问题，并通过有效的理论解释和新的激活函数设计，解决了RePU在深层网络中训练失败的问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02697v1 Announce Type: cross  Abstract: The Rectified Power Unit (RePU) activation functions, unlike the Rectified Linear Unit (ReLU), have the advantage of being a differentiable function when constructing neural networks. However, it can be experimentally observed when deep layers are stacked, neural networks constructed with RePU encounter critical issues. These issues include the values exploding or vanishing and failure of training. And these happen regardless of the hyperparameter initialization. From the perspective of effective theory, we aim to identify the causes of this phenomenon and propose a new activation function that retains the advantages of RePU while overcoming its drawbacks.
&lt;/p&gt;</description></item><item><title>该文章提出了一种Distribution-Level Memory Recall（DMR）方法，通过精确地使用高斯混合模型在分布水平上拟合旧知识的特征分布，并生成在旧任务分类边界上能够避免混淆的特征。</title><link>https://arxiv.org/abs/2408.02695</link><description>&lt;p&gt;
Distribution-Level Memory Recall for Continual Learning: Preserving Knowledge and Avoiding Confusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02695
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种Distribution-Level Memory Recall（DMR）方法，通过精确地使用高斯混合模型在分布水平上拟合旧知识的特征分布，并生成在旧任务分类边界上能够避免混淆的特征。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02695v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn new data without forgetting previously learned knowledge. The key to achieving this goal is to avoid confusion at the feature level, i.e., avoiding confusion within old tasks and between new and old tasks. Previous prototype-based CL methods generate pseudo features for old knowledge replay by adding Gaussian noise to the centroids of old classes. However, the distribution in the feature space exhibits anisotropy during the incremental process, which prevents the pseudo features from faithfully reproducing the distribution of old knowledge in the feature space, leading to confusion in classification boundaries within old tasks. To address this issue, we propose the Distribution-Level Memory Recall (DMR) method, which uses a Gaussian mixture model to precisely fit the feature distribution of old knowledge at the distribution level and generate pseudo features in
&lt;/p&gt;</description></item><item><title>该文章提出一种基于Kolmogorov-Arnold Networks的自动编码器方法，用于构建资产定价的因子模型。与传统的使用多层感知机及其ReLU激活函数的方法相比，该方法在模型精度和解释性上都得到了显著提升，为资产定价中的因子暴露建模提供了更灵活的非线性函数表达，并具有更高的模型解释力。实验结果表明，采用该方法构建的策略在投资市场上表现出了更高的收益风险比，为投资管理实践提供了一个有价值的工具。</title><link>https://arxiv.org/abs/2408.02694</link><description>&lt;p&gt;
KAN based Autoencoders for Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02694
&lt;/p&gt;
&lt;p&gt;
该文章提出一种基于Kolmogorov-Arnold Networks的自动编码器方法，用于构建资产定价的因子模型。与传统的使用多层感知机及其ReLU激活函数的方法相比，该方法在模型精度和解释性上都得到了显著提升，为资产定价中的因子暴露建模提供了更灵活的非线性函数表达，并具有更高的模型解释力。实验结果表明，采用该方法构建的策略在投资市场上表现出了更高的收益风险比，为投资管理实践提供了一个有价值的工具。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02694v1 Announce Type: cross  Abstract: Inspired by recent advances in Kolmogorov-Arnold Networks (KANs), we introduce a novel approach to latent factor conditional asset pricing models. While previous machine learning applications in asset pricing have predominantly used Multilayer Perceptrons with ReLU activation functions to model latent factor exposures, our method introduces a KAN-based autoencoder which surpasses MLP models in both accuracy and interpretability. Our model offers enhanced flexibility in approximating exposures as nonlinear functions of asset characteristics, while simultaneously providing users with an intuitive framework for interpreting latent factors. Empirical backtesting demonstrates our model's superior ability to explain cross-sectional risk exposures. Moreover, long-short portfolios constructed using our model's predictions achieve higher Sharpe ratios, highlighting its practical value in investment management.
&lt;/p&gt;</description></item><item><title>该文章开发了一种名为Diff-PIC的革命性粒子-在-细胞模拟技术，通过使用扩散模型代替传统的粒子-在-细胞模拟，大大提高了核融合研究的效率，特别是在激光-等离子体相互作用的研究中。</title><link>https://arxiv.org/abs/2408.02693</link><description>&lt;p&gt;
Diff-PIC: Revolutionizing Particle-In-Cell Simulation for Advancing Nuclear Fusion with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02693
&lt;/p&gt;
&lt;p&gt;
该文章开发了一种名为Diff-PIC的革命性粒子-在-细胞模拟技术，通过使用扩散模型代替传统的粒子-在-细胞模拟，大大提高了核融合研究的效率，特别是在激光-等离子体相互作用的研究中。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02693v1 Announce Type: cross  Abstract: Sustainable energy is a crucial global challenge, and recent breakthroughs in nuclear fusion ignition underscore the potential of harnessing energy extracted from nuclear fusion in everyday life, thereby drawing significant attention to fusion ignition research, especially Laser-Plasma Interaction (LPI). Unfortunately, the complexity of LPI at ignition scale renders theory-based analysis nearly impossible -- instead, it has to rely heavily on Particle-in-Cell (PIC) simulations, which is extremely computationally intensive, making it a major bottleneck in advancing fusion ignition. In response, this work introduces Diff-PIC, a novel paradigm that leverages conditional diffusion models as a computationally efficient alternative to PIC simulations for generating high-fidelity scientific data. Specifically, we design a distillation paradigm to distill the physical patterns captured by PIC simulations into diffusion models, demonstrating bo
&lt;/p&gt;</description></item><item><title>该文章采用注意力机制，特别是卷积块注意模块（CBAM），来改进了使用CNNs（卷积神经网络）对摩洛哥鲁拉亚水系未定理流域的洪水敏感性进行预测的能力。通过在流行的CNN模型，如ResNet18、DenseNet121和Xception中分别使用了CBAM，研究者发现这种注意力机制在提高模型的准确度、精确度、召回率和F1得分方面表现显著，特别是在DenseNet121模型中，CBAM在每个卷积块中均得到整合，实现了最佳的ROC曲线下面积（AUC）。</title><link>https://arxiv.org/abs/2408.02692</link><description>&lt;p&gt;
Attention is all you need for an improved CNN-based flash flood susceptibility modeling. The case of the ungauged Rheraya watershed, Morocco
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02692
&lt;/p&gt;
&lt;p&gt;
该文章采用注意力机制，特别是卷积块注意模块（CBAM），来改进了使用CNNs（卷积神经网络）对摩洛哥鲁拉亚水系未定理流域的洪水敏感性进行预测的能力。通过在流行的CNN模型，如ResNet18、DenseNet121和Xception中分别使用了CBAM，研究者发现这种注意力机制在提高模型的准确度、精确度、召回率和F1得分方面表现显著，特别是在DenseNet121模型中，CBAM在每个卷积块中均得到整合，实现了最佳的ROC曲线下面积（AUC）。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02692v1 Announce Type: cross  Abstract: Effective flood hazard management requires evaluating and predicting flash flood susceptibility. Convolutional neural networks (CNNs) are commonly used for this task but face issues like gradient explosion and overfitting. This study explores the use of an attention mechanism, specifically the convolutional block attention module (CBAM), to enhance CNN models for flash flood susceptibility in the ungauged Rheraya watershed, a flood prone region. We used ResNet18, DenseNet121, and Xception as backbone architectures, integrating CBAM at different locations. Our dataset included 16 conditioning factors and 522 flash flood inventory points. Performance was evaluated using accuracy, precision, recall, F1-score, and the area under the curve (AUC) of the receiver operating characteristic (ROC). Results showed that CBAM significantly improved model performance, with DenseNet121 incorporating CBAM in each convolutional block achieving the best 
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为Symmetric Graph Contrastive Learning（SGCL）的模型，用于推荐系统。该方法通过理论上的保证，确保了数据增广的对比视图不会因为失去原有的连接信息而导致的性能下降，有效提升了推荐系统的准确率。</title><link>https://arxiv.org/abs/2408.02691</link><description>&lt;p&gt;
Symmetric Graph Contrastive Learning against Noisy Views for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02691
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为Symmetric Graph Contrastive Learning（SGCL）的模型，用于推荐系统。该方法通过理论上的保证，确保了数据增广的对比视图不会因为失去原有的连接信息而导致的性能下降，有效提升了推荐系统的准确率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02691v1 Announce Type: cross  Abstract: Graph Contrastive Learning (GCL) leverages data augmentation techniques to produce contrasting views, enhancing the accuracy of recommendation systems through learning the consistency between contrastive views. However, existing augmentation methods, such as directly perturbing interaction graph (e.g., node/edge dropout), may interfere with the original connections and generate poor contrasting views, resulting in sub-optimal performance. In this paper, we define the views that share only a small amount of information with the original graph due to poor data augmentation as noisy views (i.e., the last 20% of the views with a cosine similarity value less than 0.1 to the original view). We demonstrate through detailed experiments that noisy views will significantly degrade recommendation performance. Further, we propose a model-agnostic Symmetric Graph Contrastive Learning (SGCL) method with theoretical guarantees to address this issue. 
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为STPS的模型，用于具有部分传感的长期交通预报问题，旨在降低交通管理中传感器的基础设施成本。该模型创新地使用了一种基于秩的嵌入技术来捕捉交通数据中的非线性时间序列特性和不可知的不确定性。这有助于在不完全传感的环境中预测复杂时空关联的长期交通流，同时考虑了数据的噪声和不规律的交通模式，如道路封闭。

以上总结包括了用户输入内容中的“部分传感”、“长期交通预报”、“模型”、“嵌入技术”等关键词。</title><link>https://arxiv.org/abs/2408.02689</link><description>&lt;p&gt;
Spatio-Temporal Partial Sensing Forecast for Long-term Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02689
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为STPS的模型，用于具有部分传感的长期交通预报问题，旨在降低交通管理中传感器的基础设施成本。该模型创新地使用了一种基于秩的嵌入技术来捕捉交通数据中的非线性时间序列特性和不可知的不确定性。这有助于在不完全传感的环境中预测复杂时空关联的长期交通流，同时考虑了数据的噪声和不规律的交通模式，如道路封闭。

以上总结包括了用户输入内容中的“部分传感”、“长期交通预报”、“模型”、“嵌入技术”等关键词。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02689v1 Announce Type: cross  Abstract: Traffic forecasting uses recent measurements by sensors installed at chosen locations to forecast the future road traffic. Existing work either assumes all locations are equipped with sensors or focuses on short-term forecast. This paper studies partial sensing traffic forecast of long-term traffic, assuming sensors only at some locations. The study is important in lowering the infrastructure investment cost in traffic management since deploying sensors at all locations could incur prohibitively high cost. However, the problem is challenging due to the unknown distribution at unsensed locations, the intricate spatio-temporal correlation in long-term forecasting, as well as noise in data and irregularities in traffic patterns (e.g., road closure). We propose a Spatio-Temporal Partial Sensing (STPS) forecast model for long-term traffic prediction, with several novel contributions, including a rank-based embedding technique to capture irr
&lt;/p&gt;</description></item><item><title>该文章系统地回顾了多模态深度学习在生物医学应用中的中间融合方法。</title><link>https://arxiv.org/abs/2408.02686</link><description>&lt;p&gt;
A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02686
&lt;/p&gt;
&lt;p&gt;
该文章系统地回顾了多模态深度学习在生物医学应用中的中间融合方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02686v1 Announce Type: cross  Abstract: Deep learning has revolutionized biomedical research by providing sophisticated methods to handle complex, high-dimensional data. Multimodal deep learning (MDL) further enhances this capability by integrating diverse data types such as imaging, textual data, and genetic information, leading to more robust and accurate predictive models. In MDL, differently from early and late fusion methods, intermediate fusion stands out for its ability to effectively combine modality-specific features during the learning process. This systematic review aims to comprehensively analyze and formalize current intermediate fusion methods in biomedical applications. We investigate the techniques employed, the challenges faced, and potential future directions for advancing intermediate fusion methods. Additionally, we introduce a structured notation to enhance the understanding and application of these methods beyond the biomedical domain. Our findings are 
&lt;/p&gt;</description></item><item><title>该文章总结了人工神经网络在光电应用中的创新和贡献，主要包括了神经网络类型与光电硬件实现的相关性，以及如何优化模型设计以实现高效率的任务执行。文章还讨论了人工神经网络在光电领域的最新发展和多个相关应用，这将为研究者提供理论和技术上的见解。</title><link>https://arxiv.org/abs/2408.02685</link><description>&lt;p&gt;
Artificial Neural Networks for Photonic Applications: From Algorithms to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02685
&lt;/p&gt;
&lt;p&gt;
该文章总结了人工神经网络在光电应用中的创新和贡献，主要包括了神经网络类型与光电硬件实现的相关性，以及如何优化模型设计以实现高效率的任务执行。文章还讨论了人工神经网络在光电领域的最新发展和多个相关应用，这将为研究者提供理论和技术上的见解。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02685v1 Announce Type: cross  Abstract: This tutorial-review on applications of artificial neural networks in photonics targets a broad audience, ranging from optical research and engineering communities to computer science and applied mathematics. We focus here on the research areas at the interface between these disciplines, attempting to find the right balance between technical details specific to each domain and overall clarity. First, we briefly recall key properties and peculiarities of some core neural network types, which we believe are the most relevant to photonics, also linking the layer's theoretical design to some photonics hardware realizations. After that, we elucidate the question of how to fine-tune the selected model's design to perform the required task with optimized accuracy. Then, in the review part, we discuss recent developments and progress for several selected applications of neural networks in photonics, including multiple aspects relevant to optic
&lt;/p&gt;</description></item><item><title>该文章介绍了如何使用一个专门设计的录音装置来记录佩戴者的视觉和听觉体验，并用于构建新型的基础模型。通过这种方法，可以创建出更接近真实人物情感和生理反应的基础模型，从而提高AI技术的逼真度和自然交互能力。</title><link>https://arxiv.org/abs/2408.02680</link><description>&lt;p&gt;
Recording First-person Experiences to Build a New Type of Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02680
&lt;/p&gt;
&lt;p&gt;
该文章介绍了如何使用一个专门设计的录音装置来记录佩戴者的视觉和听觉体验，并用于构建新型的基础模型。通过这种方法，可以创建出更接近真实人物情感和生理反应的基础模型，从而提高AI技术的逼真度和自然交互能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02680v1 Announce Type: new  Abstract: Foundation models have had a big impact in recent years and billions of dollars are being invested in them in the current AI boom. The more popular ones, such as Chat-GPT, are trained on large amounts of Internet data. However, it is becoming apparent that this data is likely to be exhausted soon, and technology companies are looking for new sources of data to train the next generation of foundation models.   Reinforcement learning, RAG, prompt engineering and cognitive modelling are often used to fine-tune and augment the behaviour of foundation models. These techniques have been used to replicate people, such as Caryn Marjorie. These chatbots are not based on people's actual emotional and physiological responses to their environment, so they are, at best, a surface-level approximation to the characters they are imitating.   To address these issues, we have developed a recording rig that captures what the wearer is seeing and hearing as
&lt;/p&gt;</description></item><item><title>该文章提出了一个基于患者中心的跨学科框架，该框架综合了临床数据、患者报告数据和多组学数据，利用多代理人工智能和机器学习技术，尤其关注大型语言模型，以预测和优化临床结果。这种方法旨在创建一个不断学习的医疗系统，以促进数字健康创新的实际应用，更好地服务于患者。</title><link>https://arxiv.org/abs/2408.02677</link><description>&lt;p&gt;
Patient-centered data science: an integrative framework for evaluating and predicting clinical outcomes in the digital health era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02677
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个基于患者中心的跨学科框架，该框架综合了临床数据、患者报告数据和多组学数据，利用多代理人工智能和机器学习技术，尤其关注大型语言模型，以预测和优化临床结果。这种方法旨在创建一个不断学习的医疗系统，以促进数字健康创新的实际应用，更好地服务于患者。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02677v1 Announce Type: cross  Abstract: This study proposes a novel, integrative framework for patient-centered data science in the digital health era. We developed a multidimensional model that combines traditional clinical data with patient-reported outcomes, social determinants of health, and multi-omic data to create comprehensive digital patient representations. Our framework employs a multi-agent artificial intelligence approach, utilizing various machine learning techniques including large language models, to analyze complex, longitudinal datasets. The model aims to optimize multiple patient outcomes simultaneously while addressing biases and ensuring generalizability. We demonstrate how this framework can be implemented to create a learning healthcare system that continuously refines strategies for optimal patient care. This approach has the potential to significantly improve the translation of digital health innovations into real-world clinical benefits, addressing 
&lt;/p&gt;</description></item><item><title>该文章发现并分析了英国生物银行数据库中基于眼底图像的疾病分类模型存在的巨大偏差问题，即使在整体模型表现出色的情况下，不同评估中心的个体仍然面临显著的不公平表现。这项研究不仅揭示了数据标准化过程中潜在的歧视问题，而且对比了多种现有偏差缓解方法的适用性，发现针对不同的偏差问题，不同的缓解方法效果差异甚大，这表明需要开发针对特定问题定制化的缓解偏差的方法。研究最终表明，当前的缓解手段在提高模型的公平性方面效果有限，进一步强调了对公平性算法的高要求和继续研究的重要性。</title><link>https://arxiv.org/abs/2408.02676</link><description>&lt;p&gt;
On Biases in a UK Biobank-based Retinal Image Classification Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02676
&lt;/p&gt;
&lt;p&gt;
该文章发现并分析了英国生物银行数据库中基于眼底图像的疾病分类模型存在的巨大偏差问题，即使在整体模型表现出色的情况下，不同评估中心的个体仍然面临显著的不公平表现。这项研究不仅揭示了数据标准化过程中潜在的歧视问题，而且对比了多种现有偏差缓解方法的适用性，发现针对不同的偏差问题，不同的缓解方法效果差异甚大，这表明需要开发针对特定问题定制化的缓解偏差的方法。研究最终表明，当前的缓解手段在提高模型的公平性方面效果有限，进一步强调了对公平性算法的高要求和继续研究的重要性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02676v1 Announce Type: cross  Abstract: Recent work has uncovered alarming disparities in the performance of machine learning models in healthcare. In this study, we explore whether such disparities are present in the UK Biobank fundus retinal images by training and evaluating a disease classification model on these images. We assess possible disparities across various population groups and find substantial differences despite strong overall performance of the model. In particular, we discover unfair performance for certain assessment centres, which is surprising given the rigorous data standardisation protocol. We compare how these differences emerge and apply a range of existing bias mitigation methods to each one. A key insight is that each disparity has unique properties and responds differently to the mitigation methods. We also find that these methods are largely unable to enhance fairness, highlighting the need for better bias mitigation methods tailored to the specif
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为Counterfactual Shapley Values的全新方法，旨在增强强化学习（RL）的透明度，通过结合反事实分析与Shapley值来量化和比较不同状态维度对不同动作选择的影响。通过引入新的特征值函数，如“反事实差异特征值”和“平均反事实差异特征值”，该研究解决了分析这些影响时的难题，并计算了Shapley值以评估最优动作和非最优动作之间的贡献差异。通过在GridWorld、FrozenLake和Taxi等多个RL领域内的实验，证明了CSV方法的有效性。实验结果表明，该方法不仅提高了复杂RL系统中的透明度，还量化了不同决策间的差异。</title><link>https://arxiv.org/abs/2408.02529</link><description>&lt;p&gt;
Counterfactual Shapley Values for Explaining Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02529
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为Counterfactual Shapley Values的全新方法，旨在增强强化学习（RL）的透明度，通过结合反事实分析与Shapley值来量化和比较不同状态维度对不同动作选择的影响。通过引入新的特征值函数，如“反事实差异特征值”和“平均反事实差异特征值”，该研究解决了分析这些影响时的难题，并计算了Shapley值以评估最优动作和非最优动作之间的贡献差异。通过在GridWorld、FrozenLake和Taxi等多个RL领域内的实验，证明了CSV方法的有效性。实验结果表明，该方法不仅提高了复杂RL系统中的透明度，还量化了不同决策间的差异。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02529v1 Announce Type: new  Abstract: This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the ``Counterfactual Difference Characteristic Value" and the ``Average Counterfactual Difference Characteristic Value." These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.
&lt;/p&gt;</description></item><item><title>该文章通过使用包含实际shellcodes的数据集，评估了神经机器翻译模型在从自然语言描述生成恶意软件代码方面的性能。研究发现，模型的表现因引入的上下文信息不同而有所差异，表明了适当上下文对提升模型性能的重要性。然而，过度增加上下文信息并未带来持续的改进，这表明在模型训练中存在一个最佳的上下文信息量。此外，模型还能识别并排除不必要的描述，从而更精确地生成代码。</title><link>https://arxiv.org/abs/2408.02402</link><description>&lt;p&gt;
Enhancing AI-based Generation of Software Exploits with Contextual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02402
&lt;/p&gt;
&lt;p&gt;
该文章通过使用包含实际shellcodes的数据集，评估了神经机器翻译模型在从自然语言描述生成恶意软件代码方面的性能。研究发现，模型的表现因引入的上下文信息不同而有所差异，表明了适当上下文对提升模型性能的重要性。然而，过度增加上下文信息并未带来持续的改进，这表明在模型训练中存在一个最佳的上下文信息量。此外，模型还能识别并排除不必要的描述，从而更精确地生成代码。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02402v1 Announce Type: cross  Abstract: This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability t
&lt;/p&gt;</description></item><item><title>该文章聚焦于数据评估和选择在指令微调大型语言模型中的应用，全面梳理了现有文献，为这类策略提供了一种分类清晰、层级精细的体系结构，以提高指令微调的效率和效果。</title><link>https://arxiv.org/abs/2408.02085</link><description>&lt;p&gt;
Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.02085
&lt;/p&gt;
&lt;p&gt;
该文章聚焦于数据评估和选择在指令微调大型语言模型中的应用，全面梳理了现有文献，为这类策略提供了一种分类清晰、层级精细的体系结构，以提高指令微调的效率和效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.02085v1 Announce Type: new  Abstract: Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each
&lt;/p&gt;</description></item><item><title>该文章介绍了名为DiReCT的临床笔记诊断推理数据集，它通过521份临床笔记的细致标注，评估了大型语言模型在诊断推理方面的能力以及对人类医生的解释性。数据集包含了一个诊断知识图谱，有助于LLMs进行推理，并突出了这些模型在诊断推理能力上的不足。</title><link>https://arxiv.org/abs/2408.01933</link><description>&lt;p&gt;
DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.01933
&lt;/p&gt;
&lt;p&gt;
该文章介绍了名为DiReCT的临床笔记诊断推理数据集，它通过521份临床笔记的细致标注，评估了大型语言模型在诊断推理方面的能力以及对人类医生的解释性。数据集包含了一个诊断知识图谱，有助于LLMs进行推理，并突出了这些模型在诊断推理能力上的不足。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.01933v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为FBSDiff的新方法，它可以将频率带替换到扩散特征中，用于高度可控的文本驱动图像翻译。通过这种方法，可以无需模型训练、微调和在线优化，直接将大型预训练文本到图像扩散模型转换为图像到图像模型，实现高质量和灵活的文本驱动的图像翻译。</title><link>https://arxiv.org/abs/2408.00998</link><description>&lt;p&gt;
FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00998
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为FBSDiff的新方法，它可以将频率带替换到扩散特征中，用于高度可控的文本驱动图像翻译。通过这种方法，可以无需模型训练、微调和在线优化，直接将大型预训练文本到图像扩散模型转换为图像到图像模型，实现高质量和灵活的文本驱动的图像翻译。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00998v2 Announce Type: replace  Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I gener
&lt;/p&gt;</description></item><item><title>该文章提出了一种将环境、社会和治理（ESG）因素与人工智能（AI）投资相结合的全面框架，旨在通过与28家公司合作开发的三个关键组件，帮助投资者评估AI应用的环境和社会影响，从而实现更可持续的AI发展。</title><link>https://arxiv.org/abs/2408.00965</link><description>&lt;p&gt;
Integrating ESG and AI: A Comprehensive Responsible AI Assessment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00965
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种将环境、社会和治理（ESG）因素与人工智能（AI）投资相结合的全面框架，旨在通过与28家公司合作开发的三个关键组件，帮助投资者评估AI应用的环境和社会影响，从而实现更可持续的AI发展。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00965v2 Announce Type: replace  Abstract: Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors 
&lt;/p&gt;</description></item><item><title>该文章介绍了Segment Anything Model 2 (SAM 2)在2D和3D医学图像中的广泛应用，这是对现有技术的显著扩展。通过对18种医学图像数据集的评估，证明了SAM 2在多帧3D分割和单帧2D分割中的有效性，展示了其在CT、MRI、PET等3D和X光、超声等2D医学图像分割领域的创新贡献。</title><link>https://arxiv.org/abs/2408.00756</link><description>&lt;p&gt;
Segment anything model 2: an application to 2D and 3D medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00756
&lt;/p&gt;
&lt;p&gt;
该文章介绍了Segment Anything Model 2 (SAM 2)在2D和3D医学图像中的广泛应用，这是对现有技术的显著扩展。通过对18种医学图像数据集的评估，证明了SAM 2在多帧3D分割和单帧2D分割中的有效性，展示了其在CT、MRI、PET等3D和X光、超声等2D医学图像分割领域的创新贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00756v2 Announce Type: replace  Abstract: Segment Anything Model (SAM) has gained significant attention because of its ability to segment varous objects in images given a prompt. The recently developed SAM 2 has extended this ability to video inputs. This opens an opportunity to apply SAM to 3D images, one of the fundamental tasks in the medical imaging field. In this paper, we extensively evaluate SAM 2's ability to segment both 2D and 3D medical images by first collecting 18 medical imaging datasets, including common 3D modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) as well as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of SAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are provided to one or multiple slice(s) selected from the volume, and (2) single-frame 2D segmentation, where prompts are provided to each slice. The former is only applicable to 3D modaliti
&lt;/p&gt;</description></item><item><title>该文章提出了SentenceVAE方法，通过将大型语言模型（LLMs）的输入/输出层集成以提升句子级别的LLMs（SLLMs）的推理效率，通过句子级别的预测实现了速度提升和准确性提高，同时支持长距离的上下文信息处理。</title><link>https://arxiv.org/abs/2408.00655</link><description>&lt;p&gt;
SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2408.00655
&lt;/p&gt;
&lt;p&gt;
该文章提出了SentenceVAE方法，通过将大型语言模型（LLMs）的输入/输出层集成以提升句子级别的LLMs（SLLMs）的推理效率，通过句子级别的预测实现了速度提升和准确性提高，同时支持长距离的上下文信息处理。
&lt;/p&gt;
&lt;p&gt;
arXiv:2408.00655v3 Announce Type: replace  Abstract: Current large language models (LLMs) primarily utilize next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present Sentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a Sentence Encoder and a Sentence Decoder. The Sentence Encoder can effectively condense the information within a sentence into a singular token, while the Sentence Decoder can reconstruct this compressed token back into sentence. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference method. In addition, the SentenceVAE module of SLLMs can maintain the integrity of the original semantic content by segmenting the context into sentences, thereby improving accuracy 
&lt;/p&gt;</description></item><item><title>该文章提出MoMa架构，一种创新的模式感知专家混合模型，用于处理任意序列的混合模式语言模型早期融合预训练。MoMa通过将专家模块分为专属处理特定模式组别的模式感知专家，显著提高了预训练的效率和性能。</title><link>https://arxiv.org/abs/2407.21770</link><description>&lt;p&gt;
MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21770
&lt;/p&gt;
&lt;p&gt;
该文章提出MoMa架构，一种创新的模式感知专家混合模型，用于处理任意序列的混合模式语言模型早期融合预训练。MoMa通过将专家模块分为专属处理特定模式组别的模式感知专家，显著提高了预训练的效率和性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21770v2 Announce Type: replace  Abstract: We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7x overall, with 2.6x for text and 5.2x for image processing compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3x overall FLOPs savings (3x for text
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为eSPARQL的四值逻辑查询语言，用于处理RDF-star知识图中的 agnostic（不可知论）和atheistic（无神论）信念。eSPARQL通过扩展SPARQL-star，允许查询以不同的信念执行，以处理可能存在冲突的信息来源。这使得它能够表达四个用例查询，展现了其在处理多源和冲突信念方面的创新和贡献。</title><link>https://arxiv.org/abs/2407.21483</link><description>&lt;p&gt;
eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21483
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为eSPARQL的四值逻辑查询语言，用于处理RDF-star知识图中的 agnostic（不可知论）和atheistic（无神论）信念。eSPARQL通过扩展SPARQL-star，允许查询以不同的信念执行，以处理可能存在冲突的信息来源。这使得它能够表达四个用例查询，展现了其在处理多源和冲突信念方面的创新和贡献。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21483v3 Announce Type: replace  Abstract: Over the past few years, we have seen the emergence of large knowledge graphs combining information from multiple sources. Sometimes, this information is provided in the form of assertions about other assertions, defining contexts where assertions are valid. A recent extension to RDF which admits statements over statements, called RDF-star, is in revision to become a W3C standard. However, there is no proposal for a semantics of these RDF-star statements nor a built-in facility to operate over them. In this paper, we propose a query language for epistemic RDF-star metadata based on a four-valued logic, called eSPARQL. Our proposed query language extends SPARQL-star, the query language for RDF-star, with a new type of FROM clause to facilitate operating with multiple and sometimes conflicting beliefs. We show that the proposed query language can express four use case queries, including the following features: (i) querying the belief o
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为PolyRAG的多层次检索方法，通过在RAG框架中构建知识金字塔，包括奥顿学、知识图和文本层，以提高检索的精度和召回率。这种方法通过跨层知识更新的动态处理和紧凑的知识图过滤方法，为现有方法提供了重要的改进，尤其是针对领域特定的知识检索任务，证明了该方法的有效性。</title><link>https://arxiv.org/abs/2407.21276</link><description>&lt;p&gt;
Multi-Level Querying using A Knowledge Pyramid
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.21276
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为PolyRAG的多层次检索方法，通过在RAG框架中构建知识金字塔，包括奥顿学、知识图和文本层，以提高检索的精度和召回率。这种方法通过跨层知识更新的动态处理和紧凑的知识图过滤方法，为现有方法提供了重要的改进，尤其是针对领域特定的知识检索任务，证明了该方法的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.21276v2 Announce Type: replace  Abstract: This paper addresses the need for improved precision in existing Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effec
&lt;/p&gt;</description></item><item><title>该文章提出了一种带有多个轻量级解码器的多解码器场景表示网络（MDSRN），能够为输入坐标生成多个可能预测，并计算它们的均值作为多解码器集合的预测，以及它们的方差作为可信度分数，从而支持推理时预测质量评估。</title><link>https://arxiv.org/abs/2407.19082</link><description>&lt;p&gt;
Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.19082
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种带有多个轻量级解码器的多解码器场景表示网络（MDSRN），能够为输入坐标生成多个可能预测，并计算它们的均值作为多解码器集合的预测，以及它们的方差作为可信度分数，从而支持推理时预测质量评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.19082v2 Announce Type: replace-cross  Abstract: Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN) ensemble architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be
&lt;/p&gt;</description></item><item><title>该文章提出了动态语言组混合专家模型（DLG-MoE），通过优化多语言和代码切换场景下MoE框架的效率与灵活性，并在语言路由和独立的内部分组路由方面取得了创新，实现了出色的识别性能。</title><link>https://arxiv.org/abs/2407.18581</link><description>&lt;p&gt;
Dynamic Language Group-Based MoE: Enhancing Efficiency and Flexibility for Code-Switching Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.18581
&lt;/p&gt;
&lt;p&gt;
该文章提出了动态语言组混合专家模型（DLG-MoE），通过优化多语言和代码切换场景下MoE框架的效率与灵活性，并在语言路由和独立的内部分组路由方面取得了创新，实现了出色的识别性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.18581v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) approach is ideally suited for tackling multilingual and code-switching (CS) challenges due to its multi-expert architecture. This work introduces the DLG-MoE, which is optimized for bilingual and CS scenarios. Our novel Dynamic Language Group-based MoE layer features a language router with shared weights for explicit language modeling, while independent unsupervised routers within the language group handle attributes beyond language. This structure not only enhances expert extension capabilities but also supports dynamic top-k training, allowing for flexible inference across various top-k values and improving overall performance. The model requires no pre-training and supports streaming recognition, achieving state-of-the-art (SOTA) results with unmatched flexibility compared to other methods. The Code will be released.
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CityX的全新多模式可控的程序化内容生成方法，可使用多种布局指令生成现实感十足、规模不受限的3D虚拟城市，并能实现对城市布局的精细控制。</title><link>https://arxiv.org/abs/2407.17572</link><description>&lt;p&gt;
CityX: Controllable Procedural Content Generation for Unbounded 3D Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17572
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CityX的全新多模式可控的程序化内容生成方法，可使用多种布局指令生成现实感十足、规模不受限的3D虚拟城市，并能实现对城市布局的精细控制。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17572v3 Announce Type: replace  Abstract: Generating a realistic, large-scale 3D virtual city remains a complex challenge due to the involvement of numerous 3D assets, various city styles, and strict layout constraints. Existing approaches provide promising attempts at procedural content generation to create large-scale scenes using Blender agents. However, they face crucial issues such as difficulties in scaling up generation capability and achieving fine-grained control at the semantic layout level. To address these problems, we propose a novel multi-modal controllable procedural content generation method, named CityX, which enhances realistic, unbounded 3D city generation guided by multiple layout conditions, including OSM, semantic maps, and satellite images. Specifically, the proposed method contains a general protocol for integrating various PCG plugins and a multi-agent framework for transforming instructions into executable Blender actions. Through this effective fra
&lt;/p&gt;</description></item><item><title>该文章提出了一个结合了机器学习和自动推理的循环科学发现过程，用于生成和选择解释。文章还定义了一个从社会学和认知科学中获得灵感的解释选择问题的分类，包括了现有的概念并扩展了新的属性。</title><link>https://arxiv.org/abs/2407.17454</link><description>&lt;p&gt;
Automated Explanation Selection for Scientific Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17454
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个结合了机器学习和自动推理的循环科学发现过程，用于生成和选择解释。文章还定义了一个从社会学和认知科学中获得灵感的解释选择问题的分类，包括了现有的概念并扩展了新的属性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17454v2 Announce Type: replace  Abstract: Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explanability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.
&lt;/p&gt;</description></item><item><title>该文章开发了AIR-Bench 2024，一个与最新政府法规和企业政策相一致的AI安全基准，将风险分为四个层次，并详细列出了314个风险类别，以评估和比较AI模型的安全性。</title><link>https://arxiv.org/abs/2407.17436</link><description>&lt;p&gt;
AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.17436
&lt;/p&gt;
&lt;p&gt;
该文章开发了AIR-Bench 2024，一个与最新政府法规和企业政策相一致的AI安全基准，将风险分为四个层次，并详细列出了314个风险类别，以评估和比较AI模型的安全性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.17436v2 Announce Type: replace-cross  Abstract: Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 di
&lt;/p&gt;</description></item><item><title>该文章通过分析Chat GPT收集的有关“Spring”一词的四种不同含义的1000句文本，展示了语义细胞的微小变异如何导致词义的多义性，这是该词在不同语境中获得不同意义的进化结果。</title><link>https://arxiv.org/abs/2407.16110</link><description>&lt;p&gt;
Analyzing the Polysemy Evolution using Semantic Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.16110
&lt;/p&gt;
&lt;p&gt;
该文章通过分析Chat GPT收集的有关“Spring”一词的四种不同含义的1000句文本，展示了语义细胞的微小变异如何导致词义的多义性，这是该词在不同语境中获得不同意义的进化结果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.16110v2 Announce Type: replace-cross  Abstract: The senses of words evolve. The sense of the same word may change from today to tomorrow, and multiple senses of the same word may be the result of the evolution of each other, that is, they may be parents and children. If we view Juba as an evolving ecosystem, the paradigm of learning the correct answer, which does not move with the sense of a word, is no longer valid. This paper is a case study that shows that word polysemy is an evolutionary consequence of the modification of Semantic Cells, which has al-ready been presented by the author, by introducing a small amount of diversity in its initial state as an example of analyzing the current set of short sentences. In particular, the analysis of a sentence sequence of 1000 sentences in some order for each of the four senses of the word Spring, collected using Chat GPT, shows that the word acquires the most polysemy monotonically in the analysis when the senses are arranged in
&lt;/p&gt;</description></item><item><title>该文章提出了一种利用数字孪生技术的多信号管理方法，通过整合异构网络技术以提高网络性能和效率，并在实验环境中展示了其在校园网络环境的实际应用潜力。</title><link>https://arxiv.org/abs/2407.15520</link><description>&lt;p&gt;
Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.15520
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种利用数字孪生技术的多信号管理方法，通过整合异构网络技术以提高网络性能和效率，并在实验环境中展示了其在校园网络环境的实际应用潜力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.15520v2 Announce Type: replace-cross  Abstract: Digital Twins (DTs) are set to become a key enabling technology in future wireless networks, with their use in network management increasing significantly. We developed a DT framework that leverages the heterogeneity of network access technologies as a resource for enhanced network performance and management, enabling smart data handling in the physical network. Tested in a Campus Area Network environment, our framework integrates diverse data sources to provide real-time, holistic insights into network performance and environmental sensing. We also envision that traditional analytics will evolve to rely on emerging AI models, such as Generative AI (GenAI), while leveraging current analytics capabilities. This capacity can simplify analytics processes through advanced ML models, enabling descriptive, diagnostic, predictive, and prescriptive analytics in a unified fashion. Finally, we present specific research opportunities conc
&lt;/p&gt;</description></item><item><title>该文章提出了一个名为CCVA-FL的系统，用于在医疗影像领域中通过联邦学习解决来自不同客户端的图像数据之间的差异化问题，并介绍了一种旨在最小化跨客户端差异的方法，涉及使用Scalable Diffusion Models with Transformers（DiT）生成反映目标客户端图像数据的合成图像，这些合成图像被用于共享以帮助其他客户端的图像数据进入目标客户端的图像空间，从而在分布式医疗影像数据中实现更好的模型训练效果。</title><link>https://arxiv.org/abs/2407.11652</link><description>&lt;p&gt;
CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.11652
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个名为CCVA-FL的系统，用于在医疗影像领域中通过联邦学习解决来自不同客户端的图像数据之间的差异化问题，并介绍了一种旨在最小化跨客户端差异的方法，涉及使用Scalable Diffusion Models with Transformers（DiT）生成反映目标客户端图像数据的合成图像，这些合成图像被用于共享以帮助其他客户端的图像数据进入目标客户端的图像空间，从而在分布式医疗影像数据中实现更好的模型训练效果。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.11652v4 Announce Type: replace  Abstract: Federated Learning (FL) offers a privacy-preserving approach to train models on decentralized data. Its potential in healthcare is significant, but challenges arise due to cross-client variations in medical image data, exacerbated by limited annotations. This paper introduces Cross-Client Variations Adaptive Federated Learning (CCVA-FL) to address these issues. CCVA-FL aims to minimize cross-client variations by transforming images into a common feature space. It involves expert annotation of a subset of images from each client, followed by the selection of a client with the least data complexity as the target. Synthetic medical images are then generated using Scalable Diffusion Models with Transformers (DiT) based on the target client's annotated images. These synthetic images, capturing diversity and representing the original data, are shared with other clients. Each client then translates its local images into the target image spa
&lt;/p&gt;</description></item><item><title>该文章创新性地提出了ToG2.0模型，该模型提高了大型语言模型在复杂推理和多样化查询中的性能，通过知识图谱引导的技术来增强检索增强生成（RAG）框架的逻辑一致性和信息精确性，同时确保了知识一致性和准确性。</title><link>https://arxiv.org/abs/2407.10805</link><description>&lt;p&gt;
Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.10805
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出了ToG2.0模型，该模型提高了大型语言模型在复杂推理和多样化查询中的性能，通过知识图谱引导的技术来增强检索增强生成（RAG）框架的逻辑一致性和信息精确性，同时确保了知识一致性和准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.10805v2 Announce Type: replace-cross  Abstract: Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG${2.0}$ not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential o
&lt;/p&gt;</description></item><item><title>该文章提出Autoverse，一种可进化、针对性的语言，用于学习具有鲁棒身体特征的智能体，并通过其作为开放式学习算法可扩展培训平台的实际案例。Autoverse使用类似于细胞自动机的规则描述游戏机制，支持各种游戏环境的描述，如迷宫、地下室等，这些环境对于强化学习算法的测试非常有用。每条规则可以被简化的卷积操作所表示，这允许在GPU上并行处理环境，大幅度加速强化学习的训练过程。通过使用Autoverse，该文提出了一种通过模仿学习和搜索来迅速启动开放式学习的策略。在这种策略中，首先通过进化选择Autoverse环境的规则和初始地图拓扑来最大化贪婪搜索发现新最佳解决方案的迭代次数，从而创建了一个难度逐渐增加的课程。</title><link>https://arxiv.org/abs/2407.04221</link><description>&lt;p&gt;
Autoverse: An Evolvable Game Language for Learning Robust Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.04221
&lt;/p&gt;
&lt;p&gt;
该文章提出Autoverse，一种可进化、针对性的语言，用于学习具有鲁棒身体特征的智能体，并通过其作为开放式学习算法可扩展培训平台的实际案例。Autoverse使用类似于细胞自动机的规则描述游戏机制，支持各种游戏环境的描述，如迷宫、地下室等，这些环境对于强化学习算法的测试非常有用。每条规则可以被简化的卷积操作所表示，这允许在GPU上并行处理环境，大幅度加速强化学习的训练过程。通过使用Autoverse，该文提出了一种通过模仿学习和搜索来迅速启动开放式学习的策略。在这种策略中，首先通过进化选择Autoverse环境的规则和初始地图拓扑来最大化贪婪搜索发现新最佳解决方案的迭代次数，从而创建了一个难度逐渐增加的课程。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.04221v2 Announce Type: replace  Abstract: We introduce Autoverse, an evolvable, domain-specific language for single-player 2D grid-based games, and demonstrate its use as a scalable training ground for Open-Ended Learning (OEL) algorithms. Autoverse uses cellular-automaton-like rewrite rules to describe game mechanics, allowing it to express various game environments (e.g. mazes, dungeons, sokoban puzzles) that are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite rule can be expressed as a series of simple convolutions, allowing for environments to be parallelized on the GPU, thereby drastically accelerating RL training. Using Autoverse, we propose jump-starting open-ended learning by imitation learning from search. In such an approach, we first evolve Autoverse environments (their rules and initial map topology) to maximize the number of iterations required by greedy tree search to discover a new best solution, producing a curriculum of increasingly com
&lt;/p&gt;</description></item><item><title>该文章提出了VCHAR框架，一种基于variances驱动的复杂人类活动识别方法，该方法通过生成表示来有效识别和解释视频中的复杂活动，从而减少了传统的标签化负担。</title><link>https://arxiv.org/abs/2407.03291</link><description>&lt;p&gt;
VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.03291
&lt;/p&gt;
&lt;p&gt;
该文章提出了VCHAR框架，一种基于variances驱动的复杂人类活动识别方法，该方法通过生成表示来有效识别和解释视频中的复杂活动，从而减少了传统的标签化负担。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.03291v2 Announce Type: replace-cross  Abstract: Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evalu
&lt;/p&gt;</description></item><item><title>该文章提出了一个基于空间关系的多视图图变换器（MVGT），该模型整合了时间、频率和空间域的信息，包括几何和解剖结构，以增强模型的表达能力。通过将EEG通道的空间信息融入模型中，该模型提高了感知通道空间结构的能力，并在公开数据集上的实验表明，该模型在识别EEG情绪方面的表现优于现有方法。</title><link>https://arxiv.org/abs/2407.03131</link><description>&lt;p&gt;
MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.03131
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个基于空间关系的多视图图变换器（MVGT），该模型整合了时间、频率和空间域的信息，包括几何和解剖结构，以增强模型的表达能力。通过将EEG通道的空间信息融入模型中，该模型提高了感知通道空间结构的能力，并在公开数据集上的实验表明，该模型在识别EEG情绪方面的表现优于现有方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.03131v3 Announce Type: replace-cross  Abstract: Electroencephalography (EEG), a medical imaging technique that captures scalp electrical activity of brain structures via electrodes, has been widely used in affective computing. The spatial domain of EEG is rich in affective information. However, few of the existing studies have simultaneously analyzed EEG signals from multiple perspectives of geometric and anatomical structures in spatial domain. In this paper, we propose a multi-view Graph Transformer (MVGT) based on spatial relations, which integrates information from the temporal, frequency and spatial domains, including geometric and anatomical structures, so as to enhance the expressive power of the model comprehensively. We incorporate the spatial information of EEG channels into the model as encoding, thereby improving its ability to perceive the spatial structure of the channels. Meanwhile, experimental results based on publicly available datasets demonstrate that our
&lt;/p&gt;</description></item><item><title>该文章提出了Logic-LM++，这是对Logic-LM 的一项改进，它在生成和精炼形式化表达方面解决了大型语言模型（LLMs）的局限性，通过使用LLM进行成对比较的能力来评估所建议的改进。Logic-LM++在三个自然语言推理数据集上显著优于Logic-LM和其他当前方法：FOLIO、ProofWriter和AR-LSAT，在标准提示、链式想法提示以及Logic-LM上的平均改进率分别为18.5%、12.3%和5%。</title><link>https://arxiv.org/abs/2407.02514</link><description>&lt;p&gt;
LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2407.02514
&lt;/p&gt;
&lt;p&gt;
该文章提出了Logic-LM++，这是对Logic-LM 的一项改进，它在生成和精炼形式化表达方面解决了大型语言模型（LLMs）的局限性，通过使用LLM进行成对比较的能力来评估所建议的改进。Logic-LM++在三个自然语言推理数据集上显著优于Logic-LM和其他当前方法：FOLIO、ProofWriter和AR-LSAT，在标准提示、链式想法提示以及Logic-LM上的平均改进率分别为18.5%、12.3%和5%。
&lt;/p&gt;
&lt;p&gt;
arXiv:2407.02514v3 Announce Type: replace-cross  Abstract: In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. Although recent works have started to employ formal languages as an intermediate representation for reasoning tasks, they often face challenges in accurately generating and refining these formal specifications to ensure correctness. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and other contemporary techniques across natural language reasoning tasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.
&lt;/p&gt;</description></item><item><title>该文章创新性地提出一个名为InterleavedBench的基准测试，这是首个专门用于评估随意交织文本和图像生成的工具，涵盖了多种现实世界场景。同时，文章还开发了InterleavedEval，一个基于GPT-4o的强大无基准评估指标，以确保准确性和开放性场景的质量评估。</title><link>https://arxiv.org/abs/2406.14643</link><description>&lt;p&gt;
Holistic Evaluation for Interleaved Text-and-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.14643
&lt;/p&gt;
&lt;p&gt;
该文章创新性地提出一个名为InterleavedBench的基准测试，这是首个专门用于评估随意交织文本和图像生成的工具，涵盖了多种现实世界场景。同时，文章还开发了InterleavedEval，一个基于GPT-4o的强大无基准评估指标，以确保准确性和开放性场景的质量评估。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.14643v2 Announce Type: replace  Abstract: Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate 
&lt;/p&gt;</description></item><item><title>该文章提出了Soft Prompting for Unlearning（SPUL）框架，该框架通过在查询中附加特定的提示 tokens 来无监督地实现语言模型的特定训练例子的遗忘，从而能够在不更新大型语言模型参数的前提下完美遵守数据保护法规。</title><link>https://arxiv.org/abs/2406.12038</link><description>&lt;p&gt;
Soft Prompting for Unlearning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.12038
&lt;/p&gt;
&lt;p&gt;
该文章提出了Soft Prompting for Unlearning（SPUL）框架，该框架通过在查询中附加特定的提示 tokens 来无监督地实现语言模型的特定训练例子的遗忘，从而能够在不更新大型语言模型参数的前提下完美遵守数据保护法规。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.12038v2 Announce Type: replace-cross  Abstract: The widespread popularity of Large Language Models (LLMs), partly due to their unique ability to perform in-context learning, has also brought to light the importance of ethical and safety considerations when deploying these pre-trained models. In this work, we focus on investigating machine unlearning for LLMs motivated by data protection regulations. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize the unlearning of a subset of training data. With losses designed to enforce forgetting as well as utility preservation, our framework \textbf{S}oft \textbf{P}rompting for \textbf{U}n\textbf{l}earning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed met
&lt;/p&gt;</description></item><item><title>该文章提出STAR框架，通过增强人类红队任务中参数化指令的生成和争议仲裁机制，提升对大型语言模型风险的识别覆盖与可靠性，同时降低了监测成本。</title><link>https://arxiv.org/abs/2406.11757</link><description>&lt;p&gt;
STAR: SocioTechnical Approach to Red Teaming Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.11757
&lt;/p&gt;
&lt;p&gt;
该文章提出STAR框架，通过增强人类红队任务中参数化指令的生成和争议仲裁机制，提升对大型语言模型风险的识别覆盖与可靠性，同时降低了监测成本。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.11757v3 Announce Type: replace  Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.
&lt;/p&gt;</description></item><item><title>该文章提出一个开放平台GenAI-Arena，旨在通过用户参与评估不同类型的生成模型，如文本到图像、文本到视频以及图像编辑，以提供更民主和准确的评估模型性能的方法。</title><link>https://arxiv.org/abs/2406.04485</link><description>&lt;p&gt;
GenAI Arena: An Open Evaluation Platform for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2406.04485
&lt;/p&gt;
&lt;p&gt;
该文章提出一个开放平台GenAI-Arena，旨在通过用户参与评估不同类型的生成模型，如文本到图像、文本到视频以及图像编辑，以提供更民主和准确的评估模型性能的方法。
&lt;/p&gt;
&lt;p&gt;
arXiv:2406.04485v2 Announce Type: replace-cross  Abstract: Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 27 open-source g
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为“Hummer”的偏好数据集，旨在减少偏好数据集中不同对齐目标之间的冲突，从而提高从人类反馈中学习的算法的鲁棒性和有效性。</title><link>https://arxiv.org/abs/2405.11647</link><description>&lt;p&gt;
Hummer: Towards Limited Competitive Preference Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.11647
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为“Hummer”的偏好数据集，旨在减少偏好数据集中不同对齐目标之间的冲突，从而提高从人类反馈中学习的算法的鲁棒性和有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.11647v3 Announce Type: replace  Abstract: Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present \texttt{Hummer} and its fine-grained variant, \texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives. \texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment object
&lt;/p&gt;</description></item><item><title>该文章提出了一种通过调整概念瓶颈模型中的概念对齐来改进干预效果的方法。通过训练一个概念干预对齐模块，作者发现可以改善人类专家在模型决策过程中的干预效果，通过最小化对每个图像的干预次数，从而减少了获取人类反馈的成本。</title><link>https://arxiv.org/abs/2405.01531</link><description>&lt;p&gt;
Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2405.01531
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种通过调整概念瓶颈模型中的概念对齐来改进干预效果的方法。通过训练一个概念干预对齐模块，作者发现可以改善人类专家在模型决策过程中的干预效果，通过最小化对每个图像的干预次数，从而减少了获取人类反馈的成本。
&lt;/p&gt;
&lt;p&gt;
arXiv:2405.01531v2 Announce Type: replace-cross  Abstract: Concept Bottleneck Models (CBMs) ground image classification on human-understandable concepts to allow for interpretable model decisions. Crucially, the CBM design inherently allows for human interventions, in which expert users are given the ability to modify potentially misaligned concept choices to influence the decision behavior of the model in an interpretable fashion. However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human feedback is expensive. In this paper, we find that this is noticeably driven by an independent treatment of concepts during intervention, wherein a change of one concept does not influence the use of other ones in the model's final decision. To address this issue, we introduce a trainable concept intervention realignment module, which leverages concept relations to realign concept ass
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为LMM-PCQA的方法，该方法通过给大型多模态模型（LMMs）提供文本监督来实现对点云质量评估的辅助。通过将质量标签转换为文本描述并在微调阶段使用，LMMs能够从2D点云投影中推导出质量评分logits。同时，还提取了结构特征来补偿3D领域感知能力上的损失。最后，将这些质量logits和结构特征结合起来进行回归，以获得质量分数。通过实验验证了该方法的有效性，展现了将LMMs集成到点云质量评估中的一种新颖方式，从而提高了模型在评估点云质量时的性能。</title><link>https://arxiv.org/abs/2404.18203</link><description>&lt;p&gt;
LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.18203
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为LMM-PCQA的方法，该方法通过给大型多模态模型（LMMs）提供文本监督来实现对点云质量评估的辅助。通过将质量标签转换为文本描述并在微调阶段使用，LMMs能够从2D点云投影中推导出质量评分logits。同时，还提取了结构特征来补偿3D领域感知能力上的损失。最后，将这些质量logits和结构特征结合起来进行回归，以获得质量分数。通过实验验证了该方法的有效性，展现了将LMMs集成到点云质量评估中的一种新颖方式，从而提高了模型在评估点云质量时的性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.18203v2 Announce Type: replace  Abstract: Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model under
&lt;/p&gt;</description></item><item><title>该文章引入了Oak Ridge Base Foundation Model for Earth System Predictability（ORBIT），这是一个具有113亿参数的先进视觉转换器模型，采用了创新的混合张量数据并行策略。ORBIT通过这一策略超越了当前气候人工智能模型的大小限制，实现了对环境动态复杂性和数据整合的改善，从而提高了地球系统预测的准确性。</title><link>https://arxiv.org/abs/2404.14712</link><description>&lt;p&gt;
ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.14712
&lt;/p&gt;
&lt;p&gt;
该文章引入了Oak Ridge Base Foundation Model for Earth System Predictability（ORBIT），这是一个具有113亿参数的先进视觉转换器模型，采用了创新的混合张量数据并行策略。ORBIT通过这一策略超越了当前气候人工智能模型的大小限制，实现了对环境动态复杂性和数据整合的改善，从而提高了地球系统预测的准确性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.14712v2 Announce Type: replace-cross  Abstract: Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved. Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges. To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique. As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold. Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling efficiency maintai
&lt;/p&gt;</description></item><item><title>该文章通过大规模案例研究Twitter用户资料图片，确认了AI生成图像在平台上的显著存在，并分析了其相关特征。</title><link>https://arxiv.org/abs/2404.14244</link><description>&lt;p&gt;
AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.14244
&lt;/p&gt;
&lt;p&gt;
该文章通过大规模案例研究Twitter用户资料图片，确认了AI生成图像在平台上的显著存在，并分析了其相关特征。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.14244v2 Announce Type: replace-cross  Abstract: Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of thes
&lt;/p&gt;</description></item><item><title>该文章阐述了使用混合现实(XR)技术使普通物理对象具备人工智能能力的新方法，通过实时对象识別技术和大型语言模型，允许用户通过物理对象来触发与现实场景相关的数字交互，从而增强用户的体验并开辟了新的交互模式。</title><link>https://arxiv.org/abs/2404.13274</link><description>&lt;p&gt;
Augmented Object Intelligence with XR-Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.13274
&lt;/p&gt;
&lt;p&gt;
该文章阐述了使用混合现实(XR)技术使普通物理对象具备人工智能能力的新方法，通过实时对象识別技术和大型语言模型，允许用户通过物理对象来触发与现实场景相关的数字交互，从而增强用户的体验并开辟了新的交互模式。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.13274v3 Announce Type: replace-cross  Abstract: Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper explores Artificial Object Intelligence (AOI) in the context of XR, an interaction paradigm that aims to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to digital functionalities. Our approach utilizes real-time object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions without the need for object pre-registration. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in contextually relevant ways using object-based context menus. This system enables analog objects to not on
&lt;/p&gt;</description></item><item><title>该文章详细介绍了深度学习在定理证明领域的应用，包括自动正规化、前提选择、证明步骤生成和证明搜索等任务。文中不仅总结了现有的方法，还讨论了评价指标和当前技术在处理这些任务时的表现，并对未来的研究方向提出了建议。因此，该论文的主要贡献在于全面概述了深度学习在提升定理证明效率方面的最新进展和面临的主要挑战。</title><link>https://arxiv.org/abs/2404.09939</link><description>&lt;p&gt;
A Survey on Deep Learning for Theorem Proving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.09939
&lt;/p&gt;
&lt;p&gt;
该文章详细介绍了深度学习在定理证明领域的应用，包括自动正规化、前提选择、证明步骤生成和证明搜索等任务。文中不仅总结了现有的方法，还讨论了评价指标和当前技术在处理这些任务时的表现，并对未来的研究方向提出了建议。因此，该论文的主要贡献在于全面概述了深度学习在提升定理证明效率方面的最新进展和面临的主要挑战。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.09939v2 Announce Type: replace  Abstract: Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundation
&lt;/p&gt;</description></item><item><title>该文章提出了一项名为强化学习中具有可泛化性的高斯点积法（Reinforcement Learning with Generalizable Gaussian Splatting）的先进技术，该技术通过3D高斯点积法为视觉增强型强化学习任务提供了高质量的环境表示，解决了之前环境表示存在的复杂几何描述不足、场景泛化能力不强、需要精确前景掩码等问题，并显著提高了表示的解读能力。</title><link>https://arxiv.org/abs/2404.07950</link><description>&lt;p&gt;
Reinforcement Learning with Generalizable Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.07950
&lt;/p&gt;
&lt;p&gt;
该文章提出了一项名为强化学习中具有可泛化性的高斯点积法（Reinforcement Learning with Generalizable Gaussian Splatting）的先进技术，该技术通过3D高斯点积法为视觉增强型强化学习任务提供了高质量的环境表示，解决了之前环境表示存在的复杂几何描述不足、场景泛化能力不强、需要精确前景掩码等问题，并显著提高了表示的解读能力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.07950v2 Announce Type: replace  Abstract: An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian
&lt;/p&gt;</description></item><item><title>该文章研究了如何通过向用于训练语言模型（LM）的人类偏好数据中注入不良偏好对，来攻击基于人类反馈的强化学习（RLHF）。作者提出了一种策略，通过构建不良偏好对，并测试其对两个广泛使用的偏好数据的污染效果。结果表明，偏好污染是高度有效的：即使注入的不良数据仅占原始数据集的一小部分（1%-5%），也可以成功操纵LM生成目标实体并带有目标情感。这说明恶意行为者有可能通过污染偏好数据来影响语言模型的输出。</title><link>https://arxiv.org/abs/2404.05530</link><description>&lt;p&gt;
Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.05530
&lt;/p&gt;
&lt;p&gt;
该文章研究了如何通过向用于训练语言模型（LM）的人类偏好数据中注入不良偏好对，来攻击基于人类反馈的强化学习（RLHF）。作者提出了一种策略，通过构建不良偏好对，并测试其对两个广泛使用的偏好数据的污染效果。结果表明，偏好污染是高度有效的：即使注入的不良数据仅占原始数据集的一小部分（1%-5%），也可以成功操纵LM生成目标实体并带有目标情感。这说明恶意行为者有可能通过污染偏好数据来影响语言模型的输出。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.05530v2 Announce Type: replace-cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or ne
&lt;/p&gt;</description></item><item><title>该文章通过研究不同程度谎言和警告信息对社会接受度及所作反应的影响，揭示了紧急警告信息的重要性。</title><link>https://arxiv.org/abs/2404.03745</link><description>&lt;p&gt;
Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.03745
&lt;/p&gt;
&lt;p&gt;
该文章通过研究不同程度谎言和警告信息对社会接受度及所作反应的影响，揭示了紧急警告信息的重要性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.03745v2 Announce Type: replace-cross  Abstract: The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Participants ranked content as truthful in the order of genuine, minor hallucination, and major hallucination, and user engagement behaviors mirrored this pattern. More importantly, we observed that warning im
&lt;/p&gt;</description></item><item><title>该文章探讨了大型语言模型在事实知识存储与处理方面的能力，并通过提出一种框架，能够解析出LLM在处理语句时所依赖的知识图，揭示了知识在模型内部从矢量形式到谓词集合的转化过程。文章使用了一种名为激活调制的工具，它允许实时改变模型在推理过程中对单个词元的响应，以提取隐藏在这些词元内部的实体信息。尽管这些实体知识并不直接由训练获得，而是在与输入信号交互的过程中逐步构建，但文章已经展示了如何通过这种调制的手段提取并解析这些知识，并在两个不同的句级事实判断数据集上实施了测试，探索了知识片段在模型不同层次上的体现。

简而言之，该研究揭示了LLM处理语句时所根据的知识图谱，实现了事实信息的提取解析，并且证明了通过一种新型激活调制方法可以对模型内部知识进行有效的挖掘。</title><link>https://arxiv.org/abs/2404.03623</link><description>&lt;p&gt;
Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.03623
&lt;/p&gt;
&lt;p&gt;
该文章探讨了大型语言模型在事实知识存储与处理方面的能力，并通过提出一种框架，能够解析出LLM在处理语句时所依赖的知识图，揭示了知识在模型内部从矢量形式到谓词集合的转化过程。文章使用了一种名为激活调制的工具，它允许实时改变模型在推理过程中对单个词元的响应，以提取隐藏在这些词元内部的实体信息。尽管这些实体知识并不直接由训练获得，而是在与输入信号交互的过程中逐步构建，但文章已经展示了如何通过这种调制的手段提取并解析这些知识，并在两个不同的句级事实判断数据集上实施了测试，探索了知识片段在模型不同层次上的体现。

简而言之，该研究揭示了LLM处理语句时所根据的知识图谱，实现了事实信息的提取解析，并且证明了通过一种新型激活调制方法可以对模型内部知识进行有效的挖掘。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.03623v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis hi
&lt;/p&gt;</description></item><item><title>该文章详细介绍了如何评估大型语言模型（LLMs）的推理行为，并强调了对这些模型进行更为全面的认知测试的必要性。文章通过研究，展示了对LLMs的推理行为进行深入分析的不同方法和趋势，以及这些模型在推理过程中可能遇到的局限性和挑战。通过对现有研究的总结和评论，作者提出了对LLMs推理能力的更全面理解，揭示了在评估这些系统的认知能力时应该关注的重点。</title><link>https://arxiv.org/abs/2404.01869</link><description>&lt;p&gt;
Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01869
&lt;/p&gt;
&lt;p&gt;
该文章详细介绍了如何评估大型语言模型（LLMs）的推理行为，并强调了对这些模型进行更为全面的认知测试的必要性。文章通过研究，展示了对LLMs的推理行为进行深入分析的不同方法和趋势，以及这些模型在推理过程中可能遇到的局限性和挑战。通过对现有研究的总结和评论，作者提出了对LLMs推理能力的更全面理解，揭示了在评估这些系统的认知能力时应该关注的重点。
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01869v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training d
&lt;/p&gt;</description></item><item><title>该文章介绍了一种名为CR3DT的模型，它结合了摄像头和雷达技术进行三维物体检测和多目标跟踪，通过将雷达数据的加入，显著提高了性能。</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种名为CR3DT的模型，它结合了摄像头和雷达技术进行三维物体检测和多目标跟踪，通过将雷达数据的加入，显著提高了性能。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v2 Announce Type: replace  Abstract: To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporat
&lt;/p&gt;</description></item><item><title>该文章提出了一种基于蓝图图的辩论范式（BDoG），旨在解决多模态推理中的意见简化问题和图像引入的注意力分散问题。BDoG通过将辩论限制在蓝图图中来防止通过世界级总结导致的观点简化，并通过在图分支中存储证据来减少频繁但无关概念的干扰。实验结果表明，与其他方法相比，BDoG在ScienceQA和MMBench上取得了显著的改进，验证了其在多模态推理中的有效性。</title><link>https://arxiv.org/abs/2403.14972</link><description>&lt;p&gt;
A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14972
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种基于蓝图图的辩论范式（BDoG），旨在解决多模态推理中的意见简化问题和图像引入的注意力分散问题。BDoG通过将辩论限制在蓝图图中来防止通过世界级总结导致的观点简化，并通过在图分支中存储证据来减少频繁但无关概念的干扰。实验结果表明，与其他方法相比，BDoG在ScienceQA和MMBench上取得了显著的改进，验证了其在多模态推理中的有效性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14972v2 Announce Type: replace  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate that BDoG is able to achieve state-of-the-art results in ScienceQA and MMBench with significant improvements over previous methods. The source code can be accessed at
&lt;/p&gt;</description></item><item><title>该文章提出设计了一款名为ClarifAI的自动化 propaganda检测工具，旨在通过激活“双思考系统”理论，促使读者对新闻内容进行更深入的分析和批判性思考。通过这项创新，ClarifAI使用大型语言模型识别文章中的 propaganda，并为读者提供详细的解释，从而提高他们在阅读时的批判性思维能力。通过在线实验，该研究还证明了工具的有效性，并强调了解释性内容在提升批判性思维中的重要性。</title><link>https://arxiv.org/abs/2402.19135</link><description>&lt;p&gt;
Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19135
&lt;/p&gt;
&lt;p&gt;
该文章提出设计了一款名为ClarifAI的自动化 propaganda检测工具，旨在通过激活“双思考系统”理论，促使读者对新闻内容进行更深入的分析和批判性思考。通过这项创新，ClarifAI使用大型语言模型识别文章中的 propaganda，并为读者提供详细的解释，从而提高他们在阅读时的批判性思维能力。通过在线实验，该研究还证明了工具的有效性，并强调了解释性内容在提升批判性思维中的重要性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19135v2 Announce Type: replace-cross  Abstract: In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool
&lt;/p&gt;</description></item><item><title>该文章探讨了一种名为委托博弈的模型，它模拟了人类与机器（如个人虚拟助手和自动驾驶车辆）之间的工作关系。研究人员研究了委托游戏中的两种重要失败模式：控制问题和合作问题。控制问题涉及代理人的行为与其委托人偏好不一致，而合作问题则指代理人之间在执行任务时的协作不佳。论文深入分析了这些问题，并将它们进一步划分为偏好一致性和执行能力两个方面。通过理论分析和有限观测数据，研究者证明了这些因素如何影响委托人的福利，并探讨了如何使用现有信息来估计这些问题。总的来说，该研究为理解人类与机器之间的工作协作提供了新的视角，并可能对涉及多方交互的自动化系统设计有重要启示。</title><link>https://arxiv.org/abs/2402.15821</link><description>&lt;p&gt;
Cooperation and Control in Delegation Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15821
&lt;/p&gt;
&lt;p&gt;
该文章探讨了一种名为委托博弈的模型，它模拟了人类与机器（如个人虚拟助手和自动驾驶车辆）之间的工作关系。研究人员研究了委托游戏中的两种重要失败模式：控制问题和合作问题。控制问题涉及代理人的行为与其委托人偏好不一致，而合作问题则指代理人之间在执行任务时的协作不佳。论文深入分析了这些问题，并将它们进一步划分为偏好一致性和执行能力两个方面。通过理论分析和有限观测数据，研究者证明了这些因素如何影响委托人的福利，并探讨了如何使用现有信息来估计这些问题。总的来说，该研究为理解人类与机器之间的工作协作提供了新的视角，并可能对涉及多方交互的自动化系统设计有重要启示。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15821v2 Announce Type: replace-cross  Abstract: Many settings of interest involving humans and machines -- from virtual personal assistants to autonomous vehicles -- can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf. We refer to these multi-principal, multi-agent scenarios as delegation games. In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together). In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?). We show -- theoretically and empirically -- how these measures determine the principals' welfare, how they can be estimated using limited observatio
&lt;/p&gt;</description></item><item><title>该文章提出GAOKAO-MM，一个基于中文高考的全新多模态评估基准，它要求模型展现出对图片、图表、函数图和其他10种类型图片的正确理解和分析能力，同时要求模型具有推理和处理高级认知任务的能力。在GAOKAO-MM基准中，10个大型视觉语言模型（LVLMs）表现不佳，它们的正确率都低于50%。这表明现有的LVLMs在处理复杂多模态任务方面仍有较大的提升空间。</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
该文章提出GAOKAO-MM，一个基于中文高考的全新多模态评估基准，它要求模型展现出对图片、图表、函数图和其他10种类型图片的正确理解和分析能力，同时要求模型具有推理和处理高级认知任务的能力。在GAOKAO-MM基准中，10个大型视觉语言模型（LVLMs）表现不佳，它们的正确率都低于50%。这表明现有的LVLMs在处理复杂多模态任务方面仍有较大的提升空间。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v2 Announce Type: replace-cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs ha
&lt;/p&gt;</description></item><item><title>该文章提出了利用人工智能（AI）对系统性文献回顾（SLR）进行综合审查，指出了AI在自动化SLR过程中的潜在机会与挑战。通过21种SLR工具的应用案例，分析了AI技术在文献筛选和提取阶段的应用，并特别探讨了11款采用大型语言模型进行文献搜索和学术写作辅助的最新工具，从而为提升SLR的效率和质量提供了新思路。</title><link>https://arxiv.org/abs/2402.08565</link><description>&lt;p&gt;
Artificial Intelligence for Literature Reviews: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08565
&lt;/p&gt;
&lt;p&gt;
该文章提出了利用人工智能（AI）对系统性文献回顾（SLR）进行综合审查，指出了AI在自动化SLR过程中的潜在机会与挑战。通过21种SLR工具的应用案例，分析了AI技术在文献筛选和提取阶段的应用，并特别探讨了11款采用大型语言模型进行文献搜索和学术写作辅助的最新工具，从而为提升SLR的效率和质量提供了新思路。
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08565v2 Announce Type: replace  Abstract: This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlin
&lt;/p&gt;</description></item><item><title>该文章通过实验比较了传统机器学习方法在缺失数据下的决策效果，以及一种旨在描述性预测并模拟人类决策的机器学习方法的性能。研究发现，后者更好地适应了包含不完整数据的复杂决策环境，提高了决策过程中的解释性和稳定性。</title><link>https://arxiv.org/abs/2401.11044</link><description>&lt;p&gt;
Preservation of Feature Stability in Machine Learning Under Data Uncertainty for Decision Support in Critical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11044
&lt;/p&gt;
&lt;p&gt;
该文章通过实验比较了传统机器学习方法在缺失数据下的决策效果，以及一种旨在描述性预测并模拟人类决策的机器学习方法的性能。研究发现，后者更好地适应了包含不完整数据的复杂决策环境，提高了决策过程中的解释性和稳定性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11044v2 Announce Type: replace-cross  Abstract: In a world where Machine Learning (ML) is increasingly deployed to support decision-making in critical domains, providing decision-makers with explainable, stable, and relevant inputs becomes fundamental. Understanding how machine learning works under missing data and how this affects feature variability is paramount. This is even more relevant as machine learning approaches focus on standardising decision-making approaches that rely on an idealised set of features. However, decision-making in human activities often relies on incomplete data, even in critical domains. This paper addresses this gap by conducting a set of experiments using traditional machine learning methods that look for optimal decisions in comparison to a recently deployed machine learning method focused on a classification that is more descriptive and mimics human decision making, allowing for the natural integration of explainability. We found that the ML d
&lt;/p&gt;</description></item><item><title>该文章提供了一种简单有效的方法，能够在不影响原始模型性能的前提下，通过针对特定目标语言的重新训练和一个验证步骤，降低大型语言模型在非罗马字母语言中造成的文本过度碎片化问题。</title><link>https://arxiv.org/abs/2401.10660</link><description>&lt;p&gt;
Accelerating Multilingual Language Model for Excessively Tokenized Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.10660
&lt;/p&gt;
&lt;p&gt;
该文章提供了一种简单有效的方法，能够在不影响原始模型性能的前提下，通过针对特定目标语言的重新训练和一个验证步骤，降低大型语言模型在非罗马字母语言中造成的文本过度碎片化问题。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.10660v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) have remarkably enhanced performances on a variety of tasks in multiple languages. However, tokenizers in LLMs trained primarily on English-centric corpora often overly fragment a text into character or Unicode-level tokens in non-Roman alphabetic languages, leading to inefficient text generation. We introduce a simple yet effective framework to accelerate text generation in such languages. Our approach involves employing a new language model head with a vocabulary set tailored to a specific target language for a pre-trained LLM. This is followed by fine-tuning the new head while incorporating a verification step to ensure the model's performance is preserved. We show that this targeted fine-tuning, while freezing other model parameters, effectively reduces token fragmentation for the target language. Our extensive experiments demonstrate that the proposed framework increases 
&lt;/p&gt;</description></item><item><title>该文章通过代表工程学视角重新审视越狱问题，揭示了针对大型语言模型（LLMs）安全攻击的成功率提高背后的机制，同时提出了通过在LLMs产生的表示空间中识别和调节特定安全模式的方法，可能有助于增强或减弱LLMs对抗越狱攻击的抵抗力。</title><link>https://arxiv.org/abs/2401.06824</link><description>&lt;p&gt;
Rethinking Jailbreaking through the Lens of Representation Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06824
&lt;/p&gt;
&lt;p&gt;
该文章通过代表工程学视角重新审视越狱问题，揭示了针对大型语言模型（LLMs）安全攻击的成功率提高背后的机制，同时提出了通过在LLMs产生的表示空间中识别和调节特定安全模式的方法，可能有助于增强或减弱LLMs对抗越狱攻击的抵抗力。
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06824v3 Announce Type: replace-cross  Abstract: The recent surge in jailbreaking methods has revealed the vulnerability of Large Language Models (LLMs) to malicious inputs. While earlier research has primarily concentrated on increasing the success rates of jailbreaking attacks, the underlying mechanism for safeguarding LLMs remains underexplored. This study investigates the vulnerability of safety-aligned LLMs by uncovering specific activity patterns within the representation space generated by LLMs. Such ``safety patterns'' can be identified with only a few pairs of contrastive queries in a simple method and function as ``keys'' (used as a metaphor for security defense capability) that can be used to open or lock Pandora's Box of LLMs. Extensive experiments demonstrate that the robustness of LLMs against jailbreaking can be lessened or augmented by attenuating or strengthening the identified safety patterns. These findings deepen our understanding of jailbreaking phenomena
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为PathoDuet的基础模型，用于对H&amp;E和IHC染色的病理切片进行分析。这些模型通过新的自监督学习框架进行预训练，该方法利用了特定于病理图像的特征，如不同倍率的图像和不同染色的关系。通过两个预训练任务——跨尺度定位和跨染色转换，研究者能够在H&amp;E图像上预训练模型，并将其有效迁移到IHC图像任务上。</title><link>https://arxiv.org/abs/2312.09894</link><description>&lt;p&gt;
PathoDuet: Foundation Models for Pathological Slide Analysis of H&amp;amp;E and IHC Stains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09894
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为PathoDuet的基础模型，用于对H&amp;amp;E和IHC染色的病理切片进行分析。这些模型通过新的自监督学习框架进行预训练，该方法利用了特定于病理图像的特征，如不同倍率的图像和不同染色的关系。通过两个预训练任务——跨尺度定位和跨染色转换，研究者能够在H&amp;amp;E图像上预训练模型，并将其有效迁移到IHC图像任务上。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09894v2 Announce Type: replace  Abstract: Large amounts of digitized histopathological data display a promising future for developing pathological foundation models via self-supervised learning methods. Foundation models pretrained with these methods serve as a good basis for downstream tasks. However, the gap between natural and histopathological images hinders the direct application of existing methods. In this work, we present PathoDuet, a series of pretrained models on histopathological images, and a new self-supervised learning framework in histopathology. The framework is featured by a newly-introduced pretext token and later task raisers to explicitly utilize certain relations between images, like multiple magnifications and multiple stains. Based on this, two pretext tasks, cross-scale positioning and cross-stain transferring, are designed to pretrain the model on Hematoxylin and Eosin (H&amp;amp;E) images and transfer the model to immunohistochemistry (IHC) images, respecti
&lt;/p&gt;</description></item><item><title>该文章提出了一种称为深度去学习（Deep Unlearning）的新方法，该方法能够不依赖梯度信息地高效、快速地解决类别遗忘问题，从而实现了无需重训模型即可为每个删除请求进行精确的类别更新，在不违背隐私的情况下提高了数据处理效率。</title><link>https://arxiv.org/abs/2312.00761</link><description>&lt;p&gt;
Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00761
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种称为深度去学习（Deep Unlearning）的新方法，该方法能够不依赖梯度信息地高效、快速地解决类别遗忘问题，从而实现了无需重训模型即可为每个删除请求进行精确的类别更新，在不违背隐私的情况下提高了数据处理效率。
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00761v4 Announce Type: replace-cross  Abstract: Machine unlearning is a prominent and challenging field, driven by regulatory demands for user data deletion and heightened privacy awareness. Existing approaches involve retraining model or multiple finetuning steps for each deletion request, often constrained by computational limits and restricted data access. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate specific classes from the learned model. Our algorithm first estimates the Retain and the Forget Spaces using Singular Value Decomposition on the layerwise activations for a small subset of samples from the retain and unlearn classes, respectively. We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space. Finally, we obtain the unlearned model by updating the weights to suppress the class discriminatory features from the activation spaces. 
&lt;/p&gt;</description></item><item><title>该文章提出了一个利用随机森林(RF)预测的数学表达式为其训练目标加权和的新方法，揭示了RF模型预测的线性关系，从而提供了一种针对RF预测的本地化解释方法，该方法能够为模型预测提供数据点之间的权重和训练集中的任何预测的局部解释，改进了现有特征解释方法，如SHAP。</title><link>https://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
Enhanced Local Explainability and Trust Scores with Random Forest Proximities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
该文章提出了一个利用随机森林(RF)预测的数学表达式为其训练目标加权和的新方法，揭示了RF模型预测的线性关系，从而提供了一种针对RF预测的本地化解释方法，该方法能够为模型预测提供数据点之间的权重和训练集中的任何预测的局部解释，改进了现有特征解释方法，如SHAP。
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12428v3 Announce Type: replace-cross  Abstract: We initiate a novel approach to explain the predictions and out of sample performance of random forest (RF) regression and classification models by exploiting the fact that any RF can be mathematically formulated as an adaptive weighted K nearest-neighbors model. Specifically, we employ a recent result that, for both regression and classification tasks, any RF prediction can be rewritten exactly as a weighted sum of the training targets, where the weights are RF proximities between the corresponding pairs of data points. We show that this linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established feature-based methods like SHAP, which generate attributions for a model prediction across input features. We show how this proximity-based approach to explainability can be used in conjunction
&lt;/p&gt;</description></item><item><title>该文章介绍了一种结合GPT-4和GPT-3.5的系统，旨在通过生成精确的编程提示来模拟人类导师的教学方法。系统使用GPT-4生成编程提示，并通过GPT-3.5进行验证，以提高提示的准确性和相关性。通过这种方式，该系统旨在为学生在解决编程错误时提供高质量的指导。</title><link>https://arxiv.org/abs/2310.03780</link><description>&lt;p&gt;
Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03780
&lt;/p&gt;
&lt;p&gt;
该文章介绍了一种结合GPT-4和GPT-3.5的系统，旨在通过生成精确的编程提示来模拟人类导师的教学方法。系统使用GPT-4生成编程提示，并通过GPT-3.5进行验证，以提高提示的准确性和相关性。通过这种方式，该系统旨在为学生在解决编程错误时提供高质量的指导。
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03780v4 Announce Type: replace  Abstract: Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model,
&lt;/p&gt;</description></item><item><title>该文章提出一种仅依赖查询点标注的弱 supervision学习算法，用于训练仅需少量的查询点标注即可进行卫星图像语义分割的深度学习模型，有效降低了高精度像素级标注的成本和时间。</title><link>https://arxiv.org/abs/2309.05490</link><description>&lt;p&gt;
Learning Semantic Segmentation with Query Points Supervision on Aerial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05490
&lt;/p&gt;
&lt;p&gt;
该文章提出一种仅依赖查询点标注的弱 supervision学习算法，用于训练仅需少量的查询点标注即可进行卫星图像语义分割的深度学习模型，有效降低了高精度像素级标注的成本和时间。
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05490v2 Announce Type: replace  Abstract: Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models supervised with imag
&lt;/p&gt;</description></item><item><title>该文章提出了一种名为DeepCoherentFactorModelNeuralNetwork（DeepCFMNN）的模型，通过在MQForecaster神经网络架构中加入一个基于深层高斯分解的预测模型，实现了在保持预测精度的同时，满足层级结构的预测一致性，对于能源管理、气候预测等需要层级结构预测的应用场景具有重要创新价值。</title><link>https://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
该文章提出了一种名为DeepCoherentFactorModelNeuralNetwork（DeepCFMNN）的模型，通过在MQForecaster神经网络架构中加入一个基于深层高斯分解的预测模型，实现了在保持预测精度的同时，满足层级结构的预测一致性，对于能源管理、气候预测等需要层级结构预测的应用场景具有重要创新价值。
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.09797v2 Announce Type: replace-cross  Abstract: Obtaining accurate probabilistic forecasts is an important operational challenge in many applications, perhaps most obviously in energy management, climate forecasting, supply chain planning, and resource allocation. In many of these applications, there is a natural hierarchical structure over the forecasted quantities; and forecasting systems that adhere to this hierarchical structure are said to be coherent. Furthermore, operational planning benefits from accuracy at all levels of the aggregation hierarchy. Building accurate and coherent forecasting systems, however, is challenging: classic multivariate time series tools and neural network methods are still being adapted for this purpose. In this paper, we augment an MQForecaster neural network architecture with a novel deep Gaussian factor forecasting model that achieves coherence by construction, yielding a method we call the Deep Coherent Factor Model Neural Network (DeepC
&lt;/p&gt;</description></item><item><title>该文章详细阐述了工具学习在人工智能中的应用及其对人类认知能力的借鉴意义。基于这类应用实践，文章概述了工具学习与基础模型结合的策略，旨在提升AI系统的决策能力、效率和自动化水平。文章通过系统地回顾现有研究，提出了工具学习领域中的关键挑战、机遇和未来研究方向。</title><link>https://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
Tool Learning with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
该文章详细阐述了工具学习在人工智能中的应用及其对人类认知能力的借鉴意义。基于这类应用实践，文章概述了工具学习与基础模型结合的策略，旨在提升AI系统的决策能力、效率和自动化水平。文章通过系统地回顾现有研究，提出了工具学习领域中的关键挑战、机遇和未来研究方向。
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08354v3 Announce Type: replace-cross  Abstract: Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmente
&lt;/p&gt;</description></item><item><title>该文章详细分析了核心参照解决技术，讨论了多种不同方法以及它们在不同语料库上的应用，强调了新算法和新方法对于改善自然语言处理中核心参照解决的重要性。</title><link>https://arxiv.org/abs/2211.04428</link><description>&lt;p&gt;
Review of coreference resolution in English and Persian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04428
&lt;/p&gt;
&lt;p&gt;
该文章详细分析了核心参照解决技术，讨论了多种不同方法以及它们在不同语料库上的应用，强调了新算法和新方法对于改善自然语言处理中核心参照解决的重要性。
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04428v2 Announce Type: replace-cross  Abstract: Coreference resolution (CR), identifying expressions referring to the same real-world entity, is a fundamental challenge in natural language processing (NLP). This paper explores the latest advancements in CR, spanning coreference and anaphora resolution. We critically analyze the diverse corpora that have fueled CR research, highlighting their strengths, limitations, and suitability for various tasks. We examine the spectrum of evaluation metrics used to assess CR systems, emphasizing their advantages, disadvantages, and the need for more nuanced, task-specific metrics. Tracing the evolution of CR algorithms, we provide a detailed overview of methodologies, from rule-based approaches to cutting-edge deep learning architectures. We delve into mention-pair, entity-based, cluster-ranking, sequence-to-sequence, and graph neural network models, elucidating their theoretical foundations and performance on benchmark datasets. Recogni
&lt;/p&gt;</description></item></channel></rss>